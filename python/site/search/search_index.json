{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Xeepy - The Ultimate X/Twitter Automation Toolkit","text":""},{"location":"#xeepy","title":"\ud83d\ude80 Xeepy","text":""},{"location":"#the-ultimate-xtwitter-automation-toolkit","title":"The Ultimate X/Twitter Automation Toolkit","text":"<p>No API keys required. No rate limits. No compromises.</p> <p>Get Started View on GitHub</p>"},{"location":"#what-is-xeepy","title":"\u2728 What is Xeepy?","text":"<p>Xeepy is a professional-grade Python toolkit for automating X (formerly Twitter) using browser automation. Unlike traditional API-based solutions that require expensive API access or face strict rate limits, Xeepy operates through Playwright browser automation\u2014giving you the same capabilities as a real user.</p> <ul> <li> <p> No API Keys Required</p> <p>Forget the $100/month Twitter API. Xeepy uses browser automation to work without any API credentials.</p> </li> <li> <p> No Rate Limits</p> <p>Smart rate limiting mimics human behavior. Scrape thousands of tweets without getting blocked.</p> </li> <li> <p> Stealth Mode</p> <p>Advanced anti-detection with user agent rotation, human-like delays, and fingerprint randomization.</p> </li> <li> <p> AI-Powered</p> <p>Generate content, analyze sentiment, detect bots, and find optimal posting times with AI.</p> </li> </ul>"},{"location":"#key-features","title":"\ud83c\udfaf Key Features","text":""},{"location":"#comprehensive-scraping","title":"\ud83d\udcca Comprehensive Scraping","text":"<p>Scrape anything from X/Twitter with simple Python code:</p> <pre><code>import asyncio\nfrom xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Get tweet replies (the original mission!)\n        replies = await x.scrape.replies(\"https://x.com/elonmusk/status/123456\")\n\n        # Scrape user profiles\n        profile = await x.scrape.profile(\"elonmusk\")\n\n        # Get followers with pagination\n        followers = await x.scrape.followers(\"naval\", limit=5000)\n\n        # Search tweets\n        tweets = await x.scrape.search(\"python programming\", limit=100)\n\n        # Export to CSV\n        x.export.to_csv(replies, \"replies.csv\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"#smart-followunfollow","title":"\ud83c\udfaf Smart Follow/Unfollow","text":"<p>Grow your account intelligently:</p> <pre><code>async with Xeepy() as x:\n    # Unfollow non-followers (the #1 requested feature!)\n    result = await x.unfollow.non_followers(\n        max_unfollows=100,\n        whitelist=[\"bestfriend\", \"mom\"],\n        dry_run=True  # Preview first\n    )\n\n    # Follow users from a competitor's followers\n    await x.follow.target_followers(\n        target=\"competitor\",\n        limit=50,\n        filters={\"min_followers\": 100, \"has_bio\": True}\n    )\n</code></pre>"},{"location":"#real-time-monitoring","title":"\ud83d\udcc8 Real-Time Monitoring","text":"<p>Track your account and competitors:</p> <pre><code>from xeepy import UnfollowerDetector, GrowthTracker\n\n# Detect who unfollowed you\ndetector = UnfollowerDetector(storage=storage, notifier=notifier)\nreport = await detector.detect(\"yourusername\")\nprint(f\"Lost {len(report.unfollowers)} followers today \ud83d\ude22\")\n\n# Track growth over time\ntracker = GrowthTracker(storage=storage)\ngrowth = tracker.generate_report(\"yourusername\", days=30)\nprint(f\"Gained {growth.net_change} followers this month! \ud83d\ude80\")\n</code></pre>"},{"location":"#ai-powered-automation","title":"\ud83e\udd16 AI-Powered Automation","text":"<p>Let AI do the heavy lifting:</p> <pre><code>from xeepy.ai import ContentGenerator, SentimentAnalyzer\n\n# Generate viral content\ngenerator = ContentGenerator(provider=\"openai\")\nthread = await generator.generate_thread(\n    topic=\"10 Python tips that will blow your mind\",\n    style=\"viral\",\n    num_tweets=10\n)\n\n# Analyze conversation sentiment\nanalyzer = SentimentAnalyzer()\nresult = await analyzer.analyze_conversation(replies)\nprint(f\"Overall sentiment: {result.overall_sentiment}\")\n</code></pre>"},{"location":"#multi-channel-notifications","title":"\ud83d\udd14 Multi-Channel Notifications","text":"<p>Get notified everywhere:</p> <pre><code>from xeepy.notifications import NotificationManager\n\nmanager = NotificationManager()\nmanager.add_channel(\"discord\", WebhookNotifier(discord_url))\nmanager.add_channel(\"telegram\", TelegramNotifier(bot_token, chat_id))\nmanager.add_channel(\"email\", EmailNotifier(smtp_config))\n\n# Now all your alerts go everywhere\nawait manager.notify(\n    title=\"\ud83d\udea8 Viral Tweet Alert!\",\n    message=\"Your tweet just hit 10,000 likes!\",\n    level=\"success\"\n)\n</code></pre>"},{"location":"#why-xeepy","title":"\ud83c\udfc6 Why Xeepy?","text":"Feature Twitter API Tweepy Xeepy Cost $100+/month Free (limited) Free Rate Limits 100-500/15min Strict Flexible Tweet Replies \u274c Removed \u274c Broken \u2705 Works Unfollower Detection \u274c No \u274c No \u2705 Yes AI Features \u274c No \u274c No \u2705 Built-in Browser Automation \u274c No \u274c No \u2705 Playwright Anti-Detection N/A N/A \u2705 Advanced"},{"location":"#quick-install","title":"\ud83d\ude80 Quick Install","text":"pippipxFrom Source <pre><code>pip install xeepy\nplaywright install chromium\n</code></pre> <pre><code>pipx install xeepy\nplaywright install chromium\n</code></pre> <pre><code>git clone https://github.com/nirholas/Get-Tweet-Replies-With-Python-Tweepy.git\ncd Get-Tweet-Replies-With-Python-Tweepy\npip install -e \".[all]\"\nplaywright install chromium\n</code></pre>"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li> <p> Getting Started</p> <p>New to Xeepy? Start here with installation and your first script.</p> <p> Quick Start</p> </li> <li> <p> Guides</p> <p>In-depth guides for every feature: scraping, automation, monitoring, AI.</p> <p> Explore Guides</p> </li> <li> <p> Cookbook</p> <p>Ready-to-use recipes for growth hacking, data science, and business intelligence.</p> <p> Browse Recipes</p> </li> <li> <p> API Reference</p> <p>Complete API documentation for all classes and methods.</p> <p> API Docs</p> </li> </ul>"},{"location":"#community","title":"\ud83d\udcac Community","text":"<ul> <li> <p> Discord</p> <p>Join 5,000+ members discussing X automation, sharing scripts, and getting help.</p> <p> Join Discord</p> </li> <li> <p> GitHub</p> <p>Star the repo, report issues, and contribute to Xeepy.</p> <p> GitHub</p> </li> <li> <p> X/Twitter</p> <p>Follow us for updates, tips, and automation tricks.</p> <p> @xeepy_dev</p> </li> </ul>"},{"location":"#disclaimer","title":"\u26a0\ufe0f Disclaimer","text":"<p>Educational Purposes Only</p> <p>Xeepy is designed for educational and research purposes only. </p> <ul> <li>Automating X/Twitter may violate their Terms of Service</li> <li>Use responsibly and at your own risk</li> <li>Do not use for spam, harassment, or malicious purposes</li> <li>Respect rate limits and other users</li> </ul> <p>Made with \u2764\ufe0f by the Xeepy Team</p> <p>Star on GitHub </p>"},{"location":"AGENT_DEVELOPMENT/","title":"\ud83e\udd16 AI Agent Prompts for Building Xeepy","text":"<p>Complete prompts used to build this toolkit with AI coding agents.</p> <p>This file documents the prompts used to build Xeepy using multiple AI agents working in parallel. These prompts can be used to: 1. Understand the architecture decisions 2. Extend the toolkit with new features 3. Learn about AI-assisted development</p>"},{"location":"AGENT_DEVELOPMENT/#architecture-overview","title":"Architecture Overview","text":"<p>Xeepy was built using 5 specialized AI agents working in parallel:</p> Agent Responsibility Files \ud83d\udd27 InfraScraperAgent Core infrastructure + 12 scrapers <code>core/</code>, <code>scrapers/</code> \ud83d\udd04 FollowOpsAgent Follow/unfollow operations <code>actions/follow/</code>, <code>actions/unfollow/</code> \ud83d\udc9c EngagementAgent Engagement automation <code>actions/engagement/</code> \ud83d\udcca MonitoringAgent Monitoring &amp; analytics <code>monitoring/</code>, <code>notifications/</code> \ud83e\udde0 AIFeaturesAgent AI integration <code>ai/</code>"},{"location":"AGENT_DEVELOPMENT/#agent-1-infrastructure-scrapers","title":"Agent 1: Infrastructure &amp; Scrapers","text":"<p>Purpose: Build the core foundation and all 12 scrapers.</p>"},{"location":"AGENT_DEVELOPMENT/#core-infrastructure-prompt","title":"Core Infrastructure Prompt","text":"<pre><code>You are building the core infrastructure for a Python X/Twitter automation toolkit.\n\nCreate these foundational modules:\n\n1. xeepy/core/browser.py - Playwright browser management\n   - BrowserManager class with async context manager\n   - Support headless and headed modes\n   - Handle browser lifecycle (launch, close, restart)\n   - Implement page pool for parallel operations\n\n2. xeepy/core/auth.py - Authentication\n   - Session management with cookie persistence\n   - Manual login flow (opens browser)\n   - Cookie import/export (JSON format)\n   - Session validation\n\n3. xeepy/core/rate_limiter.py - Rate limiting\n   - Token bucket algorithm\n   - Configurable limits per action type\n   - Automatic delay injection\n   - Rate limit state persistence\n\n4. xeepy/core/config.py - Configuration\n   - Pydantic settings model\n   - Environment variable support\n   - YAML/JSON config file loading\n   - Default sensible values\n\nRequirements:\n- Python 3.10+ with full type hints\n- Async/await throughout\n- Comprehensive error handling\n- Detailed docstrings\n- No stubs - complete implementations\n</code></pre>"},{"location":"AGENT_DEVELOPMENT/#scrapers-prompt","title":"Scrapers Prompt","text":"<pre><code>Build 12 comprehensive scrapers for X/Twitter data extraction.\n\nEach scraper should:\n- Inherit from BaseScraper\n- Use Playwright browser automation\n- Handle infinite scroll pagination\n- Parse data into Pydantic models\n- Support configurable limits\n- Include retry logic\n\nScrapers to implement:\n\n1. ReplyScraper - Get replies to a tweet\n2. ProfileScraper - User profile data\n3. FollowersScraper - Follower list with details\n4. FollowingScraper - Following list\n5. TweetsScraper - User's tweet history\n6. ThreadScraper - Full thread unroller\n7. HashtagScraper - Tweets by hashtag\n8. SearchScraper - Search results\n9. MediaScraper - User's media posts\n10. LikesScraper - Who liked a tweet\n11. ListsScraper - List members\n12. MentionsScraper - Mentions of a user\n\nData models needed:\n- Tweet(id, text, username, created_at, likes, retweets, replies, url, media)\n- User(id, username, name, bio, followers_count, following_count, verified)\n- Media(type, url, thumbnail_url)\n</code></pre>"},{"location":"AGENT_DEVELOPMENT/#agent-2-followunfollow-operations","title":"Agent 2: Follow/Unfollow Operations","text":"<p>Purpose: Build all follow and unfollow functionality.</p>"},{"location":"AGENT_DEVELOPMENT/#prompt","title":"Prompt","text":"<pre><code>Build follow/unfollow operations for the X/Twitter automation toolkit.\n\nFOLLOW OPERATIONS (xeepy/actions/follow/):\n\n1. follow_user.py - Follow a single user\n   - Navigate to profile, click follow button\n   - Verify follow was successful\n   - Handle already-following case\n   - Rate limit aware\n\n2. follow_by_keyword.py - Follow from search results\n   - Search for keyword\n   - Filter by criteria (min followers, verified, etc.)\n   - Follow up to limit\n   - Track who was followed\n\n3. follow_by_hashtag.py - Follow users from hashtag\n   - Scrape hashtag tweets\n   - Extract unique users\n   - Apply filters\n   - Batch follow with delays\n\n4. follow_followers.py - Follow target's followers\n   - Scrape followers of target account\n   - Filter (min followers, active recently, etc.)\n   - Batch follow operation\n\n5. follow_engagers.py - Follow users who engaged\n   - Get likers/retweeters of a tweet\n   - Follow them as they're likely interested\n\nUNFOLLOW OPERATIONS (xeepy/actions/unfollow/):\n\n1. unfollow_user.py - Unfollow single user\n   - Navigate to profile\n   - Click unfollow, confirm dialog\n   - Verify unfollow success\n\n2. unfollow_non_followers.py - Unfollow who doesn't follow back\n   - Get following list\n   - Check who follows back\n   - Unfollow non-followers\n   - Support whitelist\n   - dry_run mode for preview\n\n3. unfollow_all.py - Mass unfollow (nuclear option)\n   - Unfollow everyone\n   - Whitelist support\n   - Requires explicit confirmation\n   - Default dry_run=True\n\n4. smart_unfollow.py - Intelligent unfollow\n   - Time-based (unfollow if no follow-back in X days)\n   - Engagement-based (low engagement accounts)\n   - Activity-based (inactive accounts)\n\n5. unfollow_by_criteria.py - Filter-based unfollow\n   - By follower count (too many/too few)\n   - By tweet frequency\n   - By account age\n   - By keywords in bio\n\nAll operations must:\n- Use rate limiter\n- Log all actions\n- Store in database for history\n- Return detailed result objects\n- Handle errors gracefully\n</code></pre>"},{"location":"AGENT_DEVELOPMENT/#agent-3-engagement-automation","title":"Agent 3: Engagement Automation","text":"<p>Purpose: Build engagement features (like, comment, retweet, bookmark).</p>"},{"location":"AGENT_DEVELOPMENT/#prompt_1","title":"Prompt","text":"<pre><code>Build engagement automation for the X/Twitter toolkit.\n\nLIKE OPERATIONS (xeepy/actions/engagement/like/):\n\n1. like_tweet.py - Like a single tweet\n   - Navigate to tweet\n   - Click like button\n   - Verify liked\n   - Handle already-liked\n\n2. auto_like.py - Auto-like by criteria\n   - By keywords in tweet text\n   - By hashtags\n   - By specific users\n   - Configurable limit and delay\n   - Skip already-liked\n\n3. like_by_user.py - Like user's recent tweets\n   - Scrape user's tweets\n   - Like up to limit\n   - Smart selection (not too old)\n\nCOMMENT OPERATIONS (xeepy/actions/engagement/comment/):\n\n1. comment.py - Post a comment\n   - Navigate to tweet\n   - Click reply button\n   - Type comment\n   - Submit\n   - Verify posted\n\n2. auto_comment.py - Auto-comment\n   - Comment on matching tweets\n   - Use template system\n   - Variable substitution\n   - AI-generated option\n\n3. comment_templates.py - Template management\n   - Load templates from file\n   - Random selection\n   - Variable support ({username}, {topic}, etc.)\n\nRETWEET OPERATIONS (xeepy/actions/engagement/retweet/):\n\n1. retweet.py - Retweet a tweet\n2. quote_tweet.py - Quote tweet with comment\n3. undo_retweet.py - Remove retweet\n\nBOOKMARK OPERATIONS (xeepy/actions/engagement/bookmark/):\n\n1. bookmark_tweet.py - Add bookmark\n2. remove_bookmark.py - Remove bookmark\n3. export_bookmarks.py - Export all bookmarks\n4. bookmark_manager.py - Manage bookmark folders\n\nAll engagement must:\n- Respect rate limits\n- Log all actions with timestamps\n- Support dry_run mode\n- Return success/failure with details\n</code></pre>"},{"location":"AGENT_DEVELOPMENT/#agent-4-monitoring-analytics","title":"Agent 4: Monitoring &amp; Analytics","text":"<p>Purpose: Build monitoring, tracking, and notification systems.</p>"},{"location":"AGENT_DEVELOPMENT/#prompt_2","title":"Prompt","text":"<pre><code>Build monitoring and analytics for the X/Twitter toolkit.\n\nMONITORING (xeepy/monitoring/):\n\n1. unfollower_tracker.py - Detect unfollowers\n   - Take snapshots of followers\n   - Compare over time\n   - Detect new unfollowers\n   - Detect new followers\n   - Generate reports\n\n2. account_monitor.py - Monitor any account\n   - Track follower count changes\n   - Detect bio changes\n   - Detect profile changes\n   - Alert on significant changes\n\n3. keyword_monitor.py - Real-time keyword monitoring\n   - Watch for keywords/hashtags\n   - Stream-like functionality (poll-based)\n   - Callback on new matches\n   - Filter by criteria\n\n4. growth_tracker.py - Track growth over time\n   - Daily/weekly/monthly metrics\n   - Follower growth rate\n   - Engagement rate\n   - Best performing content\n\nANALYTICS (xeepy/monitoring/analytics.py):\n\n1. EngagementAnalytics\n   - Calculate engagement rate\n   - Best posting times\n   - Top performing tweets\n   - Audience insights\n\n2. GrowthAnalytics\n   - Growth velocity\n   - Projection modeling\n   - Trend analysis\n\nNOTIFICATIONS (xeepy/notifications/):\n\n1. discord.py - Discord webhooks\n   - Send notifications\n   - Rich embeds\n   - Configurable events\n\n2. telegram.py - Telegram bot\n   - Send messages\n   - Commands support\n\n3. email.py - Email notifications\n   - SMTP support\n   - HTML templates\n\n4. base.py - Notification base class\n   - Common interface\n   - Event filtering\n   - Rate limiting notifications\n\nStorage needs (xeepy/storage/):\n1. database.py - SQLite for local data\n2. snapshots.py - Follower snapshots\n3. timeseries.py - Time-series data\n</code></pre>"},{"location":"AGENT_DEVELOPMENT/#agent-5-ai-features","title":"Agent 5: AI Features","text":"<p>Purpose: Build AI integration (OpenAI, Anthropic, Ollama).</p>"},{"location":"AGENT_DEVELOPMENT/#prompt_3","title":"Prompt","text":"<pre><code>Build AI integration for the X/Twitter toolkit.\n\nAI PROVIDERS (xeepy/ai/providers.py):\n\nCreate abstraction layer supporting:\n1. OpenAI (GPT-4, GPT-3.5)\n2. Anthropic (Claude 3 Opus, Sonnet, Haiku)\n3. Ollama (local models)\n\nBase class:\n- async generate(prompt, **kwargs) -&gt; str\n- async generate_structured(prompt, schema) -&gt; dict\n- Support streaming\n- Handle rate limits\n- Retry logic\n\nCONTENT GENERATION (xeepy/ai/content.py):\n\n1. ContentGenerator class\n   - generate_reply(tweet_text, style, context)\n   - generate_tweet(topic, style, hashtags)\n   - generate_thread(topic, points)\n   - improve_draft(draft)\n\nStyles:\n- friendly, witty, professional, crypto, supportive\n\n2. Templates with AI enhancement\n   - Base template + AI variation\n   - Personality consistency\n   - Hashtag suggestions\n\nANALYSIS (xeepy/ai/analysis.py):\n\n1. SentimentAnalyzer\n   - analyze_tweet(tweet) -&gt; sentiment score\n   - analyze_batch(tweets) -&gt; aggregate\n   - detect_tone(text)\n\n2. BotDetector\n   - analyze_profile(user) -&gt; bot_probability\n   - Check patterns (posting frequency, content similarity)\n   - Engagement pattern analysis\n\n3. ContentClassifier\n   - Categorize tweet topics\n   - Detect spam/promotional\n   - Identify opportunities\n\nSMART FEATURES (xeepy/ai/smart.py):\n\n1. SmartTargeting\n   - Suggest accounts to follow\n   - Optimal engagement targets\n   - Growth recommendations\n\n2. TrendAnalyzer\n   - Identify trending topics in niche\n   - Predict virality\n   - Content opportunity detection\n\nConfiguration:\n- xeepy/ai/config.py - AI provider settings\n- Support for API keys via environment\n- Model selection\n- Cost tracking\n</code></pre>"},{"location":"AGENT_DEVELOPMENT/#using-these-prompts","title":"Using These Prompts","text":""},{"location":"AGENT_DEVELOPMENT/#for-development","title":"For Development","text":"<p>Copy the relevant prompt and give it to your AI coding assistant (Copilot, Cursor, Claude, etc.) to extend Xeepy.</p>"},{"location":"AGENT_DEVELOPMENT/#for-learning","title":"For Learning","text":"<p>Study the prompts to understand: - How to structure complex systems - How to specify requirements clearly - How to coordinate multiple agents</p>"},{"location":"AGENT_DEVELOPMENT/#for-contributing","title":"For Contributing","text":"<p>Use these prompts as templates when adding new features. Maintain the same: - Code style (async, type hints) - Documentation standards - Error handling patterns - Testing requirements</p>"},{"location":"AGENT_DEVELOPMENT/#success-criteria","title":"Success Criteria","text":"<p>Each agent was given these universal requirements:</p> <ol> <li>No Stubs - Every function must be fully implemented</li> <li>Type Hints - Full typing throughout</li> <li>Docstrings - Google-style docstrings</li> <li>Error Handling - Comprehensive try/except</li> <li>Async Native - All I/O operations async</li> <li>Rate Limiting - Integrate with rate limiter</li> <li>Logging - Structured logging</li> <li>Testing - Unit tests for each module</li> </ol> <p>This documentation is part of Xeepy - the most comprehensive Python toolkit for X/Twitter automation.</p>"},{"location":"AGENT_TASKS/","title":"AGENT TASKS","text":""},{"location":"AGENT_TASKS/#agent-5-api-reference","title":"\ud83e\udd16 AGENT 5: API Reference","text":"<p>Files to create (50+ total) - Structured reference documentation:</p>"},{"location":"AGENT_TASKS/#template-for-each-file","title":"Template for each file:","text":"<pre><code># ModuleName\n\nBrief description.\n\n## Classes\n\n### ClassName\n\nDescription.\n\n#### Constructor\n\n\\`\\`\\`python\nClassName(\n    param1: str,      # Description\n    param2: int = 10  # Description with default\n)\n\\`\\`\\`\n\n#### Attributes\n\n| Attribute | Type | Description |\n|-----------|------|-------------|\n| `attr1` | `str` | Description |\n\n#### Methods\n\n##### method_name()\n\n\\`\\`\\`python\nasync def method_name(\n    arg1: str,\n    arg2: Optional[int] = None\n) -&gt; ReturnType\n\\`\\`\\`\n\n**Parameters:**\n- `arg1` - Description\n- `arg2` - Description (optional)\n\n**Returns:** Description\n\n**Raises:**\n- `ErrorType` - When this happens\n\n**Example:**\n\\`\\`\\`python\nresult = await obj.method_name(\"test\")\n\\`\\`\\`\n</code></pre>"},{"location":"AGENT_TASKS/#files-to-create","title":"Files to create:","text":"<p>Core (<code>docs/api/core/</code>): 1. <code>xeepy.md</code>, <code>browser.md</code>, <code>auth.md</code>, <code>rate_limiter.md</code>, <code>config.md</code></p> <p>Scrapers (<code>docs/api/scrapers/</code>): 6-20. <code>replies.md</code>, <code>profile.md</code>, <code>followers.md</code>, <code>following.md</code>, <code>tweets.md</code>, <code>thread.md</code>, <code>search.md</code>, <code>hashtag.md</code>, <code>media.md</code>, <code>likes.md</code>, <code>lists.md</code>, <code>mentions.md</code>, <code>spaces.md</code>, <code>downloads.md</code>, <code>recommendations.md</code></p> <p>Actions (<code>docs/api/actions/</code>): 21-27. <code>follow.md</code>, <code>unfollow.md</code>, <code>engage.md</code>, <code>messaging.md</code>, <code>scheduling.md</code>, <code>polls.md</code>, <code>settings.md</code></p> <p>Monitoring (<code>docs/api/monitoring/</code>): 28-31. <code>unfollowers.md</code>, <code>growth.md</code>, <code>keywords.md</code>, <code>account.md</code></p> <p>Analytics (<code>docs/api/analytics/</code>): 32-36. <code>growth.md</code>, <code>engagement.md</code>, <code>audience.md</code>, <code>competitors.md</code>, <code>content.md</code></p> <p>AI (<code>docs/api/ai/</code>): 37-40. <code>providers.md</code>, <code>content.md</code>, <code>sentiment.md</code>, <code>detection.md</code></p> <p>API (<code>docs/api/api/</code>): 41-42. <code>graphql.md</code>, <code>server.md</code></p> <p>Models (<code>docs/api/models/</code>): 43-45. <code>tweet.md</code>, <code>user.md</code>, <code>engagement.md</code></p> <p>Storage (<code>docs/api/storage/</code>): 46-47. <code>database.md</code>, <code>export.md</code></p> <p>Notifications (<code>docs/api/notifications/</code>): 48-50. <code>discord.md</code>, <code>telegram.md</code>, <code>email.md</code></p>"},{"location":"AGENT_TASKS/#also-create","title":"Also create:","text":"<ol> <li><code>docs/migration.md</code> - Migration guide from v1 to v2</li> </ol>"},{"location":"AGENT_TASKS/#fix-required","title":"Fix required:","text":"<ol> <li>Edit <code>docs/FAQ.md</code> - Change <code>../CONTRIBUTING.md</code> to <code>community/contributing.md</code></li> </ol>"},{"location":"AGENT_TASKS/#verification-checklist","title":"\u2705 Verification Checklist","text":"<p>After all agents complete, run: <pre><code>mkdocs serve\n</code></pre></p> <p>Verify: - [ ] No warnings about missing files - [ ] No broken internal links - [ ] All code examples have correct syntax - [ ] Consistent formatting across pages</p>"},{"location":"AI_FEATURES/","title":"\ud83e\udd16 AI Features Guide","text":"<p>Xeepy integrates with leading AI providers to enable intelligent automation. This guide covers all AI-powered features.</p>"},{"location":"AI_FEATURES/#overview","title":"\ud83c\udfaf Overview","text":"Feature Description Providers Content Generation AI-generated tweets, replies, threads OpenAI, Anthropic, Ollama Sentiment Analysis Analyze tweet sentiment and emotions All providers + local Bot Detection Identify spam/bot accounts ML-based Smart Targeting AI recommendations for who to follow All providers Crypto Analysis Crypto Twitter intelligence GPT-4, Claude"},{"location":"AI_FEATURES/#setup","title":"\ud83d\udd27 Setup","text":""},{"location":"AI_FEATURES/#install-ai-dependencies","title":"Install AI Dependencies","text":"<pre><code>pip install xeepy[ai]\n</code></pre>"},{"location":"AI_FEATURES/#configure-providers","title":"Configure Providers","text":"<pre><code>from xeepy.ai import AIConfig\n\n# OpenAI\nconfig = AIConfig(\n    provider=\"openai\",\n    api_key=\"sk-...\",\n    model=\"gpt-4\"\n)\n\n# Anthropic Claude\nconfig = AIConfig(\n    provider=\"anthropic\",\n    api_key=\"sk-ant-...\",\n    model=\"claude-3-opus-20240229\"\n)\n\n# Local (Ollama)\nconfig = AIConfig(\n    provider=\"ollama\",\n    model=\"llama2\",\n    base_url=\"http://localhost:11434\"\n)\n</code></pre>"},{"location":"AI_FEATURES/#environment-variables","title":"Environment Variables","text":"<pre><code># .env file\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\nOLLAMA_BASE_URL=http://localhost:11434\n</code></pre>"},{"location":"AI_FEATURES/#content-generation","title":"\ud83d\udcdd Content Generation","text":""},{"location":"AI_FEATURES/#generate-replies","title":"Generate Replies","text":"<pre><code>from xeepy.ai import ContentGenerator\n\nai = ContentGenerator(provider=\"openai\", api_key=\"...\")\n\n# Basic reply\nreply = await ai.generate_reply(\n    tweet_text=\"Just launched my startup! \ud83d\ude80\",\n    style=\"supportive\"\n)\n# Output: \"Congrats on the launch! What problem are you solving?\"\n\n# With context\nreply = await ai.generate_reply(\n    tweet_text=\"Python is the best language\",\n    style=\"witty\",\n    context={\n        \"author\": \"pythonista42\",\n        \"author_bio\": \"Python developer | Open source contributor\"\n    }\n)\n</code></pre>"},{"location":"AI_FEATURES/#available-styles","title":"Available Styles","text":"Style Description Best For <code>supportive</code> Encouraging, positive Launches, achievements <code>witty</code> Clever, humorous Entertainment, engagement <code>professional</code> Formal, business-like B2B, corporate <code>casual</code> Friendly, conversational General engagement <code>crypto</code> Web3 vernacular (WAGMI, etc.) Crypto Twitter <code>tech</code> Technical, enthusiastic Dev community <code>helpful</code> Adds value, informative Questions, discussions"},{"location":"AI_FEATURES/#generate-tweets","title":"Generate Tweets","text":"<pre><code># Single tweet\ntweet = await ai.generate_tweet(\n    topic=\"Python async programming tips\",\n    style=\"educational\",\n    hashtags=[\"Python\", \"AsyncIO\"]\n)\n\n# Thread\nthread = await ai.generate_thread(\n    topic=\"Why async/await is game-changing in Python\",\n    num_tweets=5,\n    style=\"educational\"\n)\nfor i, tweet in enumerate(thread, 1):\n    print(f\"Tweet {i}: {tweet}\")\n</code></pre>"},{"location":"AI_FEATURES/#improve-existing-text","title":"Improve Existing Text","text":"<pre><code># Make more engaging\nimproved = await ai.improve_text(\n    text=\"Check out my new project\",\n    goal=\"engagement\"  # engagement, clarity, professionalism\n)\n# Output: \"\ud83d\ude80 Just shipped something I've been working on for months! Check it out and let me know what you think \ud83d\udc47\"\n</code></pre>"},{"location":"AI_FEATURES/#sentiment-analysis","title":"\ud83d\ude0a Sentiment Analysis","text":""},{"location":"AI_FEATURES/#analyze-single-tweet","title":"Analyze Single Tweet","text":"<pre><code>from xeepy.ai import SentimentAnalyzer\n\nanalyzer = SentimentAnalyzer()\n\nresult = await analyzer.analyze_tweet(\n    \"This product is absolutely terrible! Worst purchase ever!\"\n)\n\nprint(f\"Score: {result.score}\")      # -0.85 (negative)\nprint(f\"Label: {result.label}\")      # \"negative\"\nprint(f\"Confidence: {result.confidence}\")  # 0.92\nprint(f\"Emotions: {result.emotions}\")\n# {'anger': 0.7, 'disappointment': 0.2, 'frustration': 0.1}\n</code></pre>"},{"location":"AI_FEATURES/#analyze-conversation","title":"Analyze Conversation","text":"<pre><code># Analyze a thread or replies\ntweets = [\n    \"Just announced our new feature!\",\n    \"This is amazing! Can't wait to try it\",\n    \"Not sure about the pricing though\",\n    \"Finally! Been waiting for this\"\n]\n\nsentiment = await analyzer.analyze_conversation(tweets)\nprint(f\"Overall sentiment: {sentiment.overall_label}\")  # \"positive\"\nprint(f\"Sentiment trend: {sentiment.trend}\")  # \"stable\" or \"improving\" or \"declining\"\n</code></pre>"},{"location":"AI_FEATURES/#analyze-mentions","title":"Analyze Mentions","text":"<pre><code># Analyze how people talk about you/your brand\nreport = await analyzer.analyze_mentions(\n    username=\"your_username\",\n    limit=100\n)\n\nprint(f\"Positive mentions: {report.positive_percentage:.1%}\")\nprint(f\"Negative mentions: {report.negative_percentage:.1%}\")\nprint(f\"Common complaints: {report.negative_topics}\")\nprint(f\"Common praise: {report.positive_topics}\")\n</code></pre>"},{"location":"AI_FEATURES/#botspam-detection","title":"\ud83e\udd16 Bot/Spam Detection","text":""},{"location":"AI_FEATURES/#analyze-single-user","title":"Analyze Single User","text":"<pre><code>from xeepy.ai import SpamDetector\n\ndetector = SpamDetector()\n\nscore = await detector.analyze_user(\"suspicious_account_123\")\n\nprint(f\"Bot probability: {score.bot_probability:.1%}\")\nprint(f\"Spam probability: {score.spam_probability:.1%}\")\nprint(f\"Quality score: {score.quality_score}/100\")\nprint(f\"Red flags: {score.red_flags}\")\n# ['Default profile picture', 'Account age &lt; 30 days', \n#  'Following/follower ratio &gt; 10', 'Repetitive tweet patterns']\n</code></pre>"},{"location":"AI_FEATURES/#analyze-your-followers","title":"Analyze Your Followers","text":"<pre><code># Find bots/spam among your followers\nreport = await detector.analyze_followers(\n    username=\"your_username\",\n    sample_size=200\n)\n\nprint(f\"Estimated fake followers: {report.fake_percentage:.1%}\")\nprint(f\"High-quality followers: {report.quality_percentage:.1%}\")\nprint(f\"Suspicious accounts: {report.suspicious_accounts}\")\n</code></pre>"},{"location":"AI_FEATURES/#detection-factors","title":"Detection Factors","text":"<p>The bot detector analyzes:</p> Factor Weight Description Account age High New accounts are more suspicious Profile completeness Medium Bio, avatar, banner Tweet patterns High Repetitive content, timing Engagement ratio Medium Likes/retweets vs followers Following ratio Medium Following &gt;&gt; Followers = suspicious Content originality High Original vs copied content Activity hours Low Abnormal posting times"},{"location":"AI_FEATURES/#smart-targeting","title":"\ud83c\udfaf Smart Targeting","text":""},{"location":"AI_FEATURES/#find-accounts-to-follow","title":"Find Accounts to Follow","text":"<pre><code>from xeepy.ai import SmartTargeting\n\ntargeting = SmartTargeting(provider=\"openai\", api_key=\"...\")\n\nrecommendations = await targeting.find_targets(\n    niche=\"Python developers\",\n    goal=\"growth\",  # growth, engagement, sales\n    limit=25\n)\n\nfor rec in recommendations:\n    print(f\"@{rec.username}\")\n    print(f\"  Score: {rec.score:.2f}\")\n    print(f\"  Why: {rec.reasons}\")\n    print(f\"  Actions: {rec.recommended_actions}\")\n    print(f\"  Follow-back chance: {rec.follow_back_chance:.1%}\")\n</code></pre>"},{"location":"AI_FEATURES/#analyze-target-account","title":"Analyze Target Account","text":"<pre><code># Deep analysis of a potential account to engage with\nanalysis = await targeting.analyze_target(\"python_guru\")\n\nprint(f\"Relevance to your niche: {analysis.relevance_score:.1%}\")\nprint(f\"Engagement quality: {analysis.engagement_quality}\")\nprint(f\"Best time to engage: {analysis.best_engagement_time}\")\nprint(f\"Content themes: {analysis.content_themes}\")\nprint(f\"Recommendation: {analysis.recommendation}\")\n</code></pre>"},{"location":"AI_FEATURES/#crypto-twitter-analysis","title":"\ud83d\udcb0 Crypto Twitter Analysis","text":""},{"location":"AI_FEATURES/#token-sentiment","title":"Token Sentiment","text":"<pre><code>from xeepy.ai import CryptoAnalyzer\n\ncrypto = CryptoAnalyzer(provider=\"openai\", api_key=\"...\")\n\n# Analyze sentiment for a token\nsentiment = await crypto.analyze_token_sentiment(\n    token=\"$ETH\",\n    limit=100\n)\n\nprint(f\"Overall sentiment: {sentiment.label}\")  # bullish, bearish, neutral\nprint(f\"Sentiment score: {sentiment.score}\")\nprint(f\"Volume of discussion: {sentiment.tweet_count}\")\nprint(f\"Notable influencers talking: {sentiment.influencers}\")\n</code></pre>"},{"location":"AI_FEATURES/#find-alpha","title":"Find Alpha","text":"<pre><code># Find potentially valuable tweets\nalpha = await crypto.find_alpha(\n    keywords=[\"airdrop\", \"whitelist\", \"alpha\"],\n    limit=50\n)\n\nfor tweet in alpha:\n    print(f\"@{tweet.author}: {tweet.text}\")\n    print(f\"  Alpha score: {tweet.alpha_score}\")\n    print(f\"  Urgency: {tweet.urgency}\")\n</code></pre>"},{"location":"AI_FEATURES/#detect-shills","title":"Detect Shills","text":"<pre><code># Detect coordinated promotion\nshills = await crypto.detect_shills(\n    token=\"$NEWCOIN\",\n    limit=50\n)\n\nprint(f\"Shill probability: {shills.shill_probability:.1%}\")\nprint(f\"Suspicious accounts: {len(shills.suspicious_accounts)}\")\nprint(f\"Coordination patterns: {shills.patterns}\")\n</code></pre>"},{"location":"AI_FEATURES/#integration-examples","title":"\ud83d\udd0c Integration Examples","text":""},{"location":"AI_FEATURES/#auto-comment-with-ai","title":"Auto-Comment with AI","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import ContentGenerator\n\nasync with Xeepy() as x:\n    ai = ContentGenerator(provider=\"openai\", api_key=\"...\")\n\n    # Search for tweets to engage with\n    tweets = await x.scrape.search(\"Python tips\", limit=10)\n\n    for tweet in tweets:\n        # Generate contextual reply\n        reply = await ai.generate_reply(\n            tweet_text=tweet.text,\n            style=\"helpful\",\n            context={\"author\": tweet.author.username}\n        )\n\n        # Preview (don't auto-post without review!)\n        print(f\"Tweet: {tweet.text[:50]}...\")\n        print(f\"Reply: {reply}\")\n        print(\"---\")\n</code></pre>"},{"location":"AI_FEATURES/#smart-unfollow-with-ai","title":"Smart Unfollow with AI","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import SpamDetector\n\nasync with Xeepy() as x:\n    detector = SpamDetector()\n\n    # Get non-followers\n    non_followers = await x.unfollow.get_non_followers()\n\n    # Analyze each to keep quality accounts\n    to_unfollow = []\n    to_keep = []\n\n    for user in non_followers:\n        score = await detector.analyze_user(user)\n\n        if score.quality_score &lt; 30 or score.bot_probability &gt; 0.7:\n            to_unfollow.append(user)\n        else:\n            to_keep.append(user)\n\n    print(f\"Unfollow (low quality): {len(to_unfollow)}\")\n    print(f\"Keep (high quality): {len(to_keep)}\")\n</code></pre>"},{"location":"AI_FEATURES/#growth-suite-with-ai","title":"Growth Suite with AI","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import SmartTargeting, ContentGenerator\n\nasync with Xeepy() as x:\n    targeting = SmartTargeting(provider=\"openai\", api_key=\"...\")\n    content = ContentGenerator(provider=\"openai\", api_key=\"...\")\n\n    # Find accounts to engage with\n    targets = await targeting.find_targets(\n        niche=\"Python developers\",\n        goal=\"growth\",\n        limit=10\n    )\n\n    for target in targets:\n        # Get their recent tweet\n        tweets = await x.scrape.tweets(target.username, limit=1)\n\n        if tweets:\n            # Generate relevant reply\n            reply = await content.generate_reply(\n                tweet_text=tweets[0].text,\n                style=\"helpful\"\n            )\n\n            print(f\"Engage with @{target.username}\")\n            print(f\"  Their tweet: {tweets[0].text[:50]}...\")\n            print(f\"  Your reply: {reply}\")\n</code></pre>"},{"location":"AI_FEATURES/#best-practices","title":"\ud83d\udca1 Best Practices","text":""},{"location":"AI_FEATURES/#1-dont-auto-post-without-review","title":"1. Don't Auto-Post Without Review","text":"<pre><code># \u2705 Good: Preview before posting\nreply = await ai.generate_reply(tweet)\nprint(f\"Preview: {reply}\")\nuser_confirms = input(\"Post this? (y/n): \")\nif user_confirms == \"y\":\n    await x.engage.comment(tweet_url, reply)\n\n# \u274c Bad: Fully automated posting\nreply = await ai.generate_reply(tweet)\nawait x.engage.comment(tweet_url, reply)  # Dangerous!\n</code></pre>"},{"location":"AI_FEATURES/#2-use-appropriate-styles","title":"2. Use Appropriate Styles","text":"<pre><code># Match style to context\nif \"startup\" in tweet.lower() or \"launch\" in tweet.lower():\n    style = \"supportive\"\nelif \"help\" in tweet.lower() or \"?\" in tweet:\n    style = \"helpful\"\nelif account_is_crypto:\n    style = \"crypto\"\nelse:\n    style = \"casual\"\n</code></pre>"},{"location":"AI_FEATURES/#3-rate-limit-ai-calls","title":"3. Rate Limit AI Calls","text":"<pre><code># AI APIs have rate limits too\nimport asyncio\n\nfor tweet in tweets:\n    reply = await ai.generate_reply(tweet.text)\n    await asyncio.sleep(1)  # Don't spam the AI API\n</code></pre>"},{"location":"AI_FEATURES/#4-cache-results","title":"4. Cache Results","text":"<pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=100)\nasync def cached_sentiment(tweet_text):\n    return await analyzer.analyze_tweet(tweet_text)\n</code></pre>"},{"location":"AI_FEATURES/#privacy-ethics","title":"\ud83d\udd12 Privacy &amp; Ethics","text":"<ul> <li>Never store user data longer than necessary</li> <li>Don't pretend AI replies are from a human</li> <li>Respect user privacy and preferences</li> <li>Don't use AI for spam or harassment</li> <li>Review AI-generated content before posting</li> <li>Disclose AI usage if required by platform</li> </ul>"},{"location":"AI_FEATURES/#cost-estimation","title":"\ud83d\udcca Cost Estimation","text":"Provider Model Cost per 1K tokens OpenAI GPT-4 ~$0.03-0.06 OpenAI GPT-3.5 ~$0.002 Anthropic Claude 3 Opus ~$0.015-0.075 Anthropic Claude 3 Sonnet ~$0.003-0.015 Ollama Local Free <p>Typical usage per feature: - Reply generation: 200-500 tokens - Sentiment analysis: 100-200 tokens - Thread generation: 1000-2000 tokens</p>"},{"location":"AI_FEATURES/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<ol> <li>Examples - More AI integration examples</li> <li>API Reference - Full API documentation</li> <li>CLI Reference - AI CLI commands</li> </ol> <p> AI + Automation = \ud83d\ude80 </p>"},{"location":"API_REFERENCE/","title":"Xeepy API Reference","text":"<p>Complete API documentation for all Xeepy classes and methods.</p>"},{"location":"API_REFERENCE/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Xeepy (Main Class)</li> <li>Scrapers</li> <li>Follow/Unfollow Actions</li> <li>Engagement Actions</li> <li>Monitoring</li> <li>AI Integration</li> <li>Storage &amp; Export</li> <li>Models</li> </ul>"},{"location":"API_REFERENCE/#xeepy-main-class","title":"Xeepy (Main Class)","text":"<p>The main entry point for all Xeepy functionality.</p>"},{"location":"API_REFERENCE/#constructor","title":"Constructor","text":"<pre><code>Xeepy(\n    headless: bool = True,\n    session_path: str | None = None,\n    config: XeepyConfig | None = None,\n    rate_limit: bool = True\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>headless</code> <code>bool</code> <code>True</code> Run browser in headless mode <code>session_path</code> <code>str</code> <code>None</code> Path to saved session file <code>config</code> <code>XeepyConfig</code> <code>None</code> Configuration object <code>rate_limit</code> <code>bool</code> <code>True</code> Enable rate limiting"},{"location":"API_REFERENCE/#usage","title":"Usage","text":"<pre><code>from xeepy import Xeepy\n\n# Basic usage\nasync with Xeepy() as x:\n    # Access all features via x.scrape, x.follow, etc.\n    pass\n\n# With options\nasync with Xeepy(headless=False, session_path=\"session.json\") as x:\n    pass\n</code></pre>"},{"location":"API_REFERENCE/#properties","title":"Properties","text":"Property Type Description <code>x.scrape</code> <code>Scraper</code> Access to all scrapers <code>x.follow</code> <code>FollowActions</code> Follow operations <code>x.unfollow</code> <code>UnfollowActions</code> Unfollow operations <code>x.engage</code> <code>EngageActions</code> Engagement operations <code>x.monitor</code> <code>Monitor</code> Monitoring features <code>x.auth</code> <code>Auth</code> Authentication methods <code>x.export</code> <code>Export</code> Export utilities"},{"location":"API_REFERENCE/#scrapers","title":"Scrapers","text":""},{"location":"API_REFERENCE/#scrapereplies","title":"<code>scrape.replies()</code>","text":"<p>Scrape replies to a tweet.</p> <pre><code>async def replies(\n    tweet_url: str,\n    limit: int = 100,\n    include_author: bool = False\n) -&gt; list[Tweet]\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>tweet_url</code> <code>str</code> required URL of the tweet <code>limit</code> <code>int</code> <code>100</code> Maximum replies to fetch <code>include_author</code> <code>bool</code> <code>False</code> Include OP's replies <p>Returns: <code>list[Tweet]</code></p> <p>Example:</p> <pre><code>replies = await x.scrape.replies(\n    \"https://x.com/elonmusk/status/1234567890\",\n    limit=50\n)\n\nfor reply in replies:\n    print(f\"@{reply.username}: {reply.text}\")\n</code></pre>"},{"location":"API_REFERENCE/#scrapeprofile","title":"<code>scrape.profile()</code>","text":"<p>Scrape a user's profile information.</p> <pre><code>async def profile(username: str) -&gt; User\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>username</code> <code>str</code> required Twitter username (without @) <p>Returns: <code>User</code></p> <p>Example:</p> <pre><code>user = await x.scrape.profile(\"elonmusk\")\nprint(f\"Name: {user.name}\")\nprint(f\"Bio: {user.bio}\")\nprint(f\"Followers: {user.followers_count}\")\nprint(f\"Following: {user.following_count}\")\n</code></pre>"},{"location":"API_REFERENCE/#scrapefollowers","title":"<code>scrape.followers()</code>","text":"<p>Scrape a user's followers.</p> <pre><code>async def followers(\n    username: str,\n    limit: int = 1000\n) -&gt; list[User]\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>username</code> <code>str</code> required Twitter username <code>limit</code> <code>int</code> <code>1000</code> Maximum followers to fetch <p>Returns: <code>list[User]</code></p> <p>Example:</p> <pre><code>followers = await x.scrape.followers(\"elonmusk\", limit=100)\nfor follower in followers:\n    print(f\"@{follower.username} - {follower.followers_count} followers\")\n</code></pre>"},{"location":"API_REFERENCE/#scrapefollowing","title":"<code>scrape.following()</code>","text":"<p>Scrape who a user is following.</p> <pre><code>async def following(\n    username: str,\n    limit: int = 1000\n) -&gt; list[User]\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>username</code> <code>str</code> required Twitter username <code>limit</code> <code>int</code> <code>1000</code> Maximum users to fetch <p>Returns: <code>list[User]</code></p>"},{"location":"API_REFERENCE/#scrapetweets","title":"<code>scrape.tweets()</code>","text":"<p>Scrape a user's tweets.</p> <pre><code>async def tweets(\n    username: str,\n    limit: int = 100,\n    include_replies: bool = False,\n    include_retweets: bool = True\n) -&gt; list[Tweet]\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>username</code> <code>str</code> required Twitter username <code>limit</code> <code>int</code> <code>100</code> Maximum tweets to fetch <code>include_replies</code> <code>bool</code> <code>False</code> Include replies <code>include_retweets</code> <code>bool</code> <code>True</code> Include retweets <p>Returns: <code>list[Tweet]</code></p>"},{"location":"API_REFERENCE/#scrapehashtag","title":"<code>scrape.hashtag()</code>","text":"<p>Scrape tweets by hashtag.</p> <pre><code>async def hashtag(\n    tag: str,\n    limit: int = 100,\n    mode: str = \"latest\"\n) -&gt; list[Tweet]\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>tag</code> <code>str</code> required Hashtag (with or without #) <code>limit</code> <code>int</code> <code>100</code> Maximum tweets to fetch <code>mode</code> <code>str</code> <code>\"latest\"</code> <code>\"latest\"</code> or <code>\"top\"</code> <p>Returns: <code>list[Tweet]</code></p>"},{"location":"API_REFERENCE/#scrapesearch","title":"<code>scrape.search()</code>","text":"<p>Search for tweets.</p> <pre><code>async def search(\n    query: str,\n    limit: int = 100,\n    mode: str = \"latest\"\n) -&gt; list[Tweet]\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>query</code> <code>str</code> required Search query <code>limit</code> <code>int</code> <code>100</code> Maximum results <code>mode</code> <code>str</code> <code>\"latest\"</code> <code>\"latest\"</code> or <code>\"top\"</code> <p>Returns: <code>list[Tweet]</code></p>"},{"location":"API_REFERENCE/#followunfollow-actions","title":"Follow/Unfollow Actions","text":""},{"location":"API_REFERENCE/#followuser","title":"<code>follow.user()</code>","text":"<p>Follow a user.</p> <pre><code>async def user(username: str) -&gt; bool\n</code></pre> <p>Returns: <code>bool</code> - Success status</p>"},{"location":"API_REFERENCE/#followby_hashtag","title":"<code>follow.by_hashtag()</code>","text":"<p>Follow users from a hashtag.</p> <pre><code>async def by_hashtag(\n    hashtag: str,\n    limit: int = 50,\n    min_followers: int = 100\n) -&gt; FollowResult\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>hashtag</code> <code>str</code> required Hashtag to search <code>limit</code> <code>int</code> <code>50</code> Max users to follow <code>min_followers</code> <code>int</code> <code>100</code> Minimum follower count <p>Returns: <code>FollowResult</code></p>"},{"location":"API_REFERENCE/#unfollownon_followers","title":"<code>unfollow.non_followers()</code>","text":"<p>Unfollow users who don't follow back.</p> <pre><code>async def non_followers(\n    max_unfollows: int = 100,\n    whitelist: list[str] | None = None,\n    dry_run: bool = False\n) -&gt; UnfollowResult\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>max_unfollows</code> <code>int</code> <code>100</code> Maximum to unfollow <code>whitelist</code> <code>list[str]</code> <code>None</code> Users to never unfollow <code>dry_run</code> <code>bool</code> <code>False</code> Preview without acting <p>Returns: <code>UnfollowResult</code></p> <p>Example:</p> <pre><code># Preview first\nresult = await x.unfollow.non_followers(dry_run=True)\nprint(f\"Would unfollow: {len(result.would_unfollow)} users\")\n\n# Then execute\nresult = await x.unfollow.non_followers(\n    max_unfollows=50,\n    whitelist=[\"friend1\", \"friend2\"]\n)\nprint(f\"Unfollowed: {result.unfollowed_count}\")\n</code></pre>"},{"location":"API_REFERENCE/#unfolloweveryone","title":"<code>unfollow.everyone()</code>","text":"<p>Unfollow all users (nuclear option).</p> <pre><code>async def everyone(\n    whitelist: list[str] | None = None,\n    dry_run: bool = True  # Default is True for safety\n) -&gt; UnfollowResult\n</code></pre> <p>\u26a0\ufe0f Warning: This is irreversible. Always use <code>dry_run=True</code> first.</p>"},{"location":"API_REFERENCE/#unfollowsmart","title":"<code>unfollow.smart()</code>","text":"<p>Smart unfollow based on criteria.</p> <pre><code>async def smart(\n    days_inactive: int = 30,\n    min_engagement: float = 0.01,\n    max_unfollows: int = 100\n) -&gt; UnfollowResult\n</code></pre>"},{"location":"API_REFERENCE/#engagement-actions","title":"Engagement Actions","text":""},{"location":"API_REFERENCE/#engagelike","title":"<code>engage.like()</code>","text":"<p>Like a tweet.</p> <pre><code>async def like(tweet_url: str) -&gt; bool\n</code></pre>"},{"location":"API_REFERENCE/#engageauto_like","title":"<code>engage.auto_like()</code>","text":"<p>Auto-like tweets by criteria.</p> <pre><code>async def auto_like(\n    keywords: list[str] | None = None,\n    hashtags: list[str] | None = None,\n    users: list[str] | None = None,\n    limit: int = 50,\n    delay_range: tuple[int, int] = (2, 5)\n) -&gt; EngagementResult\n</code></pre> <p>Example:</p> <pre><code>result = await x.engage.auto_like(\n    keywords=[\"python\", \"javascript\"],\n    limit=20\n)\nprint(f\"Liked {result.liked_count} tweets\")\n</code></pre>"},{"location":"API_REFERENCE/#engagecomment","title":"<code>engage.comment()</code>","text":"<p>Post a comment on a tweet.</p> <pre><code>async def comment(\n    tweet_url: str,\n    text: str\n) -&gt; bool\n</code></pre>"},{"location":"API_REFERENCE/#engageretweet","title":"<code>engage.retweet()</code>","text":"<p>Retweet a tweet.</p> <pre><code>async def retweet(tweet_url: str) -&gt; bool\n</code></pre>"},{"location":"API_REFERENCE/#monitoring","title":"Monitoring","text":""},{"location":"API_REFERENCE/#monitorunfollowers","title":"<code>monitor.unfollowers()</code>","text":"<p>Detect who unfollowed you.</p> <pre><code>async def unfollowers() -&gt; UnfollowerReport\n</code></pre> <p>Returns: <code>UnfollowerReport</code></p> <pre><code>report = await x.monitor.unfollowers()\nprint(f\"New unfollowers: {report.unfollowers}\")\nprint(f\"New followers: {report.new_followers}\")\nprint(f\"Total followers: {report.current_count}\")\n</code></pre>"},{"location":"API_REFERENCE/#monitoraccount","title":"<code>monitor.account()</code>","text":"<p>Monitor an account for changes.</p> <pre><code>async def account(\n    username: str,\n    watch: list[str] = [\"bio\", \"followers\", \"following\"]\n) -&gt; AccountMonitor\n</code></pre>"},{"location":"API_REFERENCE/#monitorkeywords","title":"<code>monitor.keywords()</code>","text":"<p>Monitor for keyword mentions.</p> <pre><code>async def keywords(\n    keywords: list[str],\n    callback: Callable[[Tweet], None] | None = None\n) -&gt; KeywordMonitor\n</code></pre>"},{"location":"API_REFERENCE/#ai-integration","title":"AI Integration","text":""},{"location":"API_REFERENCE/#contentgenerator","title":"<code>ContentGenerator</code>","text":"<p>AI-powered content generation.</p> <pre><code>from xeepy.ai import ContentGenerator\n\nai = ContentGenerator(\n    provider: str = \"openai\",  # \"openai\", \"anthropic\", \"ollama\"\n    api_key: str | None = None,\n    model: str | None = None,\n    base_url: str | None = None  # For Ollama\n)\n</code></pre>"},{"location":"API_REFERENCE/#aigenerate_reply","title":"<code>ai.generate_reply()</code>","text":"<p>Generate an AI reply to a tweet.</p> <pre><code>async def generate_reply(\n    tweet_text: str,\n    style: str = \"friendly\",  # friendly, witty, professional, crypto\n    context: str | None = None,\n    max_length: int = 280\n) -&gt; str\n</code></pre> <p>Example:</p> <pre><code>reply = await ai.generate_reply(\n    tweet_text=\"Just shipped a new feature!\",\n    style=\"supportive\",\n    max_length=280\n)\nprint(reply)  # \"Congrats on the launch! \ud83c\udf89 What problem does it solve?\"\n</code></pre>"},{"location":"API_REFERENCE/#aianalyze_sentiment","title":"<code>ai.analyze_sentiment()</code>","text":"<p>Analyze sentiment of tweets.</p> <pre><code>async def analyze_sentiment(\n    tweets: list[Tweet]\n) -&gt; SentimentResult\n</code></pre>"},{"location":"API_REFERENCE/#aidetect_bot","title":"<code>ai.detect_bot()</code>","text":"<p>Detect if an account is a bot.</p> <pre><code>async def detect_bot(user: User) -&gt; BotDetectionResult\n</code></pre>"},{"location":"API_REFERENCE/#storage-export","title":"Storage &amp; Export","text":""},{"location":"API_REFERENCE/#exportto_csv","title":"<code>export.to_csv()</code>","text":"<p>Export data to CSV.</p> <pre><code>def to_csv(\n    data: list[Tweet | User],\n    filepath: str,\n    columns: list[str] | None = None\n) -&gt; str\n</code></pre>"},{"location":"API_REFERENCE/#exportto_json","title":"<code>export.to_json()</code>","text":"<p>Export data to JSON.</p> <pre><code>def to_json(\n    data: list[Tweet | User],\n    filepath: str,\n    indent: int = 2\n) -&gt; str\n</code></pre>"},{"location":"API_REFERENCE/#exportto_excel","title":"<code>export.to_excel()</code>","text":"<p>Export data to Excel.</p> <pre><code>def to_excel(\n    data: list[Tweet | User],\n    filepath: str,\n    sheet_name: str = \"Data\"\n) -&gt; str\n</code></pre>"},{"location":"API_REFERENCE/#models","title":"Models","text":""},{"location":"API_REFERENCE/#tweet","title":"<code>Tweet</code>","text":"<pre><code>@dataclass\nclass Tweet:\n    id: str\n    text: str\n    username: str\n    user_id: str\n    created_at: datetime\n    likes: int\n    retweets: int\n    replies: int\n    url: str\n    media: list[Media] | None = None\n    is_reply: bool = False\n    is_retweet: bool = False\n</code></pre>"},{"location":"API_REFERENCE/#user","title":"<code>User</code>","text":"<pre><code>@dataclass\nclass User:\n    id: str\n    username: str\n    name: str\n    bio: str | None\n    followers_count: int\n    following_count: int\n    tweet_count: int\n    created_at: datetime\n    verified: bool\n    profile_image_url: str | None\n    banner_url: str | None\n    location: str | None\n    website: str | None\n</code></pre>"},{"location":"API_REFERENCE/#unfollowresult","title":"<code>UnfollowResult</code>","text":"<pre><code>@dataclass\nclass UnfollowResult:\n    unfollowed_count: int\n    unfollowed_users: list[str]\n    would_unfollow: list[str]  # For dry_run\n    skipped_whitelist: list[str]\n    errors: list[str]\n</code></pre>"},{"location":"API_REFERENCE/#engagementresult","title":"<code>EngagementResult</code>","text":"<pre><code>@dataclass\nclass EngagementResult:\n    liked_count: int\n    liked_tweets: list[str]\n    commented_count: int\n    retweeted_count: int\n    errors: list[str]\n</code></pre>"},{"location":"API_REFERENCE/#configuration","title":"Configuration","text":""},{"location":"API_REFERENCE/#xeepyconfig","title":"<code>XeepyConfig</code>","text":"<pre><code>@dataclass\nclass XeepyConfig:\n    # Browser settings\n    headless: bool = True\n    slow_mo: int = 0\n    timeout: int = 30000\n\n    # Rate limiting\n    follow_delay: tuple[int, int] = (3, 8)\n    unfollow_delay: tuple[int, int] = (2, 6)\n    like_delay: tuple[int, int] = (1, 3)\n    comment_delay: tuple[int, int] = (30, 90)\n\n    # Limits per day\n    max_follows_per_day: int = 100\n    max_unfollows_per_day: int = 150\n    max_likes_per_day: int = 500\n    max_comments_per_day: int = 50\n\n    # Storage\n    database_path: str = \"~/.xeepy/data.db\"\n    session_path: str = \"~/.xeepy/session.json\"\n</code></pre>"},{"location":"API_REFERENCE/#error-handling","title":"Error Handling","text":""},{"location":"API_REFERENCE/#exceptions","title":"Exceptions","text":"<pre><code>from xeepy.exceptions import (\n    XeepyError,           # Base exception\n    AuthenticationError,   # Login/session issues\n    RateLimitError,        # Rate limit exceeded\n    ScraperError,          # Scraping failed\n    ActionError,           # Action failed (follow, like, etc.)\n    NetworkError,          # Network issues\n)\n</code></pre>"},{"location":"API_REFERENCE/#example","title":"Example","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.exceptions import AuthenticationError, RateLimitError\n\nasync with Xeepy() as x:\n    try:\n        await x.follow.user(\"username\")\n    except AuthenticationError:\n        print(\"Please login first\")\n        await x.auth.login()\n    except RateLimitError as e:\n        print(f\"Rate limited. Wait {e.retry_after} seconds\")\n</code></pre>"},{"location":"API_REFERENCE/#next-steps","title":"Next Steps","text":"<ul> <li>Examples - See more code examples</li> <li>CLI Reference - Command-line usage</li> <li>FAQ - Common questions</li> </ul>"},{"location":"CLI_REFERENCE/","title":"\ud83d\udda5\ufe0f CLI Reference","text":"<p>Complete command-line interface documentation for Xeepy.</p>"},{"location":"CLI_REFERENCE/#installation","title":"Installation","text":"<pre><code>pip install xeepy\n</code></pre> <p>After installation, the <code>xeepy</code> command is available globally.</p>"},{"location":"CLI_REFERENCE/#global-options","title":"Global Options","text":"<pre><code>xeepy [OPTIONS] COMMAND [ARGS]\n\nOptions:\n  -c, --config PATH    Config file path (default: ~/.xeepy/config.yaml)\n  -v, --verbose        Enable verbose output\n  --headless/--no-headless  Run browser headless (default: headless)\n  --version            Show version\n  --help               Show help\n</code></pre>"},{"location":"CLI_REFERENCE/#authentication-commands","title":"Authentication Commands","text":""},{"location":"CLI_REFERENCE/#xeepy-auth-login","title":"<code>xeepy auth login</code>","text":"<p>Setup authentication with your X/Twitter session.</p> <pre><code>xeepy auth login [OPTIONS]\n\nOptions:\n  --token TEXT    Your auth_token cookie value\n  --interactive   Interactive setup wizard\n</code></pre> <p>Examples:</p> <pre><code># Interactive login\nxeepy auth login --interactive\n\n# Direct token\nxeepy auth login --token \"your_auth_token_here\"\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-auth-logout","title":"<code>xeepy auth logout</code>","text":"<p>Remove saved authentication.</p> <pre><code>xeepy auth logout\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-auth-status","title":"<code>xeepy auth status</code>","text":"<p>Check authentication status.</p> <pre><code>xeepy auth status\n\n# Output:\n# \u2705 Authenticated as @your_username\n# Session expires: 2026-03-15\n</code></pre>"},{"location":"CLI_REFERENCE/#scraping-commands","title":"Scraping Commands","text":""},{"location":"CLI_REFERENCE/#xeepy-scrape-replies","title":"<code>xeepy scrape replies</code>","text":"<p>Scrape replies to a tweet.</p> <pre><code>xeepy scrape replies URL [OPTIONS]\n\nArguments:\n  URL    Tweet URL\n\nOptions:\n  -l, --limit INT      Max replies (default: 100)\n  -o, --output PATH    Output file (.json or .csv)\n  --format TEXT        Output format: json, csv (default: json)\n</code></pre> <p>Examples:</p> <pre><code># Basic\nxeepy scrape replies https://x.com/user/status/123\n\n# With options\nxeepy scrape replies https://x.com/user/status/123 -l 200 -o replies.csv\n\n# JSON output\nxeepy scrape replies https://x.com/user/status/123 --format json -o replies.json\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-scrape-profile","title":"<code>xeepy scrape profile</code>","text":"<p>Scrape user profile information.</p> <pre><code>xeepy scrape profile USERNAME [OPTIONS]\n\nArguments:\n  USERNAME    X/Twitter username (without @)\n\nOptions:\n  -o, --output PATH    Output file\n</code></pre> <p>Examples:</p> <pre><code>xeepy scrape profile elonmusk\nxeepy scrape profile python -o python_profile.json\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-scrape-followers","title":"<code>xeepy scrape followers</code>","text":"<p>Scrape a user's followers.</p> <pre><code>xeepy scrape followers USERNAME [OPTIONS]\n\nArguments:\n  USERNAME    X/Twitter username\n\nOptions:\n  -l, --limit INT      Max followers (default: 100)\n  -o, --output PATH    Output file\n  --format TEXT        json or csv\n</code></pre> <p>Examples:</p> <pre><code>xeepy scrape followers python -l 500 -o followers.csv\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-scrape-following","title":"<code>xeepy scrape following</code>","text":"<p>Scrape who a user follows.</p> <pre><code>xeepy scrape following USERNAME [OPTIONS]\n\nArguments:\n  USERNAME    X/Twitter username\n\nOptions:\n  -l, --limit INT      Max accounts (default: 100)\n  -o, --output PATH    Output file\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-scrape-tweets","title":"<code>xeepy scrape tweets</code>","text":"<p>Scrape a user's tweets.</p> <pre><code>xeepy scrape tweets USERNAME [OPTIONS]\n\nArguments:\n  USERNAME    X/Twitter username\n\nOptions:\n  -l, --limit INT          Max tweets (default: 100)\n  -o, --output PATH        Output file\n  --include-replies        Include replies\n  --include-retweets       Include retweets\n</code></pre> <p>Examples:</p> <pre><code>xeepy scrape tweets python -l 200 -o tweets.json\nxeepy scrape tweets python --include-replies\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-scrape-search","title":"<code>xeepy scrape search</code>","text":"<p>Search for tweets.</p> <pre><code>xeepy scrape search QUERY [OPTIONS]\n\nArguments:\n  QUERY    Search query\n\nOptions:\n  -l, --limit INT      Max results (default: 50)\n  -o, --output PATH    Output file\n  --filter TEXT        top, latest, people, media (default: latest)\n</code></pre> <p>Examples:</p> <pre><code>xeepy scrape search \"Python tutorial\" -l 100 --filter latest\nxeepy scrape search \"from:python min_faves:100\" -o popular.json\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-scrape-hashtag","title":"<code>xeepy scrape hashtag</code>","text":"<p>Scrape tweets with a hashtag.</p> <pre><code>xeepy scrape hashtag TAG [OPTIONS]\n\nArguments:\n  TAG    Hashtag (without #)\n\nOptions:\n  -l, --limit INT      Max tweets (default: 100)\n  -o, --output PATH    Output file\n  --filter TEXT        top or latest\n</code></pre> <p>Examples:</p> <pre><code>xeepy scrape hashtag Python -l 200 -o python_hashtag.csv\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-scrape-thread","title":"<code>xeepy scrape thread</code>","text":"<p>Unroll and scrape a thread.</p> <pre><code>xeepy scrape thread URL [OPTIONS]\n\nArguments:\n  URL    Thread URL (first tweet)\n\nOptions:\n  -o, --output PATH    Output file\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-scrape-media","title":"<code>xeepy scrape media</code>","text":"<p>Scrape a user's media posts.</p> <pre><code>xeepy scrape media USERNAME [OPTIONS]\n\nArguments:\n  USERNAME    X/Twitter username\n\nOptions:\n  -l, --limit INT      Max media (default: 50)\n  -o, --output PATH    Output file\n  --download           Download media files\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-scrape-likes","title":"<code>xeepy scrape likes</code>","text":"<p>Scrape who liked a tweet.</p> <pre><code>xeepy scrape likes URL [OPTIONS]\n\nArguments:\n  URL    Tweet URL\n\nOptions:\n  -l, --limit INT      Max likers (default: 100)\n  -o, --output PATH    Output file\n</code></pre>"},{"location":"CLI_REFERENCE/#unfollow-commands","title":"Unfollow Commands","text":""},{"location":"CLI_REFERENCE/#xeepy-unfollow-non-followers","title":"<code>xeepy unfollow non-followers</code>","text":"<p>Unfollow users who don't follow you back.</p> <pre><code>xeepy unfollow non-followers [OPTIONS]\n\nOptions:\n  -m, --max INT            Max unfollows (default: 100)\n  -w, --whitelist TEXT     Users to never unfollow (can repeat)\n  --min-followers INT      Keep if they have &gt;= this many followers\n  --dry-run                Preview without unfollowing\n  -o, --output PATH        Export unfollowed list\n</code></pre> <p>Examples:</p> <pre><code># Dry run first!\nxeepy unfollow non-followers --dry-run\n\n# With whitelist\nxeepy unfollow non-followers --max 50 -w friend1 -w friend2\n\n# Keep accounts with 10k+ followers\nxeepy unfollow non-followers --min-followers 10000\n\n# Export list\nxeepy unfollow non-followers -o unfollowed.txt\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-unfollow-everyone","title":"<code>xeepy unfollow everyone</code>","text":"<p>\u26a0\ufe0f Unfollow ALL accounts.</p> <pre><code>xeepy unfollow everyone [OPTIONS]\n\nOptions:\n  -m, --max INT        Max unfollows (default: 500)\n  --dry-run            Preview without unfollowing\n  --export-first       Export following list before (recommended!)\n  --confirm            Skip confirmation prompt\n</code></pre> <p>Examples:</p> <pre><code># Always dry run first!\nxeepy unfollow everyone --dry-run\n\n# With backup\nxeepy unfollow everyone --export-first -o following_backup.json\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-unfollow-smart","title":"<code>xeepy unfollow smart</code>","text":"<p>Smart time-based unfollow.</p> <pre><code>xeepy unfollow smart [OPTIONS]\n\nOptions:\n  --days INT           Days to wait for follow-back (default: 3)\n  -m, --max INT        Max unfollows (default: 50)\n  --dry-run            Preview only\n</code></pre> <p>Examples:</p> <pre><code>xeepy unfollow smart --days 5 --max 25\n</code></pre>"},{"location":"CLI_REFERENCE/#follow-commands","title":"Follow Commands","text":""},{"location":"CLI_REFERENCE/#xeepy-follow-user","title":"<code>xeepy follow user</code>","text":"<p>Follow a specific user.</p> <pre><code>xeepy follow user USERNAME\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-follow-by-keyword","title":"<code>xeepy follow by-keyword</code>","text":"<p>Follow users tweeting about keywords.</p> <pre><code>xeepy follow by-keyword KEYWORDS... [OPTIONS]\n\nArguments:\n  KEYWORDS    Keywords to search (space-separated)\n\nOptions:\n  -m, --max INT            Max follows (default: 50)\n  --min-followers INT      Min followers filter (default: 100)\n  --max-followers INT      Max followers filter (default: 100000)\n</code></pre> <p>Examples:</p> <pre><code>xeepy follow by-keyword Python \"machine learning\" -m 25\nxeepy follow by-keyword coding --min-followers 500\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-follow-by-hashtag","title":"<code>xeepy follow by-hashtag</code>","text":"<p>Follow users using specific hashtags.</p> <pre><code>xeepy follow by-hashtag HASHTAGS... [OPTIONS]\n\nArguments:\n  HASHTAGS    Hashtags (without #)\n\nOptions:\n  -m, --max INT    Max follows (default: 50)\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-follow-followers-of","title":"<code>xeepy follow followers-of</code>","text":"<p>Follow followers of a target account.</p> <pre><code>xeepy follow followers-of USERNAME [OPTIONS]\n\nArguments:\n  USERNAME    Target account\n\nOptions:\n  -m, --max INT    Max follows (default: 50)\n  --mode TEXT      followers or following (default: followers)\n</code></pre> <p>Examples:</p> <pre><code>xeepy follow followers-of python -m 30\nxeepy follow followers-of competitor --mode following\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-follow-engagers","title":"<code>xeepy follow engagers</code>","text":"<p>Follow users who engaged with specific tweets.</p> <pre><code>xeepy follow engagers URLS... [OPTIONS]\n\nArguments:\n  URLS    Tweet URLs\n\nOptions:\n  -m, --max INT       Max follows (default: 50)\n  --type TEXT         likers, retweeters, commenters, all (default: likers)\n</code></pre>"},{"location":"CLI_REFERENCE/#engagement-commands","title":"Engagement Commands","text":""},{"location":"CLI_REFERENCE/#xeepy-engage-like","title":"<code>xeepy engage like</code>","text":"<p>Like a specific tweet.</p> <pre><code>xeepy engage like URL\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-engage-auto-like","title":"<code>xeepy engage auto-like</code>","text":"<p>Auto-like tweets by criteria.</p> <pre><code>xeepy engage auto-like [OPTIONS]\n\nOptions:\n  -k, --keyword TEXT      Keywords to match (can repeat)\n  -h, --hashtag TEXT      Hashtags to match (can repeat)\n  -m, --max INT           Max likes (default: 50)\n  -d, --duration INT      Duration in minutes (default: 30)\n</code></pre> <p>Examples:</p> <pre><code>xeepy engage auto-like -k Python -k coding -m 25\nxeepy engage auto-like -h 100DaysOfCode --duration 15\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-engage-comment","title":"<code>xeepy engage comment</code>","text":"<p>Post a comment on a tweet.</p> <pre><code>xeepy engage comment URL TEXT\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-engage-bookmark","title":"<code>xeepy engage bookmark</code>","text":"<p>Bookmark a tweet.</p> <pre><code>xeepy engage bookmark URL\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-engage-export-bookmarks","title":"<code>xeepy engage export-bookmarks</code>","text":"<p>Export all bookmarks.</p> <pre><code>xeepy engage export-bookmarks [OPTIONS]\n\nOptions:\n  -o, --output PATH    Output file (default: bookmarks.json)\n  --format TEXT        json or csv\n</code></pre>"},{"location":"CLI_REFERENCE/#monitoring-commands","title":"Monitoring Commands","text":""},{"location":"CLI_REFERENCE/#xeepy-monitor-unfollowers","title":"<code>xeepy monitor unfollowers</code>","text":"<p>Detect who unfollowed you.</p> <pre><code>xeepy monitor unfollowers [OPTIONS]\n\nOptions:\n  --notify             Send notification\n  -o, --output PATH    Export report\n</code></pre> <p>Examples:</p> <pre><code>xeepy monitor unfollowers\nxeepy monitor unfollowers --notify -o report.json\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-monitor-account","title":"<code>xeepy monitor account</code>","text":"<p>Monitor changes to an account.</p> <pre><code>xeepy monitor account USERNAME [OPTIONS]\n\nArguments:\n  USERNAME    Account to monitor\n\nOptions:\n  --since INT    Hours to look back (default: 24)\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-monitor-keywords","title":"<code>xeepy monitor keywords</code>","text":"<p>Monitor X for keywords in real-time.</p> <pre><code>xeepy monitor keywords KEYWORDS... [OPTIONS]\n\nArguments:\n  KEYWORDS    Keywords to monitor\n\nOptions:\n  --interval INT    Check interval in seconds (default: 60)\n  --notify          Send notifications on match\n</code></pre>"},{"location":"CLI_REFERENCE/#analytics-commands","title":"Analytics Commands","text":""},{"location":"CLI_REFERENCE/#xeepy-analytics-growth","title":"<code>xeepy analytics growth</code>","text":"<p>Show growth statistics.</p> <pre><code>xeepy analytics growth [OPTIONS]\n\nOptions:\n  --days INT    Days of history (default: 30)\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-analytics-engagement","title":"<code>xeepy analytics engagement</code>","text":"<p>Analyze engagement on your tweets.</p> <pre><code>xeepy analytics engagement [OPTIONS]\n\nOptions:\n  --limit INT    Tweets to analyze (default: 100)\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-analytics-best-time","title":"<code>xeepy analytics best-time</code>","text":"<p>Find optimal posting times.</p> <pre><code>xeepy analytics best-time [OPTIONS]\n\nOptions:\n  --limit INT    Tweets to analyze (default: 200)\n</code></pre>"},{"location":"CLI_REFERENCE/#ai-commands","title":"AI Commands","text":""},{"location":"CLI_REFERENCE/#xeepy-ai-reply","title":"<code>xeepy ai reply</code>","text":"<p>Generate an AI reply.</p> <pre><code>xeepy ai reply TEXT [OPTIONS]\n\nArguments:\n  TEXT    Tweet text to reply to\n\nOptions:\n  --style TEXT      Reply style (default: helpful)\n  --provider TEXT   openai, anthropic, ollama\n</code></pre> <p>Examples:</p> <pre><code>xeepy ai reply \"Just launched my startup!\" --style supportive\nxeepy ai reply \"Python vs JavaScript?\" --style witty\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-ai-sentiment","title":"<code>xeepy ai sentiment</code>","text":"<p>Analyze sentiment of text.</p> <pre><code>xeepy ai sentiment TEXT\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-ai-detect-bot","title":"<code>xeepy ai detect-bot</code>","text":"<p>Analyze if an account is a bot.</p> <pre><code>xeepy ai detect-bot USERNAME\n</code></pre>"},{"location":"CLI_REFERENCE/#configuration-commands","title":"Configuration Commands","text":""},{"location":"CLI_REFERENCE/#xeepy-config-show","title":"<code>xeepy config show</code>","text":"<p>Show current configuration.</p> <pre><code>xeepy config show\n</code></pre>"},{"location":"CLI_REFERENCE/#xeepy-config-set","title":"<code>xeepy config set</code>","text":"<p>Set a configuration value.</p> <pre><code>xeepy config set KEY VALUE\n</code></pre> <p>Examples:</p> <pre><code>xeepy config set headless true\nxeepy config set rate_limit.delay 5\n</code></pre>"},{"location":"CLI_REFERENCE/#output-formats","title":"Output Formats","text":"<p>All scraping commands support these output formats:</p> Format Extension Description JSON <code>.json</code> Structured data CSV <code>.csv</code> Spreadsheet compatible <p>Auto-detection: Output format is detected from file extension.</p> <pre><code>xeepy scrape followers python -o followers.json  # JSON\nxeepy scrape followers python -o followers.csv   # CSV\n</code></pre>"},{"location":"CLI_REFERENCE/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success 1 General error 2 Authentication error 3 Rate limit error 4 Not found error"},{"location":"CLI_REFERENCE/#environment-variables","title":"Environment Variables","text":"Variable Description <code>XEEPY_AUTH_TOKEN</code> X/Twitter auth token <code>XEEPY_CONFIG</code> Config file path <code>OPENAI_API_KEY</code> OpenAI API key for AI features <code>ANTHROPIC_API_KEY</code> Anthropic API key"},{"location":"CLI_REFERENCE/#cron-examples","title":"Cron Examples","text":"<pre><code># Daily unfollower check at 9 AM\n0 9 * * * xeepy monitor unfollowers --notify\n\n# Weekly cleanup on Sundays\n0 20 * * 0 xeepy unfollow non-followers --max 50\n\n# Hourly engagement\n0 * * * * xeepy engage auto-like -k Python --max 10 --duration 5\n</code></pre> <p> Need help? Run `xeepy --help` </p>"},{"location":"COMPARISON/","title":"\ud83d\ude80 Xeepy vs Alternatives Comparison","text":"<p>Why Xeepy is the best Python toolkit for X/Twitter automation.</p>"},{"location":"COMPARISON/#feature-comparison-matrix","title":"Feature Comparison Matrix","text":"Feature Xeepy Tweepy Snscrape Twint Nitter No API Required \u2705 \u274c \u2705 \u2705 \u2705 Currently Working \u2705 \u26a0\ufe0f Limited \u274c Broken \u274c Dead \u26a0\ufe0f Unstable Get Tweet Replies \u2705 \u274c \u274c \u274c \u274c Async Support \u2705 \u2705 \u274c \u274c \u274c Follow/Unfollow \u2705 \u2705* \u274c \u274c \u274c Mass Unfollow \u2705 \u274c \u274c \u274c \u274c Auto-Like \u2705 \u26a0\ufe0f \u274c \u274c \u274c AI Integration \u2705 \u274c \u274c \u274c \u274c CLI Tool \u2705 \u274c \u26a0\ufe0f \u2705 \u274c Active Development \u2705 \u26a0\ufe0f \u274c \u274c \u26a0\ufe0f Python 3.10+ \u2705 \u2705 \u2705 \u274c N/A Rate Limiting \u2705 Manual N/A \u274c N/A Session Management \u2705 API Keys N/A Cookies N/A <p>*Tweepy requires expensive API access ($100-5000/month)</p>"},{"location":"COMPARISON/#why-other-tools-dont-work","title":"Why Other Tools Don't Work","text":""},{"location":"COMPARISON/#tweepy","title":"Tweepy","text":"<p>Problem: Twitter API v2 requires paid access - Basic tier: $100/month (limited) - Pro tier: $5000/month (full access) - Most endpoints deprecated or removed</p> <pre><code># \u274c This no longer works without paid API\nimport tweepy\napi.search(q=\"to:username\")  # DEPRECATED\n</code></pre>"},{"location":"COMPARISON/#snscrape","title":"Snscrape","text":"<p>Problem: Completely broken since X changes - Last working: 2023 - No longer maintained - All scrapers fail</p> <pre><code># \u274c This is broken\nimport snscrape.modules.twitter as sntwitter\n# ERROR: snscrape.base.ScraperException\n</code></pre>"},{"location":"COMPARISON/#twint","title":"Twint","text":"<p>Problem: Project abandoned - Last commit: 2022 - Doesn't work with current X - No Python 3.10+ support</p> <pre><code># \u274c This project is dead\nimport twint\n# ModuleNotFoundError or immediate errors\n</code></pre>"},{"location":"COMPARISON/#nitter-instances","title":"Nitter Instances","text":"<p>Problem: Unstable and limited - Instances frequently go down - No action support (can't like, follow) - Rate limited heavily</p>"},{"location":"COMPARISON/#xeepy-advantages","title":"Xeepy Advantages","text":""},{"location":"COMPARISON/#1-browser-automation-no-api-needed","title":"1. Browser Automation = No API Needed","text":"<pre><code>from xeepy import Xeepy\n\n# \u2705 Works without API keys\nasync with Xeepy() as x:\n    replies = await x.scrape.replies(tweet_url)\n</code></pre>"},{"location":"COMPARISON/#2-full-feature-set","title":"2. Full Feature Set","text":"<pre><code># \u2705 Everything works\nawait x.scrape.replies(url)\nawait x.scrape.followers(\"user\")\nawait x.unfollow.non_followers()\nawait x.engage.auto_like(keywords=[\"python\"])\nawait x.monitor.unfollowers()\n</code></pre>"},{"location":"COMPARISON/#3-ai-powered","title":"3. AI-Powered","text":"<pre><code># \u2705 Built-in AI support\nfrom xeepy.ai import ContentGenerator\n\nai = ContentGenerator(provider=\"openai\")\nreply = await ai.generate_reply(tweet_text)\n</code></pre>"},{"location":"COMPARISON/#4-modern-python","title":"4. Modern Python","text":"<pre><code># \u2705 Async/await native\n# \u2705 Type hints throughout\n# \u2705 Pydantic models\n# \u2705 Python 3.10+\n</code></pre>"},{"location":"COMPARISON/#5-production-ready","title":"5. Production Ready","text":"<ul> <li>Rate limiting built-in</li> <li>Session management</li> <li>Error handling</li> <li>Export to CSV/JSON/Excel</li> <li>Notification webhooks</li> </ul>"},{"location":"COMPARISON/#cost-comparison","title":"Cost Comparison","text":"Solution Monthly Cost Features Xeepy $0 All features Twitter API Basic $100 Limited endpoints Twitter API Pro $5000 Full access Enterprise Custom ($) Full access"},{"location":"COMPARISON/#migration-guide","title":"Migration Guide","text":""},{"location":"COMPARISON/#from-tweepy","title":"From Tweepy","text":"<pre><code># OLD (Tweepy)\nimport tweepy\n\nauth = tweepy.OAuthHandler(key, secret)\napi = tweepy.API(auth)\ntweets = api.search_tweets(q=\"python\")\n\n# NEW (Xeepy)\nfrom xeepy import Xeepy\n\nasync with Xeepy() as x:\n    tweets = await x.scrape.search(\"python\")\n</code></pre>"},{"location":"COMPARISON/#from-snscrape","title":"From Snscrape","text":"<pre><code># OLD (snscrape - broken)\nimport snscrape.modules.twitter as sntwitter\ntweets = sntwitter.TwitterSearchScraper(\"python\").get_items()\n\n# NEW (Xeepy)\nfrom xeepy import Xeepy\n\nasync with Xeepy() as x:\n    tweets = await x.scrape.search(\"python\")\n</code></pre>"},{"location":"COMPARISON/#conclusion","title":"Conclusion","text":"<p>Xeepy is the only Python toolkit that: - \u2705 Actually works in 2024 - \u2705 Requires no API keys - \u2705 Has full feature support - \u2705 Includes AI integration - \u2705 Is actively maintained</p> <p>Stop fighting with broken tools. Use Xeepy.</p> <pre><code>pip install xeepy\n</code></pre>"},{"location":"EXAMPLES/","title":"\ud83d\udcda Code Examples","text":"<p>Complete, copy-paste-ready examples for every Xeepy feature.</p>"},{"location":"EXAMPLES/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Scraping Examples</li> <li>Unfollow Examples</li> <li>Follow Examples</li> <li>Engagement Examples</li> <li>Monitoring Examples</li> <li>AI Examples</li> <li>Advanced Workflows</li> </ol>"},{"location":"EXAMPLES/#scraping-examples","title":"\ud83d\udcca Scraping Examples","text":""},{"location":"EXAMPLES/#get-tweet-replies-original-repo-feature","title":"Get Tweet Replies (Original Repo Feature!)","text":"<pre><code>\"\"\"\nGet all replies to a specific tweet.\nThis is what the original repo was meant to do!\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def get_replies():\n    async with Xeepy() as x:\n        replies = await x.scrape.replies(\n            tweet_url=\"https://x.com/elonmusk/status/1234567890\",\n            limit=200\n        )\n\n        print(f\"Found {len(replies)} replies\\n\")\n\n        for reply in replies[:10]:\n            print(f\"@{reply.username} ({reply.likes} likes)\")\n            print(f\"  {reply.text[:100]}...\")\n            print()\n\n        # Export to CSV (like original repo)\n        x.export.to_csv(replies, \"replies_clean.csv\")\n        print(\"Exported to replies_clean.csv\")\n\nasyncio.run(get_replies())\n</code></pre>"},{"location":"EXAMPLES/#scrape-user-profile","title":"Scrape User Profile","text":"<pre><code>\"\"\"\nGet detailed profile information for any user.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def get_profile():\n    async with Xeepy() as x:\n        profile = await x.scrape.profile(\"elonmusk\")\n\n        print(f\"Name: {profile.display_name}\")\n        print(f\"Username: @{profile.username}\")\n        print(f\"Bio: {profile.bio}\")\n        print(f\"Location: {profile.location}\")\n        print(f\"Followers: {profile.followers_count:,}\")\n        print(f\"Following: {profile.following_count:,}\")\n        print(f\"Tweets: {profile.tweets_count:,}\")\n        print(f\"Joined: {profile.joined_date}\")\n        print(f\"Verified: {profile.is_verified}\")\n\n        # Export\n        x.export.to_json(profile, \"profile.json\")\n\nasyncio.run(get_profile())\n</code></pre>"},{"location":"EXAMPLES/#scrape-followers","title":"Scrape Followers","text":"<pre><code>\"\"\"\nGet a user's complete followers list.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def get_followers():\n    async with Xeepy() as x:\n        followers = await x.scrape.followers(\n            username=\"python\",\n            limit=500,\n            on_progress=lambda p: print(f\"Progress: {p}%\")\n        )\n\n        print(f\"Found {len(followers)} followers\")\n\n        # Filter by criteria\n        quality_followers = [\n            f for f in followers \n            if f.followers_count &gt; 100 and f.bio\n        ]\n        print(f\"Quality followers: {len(quality_followers)}\")\n\n        # Export\n        x.export.to_csv(followers, \"followers.csv\")\n        x.export.to_json(followers, \"followers.json\")\n\nasyncio.run(get_followers())\n</code></pre>"},{"location":"EXAMPLES/#scrape-user-tweets","title":"Scrape User Tweets","text":"<pre><code>\"\"\"\nGet tweets from a user's timeline.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def get_tweets():\n    async with Xeepy() as x:\n        tweets = await x.scrape.tweets(\n            username=\"python\",\n            limit=100,\n            include_replies=False,\n            include_retweets=False\n        )\n\n        print(f\"Found {len(tweets)} tweets\\n\")\n\n        # Sort by engagement\n        tweets.sort(key=lambda t: t.likes + t.retweets, reverse=True)\n\n        print(\"Top 5 tweets by engagement:\")\n        for i, tweet in enumerate(tweets[:5], 1):\n            print(f\"\\n{i}. {tweet.text[:80]}...\")\n            print(f\"   \u2764\ufe0f {tweet.likes} | \ud83d\udd04 {tweet.retweets} | \ud83d\udcac {tweet.replies}\")\n\nasyncio.run(get_tweets())\n</code></pre>"},{"location":"EXAMPLES/#search-tweets","title":"Search Tweets","text":"<pre><code>\"\"\"\nSearch for tweets matching a query.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def search_tweets():\n    async with Xeepy() as x:\n        # Basic search\n        results = await x.scrape.search(\n            query=\"Python tutorial\",\n            limit=50,\n            filter=\"latest\"  # \"top\", \"latest\", \"people\", \"media\"\n        )\n\n        print(f\"Found {len(results)} tweets\")\n\n        # Advanced search\n        results = await x.scrape.search(\n            query=\"from:python lang:en min_faves:100\",\n            limit=50\n        )\n\n        for tweet in results[:5]:\n            print(f\"@{tweet.author.username}: {tweet.text[:60]}...\")\n\nasyncio.run(search_tweets())\n</code></pre>"},{"location":"EXAMPLES/#scrape-hashtag","title":"Scrape Hashtag","text":"<pre><code>\"\"\"\nGet tweets containing a specific hashtag.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def scrape_hashtag():\n    async with Xeepy() as x:\n        tweets = await x.scrape.hashtag(\n            hashtag=\"Python\",\n            limit=100,\n            filter=\"latest\"\n        )\n\n        # Analyze hashtag usage\n        users = {}\n        for tweet in tweets:\n            users[tweet.author.username] = users.get(tweet.author.username, 0) + 1\n\n        print(\"Top users in #Python:\")\n        for user, count in sorted(users.items(), key=lambda x: -x[1])[:10]:\n            print(f\"  @{user}: {count} tweets\")\n\nasyncio.run(scrape_hashtag())\n</code></pre>"},{"location":"EXAMPLES/#scrape-thread","title":"Scrape Thread","text":"<pre><code>\"\"\"\nUnroll and extract a complete thread.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def scrape_thread():\n    async with Xeepy() as x:\n        thread = await x.scrape.thread(\n            \"https://x.com/user/status/1234567890\"\n        )\n\n        print(f\"Thread by @{thread.author.username}\")\n        print(f\"Total tweets: {len(thread.tweets)}\\n\")\n\n        for i, tweet in enumerate(thread.tweets, 1):\n            print(f\"{i}/{len(thread.tweets)}: {tweet.text}\")\n            print()\n\nasyncio.run(scrape_thread())\n</code></pre>"},{"location":"EXAMPLES/#unfollow-examples","title":"\ud83d\udd04 Unfollow Examples","text":""},{"location":"EXAMPLES/#unfollow-non-followers","title":"Unfollow Non-Followers","text":"<pre><code>\"\"\"\nUnfollow everyone who doesn't follow you back.\nThe most requested feature!\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def unfollow_non_followers():\n    async with Xeepy() as x:\n        # Step 1: Preview (dry run)\n        preview = await x.unfollow.non_followers(\n            max_unfollows=100,\n            whitelist=[\n                \"friend1\",\n                \"important_brand\",\n                \"mom\"\n            ],\n            min_followers=10000,  # Keep if they have 10k+ followers\n            dry_run=True\n        )\n\n        print(f\"Would unfollow {len(preview.unfollowed_users)} users:\")\n        for user in preview.unfollowed_users[:10]:\n            print(f\"  - @{user}\")\n\n        # Step 2: Confirm and execute\n        if input(\"\\nProceed? (y/n): \").lower() == \"y\":\n            result = await x.unfollow.non_followers(\n                max_unfollows=100,\n                whitelist=[\"friend1\", \"important_brand\", \"mom\"],\n                dry_run=False\n            )\n\n            print(f\"\\n\u2705 Unfollowed {result.success_count} users\")\n            print(f\"\u274c Failed: {result.failed_count}\")\n\n            # Save list of unfollowed users\n            with open(\"unfollowed.txt\", \"w\") as f:\n                for user in result.unfollowed_users:\n                    f.write(f\"{user}\\n\")\n\nasyncio.run(unfollow_non_followers())\n</code></pre>"},{"location":"EXAMPLES/#unfollow-everyone-nuclear-option","title":"Unfollow Everyone (Nuclear Option)","text":"<pre><code>\"\"\"\n\u26a0\ufe0f WARNING: This unfollows EVERYONE!\nUse with extreme caution.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def unfollow_everyone():\n    async with Xeepy() as x:\n        # ALWAYS do dry run first\n        preview = await x.unfollow.everyone(\n            max_unfollows=500,\n            export_before=True,  # Save following list first\n            dry_run=True\n        )\n\n        print(f\"\u26a0\ufe0f Would unfollow {len(preview.unfollowed_users)} users\")\n        print(\"Following list saved to: following_backup.json\")\n\n        confirm = input(\"\\n\u26a0\ufe0f TYPE 'unfollow everyone' TO CONFIRM: \")\n        if confirm == \"unfollow everyone\":\n            result = await x.unfollow.everyone(dry_run=False)\n            print(f\"Unfollowed {result.success_count} users\")\n\nasyncio.run(unfollow_everyone())\n</code></pre>"},{"location":"EXAMPLES/#smart-unfollow-time-based","title":"Smart Unfollow (Time-Based)","text":"<pre><code>\"\"\"\nUnfollow users who didn't follow back within X days.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def smart_unfollow():\n    async with Xeepy() as x:\n        result = await x.unfollow.smart(\n            days_threshold=3,  # Unfollow if no follow-back in 3 days\n            max_unfollows=25,\n            check_engagement=True,  # Keep if they engaged with you\n            whitelist=[\"important_people\"]\n        )\n\n        print(f\"Unfollowed {result.success_count} users who didn't follow back\")\n\n        # Show stats\n        stats = await x.storage.get_follow_stats()\n        print(f\"\\nFollow-back rate: {stats.follow_back_rate:.1%}\")\n\nasyncio.run(smart_unfollow())\n</code></pre>"},{"location":"EXAMPLES/#follow-examples","title":"\u2795 Follow Examples","text":""},{"location":"EXAMPLES/#follow-by-keywords","title":"Follow by Keywords","text":"<pre><code>\"\"\"\nFollow users who tweet about specific topics.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\nfrom xeepy.actions.follow import FollowFilters\n\nasync def follow_by_keyword():\n    async with Xeepy() as x:\n        result = await x.follow.by_keyword(\n            keywords=[\"Python\", \"machine learning\", \"data science\"],\n            max_follows=25,\n            filters=FollowFilters(\n                min_followers=100,\n                max_followers=50000,\n                min_tweets=50,\n                must_have_bio=True,\n                must_have_profile_pic=True\n            )\n        )\n\n        print(f\"\u2705 Followed {result.success_count} users\")\n        print(f\"\u23ed\ufe0f Skipped {result.skipped_count} (didn't match filters)\")\n\n        for user in result.followed_users:\n            print(f\"  + @{user}\")\n\nasyncio.run(follow_by_keyword())\n</code></pre>"},{"location":"EXAMPLES/#follow-targets-followers","title":"Follow Target's Followers","text":"<pre><code>\"\"\"\nFollow the followers of a competitor/influencer.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def follow_competitors_followers():\n    async with Xeepy() as x:\n        result = await x.follow.followers_of(\n            target_username=\"python\",  # Follow @python's followers\n            max_follows=30,\n            mode=\"followers\",  # or \"following\"\n            skip_mutual=True,  # Skip if already following\n            filters=FollowFilters(\n                min_followers=50,\n                must_have_bio=True\n            )\n        )\n\n        print(f\"Followed {result.success_count} of @python's followers\")\n\nasyncio.run(follow_competitors_followers())\n</code></pre>"},{"location":"EXAMPLES/#follow-engagers","title":"Follow Engagers","text":"<pre><code>\"\"\"\nFollow users who engaged with specific tweets.\nThese are highly relevant users!\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def follow_engagers():\n    async with Xeepy() as x:\n        result = await x.follow.engagers(\n            tweet_urls=[\n                \"https://x.com/user/status/123\",\n                \"https://x.com/user/status/456\"\n            ],\n            engagement_type=\"likers\",  # \"likers\", \"retweeters\", \"commenters\", \"all\"\n            max_follows=25\n        )\n\n        print(f\"Followed {result.success_count} users who engaged\")\n\nasyncio.run(follow_engagers())\n</code></pre>"},{"location":"EXAMPLES/#engagement-examples","title":"\ud83d\udc9c Engagement Examples","text":""},{"location":"EXAMPLES/#auto-like-by-keywords","title":"Auto-Like by Keywords","text":"<pre><code>\"\"\"\nAutomatically like tweets containing specific keywords.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\nfrom xeepy.actions.engagement import AutoLikeConfig\n\nasync def auto_like():\n    async with Xeepy() as x:\n        result = await x.engage.auto_like(\n            config=AutoLikeConfig(\n                keywords=[\"Python\", \"coding\", \"developer\"],\n                hashtags=[\"100DaysOfCode\"],\n                min_likes=5,\n                max_likes=1000,\n                exclude_retweets=True,\n                max_likes_per_session=50\n            ),\n            duration_minutes=15\n        )\n\n        print(f\"Liked {result.success_count} tweets!\")\n\nasyncio.run(auto_like())\n</code></pre>"},{"location":"EXAMPLES/#like-by-user","title":"Like by User","text":"<pre><code>\"\"\"\nLike all recent tweets from specific users.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def like_user_tweets():\n    async with Xeepy() as x:\n        for username in [\"friend1\", \"mentor\", \"favorite_account\"]:\n            result = await x.engage.like_user(\n                username=username,\n                max_likes=5\n            )\n            print(f\"Liked {result.success_count} tweets from @{username}\")\n\nasyncio.run(like_user_tweets())\n</code></pre>"},{"location":"EXAMPLES/#auto-comment-with-templates","title":"Auto-Comment with Templates","text":"<pre><code>\"\"\"\nAutomatically comment using templates.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\nfrom xeepy.actions.engagement import AutoCommentConfig\nfrom xeepy.templates import CommentTemplates\n\nasync def auto_comment():\n    async with Xeepy() as x:\n        result = await x.engage.auto_comment(\n            config=AutoCommentConfig(\n                keywords=[\"shipped\", \"launched\", \"released\"],\n                templates=CommentTemplates.APPRECIATION,\n                max_comments_per_session=5,\n                review_before_post=True  # Show preview\n            ),\n            duration_minutes=30\n        )\n\nasyncio.run(auto_comment())\n</code></pre>"},{"location":"EXAMPLES/#bookmark-management","title":"Bookmark Management","text":"<pre><code>\"\"\"\nManage your bookmarks - add, remove, export.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def manage_bookmarks():\n    async with Xeepy() as x:\n        # Add bookmark\n        await x.engage.bookmark(\"https://x.com/user/status/123\")\n\n        # Export all bookmarks\n        count = await x.engage.export_bookmarks(\n            filepath=\"my_bookmarks.json\",\n            include_content=True\n        )\n        print(f\"Exported {count} bookmarks\")\n\n        # Remove bookmark\n        await x.engage.remove_bookmark(\"https://x.com/user/status/123\")\n\nasyncio.run(manage_bookmarks())\n</code></pre>"},{"location":"EXAMPLES/#monitoring-examples","title":"\ud83d\udcc8 Monitoring Examples","text":""},{"location":"EXAMPLES/#detect-unfollowers","title":"Detect Unfollowers","text":"<pre><code>\"\"\"\nFind out who unfollowed you.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def detect_unfollowers():\n    async with Xeepy() as x:\n        report = await x.monitor.unfollowers()\n\n        print(f\"\ud83d\udcca Follower Report\")\n        print(f\"   Before: {report.total_followers_before:,}\")\n        print(f\"   After: {report.total_followers_after:,}\")\n        print(f\"   Change: {report.net_change:+,}\")\n\n        if report.unfollowers:\n            print(f\"\\n\ud83d\ude22 Unfollowers ({len(report.unfollowers)}):\")\n            for user in report.unfollowers:\n                print(f\"   - @{user}\")\n\n        if report.new_followers:\n            print(f\"\\n\ud83c\udf89 New Followers ({len(report.new_followers)}):\")\n            for user in report.new_followers:\n                print(f\"   + @{user}\")\n\nasyncio.run(detect_unfollowers())\n</code></pre>"},{"location":"EXAMPLES/#monitor-account-changes","title":"Monitor Account Changes","text":"<pre><code>\"\"\"\nMonitor any account for changes.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def monitor_account():\n    async with Xeepy() as x:\n        # One-time check\n        changes = await x.monitor.account_changes(\n            username=\"competitor\",\n            since_hours=24\n        )\n\n        print(f\"Changes for @competitor in last 24h:\")\n        print(f\"  Followers: {changes.followers_change:+,}\")\n        print(f\"  Following: {changes.following_change:+,}\")\n        print(f\"  New tweets: {len(changes.new_tweets)}\")\n\n        if changes.bio_changed:\n            print(f\"  Bio changed!\")\n            print(f\"    Old: {changes.old_bio}\")\n            print(f\"    New: {changes.new_bio}\")\n\nasyncio.run(monitor_account())\n</code></pre>"},{"location":"EXAMPLES/#real-time-keyword-monitoring","title":"Real-Time Keyword Monitoring","text":"<pre><code>\"\"\"\nMonitor X for specific keywords in real-time.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def on_match(tweet):\n    print(f\"\ud83d\udd14 Match found!\")\n    print(f\"   @{tweet.author.username}: {tweet.text[:80]}...\")\n\nasync def monitor_keywords():\n    async with Xeepy() as x:\n        await x.monitor.keywords(\n            keywords=[\"your_brand\", \"your_product\"],\n            hashtags=[\"YourHashtag\"],\n            interval_seconds=60,\n            on_match=on_match\n        )\n\nasyncio.run(monitor_keywords())\n</code></pre>"},{"location":"EXAMPLES/#growth-tracking","title":"Growth Tracking","text":"<pre><code>\"\"\"\nTrack your follower growth over time.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def track_growth():\n    async with Xeepy() as x:\n        # Record today's snapshot\n        await x.analytics.record_snapshot()\n\n        # Get growth history\n        history = await x.analytics.growth_history(days=30)\n\n        print(\"\ud83d\udcc8 30-Day Growth\")\n        for day in history[-7:]:  # Last 7 days\n            print(f\"  {day.date}: {day.followers:,} ({day.change:+,})\")\n\n        # Calculate growth rate\n        rate = await x.analytics.growth_rate(days=7)\n        print(f\"\\n\ud83d\udcca 7-day growth rate: {rate:.1%}\")\n\nasyncio.run(track_growth())\n</code></pre>"},{"location":"EXAMPLES/#ai-examples","title":"\ud83e\udd16 AI Examples","text":""},{"location":"EXAMPLES/#ai-reply-generator","title":"AI Reply Generator","text":"<pre><code>\"\"\"\nGenerate contextual replies using AI.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\nfrom xeepy.ai import ContentGenerator\n\nasync def ai_replies():\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"sk-...\")\n\n        # Get recent tweets to reply to\n        tweets = await x.scrape.search(\"Python tips\", limit=5)\n\n        for tweet in tweets:\n            reply = await ai.generate_reply(\n                tweet_text=tweet.text,\n                style=\"helpful\"\n            )\n\n            print(f\"Tweet: {tweet.text[:50]}...\")\n            print(f\"Reply: {reply}\")\n            print(\"---\")\n\nasyncio.run(ai_replies())\n</code></pre>"},{"location":"EXAMPLES/#sentiment-analysis-dashboard","title":"Sentiment Analysis Dashboard","text":"<pre><code>\"\"\"\nAnalyze sentiment of your mentions.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\nfrom xeepy.ai import SentimentAnalyzer\n\nasync def sentiment_dashboard():\n    async with Xeepy() as x:\n        analyzer = SentimentAnalyzer()\n\n        # Get your mentions\n        mentions = await x.scrape.mentions(\"your_username\", limit=100)\n\n        # Analyze each\n        positive, negative, neutral = 0, 0, 0\n\n        for mention in mentions:\n            result = await analyzer.analyze_tweet(mention.text)\n\n            if result.label == \"positive\":\n                positive += 1\n            elif result.label == \"negative\":\n                negative += 1\n                print(f\"\u26a0\ufe0f Negative: @{mention.author}: {mention.text[:50]}...\")\n            else:\n                neutral += 1\n\n        print(f\"\\n\ud83d\udcca Sentiment Summary\")\n        print(f\"   Positive: {positive} ({positive/len(mentions):.1%})\")\n        print(f\"   Neutral: {neutral} ({neutral/len(mentions):.1%})\")\n        print(f\"   Negative: {negative} ({negative/len(mentions):.1%})\")\n\nasyncio.run(sentiment_dashboard())\n</code></pre>"},{"location":"EXAMPLES/#advanced-workflows","title":"\ud83d\udd27 Advanced Workflows","text":""},{"location":"EXAMPLES/#complete-growth-suite","title":"Complete Growth Suite","text":"<pre><code>\"\"\"\nAll-in-one growth automation.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\nfrom xeepy.ai import SmartTargeting\n\nasync def growth_suite():\n    async with Xeepy() as x:\n        targeting = SmartTargeting(provider=\"openai\", api_key=\"sk-...\")\n\n        print(\"\ud83d\ude80 Starting Growth Suite\")\n\n        # Phase 1: Clean up (unfollow non-followers)\n        print(\"\\n\ud83d\udccd Phase 1: Cleaning up...\")\n        cleanup = await x.unfollow.non_followers(\n            max_unfollows=20,\n            whitelist=[\"important_accounts\"],\n            dry_run=False\n        )\n        print(f\"   Unfollowed {cleanup.success_count} non-followers\")\n\n        await asyncio.sleep(60)  # Pause between phases\n\n        # Phase 2: Smart follow\n        print(\"\\n\ud83d\udccd Phase 2: Finding targets...\")\n        targets = await targeting.find_targets(\n            niche=\"Python developers\",\n            goal=\"growth\",\n            limit=15\n        )\n\n        for target in targets:\n            await x.follow.user(target.username)\n            print(f\"   Followed @{target.username}\")\n            await asyncio.sleep(5)\n\n        await asyncio.sleep(60)\n\n        # Phase 3: Engage\n        print(\"\\n\ud83d\udccd Phase 3: Engaging...\")\n        likes = await x.engage.auto_like(\n            keywords=[\"Python\", \"coding\"],\n            max_likes=20,\n            duration_minutes=10\n        )\n        print(f\"   Liked {likes.success_count} tweets\")\n\n        print(\"\\n\u2705 Growth suite complete!\")\n\nasyncio.run(growth_suite())\n</code></pre>"},{"location":"EXAMPLES/#scheduled-automation","title":"Scheduled Automation","text":"<pre><code>\"\"\"\nRun automations on a schedule.\n\"\"\"\nimport asyncio\nfrom datetime import datetime\nfrom xeepy import Xeepy\n\nasync def daily_routine():\n    async with Xeepy() as x:\n        now = datetime.now()\n\n        # Morning: Check unfollowers\n        if 8 &lt;= now.hour &lt; 9:\n            report = await x.monitor.unfollowers()\n            print(f\"Morning report: {report.net_change:+} followers\")\n\n        # Afternoon: Engage\n        elif 14 &lt;= now.hour &lt; 15:\n            await x.engage.auto_like(\n                keywords=[\"Python\"],\n                max_likes=25\n            )\n\n        # Evening: Clean up\n        elif 20 &lt;= now.hour &lt; 21:\n            await x.unfollow.non_followers(\n                max_unfollows=15,\n                dry_run=False\n            )\n\n# Run with cron or scheduler\nasyncio.run(daily_routine())\n</code></pre>"},{"location":"EXAMPLES/#export-everything","title":"Export Everything","text":"<pre><code>\"\"\"\nFull data export for backup/analysis.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def full_export():\n    async with Xeepy() as x:\n        username = \"your_username\"\n\n        print(\"\ud83d\udce6 Exporting all data...\")\n\n        # Profile\n        profile = await x.scrape.profile(username)\n        x.export.to_json(profile, f\"export/{username}_profile.json\")\n\n        # Followers\n        followers = await x.scrape.followers(username, limit=5000)\n        x.export.to_csv(followers, f\"export/{username}_followers.csv\")\n\n        # Following\n        following = await x.scrape.following(username, limit=5000)\n        x.export.to_csv(following, f\"export/{username}_following.csv\")\n\n        # Tweets\n        tweets = await x.scrape.tweets(username, limit=1000)\n        x.export.to_json(tweets, f\"export/{username}_tweets.json\")\n\n        # Bookmarks\n        await x.engage.export_bookmarks(f\"export/{username}_bookmarks.json\")\n\n        print(\"\u2705 Export complete!\")\n\nasyncio.run(full_export())\n</code></pre>"},{"location":"EXAMPLES/#error-handling","title":"\ud83c\udd98 Error Handling","text":"<pre><code>\"\"\"\nProper error handling pattern.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\nfrom xeepy.core.exceptions import (\n    AuthenticationError,\n    RateLimitError,\n    ElementNotFoundError\n)\n\nasync def safe_automation():\n    try:\n        async with Xeepy() as x:\n            result = await x.unfollow.non_followers()\n\n    except AuthenticationError:\n        print(\"\u274c Session expired. Please re-authenticate.\")\n\n    except RateLimitError as e:\n        print(f\"\u23f3 Rate limited. Wait {e.retry_after} seconds.\")\n\n    except ElementNotFoundError:\n        print(\"\u26a0\ufe0f X/Twitter UI may have changed. Check for updates.\")\n\n    except Exception as e:\n        print(f\"\u274c Unexpected error: {e}\")\n\nasyncio.run(safe_automation())\n</code></pre> <p> Happy automating! \ud83d\ude80 </p>"},{"location":"FAQ/","title":"\u2753 Frequently Asked Questions","text":"<p>Common questions about Xeepy answered.</p>"},{"location":"FAQ/#general-questions","title":"General Questions","text":""},{"location":"FAQ/#what-is-xeepy","title":"What is Xeepy?","text":"<p>Xeepy is a Python toolkit for X/Twitter automation. It provides: - 12 different scrapers (replies, followers, tweets, etc.) - Follow/unfollow automation - Engagement tools (auto-like, auto-comment) - Monitoring (unfollower detection, account tracking) - AI integration (GPT, Claude, Ollama)</p>"},{"location":"FAQ/#is-xeepy-free","title":"Is Xeepy free?","text":"<p>Yes! Xeepy is completely free and open-source (MIT License).</p>"},{"location":"FAQ/#do-i-need-a-twitter-api-key","title":"Do I need a Twitter API key?","text":"<p>No. Xeepy uses browser automation instead of the Twitter API. This means: - No API costs ($100-5000/month saved) - No rate limit restrictions - No application approval needed - Works with any X/Twitter account</p>"},{"location":"FAQ/#is-xeepy-safe-to-use","title":"Is Xeepy safe to use?","text":"<p>Xeepy includes: - Built-in rate limiting - Random delays between actions - Stealth browser settings</p> <p>However, any automation may risk account restrictions. Use responsibly and start with small numbers.</p>"},{"location":"FAQ/#installation-questions","title":"Installation Questions","text":""},{"location":"FAQ/#how-do-i-install-xeepy","title":"How do I install Xeepy?","text":"<pre><code>pip install xeepy\n</code></pre> <p>Or from source: <pre><code>git clone https://github.com/nirholas/Get-Tweet-Replies-With-Python-Tweepy.git\npip install -e .\n</code></pre></p>"},{"location":"FAQ/#what-python-version-do-i-need","title":"What Python version do I need?","text":"<p>Python 3.10 or higher is required.</p>"},{"location":"FAQ/#what-are-the-dependencies","title":"What are the dependencies?","text":"<p>Core dependencies: - <code>playwright</code> - Browser automation - <code>loguru</code> - Logging - <code>pydantic</code> - Data models - <code>click</code> - CLI - <code>rich</code> - Terminal output</p> <p>AI dependencies (optional): - <code>openai</code> - GPT integration - <code>anthropic</code> - Claude integration</p> <p>Install all with: <pre><code>pip install xeepy[ai]\n</code></pre></p>"},{"location":"FAQ/#browser-installation-failed","title":"Browser installation failed?","text":"<p>Run: <pre><code>playwright install chromium\n</code></pre></p>"},{"location":"FAQ/#authentication-questions","title":"Authentication Questions","text":""},{"location":"FAQ/#how-do-i-authenticate","title":"How do I authenticate?","text":"<ol> <li>Open x.com in your browser</li> <li>Open Developer Tools (F12)</li> <li>Go to Application \u2192 Cookies \u2192 x.com</li> <li>Copy the <code>auth_token</code> value</li> <li>Set it in Xeepy:</li> </ol> <pre><code>async with Xeepy(auth_token=\"your_token\") as x:\n    ...\n</code></pre> <p>Or via environment variable: <pre><code>export XEEPY_AUTH_TOKEN=your_token\n</code></pre></p>"},{"location":"FAQ/#my-authentication-expired-what-do-i-do","title":"My authentication expired. What do I do?","text":"<p>Re-fetch your <code>auth_token</code> from the browser. Tokens typically expire after a few weeks.</p>"},{"location":"FAQ/#can-i-use-multiple-accounts","title":"Can I use multiple accounts?","text":"<p>Yes, pass different auth tokens:</p> <pre><code>async with Xeepy(auth_token=\"account1_token\") as x1:\n    ...\n\nasync with Xeepy(auth_token=\"account2_token\") as x2:\n    ...\n</code></pre>"},{"location":"FAQ/#feature-questions","title":"Feature Questions","text":""},{"location":"FAQ/#does-the-tweet-reply-scraper-work","title":"Does the tweet reply scraper work?","text":"<p>Yes! This is the main fix for the original repo. The original used Tweepy's deprecated search API. Xeepy uses browser automation which works reliably.</p> <pre><code>async with Xeepy() as x:\n    replies = await x.scrape.replies(\"https://x.com/user/status/123\")\n</code></pre>"},{"location":"FAQ/#can-i-unfollow-non-followers","title":"Can I unfollow non-followers?","text":"<p>Yes, this is the most popular feature:</p> <pre><code>async with Xeepy() as x:\n    await x.unfollow.non_followers(max_unfollows=100)\n</code></pre>"},{"location":"FAQ/#can-i-detect-who-unfollowed-me","title":"Can I detect who unfollowed me?","text":"<p>Yes:</p> <pre><code>async with Xeepy() as x:\n    report = await x.monitor.unfollowers()\n    print(report.unfollowers)\n</code></pre>"},{"location":"FAQ/#does-auto-like-work","title":"Does auto-like work?","text":"<p>Yes:</p> <pre><code>async with Xeepy() as x:\n    await x.engage.auto_like(keywords=[\"Python\"], max_likes=25)\n</code></pre>"},{"location":"FAQ/#can-i-use-ai-to-generate-replies","title":"Can I use AI to generate replies?","text":"<p>Yes, with OpenAI, Claude, or local models:</p> <pre><code>from xeepy.ai import ContentGenerator\n\nai = ContentGenerator(provider=\"openai\", api_key=\"sk-...\")\nreply = await ai.generate_reply(\"Great tweet!\", style=\"supportive\")\n</code></pre>"},{"location":"FAQ/#rate-limits-safety","title":"Rate Limits &amp; Safety","text":""},{"location":"FAQ/#what-are-the-rate-limits","title":"What are the rate limits?","text":"<p>Xeepy includes conservative defaults:</p> Action Per Hour Per Day Delay Follow 20 100 3-8 sec Unfollow 25 150 2-6 sec Like 50 500 1-3 sec Comment 10 50 30-90 sec"},{"location":"FAQ/#can-i-change-rate-limits","title":"Can I change rate limits?","text":"<p>Yes:</p> <pre><code>from xeepy.core.rate_limiter import RateLimiter\n\nlimiter = RateLimiter()\nlimiter.set_limit(\"follow\", per_hour=10, delay=(5, 10))\n</code></pre>"},{"location":"FAQ/#will-i-get-banned","title":"Will I get banned?","text":"<p>Risk depends on your usage. To minimize risk: - Start with small numbers (10-25 actions) - Use dry-run mode first - Don't run continuously - Take breaks between sessions - Don't spam</p>"},{"location":"FAQ/#what-if-i-hit-a-rate-limit","title":"What if I hit a rate limit?","text":"<p>Xeepy will automatically wait and retry. You'll see:</p> <pre><code>\u23f3 Rate limited. Waiting 60 seconds...\n</code></pre>"},{"location":"FAQ/#troubleshooting","title":"Troubleshooting","text":""},{"location":"FAQ/#authentication-failed","title":"\"Authentication failed\"","text":"<ol> <li>Re-fetch your <code>auth_token</code> from browser</li> <li>Make sure you're logged into X/Twitter</li> <li>Clear browser cookies and re-login</li> </ol>"},{"location":"FAQ/#element-not-found","title":"\"Element not found\"","text":"<p>X/Twitter may have updated their UI. Solutions: 1. Update Xeepy: <code>pip install --upgrade xeepy</code> 2. Check for open issues on GitHub 3. Try running with <code>headless=False</code> to see what's happening</p>"},{"location":"FAQ/#browser-crashed","title":"\"Browser crashed\"","text":"<ol> <li>Make sure Playwright is installed: <code>playwright install chromium</code></li> <li>Try running non-headless: <code>Xeepy(headless=False)</code></li> <li>Check available memory</li> </ol>"},{"location":"FAQ/#timeout-waiting-for-element","title":"\"Timeout waiting for element\"","text":"<p>X/Twitter might be loading slowly: 1. Increase timeout in config 2. Check your internet connection 3. Try running at off-peak hours</p>"},{"location":"FAQ/#scraping-returns-empty-results","title":"Scraping returns empty results","text":"<ol> <li>Check if the account is private</li> <li>Verify the URL/username is correct</li> <li>Some features require authentication</li> </ol>"},{"location":"FAQ/#comparison-questions","title":"Comparison Questions","text":""},{"location":"FAQ/#xeepy-vs-tweepy","title":"Xeepy vs Tweepy?","text":"Feature Xeepy Tweepy API Required \u274c No \u2705 Yes Cost Free $100-5000/mo Reply Scraping \u2705 Works \u274c Deprecated Rate Limits Flexible Strict Setup Easy Complex"},{"location":"FAQ/#xeepy-vs-twitter-api-v2","title":"Xeepy vs Twitter API v2?","text":"Feature Xeepy API v2 Cost Free $100-5000/mo Approval Not needed Required Rate Limits No hard limits 500k tweets/mo (Enterprise) Real-time Near real-time Yes (streaming)"},{"location":"FAQ/#xeepy-vs-hypefurytweethunter","title":"Xeepy vs Hypefury/Tweethunter?","text":"Feature Xeepy Hypefury/TweetHunter Cost Free $29-99/mo Open Source \u2705 Yes \u274c No Self-hosted \u2705 Yes \u274c No Customizable \u2705 Fully Limited AI Features \u2705 Yes \u2705 Yes"},{"location":"FAQ/#contribution-questions","title":"Contribution Questions","text":""},{"location":"FAQ/#how-can-i-contribute","title":"How can I contribute?","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Submit a pull request</li> </ol> <p>See Contributing Guide for details.</p>"},{"location":"FAQ/#how-do-i-report-a-bug","title":"How do I report a bug?","text":"<p>Open an issue on GitHub with: 1. Description of the bug 2. Steps to reproduce 3. Expected vs actual behavior 4. Xeepy version 5. Python version</p>"},{"location":"FAQ/#how-do-i-request-a-feature","title":"How do I request a feature?","text":"<p>Open an issue on GitHub with: 1. Description of the feature 2. Use case 3. Expected behavior</p>"},{"location":"FAQ/#legal-questions","title":"Legal Questions","text":""},{"location":"FAQ/#is-web-scraping-legal","title":"Is web scraping legal?","text":"<p>Web scraping public data is generally legal, but: - Respect robots.txt - Don't overload servers - Don't scrape private data - Check local laws - Review X/Twitter ToS</p>"},{"location":"FAQ/#can-i-use-xeepy-commercially","title":"Can I use Xeepy commercially?","text":"<p>Yes, under the MIT License. However: - Automation may violate X/Twitter ToS - Don't use for spam or harassment - Users are responsible for their usage</p>"},{"location":"FAQ/#who-is-responsible-if-i-get-banned","title":"Who is responsible if I get banned?","text":"<p>You are. Xeepy is provided \"as-is\" for educational purposes. The authors are not responsible for any account restrictions.</p>"},{"location":"FAQ/#getting-help","title":"Getting Help","text":""},{"location":"FAQ/#where-can-i-get-help","title":"Where can I get help?","text":"<ol> <li>Documentation: Start here!</li> <li>GitHub Issues: For bugs and features</li> <li>Examples: See EXAMPLES.md</li> </ol>"},{"location":"FAQ/#still-have-questions","title":"Still have questions?","text":"<p>Open an issue on GitHub or tweet @nichxbt.</p> <p> Didn't find your answer? Open an issue! </p>"},{"location":"INSTALLATION/","title":"Documentation: Installation Guide","text":"<p>Complete installation guide for Xeepy on all platforms.</p>"},{"location":"INSTALLATION/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Requirements</li> <li>Quick Install</li> <li>Platform-Specific Instructions</li> <li>Development Installation</li> <li>Docker Installation</li> <li>Troubleshooting</li> </ul>"},{"location":"INSTALLATION/#requirements","title":"Requirements","text":""},{"location":"INSTALLATION/#system-requirements","title":"System Requirements","text":"Requirement Minimum Recommended Python 3.10 3.11+ RAM 2 GB 4 GB+ Disk Space 500 MB 1 GB Browser Chromium Chromium"},{"location":"INSTALLATION/#dependencies","title":"Dependencies","text":"<p>Core dependencies are automatically installed:</p> <ul> <li><code>playwright</code> - Browser automation</li> <li><code>aiohttp</code> - Async HTTP client</li> <li><code>pydantic</code> - Data validation</li> <li><code>rich</code> - CLI interface</li> <li><code>typer</code> - CLI framework</li> </ul> <p>Optional dependencies:</p> <ul> <li><code>openai</code> - OpenAI GPT integration</li> <li><code>anthropic</code> - Claude integration</li> <li><code>ollama</code> - Local LLM support</li> </ul>"},{"location":"INSTALLATION/#quick-install","title":"Quick Install","text":""},{"location":"INSTALLATION/#from-pypi-recommended","title":"From PyPI (Recommended)","text":"<pre><code># Install Xeepy\npip install xeepy\n\n# Install browser (required)\nplaywright install chromium\n\n# Verify installation\nxeepy --version\n</code></pre>"},{"location":"INSTALLATION/#from-source","title":"From Source","text":"<pre><code># Clone repository\ngit clone https://github.com/nirholas/Get-Tweet-Replies-With-Python-Tweepy.git\ncd Get-Tweet-Replies-With-Python-Tweepy\n\n# Install\npip install .\n\n# Install browser\nplaywright install chromium\n</code></pre>"},{"location":"INSTALLATION/#platform-specific-instructions","title":"Platform-Specific Instructions","text":""},{"location":"INSTALLATION/#macos","title":"macOS","text":"<pre><code># Ensure Python 3.10+ is installed\npython3 --version\n\n# If not, install via Homebrew\nbrew install python@3.11\n\n# Install Xeepy\npip3 install xeepy\n\n# Install browser\nplaywright install chromium\n\n# Verify\nxeepy --version\n</code></pre>"},{"location":"INSTALLATION/#windows","title":"Windows","text":"<pre><code># Ensure Python 3.10+ is installed\npython --version\n\n# If not, download from python.org or use winget\nwinget install Python.Python.3.11\n\n# Install Xeepy\npip install xeepy\n\n# Install browser\nplaywright install chromium\n\n# Verify\nxeepy --version\n</code></pre>"},{"location":"INSTALLATION/#linux-ubuntudebian","title":"Linux (Ubuntu/Debian)","text":"<pre><code># Install Python 3.10+ if needed\nsudo apt update\nsudo apt install python3.11 python3.11-venv python3-pip\n\n# Install Xeepy\npip3 install xeepy\n\n# Install browser and dependencies\nplaywright install chromium\nplaywright install-deps chromium\n\n# Verify\nxeepy --version\n</code></pre>"},{"location":"INSTALLATION/#linux-fedorarhel","title":"Linux (Fedora/RHEL)","text":"<pre><code># Install Python\nsudo dnf install python3.11 python3-pip\n\n# Install Xeepy\npip3 install xeepy\n\n# Install browser\nplaywright install chromium\n\n# Verify\nxeepy --version\n</code></pre>"},{"location":"INSTALLATION/#linux-arch","title":"Linux (Arch)","text":"<pre><code># Install Python\nsudo pacman -S python python-pip\n\n# Install Xeepy\npip install xeepy\n\n# Install browser\nplaywright install chromium\n\n# Verify\nxeepy --version\n</code></pre>"},{"location":"INSTALLATION/#development-installation","title":"Development Installation","text":"<p>For contributing or modifying Xeepy:</p> <pre><code># Clone repository\ngit clone https://github.com/nirholas/Get-Tweet-Replies-With-Python-Tweepy.git\ncd Get-Tweet-Replies-With-Python-Tweepy\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # Windows: venv\\Scripts\\activate\n\n# Install in development mode with all dependencies\npip install -e \".[dev,ai]\"\n\n# Install browser\nplaywright install chromium\n\n# Install pre-commit hooks\npre-commit install\n\n# Run tests\npytest\n\n# Verify\nxeepy --version\n</code></pre>"},{"location":"INSTALLATION/#dev-dependencies","title":"Dev Dependencies","text":"<p>The <code>[dev]</code> extra includes:</p> <ul> <li><code>pytest</code> - Testing framework</li> <li><code>pytest-asyncio</code> - Async test support</li> <li><code>pytest-cov</code> - Coverage reporting</li> <li><code>black</code> - Code formatter</li> <li><code>ruff</code> - Linter</li> <li><code>pre-commit</code> - Git hooks</li> <li><code>mypy</code> - Type checking</li> </ul>"},{"location":"INSTALLATION/#docker-installation","title":"Docker Installation","text":""},{"location":"INSTALLATION/#using-pre-built-image","title":"Using Pre-built Image","text":"<pre><code># Pull image (when available)\ndocker pull nirholas/xeepy:latest\n\n# Run with session volume\ndocker run -it -v ~/.xeepy:/root/.xeepy nirholas/xeepy\n</code></pre>"},{"location":"INSTALLATION/#building-locally","title":"Building Locally","text":"<pre><code># Dockerfile\nFROM mcr.microsoft.com/playwright/python:v1.40.0-jammy\n\nWORKDIR /app\n\n# Install Xeepy\nCOPY . .\nRUN pip install .\n\n# Install browser\nRUN playwright install chromium\n\nENTRYPOINT [\"xeepy\"]\n</code></pre> <pre><code># Build\ndocker build -t xeepy .\n\n# Run\ndocker run -it -v ~/.xeepy:/root/.xeepy xeepy --help\n</code></pre>"},{"location":"INSTALLATION/#docker-compose","title":"Docker Compose","text":"<pre><code># docker-compose.yml\nversion: '3.8'\nservices:\n  xeepy:\n    build: .\n    volumes:\n      - ~/.xeepy:/root/.xeepy\n      - ./data:/app/data\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n</code></pre>"},{"location":"INSTALLATION/#virtual-environment-setup","title":"Virtual Environment Setup","text":""},{"location":"INSTALLATION/#using-venv-recommended","title":"Using venv (Recommended)","text":"<pre><code># Create environment\npython -m venv xeepy-env\n\n# Activate\n# macOS/Linux:\nsource xeepy-env/bin/activate\n# Windows:\nxeepy-env\\Scripts\\activate\n\n# Install\npip install xeepy\nplaywright install chromium\n\n# Deactivate when done\ndeactivate\n</code></pre>"},{"location":"INSTALLATION/#using-conda","title":"Using conda","text":"<pre><code># Create environment\nconda create -n xeepy python=3.11\n\n# Activate\nconda activate xeepy\n\n# Install\npip install xeepy\nplaywright install chromium\n\n# Deactivate\nconda deactivate\n</code></pre>"},{"location":"INSTALLATION/#using-pipx-for-cli-only","title":"Using pipx (for CLI only)","text":"<pre><code># Install pipx if needed\npip install pipx\n\n# Install Xeepy\npipx install xeepy\n\n# Inject playwright\npipx inject xeepy playwright\nplaywright install chromium\n</code></pre>"},{"location":"INSTALLATION/#ai-features-installation","title":"AI Features Installation","text":""},{"location":"INSTALLATION/#openai-integration","title":"OpenAI Integration","text":"<pre><code># Install with OpenAI support\npip install \"xeepy[ai]\"\n\n# Or install openai separately\npip install openai\n\n# Set API key\nexport OPENAI_API_KEY=\"sk-...\"\n</code></pre>"},{"location":"INSTALLATION/#anthropic-integration","title":"Anthropic Integration","text":"<pre><code># Install with AI support\npip install \"xeepy[ai]\"\n\n# Or install anthropic separately\npip install anthropic\n\n# Set API key\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\n</code></pre>"},{"location":"INSTALLATION/#local-llms-ollama","title":"Local LLMs (Ollama)","text":"<pre><code># Install Ollama\ncurl https://ollama.ai/install.sh | sh\n\n# Pull a model\nollama pull llama2\n\n# Xeepy will auto-detect local Ollama\n</code></pre>"},{"location":"INSTALLATION/#troubleshooting","title":"Troubleshooting","text":""},{"location":"INSTALLATION/#browser-installation-failed","title":"Browser Installation Failed","text":"<pre><code># Try with dependencies\nplaywright install-deps chromium\nplaywright install chromium\n\n# Or use system chromium\npip install xeepy[chromium]\n</code></pre>"},{"location":"INSTALLATION/#permission-denied","title":"Permission Denied","text":"<pre><code># Use user installation\npip install --user xeepy\n\n# Or fix permissions\nsudo chown -R $USER ~/.cache/ms-playwright\n</code></pre>"},{"location":"INSTALLATION/#ssl-certificate-errors","title":"SSL Certificate Errors","text":"<pre><code># Update certificates\npip install --upgrade certifi\n\n# Or skip SSL verification (not recommended)\nexport PYTHONHTTPSVERIFY=0\n</code></pre>"},{"location":"INSTALLATION/#module-not-found","title":"Module Not Found","text":"<pre><code># Ensure correct Python\nwhich python\npython --version\n\n# Reinstall\npip uninstall xeepy\npip install xeepy\n</code></pre>"},{"location":"INSTALLATION/#playwright-errors","title":"Playwright Errors","text":"<pre><code># Update Playwright\npip install --upgrade playwright\nplaywright install chromium\n\n# Check browser exists\nls ~/.cache/ms-playwright/\n</code></pre>"},{"location":"INSTALLATION/#rate-limit-errors","title":"Rate Limit Errors","text":"<p>Xeepy includes rate limiting. If you see rate limit errors:</p> <ol> <li>Wait and retry</li> <li>Check your configuration</li> <li>Reduce operation frequency</li> </ol>"},{"location":"INSTALLATION/#verify-installation","title":"Verify Installation","text":"<pre><code># Check version\nxeepy --version\n\n# Run test command\nxeepy --help\n\n# Test Python import\npython -c \"from xeepy import Xeepy; print('OK')\"\n</code></pre>"},{"location":"INSTALLATION/#uninstallation","title":"Uninstallation","text":"<pre><code># Remove package\npip uninstall xeepy\n\n# Remove browser (optional)\nplaywright uninstall chromium\n\n# Remove configuration (optional)\nrm -rf ~/.xeepy\n</code></pre>"},{"location":"INSTALLATION/#next-steps","title":"Next Steps","text":"<p>After installation:</p> <ol> <li>Quick Start Guide - Get started in 5 minutes</li> <li>CLI Reference - Learn CLI commands</li> <li>Examples - See code examples</li> <li>AI Features - Set up AI integration</li> </ol>"},{"location":"QUICKSTART/","title":"\u26a1 Quick Start Guide","text":"<p>Get up and running with Xeepy in under 5 minutes.</p>"},{"location":"QUICKSTART/#installation","title":"\ud83d\udce6 Installation","text":""},{"location":"QUICKSTART/#option-1-pip-recommended","title":"Option 1: pip (Recommended)","text":"<pre><code>pip install xeepy\n</code></pre>"},{"location":"QUICKSTART/#option-2-from-source","title":"Option 2: From Source","text":"<pre><code>git clone https://github.com/nirholas/Get-Tweet-Replies-With-Python-Tweepy.git\ncd Get-Tweet-Replies-With-Python-Tweepy\npip install -e .\n</code></pre>"},{"location":"QUICKSTART/#option-3-with-ai-features","title":"Option 3: With AI Features","text":"<pre><code>pip install xeepy[ai]  # Includes OpenAI, Anthropic clients\n</code></pre>"},{"location":"QUICKSTART/#authentication","title":"\ud83d\udd10 Authentication","text":"<p>Xeepy uses cookie-based authentication (no API keys needed!).</p>"},{"location":"QUICKSTART/#step-1-get-your-session-cookie","title":"Step 1: Get Your Session Cookie","text":"<ol> <li>Open x.com in your browser</li> <li>Open Developer Tools (F12 or Cmd+Option+I)</li> <li>Go to Application \u2192 Cookies \u2192 x.com</li> <li>Find the <code>auth_token</code> cookie and copy its value</li> </ol>"},{"location":"QUICKSTART/#step-2-configure-xeepy","title":"Step 2: Configure Xeepy","text":"<pre><code>from xeepy import Xeepy\n\n# Option A: Pass directly\nasync with Xeepy(auth_token=\"your_auth_token_here\") as x:\n    ...\n\n# Option B: Environment variable (recommended)\n# export XEEPY_AUTH_TOKEN=your_auth_token_here\nasync with Xeepy() as x:\n    ...\n\n# Option C: Config file\n# Create ~/.xeepy/config.yaml\n</code></pre> <p>~/.xeepy/config.yaml: <pre><code>auth:\n  auth_token: \"your_auth_token_here\"\n\nsettings:\n  headless: true\n  rate_limit: true\n</code></pre></p>"},{"location":"QUICKSTART/#5-minute-examples","title":"\ud83c\udfaf 5-Minute Examples","text":""},{"location":"QUICKSTART/#example-1-get-tweet-replies","title":"Example 1: Get Tweet Replies","text":"<p>The original purpose of this repo - finally working!</p> <pre><code>import asyncio\nfrom xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Get replies to any tweet\n        replies = await x.scrape.replies(\n            \"https://x.com/elonmusk/status/1234567890\",\n            limit=50\n        )\n\n        print(f\"Found {len(replies)} replies!\")\n\n        for reply in replies[:5]:\n            print(f\"@{reply.username}: {reply.text[:100]}...\")\n\n        # Export to CSV\n        x.export.to_csv(replies, \"replies.csv\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"QUICKSTART/#example-2-unfollow-non-followers","title":"Example 2: Unfollow Non-Followers","text":"<p>The most requested feature!</p> <pre><code>import asyncio\nfrom xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # First, do a dry run to see who would be unfollowed\n        preview = await x.unfollow.non_followers(\n            max_unfollows=50,\n            whitelist=[\"friend1\", \"important_account\"],\n            dry_run=True\n        )\n\n        print(f\"Would unfollow {len(preview.unfollowed_users)} users:\")\n        for user in preview.unfollowed_users[:10]:\n            print(f\"  - @{user}\")\n\n        # If you're sure, run for real\n        # result = await x.unfollow.non_followers(dry_run=False)\n\nasyncio.run(main())\n</code></pre>"},{"location":"QUICKSTART/#example-3-auto-like-by-keywords","title":"Example 3: Auto-Like by Keywords","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Auto-like tweets containing keywords\n        result = await x.engage.auto_like(\n            keywords=[\"python\", \"machinelearning\", \"AI\"],\n            max_likes=25,\n            duration_minutes=15\n        )\n\n        print(f\"Liked {result.success_count} tweets!\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"QUICKSTART/#example-4-detect-unfollowers","title":"Example 4: Detect Unfollowers","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Check who unfollowed you\n        report = await x.monitor.unfollowers()\n\n        if report.unfollowers:\n            print(f\"\ud83d\ude22 {len(report.unfollowers)} people unfollowed you:\")\n            for user in report.unfollowers:\n                print(f\"  - @{user}\")\n        else:\n            print(\"\u2705 No one unfollowed you!\")\n\n        if report.new_followers:\n            print(f\"\ud83c\udf89 {len(report.new_followers)} new followers!\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"QUICKSTART/#example-5-ai-powered-reply-generation","title":"Example 5: AI-Powered Reply Generation","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom xeepy.ai import ContentGenerator\n\nasync def main():\n    async with Xeepy() as x:\n        # Initialize AI (requires API key)\n        ai = ContentGenerator(\n            provider=\"openai\",\n            api_key=\"sk-...\"\n        )\n\n        # Generate reply for a tweet\n        tweet = \"Just shipped my first Python package! \ud83d\udc0d\"\n\n        reply = await ai.generate_reply(\n            tweet_text=tweet,\n            style=\"supportive\"\n        )\n\n        print(f\"Suggested reply: {reply}\")\n        # Output: \"Congrats! \ud83c\udf89 What does it do? Would love to check it out!\"\n\nasyncio.run(main())\n</code></pre>"},{"location":"QUICKSTART/#cli-quick-start","title":"\ud83d\udda5\ufe0f CLI Quick Start","text":"<pre><code># Setup authentication\nxeepy auth login\n\n# Get tweet replies\nxeepy scrape replies https://x.com/user/status/123 -o replies.csv\n\n# Unfollow non-followers (dry run)\nxeepy unfollow non-followers --dry-run\n\n# Auto-like by keyword\nxeepy engage auto-like \"python\" --max 25\n\n# Check unfollowers\nxeepy monitor unfollowers\n</code></pre>"},{"location":"QUICKSTART/#project-structure","title":"\ud83d\udcc1 Project Structure","text":"<p>After installation, here's how to organize your project:</p> <pre><code>my_twitter_project/\n\u251c\u2500\u2500 main.py              # Your main script\n\u251c\u2500\u2500 config.yaml          # Configuration (optional)\n\u251c\u2500\u2500 .env                 # Environment variables\n\u2514\u2500\u2500 output/              # Exported data\n    \u251c\u2500\u2500 replies.csv\n    \u251c\u2500\u2500 followers.json\n    \u2514\u2500\u2500 unfollowers.json\n</code></pre> <p>main.py: <pre><code>import asyncio\nfrom xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Your automation code here\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre></p> <p>.env: <pre><code>XEEPY_AUTH_TOKEN=your_auth_token\nOPENAI_API_KEY=sk-...  # Optional, for AI features\n</code></pre></p>"},{"location":"QUICKSTART/#important-notes","title":"\u26a0\ufe0f Important Notes","text":""},{"location":"QUICKSTART/#rate-limits","title":"Rate Limits","text":"<p>Xeepy automatically handles rate limiting, but be aware: - Don't run multiple instances simultaneously - Start with small numbers (10-25 actions) - Wait between sessions (15-30 minutes)</p>"},{"location":"QUICKSTART/#session-expiry","title":"Session Expiry","text":"<p>Your auth_token may expire. If you get authentication errors: 1. Re-fetch your auth_token from the browser 2. Update your configuration</p>"},{"location":"QUICKSTART/#headless-mode","title":"Headless Mode","text":"<p>By default, Xeepy runs headless (no visible browser). To see what's happening:</p> <pre><code>async with Xeepy(headless=False) as x:\n    # Browser window will be visible\n    pass\n</code></pre>"},{"location":"QUICKSTART/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<ol> <li>Full API Reference - All available methods</li> <li>Examples - More code examples</li> <li>AI Features - AI integration guide</li> <li>CLI Reference - All CLI commands</li> </ol>"},{"location":"QUICKSTART/#troubleshooting","title":"\ud83c\udd98 Troubleshooting","text":""},{"location":"QUICKSTART/#authentication-failed","title":"\"Authentication failed\"","text":"<ul> <li>Re-fetch your auth_token from the browser</li> <li>Make sure you're logged into X/Twitter</li> </ul>"},{"location":"QUICKSTART/#element-not-found","title":"\"Element not found\"","text":"<ul> <li>X/Twitter may have updated their UI</li> <li>Check for Xeepy updates: <code>pip install --upgrade xeepy</code></li> </ul>"},{"location":"QUICKSTART/#rate-limited","title":"\"Rate limited\"","text":"<ul> <li>You're making too many requests</li> <li>Wait 15-30 minutes before continuing</li> <li>Reduce your action limits</li> </ul>"},{"location":"QUICKSTART/#need-help","title":"Need help?","text":"<ul> <li>Open an issue</li> <li>Check FAQ</li> </ul> <p> Ready to automate? Let's go! \ud83d\ude80 </p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to Xeepy are documented here.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>GraphQL API client for higher rate limits</li> <li>Twitter Spaces scraping and audio capture</li> <li>Media download functionality (photos, videos, HQ)</li> <li>Trends and recommendations scraper</li> <li>Poll creation and voting</li> <li>DM management (send, inbox, history, search, delete)</li> <li>Scheduled tweets and drafts</li> <li>Account settings management</li> <li>Cookie session save/load/import</li> <li>AI-powered content generation with OpenAI, Anthropic, Ollama</li> <li>Sentiment analysis</li> <li>Bot detection</li> <li>Comprehensive monitoring and analytics</li> <li>Discord, Telegram, email notifications</li> <li>Full CLI interface</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Improved rate limiting with per-operation limits</li> <li>Better error messages and handling</li> <li>Performance optimizations for large scraping jobs</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Session expiration handling</li> <li>Memory usage for large datasets</li> <li>Thread unrolling accuracy</li> </ul>"},{"location":"changelog/#100-2024-01-15","title":"[1.0.0] - 2024-01-15","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Initial public release</li> <li>Core scraping functionality:</li> <li>Tweet replies</li> <li>User profiles</li> <li>Followers/following lists</li> <li>User tweets</li> <li>Search results</li> <li>Hashtag tweets</li> <li>Thread unrolling</li> <li>Follow/unfollow operations:</li> <li>Follow user</li> <li>Follow by hashtag</li> <li>Unfollow non-followers</li> <li>Smart unfollow</li> <li>Whitelist support</li> <li>Engagement actions:</li> <li>Like tweets</li> <li>Retweet</li> <li>Reply</li> <li>Bookmark</li> <li>Auto-like by keyword</li> <li>Export functionality:</li> <li>CSV</li> <li>JSON</li> <li>Excel</li> <li>Database (SQLite, PostgreSQL, MySQL)</li> <li>Basic monitoring:</li> <li>Unfollower detection</li> <li>Growth tracking</li> <li>Authentication:</li> <li>Browser-based login</li> <li>Session persistence</li> <li>Cookie import</li> <li>CLI interface</li> <li>Documentation</li> </ul>"},{"location":"changelog/#security","title":"Security","text":"<ul> <li>Encrypted session storage option</li> <li>Secure credential handling</li> </ul>"},{"location":"changelog/#090-2024-01-01-beta","title":"[0.9.0] - 2024-01-01 (Beta)","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Beta release for testing</li> <li>Core functionality implementation</li> <li>Initial documentation</li> </ul>"},{"location":"changelog/#known-issues","title":"Known Issues","text":"<ul> <li>Rate limiting needs tuning</li> <li>Some UI element selectors may break</li> </ul>"},{"location":"changelog/#version-history-summary","title":"Version History Summary","text":"Version Date Highlights 1.0.0 2024-01-15 Initial public release 0.9.0 2024-01-01 Beta release"},{"location":"changelog/#upgrade-guide","title":"Upgrade Guide","text":""},{"location":"changelog/#upgrading-to-100","title":"Upgrading to 1.0.0","text":"<pre><code>pip install --upgrade xeepy\n</code></pre> <p>No breaking changes from 0.9.0.</p>"},{"location":"changelog/#breaking-changes-log","title":"Breaking Changes Log","text":"<p>None yet. We're committed to backwards compatibility.</p>"},{"location":"changelog/#links","title":"Links","text":"<ul> <li>GitHub Releases</li> <li>PyPI</li> <li>Migration Guides</li> </ul>"},{"location":"migration/","title":"Migration Guide","text":"<p>This guide helps you migrate from Xeepy v1.x to v2.x, or from other Twitter libraries.</p>"},{"location":"migration/#from-xeepy-v1x-to-v2x","title":"From Xeepy v1.x to v2.x","text":""},{"location":"migration/#breaking-changes-in-v20","title":"Breaking Changes in v2.0","text":""},{"location":"migration/#1-async-context-manager-required","title":"1. Async Context Manager Required","text":"<pre><code># \u274c Old (v1.x)\nx = Xeepy()\nawait x.scrape.replies(url)\nawait x.close()\n\n# \u2705 New (v2.x)\nasync with Xeepy() as x:\n    await x.scrape.replies(url)\n</code></pre>"},{"location":"migration/#2-method-renames","title":"2. Method Renames","text":"v1.x v2.x Notes <code>x.get_replies()</code> <code>x.scrape.replies()</code> Moved to scrape manager <code>x.get_followers()</code> <code>x.scrape.followers()</code> Moved to scrape manager <code>x.get_following()</code> <code>x.scrape.following()</code> Moved to scrape manager <code>x.like_tweet()</code> <code>x.engage.like()</code> Moved to engage manager <code>x.retweet()</code> <code>x.engage.retweet()</code> Moved to engage manager <code>x.follow_user()</code> <code>x.follow.user()</code> Moved to follow manager <code>x.unfollow_user()</code> <code>x.unfollow.user()</code> Moved to unfollow manager"},{"location":"migration/#3-return-type-changes","title":"3. Return Type Changes","text":"<p>All scraping methods now return <code>ScrapeResult</code> objects instead of raw lists:</p> <pre><code># \u274c Old (v1.x)\nreplies = await x.get_replies(url)  # Returns List[Tweet]\nfor reply in replies:\n    print(reply.text)\n\n# \u2705 New (v2.x)\nresult = await x.scrape.replies(url)  # Returns ScrapeResult[Tweet]\nprint(f\"Found {result.count} replies\")\nfor reply in result.items:\n    print(reply.text)\n</code></pre>"},{"location":"migration/#4-configuration-changes","title":"4. Configuration Changes","text":"<pre><code># \u274c Old (v1.x)\nx = Xeepy(browser=\"chromium\", headless=True)\n\n# \u2705 New (v2.x)\nx = Xeepy(\n    headless=True,\n    config_path=\"xeepy.yml\"  # Optional external config\n)\n</code></pre>"},{"location":"migration/#5-rate-limiter-integration","title":"5. Rate Limiter Integration","text":"<p>Rate limiting is now built-in and enabled by default:</p> <pre><code># \u274c Old (v1.x) - Manual delays\nimport asyncio\nfor user in users:\n    await x.follow_user(user)\n    await asyncio.sleep(5)\n\n# \u2705 New (v2.x) - Automatic rate limiting\nfor user in users:\n    await x.follow.user(user)  # Automatic delays\n</code></pre>"},{"location":"migration/#from-tweepy-to-xeepy","title":"From Tweepy to Xeepy","text":""},{"location":"migration/#why-migrate","title":"Why Migrate?","text":"Feature Tweepy Xeepy Twitter API Required \u2705 Yes \u274c No API Costs $100+/month Free Rate Limits Strict (API) Browser-based Full Access Limited by tier Full access"},{"location":"migration/#quick-conversion-guide","title":"Quick Conversion Guide","text":""},{"location":"migration/#authentication","title":"Authentication","text":"<pre><code># \u274c Tweepy\nimport tweepy\nauth = tweepy.OAuth1UserHandler(\n    consumer_key, consumer_secret,\n    access_token, access_token_secret\n)\napi = tweepy.API(auth)\n\n# \u2705 Xeepy\nfrom xeepy import Xeepy\nasync with Xeepy() as x:\n    await x.auth.login()  # Browser-based auth\n</code></pre>"},{"location":"migration/#get-tweet-replies","title":"Get Tweet Replies","text":"<pre><code># \u274c Tweepy (v1 API - deprecated)\n# No native method, requires search workarounds\n\n# \u2705 Xeepy\nasync with Xeepy() as x:\n    replies = await x.scrape.replies(\"https://x.com/user/status/123\")\n</code></pre>"},{"location":"migration/#get-user-timeline","title":"Get User Timeline","text":"<pre><code># \u274c Tweepy\ntweets = api.user_timeline(screen_name=\"username\", count=100)\n\n# \u2705 Xeepy\nasync with Xeepy() as x:\n    result = await x.scrape.tweets(\"username\", limit=100)\n    tweets = result.items\n</code></pre>"},{"location":"migration/#get-followers","title":"Get Followers","text":"<pre><code># \u274c Tweepy\nfollowers = tweepy.Cursor(api.get_followers, screen_name=\"username\").items(1000)\n\n# \u2705 Xeepy\nasync with Xeepy() as x:\n    result = await x.scrape.followers(\"username\", limit=1000)\n    followers = result.items\n</code></pre>"},{"location":"migration/#like-a-tweet","title":"Like a Tweet","text":"<pre><code># \u274c Tweepy\napi.create_favorite(tweet_id)\n\n# \u2705 Xeepy\nasync with Xeepy() as x:\n    await x.engage.like(\"https://x.com/user/status/123\")\n</code></pre>"},{"location":"migration/#follow-a-user","title":"Follow a User","text":"<pre><code># \u274c Tweepy\napi.create_friendship(screen_name=\"username\")\n\n# \u2705 Xeepy\nasync with Xeepy() as x:\n    await x.follow.user(\"username\")\n</code></pre>"},{"location":"migration/#search-tweets","title":"Search Tweets","text":"<pre><code># \u274c Tweepy\ntweets = api.search_tweets(q=\"python\", count=100)\n\n# \u2705 Xeepy\nasync with Xeepy() as x:\n    result = await x.scrape.search(\"python\", limit=100)\n    tweets = result.items\n</code></pre>"},{"location":"migration/#from-snscrape-to-xeepy","title":"From Snscrape to Xeepy","text":""},{"location":"migration/#key-differences","title":"Key Differences","text":"Feature Snscrape Xeepy Maintenance Abandoned Active Actions Read-only Full actions Rate Limits Strict Manageable Anti-Detection None Built-in"},{"location":"migration/#common-migrations","title":"Common Migrations","text":""},{"location":"migration/#scrape-user-tweets","title":"Scrape User Tweets","text":"<pre><code># \u274c Snscrape\nimport snscrape.modules.twitter as sntwitter\ntweets = []\nfor tweet in sntwitter.TwitterUserScraper(\"username\").get_items():\n    tweets.append(tweet)\n    if len(tweets) &gt;= 100:\n        break\n\n# \u2705 Xeepy\nasync with Xeepy() as x:\n    result = await x.scrape.tweets(\"username\", limit=100)\n    tweets = result.items\n</code></pre>"},{"location":"migration/#search-tweets_1","title":"Search Tweets","text":"<pre><code># \u274c Snscrape\nfor tweet in sntwitter.TwitterSearchScraper(\"python\").get_items():\n    print(tweet.content)\n\n# \u2705 Xeepy\nasync with Xeepy() as x:\n    result = await x.scrape.search(\"python\")\n    for tweet in result.items:\n        print(tweet.text)\n</code></pre>"},{"location":"migration/#from-twitter-api-v2-to-xeepy","title":"From Twitter API v2 to Xeepy","text":""},{"location":"migration/#cost-comparison","title":"Cost Comparison","text":"Twitter API Tier Cost Tweets/Month Xeepy Free $0 1,500 Unlimited Basic $100 10,000 Unlimited Pro $5,000 1,000,000 Unlimited Enterprise $$$$ Custom Unlimited"},{"location":"migration/#endpoint-mapping","title":"Endpoint Mapping","text":"Twitter API v2 Endpoint Xeepy Method <code>GET /2/tweets/:id</code> <code>x.scrape.tweet(url)</code> <code>GET /2/users/:id</code> <code>x.scrape.profile(username)</code> <code>GET /2/users/:id/tweets</code> <code>x.scrape.tweets(username)</code> <code>GET /2/users/:id/followers</code> <code>x.scrape.followers(username)</code> <code>GET /2/users/:id/following</code> <code>x.scrape.following(username)</code> <code>POST /2/users/:id/likes</code> <code>x.engage.like(url)</code> <code>POST /2/users/:id/retweets</code> <code>x.engage.retweet(url)</code> <code>POST /2/users/:id/following</code> <code>x.follow.user(username)</code> <code>DELETE /2/users/:id/following</code> <code>x.unfollow.user(username)</code> <code>POST /2/tweets</code> GraphQL API <code>POST /2/dm_conversations</code> <code>x.dm.send(message, recipients)</code>"},{"location":"migration/#data-model-mapping","title":"Data Model Mapping","text":""},{"location":"migration/#tweet-object","title":"Tweet Object","text":"<pre><code># Twitter API v2 Tweet\n{\n    \"id\": \"123456789\",\n    \"text\": \"Hello world\",\n    \"created_at\": \"2024-01-01T12:00:00.000Z\",\n    \"author_id\": \"987654321\",\n    \"public_metrics\": {\n        \"like_count\": 100,\n        \"retweet_count\": 50\n    }\n}\n\n# Xeepy Tweet\nTweet(\n    id=\"123456789\",\n    text=\"Hello world\",\n    created_at=datetime(2024, 1, 1, 12, 0, 0),\n    author=User(id=\"987654321\", ...),\n    likes=100,\n    retweets=50\n)\n</code></pre>"},{"location":"migration/#user-object","title":"User Object","text":"<pre><code># Twitter API v2 User\n{\n    \"id\": \"987654321\",\n    \"username\": \"johndoe\",\n    \"name\": \"John Doe\",\n    \"public_metrics\": {\n        \"followers_count\": 1000,\n        \"following_count\": 500\n    }\n}\n\n# Xeepy User\nUser(\n    id=\"987654321\",\n    username=\"johndoe\",\n    name=\"John Doe\",\n    followers_count=1000,\n    following_count=500\n)\n</code></pre>"},{"location":"migration/#migration-checklist","title":"Migration Checklist","text":""},{"location":"migration/#before-you-start","title":"Before You Start","text":"<ul> <li> Backup your existing code</li> <li> Note your current API usage patterns</li> <li> Install Xeepy: <code>pip install xeepy</code></li> <li> Install Playwright: <code>playwright install chromium</code></li> </ul>"},{"location":"migration/#during-migration","title":"During Migration","text":"<ul> <li> Replace authentication code</li> <li> Update method calls to new namespaces</li> <li> Handle <code>ScrapeResult</code> return types</li> <li> Remove manual rate limiting (now automatic)</li> <li> Update error handling</li> </ul>"},{"location":"migration/#after-migration","title":"After Migration","text":"<ul> <li> Test all functionality</li> <li> Remove old library dependencies</li> <li> Update environment variables</li> <li> Review rate limit settings</li> </ul>"},{"location":"migration/#common-issues","title":"Common Issues","text":""},{"location":"migration/#1-asyncawait-errors","title":"1. Async/Await Errors","text":"<p>All Xeepy methods are async:</p> <pre><code># \u274c Wrong\nreplies = x.scrape.replies(url)\n\n# \u2705 Correct\nreplies = await x.scrape.replies(url)\n</code></pre>"},{"location":"migration/#2-browser-not-installed","title":"2. Browser Not Installed","text":"<pre><code># Install browser\nplaywright install chromium\n</code></pre>"},{"location":"migration/#3-authentication-issues","title":"3. Authentication Issues","text":"<pre><code># Clear old session and re-authenticate\nasync with Xeepy() as x:\n    await x.auth.login()  # Opens browser for manual login\n    await x.auth.save_cookies(\"session.json\")\n</code></pre>"},{"location":"migration/#4-rate-limit-adjustments","title":"4. Rate Limit Adjustments","text":"<pre><code># Customize rate limits if needed\nfrom xeepy.core.config import Config\n\nconfig = Config()\nconfig.rate_limits.actions_per_hour = 50\nconfig.rate_limits.scrapes_per_hour = 200\n\nasync with Xeepy(config=config) as x:\n    ...\n</code></pre>"},{"location":"migration/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcd6 Documentation</li> <li>\ud83d\udcac Discord Community</li> <li>\ud83d\udc1b GitHub Issues</li> <li>\ud83e\udd1d Contributing</li> </ul>"},{"location":"migration/#see-also","title":"See Also","text":"<ul> <li>Quick Start Guide</li> <li>Installation</li> <li>Configuration</li> <li>API Reference</li> </ul>"},{"location":"why-xeepy/","title":"Why Xeepy?","text":""},{"location":"why-xeepy/#the-problem-we-solved","title":"The Problem We Solved","text":"<p>In 2023, Twitter (now X) drastically changed their API:</p> <ul> <li>Basic API: $100/month, severely rate-limited</li> <li>Pro API: $5,000/month for reasonable access</li> <li>Enterprise: Contact sales (read: very expensive)</li> </ul> <p>Worse, many endpoints were removed entirely. Tweet replies? Gone from the free tier. Full-archive search? Enterprise only. Unfollower detection? Never existed.</p> <p>The original <code>twitter_reply.py</code> script in this repo used Tweepy's search API\u2014which Twitter broke in 2023. Xeepy was born from the ashes of that broken script.</p>"},{"location":"why-xeepy/#our-solution-browser-automation","title":"Our Solution: Browser Automation","text":"<p>Instead of fighting API restrictions, we went around them entirely. Xeepy uses Playwright to automate a real browser, giving you the same access as any X user:</p> <pre><code># What used to require expensive API access\n# now works with simple browser automation\n\nasync with Xeepy() as x:\n    # This \"impossible\" query now just works\n    replies = await x.scrape.replies(tweet_url)\n</code></pre>"},{"location":"why-xeepy/#xeepy-vs-alternatives","title":"Xeepy vs. Alternatives","text":""},{"location":"why-xeepy/#vs-twitter-api-official","title":"vs. Twitter API (Official)","text":"Aspect Twitter API Xeepy Monthly Cost \\(100-\\)5,000+ Free Tweet Replies \u274c Premium only \u2705 Included Full Archive \u274c Enterprise only \u2705 Included Rate Limits Strict, per-endpoint Flexible, human-like Unfollower Detection \u274c Not available \u2705 Built-in Setup API keys, OAuth Just cookies"},{"location":"why-xeepy/#vs-tweepy","title":"vs. Tweepy","text":"<p>Tweepy is a great library\u2014but it depends on the Twitter API:</p> <pre><code># Tweepy (BROKEN since 2023)\nimport tweepy\napi.search_tweets(q=f\"to:{username}\")  # \u274c No longer works!\n\n# Xeepy (WORKS)\nfrom xeepy import Xeepy\nawait x.scrape.replies(tweet_url)  # \u2705 Works perfectly\n</code></pre>"},{"location":"why-xeepy/#vs-snscrape","title":"vs. SNScrape","text":"<p>SNScrape was popular but is now unmaintained and broken:</p> Aspect SNScrape Xeepy Status \u274c Unmaintained \u2705 Actively developed Login Required No (but limited) Yes (full access) Anti-Detection Basic Advanced Actions Scraping only Scraping + Actions AI Features \u274c No \u2705 Built-in"},{"location":"why-xeepy/#vs-nitter","title":"vs. Nitter","text":"<p>Nitter instances are unreliable and frequently go down:</p> Aspect Nitter Xeepy Reliability \u274c Instances die often \u2705 Your own browser Authentication \u274c Can't log in \u2705 Full account access Actions \u274c Read only \u2705 Full automation Rate Limits Per-instance Your control"},{"location":"why-xeepy/#unique-features","title":"Unique Features","text":"<p>Xeepy isn't just a scraper\u2014it's a complete automation toolkit:</p>"},{"location":"why-xeepy/#unfollower-detection","title":"\ud83c\udfaf Unfollower Detection","text":"<p>The #1 requested feature that no API provides:</p> <pre><code>from xeepy import UnfollowerDetector\n\ndetector = UnfollowerDetector(storage, notifier)\nreport = await detector.detect(\"yourusername\")\n\nprint(f\"Lost followers: {report.unfollowers}\")\nprint(f\"New followers: {report.new_followers}\")\nprint(f\"Net change: {report.net_change}\")\n</code></pre>"},{"location":"why-xeepy/#advanced-analytics","title":"\ud83d\udcca Advanced Analytics","text":"<p>Built-in analytics that would cost $50+/month elsewhere:</p> <pre><code>from xeepy.analytics import BestTimeAnalyzer, AudienceInsights\n\n# Find YOUR optimal posting times\nanalyzer = BestTimeAnalyzer()\nschedule = await analyzer.analyze(\"yourusername\")\nprint(schedule.get_schedule_text())\n# \"Best time to post: Tuesday at 9:00 AM\"\n\n# Understand your audience\ninsights = AudienceInsights()\nreport = await insights.analyze(\"yourusername\")\nprint(f\"Top locations: {report.locations}\")\nprint(f\"Bot percentage: {report.likely_bots_percentage}%\")\n</code></pre>"},{"location":"why-xeepy/#ai-integration","title":"\ud83e\udd16 AI Integration","text":"<p>Native AI support for content and analysis:</p> <pre><code>from xeepy.ai import ContentGenerator, SentimentAnalyzer\n\n# Generate viral content\ngenerator = ContentGenerator(provider=\"openai\")\nthread = await generator.generate_thread(\n    topic=\"Python async tips\",\n    style=\"viral\",\n    num_tweets=10\n)\n\n# Analyze sentiment of replies\nanalyzer = SentimentAnalyzer()\nfor reply in replies:\n    sentiment = await analyzer.analyze(reply.text)\n    if sentiment.label == \"negative\":\n        print(f\"Hater detected: @{reply.author}\")\n</code></pre>"},{"location":"why-xeepy/#multi-channel-notifications","title":"\ud83d\udd14 Multi-Channel Notifications","text":"<p>Get alerts everywhere:</p> <pre><code>from xeepy.notifications import NotificationManager\n\nmanager = NotificationManager()\nmanager.add_channel(\"discord\", discord_webhook)\nmanager.add_channel(\"telegram\", telegram_bot)\nmanager.add_channel(\"email\", email_config)\nmanager.add_channel(\"slack\", slack_webhook)\n\n# All channels notified instantly\nawait manager.notify(\"\ud83d\udea8 Alert!\", \"Someone important followed you!\")\n</code></pre>"},{"location":"why-xeepy/#advanced-stealth","title":"\ud83d\udee1\ufe0f Advanced Stealth","text":"<p>Undetectable automation:</p> <pre><code>from xeepy import Xeepy\nfrom xeepy.core import BrowserConfig\n\nconfig = BrowserConfig(\n    stealth_mode=True,        # Anti-fingerprinting\n    rotate_user_agent=True,   # Random user agents\n    human_delays=True,        # Natural timing\n    proxy_rotation=True,      # IP rotation\n)\n\nasync with Xeepy(browser_config=config) as x:\n    # Scrape like a human\n    ...\n</code></pre>"},{"location":"why-xeepy/#who-uses-xeepy","title":"Who Uses Xeepy?","text":"<ul> <li> <p> Growth Hackers</p> <p>Automate follow/unfollow, find optimal posting times, track competitor growth.</p> </li> <li> <p> Data Scientists</p> <p>Scrape large datasets for research, sentiment analysis, network analysis.</p> </li> <li> <p> Marketing Teams</p> <p>Monitor brand mentions, track campaigns, generate engagement reports.</p> </li> <li> <p> AI Developers</p> <p>Build AI-powered bots, train models on Twitter data, automate content.</p> </li> <li> <p> Researchers</p> <p>Academic research, discourse analysis, misinformation tracking.</p> </li> <li> <p> Personal Users</p> <p>Clean up following list, track unfollowers, optimize posting schedule.</p> </li> </ul>"},{"location":"why-xeepy/#the-bottom-line","title":"The Bottom Line","text":"Need Solution Scrape tweet replies \u2705 Xeepy Detect unfollowers \u2705 Xeepy Automate follows \u2705 Xeepy Analyze engagement \u2705 Xeepy Generate AI content \u2705 Xeepy $0 monthly cost \u2705 Xeepy <p>Ready to get started?</p> <p>Quick Start Guide </p>"},{"location":"advanced/","title":"Advanced Topics","text":"<p>Deep dives into Xeepy internals and advanced usage patterns.</p>"},{"location":"advanced/#architecture","title":"Architecture","text":"<ul> <li> <p> System Architecture</p> <p>Understand how Xeepy components work together</p> </li> <li> <p> Plugin System</p> <p>Extend Xeepy with custom plugins</p> </li> <li> <p> Custom Scrapers</p> <p>Build your own scrapers</p> </li> <li> <p> Performance Tuning</p> <p>Optimize for speed and efficiency</p> </li> </ul>"},{"location":"advanced/#infrastructure","title":"Infrastructure","text":"<ul> <li> <p> Proxies &amp; Rotation</p> <p>Configure proxy rotation for stealth</p> </li> <li> <p> Stealth Mode</p> <p>Avoid detection and blocks</p> </li> <li> <p> Distributed Scraping</p> <p>Scale across multiple machines</p> </li> <li> <p> Docker Deployment</p> <p>Production container deployment</p> </li> </ul>"},{"location":"advanced/#development","title":"Development","text":"<ul> <li> <p> Testing Guide</p> <p>Test your Xeepy integrations</p> </li> <li> <p> Error Handling</p> <p>Graceful error recovery</p> </li> <li> <p> Webhooks &amp; Events</p> <p>Event-driven integrations</p> </li> <li> <p> REST API Server</p> <p>Run Xeepy as a service</p> </li> </ul>"},{"location":"advanced/#quick-links","title":"Quick Links","text":"Topic Description Difficulty Architecture System design overview Intermediate Custom Scrapers Build new scrapers Advanced Proxies Proxy configuration Intermediate Stealth Detection avoidance Advanced Distributed Multi-machine scaling Expert Performance Speed optimization Intermediate Docker Container deployment Intermediate Testing Testing strategies Intermediate Errors Error handling Beginner Plugins Plugin development Advanced"},{"location":"advanced/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         Xeepy                                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502  \u2502 Scrapers \u2502  \u2502 Actions  \u2502  \u2502 Monitor  \u2502  \u2502 Analytics\u2502        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2502       \u2502             \u2502             \u2502             \u2502               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502                    Core                            \u2502         \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502\n\u2502  \u2502  \u2502  Browser   \u2502  \u2502    Auth    \u2502  \u2502Rate Limiter\u2502  \u2502         \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502                          \u2502                                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502                    Storage                         \u2502         \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502\n\u2502  \u2502  \u2502 SQLite \u2502  \u2502  CSV   \u2502  \u2502  JSON  \u2502  \u2502  Excel \u2502  \u2502         \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"advanced/#key-concepts","title":"Key Concepts","text":""},{"location":"advanced/#browser-management","title":"Browser Management","text":"<p>Xeepy uses Playwright for browser automation:</p> <pre><code>from xeepy.core.browser import BrowserManager\n\nclass BrowserManager:\n    \"\"\"Manages browser lifecycle and page pool.\"\"\"\n\n    async def start(self) -&gt; None:\n        \"\"\"Launch browser with stealth configuration.\"\"\"\n\n    async def new_page(self) -&gt; Page:\n        \"\"\"Create new page with rate limiting.\"\"\"\n\n    async def close(self) -&gt; None:\n        \"\"\"Clean shutdown with session save.\"\"\"\n</code></pre>"},{"location":"advanced/#rate-limiting","title":"Rate Limiting","text":"<p>Intelligent rate limiting protects your account:</p> <pre><code>from xeepy.core.rate_limiter import RateLimiter\n\nclass RateLimiter:\n    \"\"\"Adaptive rate limiter with backoff.\"\"\"\n\n    def __init__(\n        self,\n        requests_per_minute: int = 20,\n        burst_limit: int = 5,\n        backoff_factor: float = 2.0\n    ): ...\n\n    async def wait(self) -&gt; None:\n        \"\"\"Wait for rate limit clearance.\"\"\"\n\n    def record_response(self, status: int) -&gt; None:\n        \"\"\"Adjust limits based on response.\"\"\"\n</code></pre>"},{"location":"advanced/#event-system","title":"Event System","text":"<p>Xeepy emits events for monitoring:</p> <pre><code>from xeepy.core.events import EventEmitter\n\nclass Xeepy(EventEmitter):\n    \"\"\"Emits events during operations.\"\"\"\n\n    # Events:\n    # - \"scrape:start\", \"scrape:complete\", \"scrape:error\"\n    # - \"action:start\", \"action:complete\", \"action:error\"\n    # - \"auth:login\", \"auth:logout\", \"auth:expired\"\n    # - \"rate_limit:warning\", \"rate_limit:hit\"\n\n# Subscribe to events\nx.on(\"scrape:complete\", lambda data: print(f\"Scraped {len(data)} items\"))\nx.on(\"rate_limit:warning\", lambda: print(\"Slowing down...\"))\n</code></pre>"},{"location":"advanced/#configuration-hierarchy","title":"Configuration Hierarchy","text":"<p>Configuration is loaded in this order (later overrides earlier):</p> <ol> <li>Defaults - Built-in defaults</li> <li>System config - <code>/etc/xeepy/config.toml</code></li> <li>User config - <code>~/.config/xeepy/config.toml</code></li> <li>Project config - <code>./xeepy.toml</code></li> <li>Environment variables - <code>XEEPY_*</code></li> <li>CLI arguments - <code>--option value</code></li> <li>Runtime - <code>x.config.setting = value</code></li> </ol>"},{"location":"advanced/#extension-points","title":"Extension Points","text":"<p>Xeepy is designed for extensibility:</p>"},{"location":"advanced/#custom-scrapers","title":"Custom Scrapers","text":"<pre><code>from xeepy.scrapers.base import BaseScraper\n\nclass MyScraper(BaseScraper):\n    \"\"\"Custom scraper implementation.\"\"\"\n\n    async def scrape(self, target: str, **kwargs) -&gt; ScrapeResult:\n        # Your implementation\n        pass\n</code></pre>"},{"location":"advanced/#custom-actions","title":"Custom Actions","text":"<pre><code>from xeepy.actions.base import BaseAction\n\nclass MyAction(BaseAction):\n    \"\"\"Custom action implementation.\"\"\"\n\n    async def execute(self, **kwargs) -&gt; ActionResult:\n        # Your implementation\n        pass\n</code></pre>"},{"location":"advanced/#custom-notifications","title":"Custom Notifications","text":"<pre><code>from xeepy.notifications.base import BaseNotifier\n\nclass MyNotifier(BaseNotifier):\n    \"\"\"Custom notification channel.\"\"\"\n\n    async def send(self, message: str, **kwargs) -&gt; bool:\n        # Your implementation\n        pass\n</code></pre>"},{"location":"advanced/#performance-characteristics","title":"Performance Characteristics","text":"Operation Typical Speed Memory Usage Profile scrape 1-2 sec ~50 MB 100 tweets 10-20 sec ~100 MB 1000 followers 60-120 sec ~200 MB Follow action 2-3 sec ~50 MB Like action 1-2 sec ~50 MB"},{"location":"advanced/#security-considerations","title":"Security Considerations","text":"<ul> <li>Sessions are stored encrypted by default</li> <li>Credentials never logged</li> <li>Rate limiting protects accounts</li> <li>Proxy support for IP rotation</li> <li>Stealth mode to avoid detection</li> </ul>"},{"location":"advanced/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture Deep Dive - Understand the internals</li> <li>Performance Tuning - Optimize your usage</li> <li>Stealth Mode - Avoid detection</li> <li>Distributed Scraping - Scale up</li> </ul>"},{"location":"advanced/api-server/","title":"Running XTools as REST API Service","text":"<p>Deploy XTools as a FastAPI REST service for programmatic access to X/Twitter automation.</p>"},{"location":"advanced/api-server/#quick-start","title":"Quick Start","text":"<pre><code>from fastapi import FastAPI, HTTPException, BackgroundTasks\nfrom pydantic import BaseModel\nfrom xtools import XTools\n\napp = FastAPI(title=\"XTools API\", version=\"1.0.0\")\nxtools_client = None\n\n@app.on_event(\"startup\")\nasync def startup():\n    global xtools_client\n    xtools_client = XTools(headless=True)\n    await xtools_client.auth.load_cookies(\"session.json\")\n\n@app.on_event(\"shutdown\")\nasync def shutdown():\n    if xtools_client:\n        await xtools_client.close()\n</code></pre>"},{"location":"advanced/api-server/#scraping-endpoints","title":"Scraping Endpoints","text":"<pre><code>class ScrapeRequest(BaseModel):\n    url: str = None\n    username: str = None\n    limit: int = 100\n\n@app.post(\"/api/scrape/replies\")\nasync def scrape_replies(request: ScrapeRequest):\n    \"\"\"Scrape replies from a tweet.\"\"\"\n    try:\n        replies = await xtools_client.scrape.replies(request.url, limit=request.limit)\n        return {\"status\": \"success\", \"data\": replies, \"count\": len(replies)}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/api/scrape/profile\")\nasync def scrape_profile(request: ScrapeRequest):\n    \"\"\"Get user profile information.\"\"\"\n    profile = await xtools_client.scrape.profile(request.username)\n    return {\"status\": \"success\", \"data\": profile.dict()}\n\n@app.post(\"/api/scrape/followers\")\nasync def scrape_followers(request: ScrapeRequest):\n    \"\"\"Get user's followers list.\"\"\"\n    followers = await xtools_client.scrape.followers(request.username, limit=request.limit)\n    return {\"status\": \"success\", \"data\": followers}\n</code></pre> <p>Use Background Tasks for Long Operations</p> <p>For operations that take longer than a few seconds, use FastAPI's BackgroundTasks.</p>"},{"location":"advanced/api-server/#action-endpoints","title":"Action Endpoints","text":"<pre><code>class FollowRequest(BaseModel):\n    username: str\n\nclass EngageRequest(BaseModel):\n    tweet_url: str\n    text: str = None\n\n@app.post(\"/api/actions/follow\")\nasync def follow_user(request: FollowRequest):\n    \"\"\"Follow a user.\"\"\"\n    await xtools_client.follow.user(request.username)\n    return {\"status\": \"success\", \"message\": f\"Followed @{request.username}\"}\n\n@app.post(\"/api/actions/like\")\nasync def like_tweet(request: EngageRequest):\n    \"\"\"Like a tweet.\"\"\"\n    await xtools_client.engage.like(request.tweet_url)\n    return {\"status\": \"success\", \"message\": \"Tweet liked\"}\n</code></pre>"},{"location":"advanced/api-server/#background-jobs","title":"Background Jobs","text":"<pre><code>from uuid import uuid4\njobs = {}\n\n@app.post(\"/api/jobs/unfollow-non-followers\")\nasync def start_unfollow_job(background_tasks: BackgroundTasks):\n    \"\"\"Start background unfollow job.\"\"\"\n    job_id = str(uuid4())\n    jobs[job_id] = {\"status\": \"running\", \"result\": None}\n    background_tasks.add_task(run_unfollow_job, job_id)\n    return {\"job_id\": job_id, \"status\": \"started\"}\n\nasync def run_unfollow_job(job_id: str):\n    try:\n        result = await xtools_client.unfollow.non_followers(max_unfollows=50)\n        jobs[job_id] = {\"status\": \"completed\", \"result\": result}\n    except Exception as e:\n        jobs[job_id] = {\"status\": \"failed\", \"result\": str(e)}\n\n@app.get(\"/api/jobs/{job_id}\")\nasync def get_job_status(job_id: str):\n    if job_id not in jobs:\n        raise HTTPException(status_code=404, detail=\"Job not found\")\n    return jobs[job_id]\n</code></pre> <p>Authentication Required</p> <p>Add API key authentication for production deployments: <pre><code>from fastapi.security import APIKeyHeader\napi_key = APIKeyHeader(name=\"X-API-Key\")\n</code></pre></p>"},{"location":"advanced/api-server/#running-the-server","title":"Running the Server","text":"<pre><code># Development\nuvicorn api:app --reload --port 8000\n\n# Production with gunicorn\ngunicorn api:app -w 4 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8000\n</code></pre>"},{"location":"advanced/api-server/#docker-deployment","title":"Docker Deployment","text":"<pre><code>FROM python:3.11-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt &amp;&amp; playwright install chromium --with-deps\nCOPY . .\nCMD [\"uvicorn\", \"api:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre> <p>API Documentation</p> <p>FastAPI auto-generates OpenAPI docs at <code>/docs</code> (Swagger UI) and <code>/redoc</code>.</p>"},{"location":"advanced/architecture/","title":"System Architecture","text":"<p>XTools follows a modular architecture designed for extensibility, performance, and maintainability.</p>"},{"location":"advanced/architecture/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      XTools Client                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 Scrapers\u2502  \u2502 Actions \u2502  \u2502 Monitor  \u2502  \u2502   AI Engine  \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502       \u2502            \u2502            \u2502               \u2502          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                    Core Layer                         \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  \u2502\n\u2502  \u2502  \u2502  Browser   \u2502 \u2502    Auth     \u2502 \u2502   Rate Limiter   \u2502 \u2502  \u2502\n\u2502  \u2502  \u2502  Manager   \u2502 \u2502   Manager   \u2502 \u2502                  \u2502 \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    Storage Layer                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n\u2502  \u2502  SQLite  \u2502  \u2502  Export  \u2502  \u2502   Cache   \u2502                 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"advanced/architecture/#core-components","title":"Core Components","text":""},{"location":"advanced/architecture/#browser-manager","title":"Browser Manager","text":"<p>The <code>BrowserManager</code> handles Playwright browser lifecycle and page management.</p> <pre><code>from xtools.core.browser import BrowserManager\n\nasync def browser_lifecycle_example():\n    \"\"\"Demonstrates browser manager usage.\"\"\"\n    browser = BrowserManager(\n        headless=True,\n        proxy=\"http://proxy:8080\",\n        user_agent=\"custom-agent\"\n    )\n\n    async with browser:\n        # Get a new page context\n        page = await browser.new_page()\n\n        # Navigate with retry logic\n        await browser.navigate(page, \"https://x.com\")\n\n        # Screenshot for debugging\n        await browser.screenshot(page, \"debug.png\")\n\n        # Close specific page\n        await browser.close_page(page)\n</code></pre>"},{"location":"advanced/architecture/#auth-manager","title":"Auth Manager","text":"<p>Handles session persistence, cookie management, and authentication state.</p> <pre><code>from xtools.core.auth import AuthManager\n\nasync def auth_example():\n    \"\"\"Authentication management.\"\"\"\n    auth = AuthManager(browser_manager)\n\n    # Load existing session\n    if await auth.load_cookies(\"session.json\"):\n        print(\"Session restored\")\n    else:\n        # Manual login required\n        await auth.login()\n        await auth.save_cookies(\"session.json\")\n\n    # Verify session is valid\n    if await auth.is_authenticated():\n        tokens = auth.get_auth_tokens()\n        print(f\"CSRF Token: {tokens['ct0']}\")\n</code></pre>"},{"location":"advanced/architecture/#rate-limiter","title":"Rate Limiter","text":"<p>Protects accounts with intelligent request throttling.</p> <pre><code>from xtools.core.rate_limiter import RateLimiter, RateLimitConfig\n\nasync def rate_limiter_example():\n    \"\"\"Rate limiting configuration.\"\"\"\n    config = RateLimitConfig(\n        requests_per_minute=30,\n        requests_per_hour=500,\n        burst_limit=5,\n        cooldown_seconds=60\n    )\n\n    limiter = RateLimiter(config)\n\n    async def make_request():\n        async with limiter.acquire(\"scrape\"):\n            # Request is rate-limited\n            await do_scrape()\n\n    # Check current usage\n    stats = limiter.get_stats()\n    print(f\"Requests this hour: {stats['hourly_count']}\")\n</code></pre> <p>Rate Limit Defaults</p> <p>XTools ships with conservative defaults to protect your account. Adjust based on your needs.</p>"},{"location":"advanced/architecture/#component-interaction","title":"Component Interaction","text":"<pre><code>from xtools import XTools\n\nasync def component_interaction():\n    \"\"\"Shows how components work together.\"\"\"\n    async with XTools() as x:\n        # Browser manager creates page\n        # Auth manager validates session\n        # Rate limiter controls request flow\n        # Scraper uses all three\n\n        replies = await x.scrape.replies(\n            \"https://x.com/user/status/123\",\n            limit=100\n        )\n\n        # Storage layer handles persistence\n        x.export.to_csv(replies, \"output.csv\")\n</code></pre> <p>Dependency Injection</p> <p>All components accept dependencies via constructor, enabling easy testing and customization.</p>"},{"location":"advanced/architecture/#event-system","title":"Event System","text":"<p>XTools uses an event-driven architecture for extensibility:</p> <pre><code>from xtools.core.events import EventBus\n\nasync def event_system_example():\n    \"\"\"Event-driven architecture.\"\"\"\n    bus = EventBus()\n\n    @bus.on(\"scrape.complete\")\n    async def on_scrape_complete(data):\n        print(f\"Scraped {len(data)} items\")\n\n    @bus.on(\"rate_limit.hit\")\n    async def on_rate_limit(endpoint):\n        print(f\"Rate limited on {endpoint}\")\n\n    # Events are emitted automatically by components\n    await bus.emit(\"scrape.complete\", replies)\n</code></pre> <p>Thread Safety</p> <p>The event bus is async-safe but not thread-safe. Use within a single event loop.</p>"},{"location":"advanced/custom-scrapers/","title":"Building Custom Scrapers","text":"<p>Extend XTools by creating custom scrapers that inherit from <code>BaseScraper</code>.</p>"},{"location":"advanced/custom-scrapers/#basescraper-overview","title":"BaseScraper Overview","text":"<p>All scrapers inherit from <code>BaseScraper</code>, which provides common functionality:</p> <pre><code>from xtools.scrapers.base import BaseScraper, ScrapeResult\nfrom xtools.models import Tweet, User\n\nclass BaseScraper:\n    \"\"\"Base class for all scrapers.\"\"\"\n\n    def __init__(self, browser_manager, rate_limiter=None):\n        self.browser = browser_manager\n        self.limiter = rate_limiter\n\n    async def scrape(self, **kwargs) -&gt; ScrapeResult:\n        \"\"\"Override this method in subclasses.\"\"\"\n        raise NotImplementedError\n\n    async def navigate(self, url: str):\n        \"\"\"Navigate with rate limiting.\"\"\"\n        if self.limiter:\n            await self.limiter.acquire(\"navigate\")\n        await self.browser.navigate(self.page, url)\n\n    async def scroll_and_collect(self, selector: str, limit: int):\n        \"\"\"Scroll page and collect elements.\"\"\"\n        # Built-in infinite scroll handling\n        pass\n</code></pre>"},{"location":"advanced/custom-scrapers/#creating-a-custom-scraper","title":"Creating a Custom Scraper","text":"<pre><code>from xtools.scrapers.base import BaseScraper, ScrapeResult\nfrom xtools.models import Tweet\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass BookmarkResult(ScrapeResult):\n    \"\"\"Result from bookmarks scraper.\"\"\"\n    bookmarks: List[Tweet]\n    cursor: str = None\n\nclass BookmarksScraper(BaseScraper):\n    \"\"\"Scrape user's bookmarked tweets.\"\"\"\n\n    async def scrape(\n        self,\n        limit: int = 100,\n        cursor: str = None\n    ) -&gt; BookmarkResult:\n        \"\"\"\n        Scrape bookmarked tweets.\n\n        Args:\n            limit: Maximum bookmarks to retrieve\n            cursor: Pagination cursor for continuation\n\n        Returns:\n            BookmarkResult with list of bookmarked tweets\n        \"\"\"\n        page = await self.browser.new_page()\n\n        try:\n            await self.navigate(\"https://x.com/i/bookmarks\")\n            await self._wait_for_content(page)\n\n            bookmarks = []\n            last_cursor = cursor\n\n            while len(bookmarks) &lt; limit:\n                # Extract tweets from current view\n                new_tweets = await self._extract_tweets(page)\n                bookmarks.extend(new_tweets)\n\n                # Scroll for more content\n                last_cursor = await self._scroll_and_wait(page)\n                if not last_cursor:\n                    break  # No more content\n\n            return BookmarkResult(\n                bookmarks=bookmarks[:limit],\n                cursor=last_cursor\n            )\n        finally:\n            await self.browser.close_page(page)\n\n    async def _extract_tweets(self, page) -&gt; List[Tweet]:\n        \"\"\"Extract tweet data from page.\"\"\"\n        tweets = []\n        elements = await page.query_selector_all('[data-testid=\"tweet\"]')\n\n        for el in elements:\n            tweet = await self._parse_tweet_element(el)\n            if tweet:\n                tweets.append(tweet)\n\n        return tweets\n\n    async def _parse_tweet_element(self, element) -&gt; Tweet:\n        \"\"\"Parse a tweet DOM element into Tweet model.\"\"\"\n        text = await element.query_selector('[data-testid=\"tweetText\"]')\n        user = await element.query_selector('[data-testid=\"User-Name\"]')\n\n        return Tweet(\n            text=await text.inner_text() if text else \"\",\n            author=await self._extract_username(user)\n        )\n</code></pre> <p>Error Handling</p> <p>Always wrap page operations in try/finally to ensure pages are closed.</p>"},{"location":"advanced/custom-scrapers/#registering-custom-scrapers","title":"Registering Custom Scrapers","text":"<pre><code>from xtools import XTools\n\nasync def use_custom_scraper():\n    \"\"\"Register and use custom scraper.\"\"\"\n    async with XTools() as x:\n        # Register scraper\n        x.scrape.register(\"bookmarks\", BookmarksScraper)\n\n        # Use like built-in scrapers\n        result = await x.scrape.bookmarks(limit=50)\n\n        for tweet in result.bookmarks:\n            print(f\"Bookmarked: {tweet.text[:50]}\")\n</code></pre>"},{"location":"advanced/custom-scrapers/#graphql-based-scraper","title":"GraphQL-Based Scraper","text":"<p>For higher rate limits, build scrapers using the GraphQL API:</p> <pre><code>from xtools.scrapers.base import BaseScraper, ScrapeResult\nfrom xtools.api.graphql import GraphQLClient\n\nclass GraphQLScraper(BaseScraper):\n    \"\"\"Base class for GraphQL-based scrapers.\"\"\"\n\n    def __init__(self, browser_manager, cookies: dict):\n        super().__init__(browser_manager)\n        self.gql = GraphQLClient(cookies=cookies)\n\n    async def close(self):\n        await self.gql.close()\n\nclass UserLikesScraper(GraphQLScraper):\n    \"\"\"Scrape tweets liked by a user via GraphQL.\"\"\"\n\n    QUERY_ID = \"eSSNbhECHHWWALkkQq-YTA\"\n\n    async def scrape(self, user_id: str, limit: int = 100):\n        \"\"\"Fetch liked tweets via GraphQL.\"\"\"\n        variables = {\n            \"userId\": user_id,\n            \"count\": min(limit, 100),\n            \"includePromotedContent\": False\n        }\n\n        response = await self.gql.query(\n            self.QUERY_ID,\n            \"Likes\",\n            variables\n        )\n\n        return self._parse_response(response)\n\n    def _parse_response(self, response: dict) -&gt; List[Tweet]:\n        \"\"\"Parse GraphQL response into Tweet models.\"\"\"\n        entries = response[\"data\"][\"user\"][\"result\"][\"timeline_v2\"][\"timeline\"][\"instructions\"]\n        # Parse tweet entries...\n        return tweets\n</code></pre> <p>GraphQL Endpoints</p> <p>X's GraphQL endpoints change frequently. Monitor for breaking changes.</p>"},{"location":"advanced/custom-scrapers/#testing-custom-scrapers","title":"Testing Custom Scrapers","text":"<pre><code>import pytest\nfrom unittest.mock import AsyncMock, MagicMock\n\n@pytest.fixture\ndef mock_browser():\n    browser = MagicMock()\n    browser.new_page = AsyncMock()\n    browser.navigate = AsyncMock()\n    return browser\n\n@pytest.mark.asyncio\nasync def test_bookmarks_scraper(mock_browser):\n    \"\"\"Test BookmarksScraper.\"\"\"\n    scraper = BookmarksScraper(mock_browser)\n\n    # Mock page content\n    mock_page = AsyncMock()\n    mock_browser.new_page.return_value = mock_page\n\n    result = await scraper.scrape(limit=10)\n\n    assert isinstance(result, BookmarkResult)\n    mock_browser.navigate.assert_called()\n</code></pre>"},{"location":"advanced/distributed/","title":"Distributed Scraping","text":"<p>Scale XTools across multiple machines for high-volume data collection.</p>"},{"location":"advanced/distributed/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Coordinator                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502 Task Queue  \u2502  \u2502   Results    \u2502  \u2502   Worker Manager  \u2502   \u2502\n\u2502  \u2502   (Redis)   \u2502  \u2502   Storage    \u2502  \u2502                   \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502               \u2502               \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n    \u2502 Worker 1\u2502    \u2502 Worker 2\u2502    \u2502 Worker 3\u2502\n    \u2502 (XTools)\u2502    \u2502 (XTools)\u2502    \u2502 (XTools)\u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"advanced/distributed/#task-queue-setup","title":"Task Queue Setup","text":"<p>Use Redis for distributed task coordination:</p> <pre><code>from xtools.distributed import TaskQueue, Task\nimport redis.asyncio as redis\n\n# Initialize task queue\nasync def setup_queue():\n    redis_client = await redis.from_url(\"redis://localhost:6379\")\n    queue = TaskQueue(redis_client, name=\"xtools-tasks\")\n    return queue\n\n# Producer: Add tasks to queue\nasync def enqueue_tasks():\n    queue = await setup_queue()\n\n    usernames = [\"user1\", \"user2\", \"user3\", ...]\n\n    for username in usernames:\n        task = Task(\n            type=\"scrape_followers\",\n            payload={\"username\": username, \"limit\": 1000},\n            priority=1\n        )\n        await queue.enqueue(task)\n\n    print(f\"Enqueued {len(usernames)} tasks\")\n</code></pre>"},{"location":"advanced/distributed/#worker-implementation","title":"Worker Implementation","text":"<pre><code>from xtools import XTools\nfrom xtools.distributed import Worker, TaskQueue\n\nclass ScraperWorker(Worker):\n    \"\"\"Distributed scraping worker.\"\"\"\n\n    def __init__(self, worker_id: str, queue: TaskQueue):\n        self.worker_id = worker_id\n        self.queue = queue\n        self.xtools = None\n\n    async def start(self):\n        \"\"\"Start processing tasks.\"\"\"\n        self.xtools = await XTools().__aenter__()\n\n        while True:\n            task = await self.queue.dequeue()\n            if task:\n                await self.process_task(task)\n\n    async def process_task(self, task):\n        \"\"\"Process a single task.\"\"\"\n        try:\n            if task.type == \"scrape_followers\":\n                result = await self.xtools.scrape.followers(\n                    task.payload[\"username\"],\n                    limit=task.payload[\"limit\"]\n                )\n                await self.queue.complete(task, result)\n\n            elif task.type == \"scrape_profile\":\n                result = await self.xtools.scrape.profile(\n                    task.payload[\"username\"]\n                )\n                await self.queue.complete(task, result)\n\n        except Exception as e:\n            await self.queue.fail(task, str(e))\n\n    async def stop(self):\n        if self.xtools:\n            await self.xtools.__aexit__(None, None, None)\n\n# Run worker\nasync def run_worker(worker_id: str):\n    queue = await setup_queue()\n    worker = ScraperWorker(worker_id, queue)\n    await worker.start()\n</code></pre> <p>Worker Scaling</p> <p>Run multiple workers on different machines with unique worker IDs.</p>"},{"location":"advanced/distributed/#coordinator-service","title":"Coordinator Service","text":"<pre><code>from xtools.distributed import Coordinator\nimport asyncio\n\nclass ScrapingCoordinator(Coordinator):\n    \"\"\"Manages distributed scraping tasks.\"\"\"\n\n    def __init__(self, queue: TaskQueue):\n        self.queue = queue\n        self.results = {}\n\n    async def scrape_many_profiles(self, usernames: list):\n        \"\"\"Distribute profile scraping across workers.\"\"\"\n        # Enqueue all tasks\n        task_ids = []\n        for username in usernames:\n            task = Task(\n                type=\"scrape_profile\",\n                payload={\"username\": username}\n            )\n            task_id = await self.queue.enqueue(task)\n            task_ids.append(task_id)\n\n        # Wait for all results\n        results = await self.queue.wait_for_results(task_ids)\n        return results\n\n    async def monitor_progress(self):\n        \"\"\"Monitor worker progress.\"\"\"\n        while True:\n            stats = await self.queue.get_stats()\n            print(f\"Pending: {stats['pending']}\")\n            print(f\"Processing: {stats['processing']}\")\n            print(f\"Completed: {stats['completed']}\")\n            print(f\"Failed: {stats['failed']}\")\n            await asyncio.sleep(5)\n</code></pre>"},{"location":"advanced/distributed/#result-aggregation","title":"Result Aggregation","text":"<pre><code>from xtools.distributed import ResultStore\nfrom xtools.storage import Database\n\nclass DistributedResultStore(ResultStore):\n    \"\"\"Store and aggregate results from workers.\"\"\"\n\n    def __init__(self, db_url: str):\n        self.db = Database(db_url)\n\n    async def store(self, task_id: str, result: dict):\n        \"\"\"Store result from worker.\"\"\"\n        await self.db.insert(\"results\", {\n            \"task_id\": task_id,\n            \"data\": result,\n            \"timestamp\": datetime.now()\n        })\n\n    async def aggregate(self, task_type: str):\n        \"\"\"Aggregate all results of a task type.\"\"\"\n        results = await self.db.query(\n            \"SELECT * FROM results WHERE task_type = ?\",\n            [task_type]\n        )\n        return [r[\"data\"] for r in results]\n</code></pre>"},{"location":"advanced/distributed/#configuration","title":"Configuration","text":"<pre><code># distributed.yaml\ncoordinator:\n  host: 0.0.0.0\n  port: 8000\n\nredis:\n  url: redis://redis:6379\n\nworkers:\n  count: 5\n  per_machine: 2\n\nqueue:\n  name: xtools-tasks\n  max_retries: 3\n  retry_delay: 60\n\nstorage:\n  type: postgresql\n  url: postgresql://user:pass@db:5432/xtools\n</code></pre> <pre><code>from xtools.distributed import load_config, start_cluster\n\nconfig = load_config(\"distributed.yaml\")\n\n# Start coordinator\nawait start_cluster(config, role=\"coordinator\")\n\n# Start worker (on different machine)\nawait start_cluster(config, role=\"worker\")\n</code></pre>"},{"location":"advanced/distributed/#load-balancing","title":"Load Balancing","text":"<pre><code>from xtools.distributed import LoadBalancer\n\nbalancer = LoadBalancer(\n    strategy=\"least_loaded\",  # or \"round_robin\", \"random\"\n    health_check_interval=30\n)\n\n# Register workers\nbalancer.register_worker(\"worker-1\", \"http://worker1:8000\")\nbalancer.register_worker(\"worker-2\", \"http://worker2:8000\")\n\n# Distribute tasks\nasync def distribute_task(task):\n    worker = await balancer.get_worker()\n    await worker.submit(task)\n</code></pre> <p>Network Partitions</p> <p>Handle network failures gracefully with retries and task reassignment.</p>"},{"location":"advanced/distributed/#monitoring-dashboard","title":"Monitoring Dashboard","text":"<pre><code>from xtools.distributed import Dashboard\nfrom fastapi import FastAPI\n\napp = FastAPI()\ndashboard = Dashboard(queue, result_store)\n\n@app.get(\"/stats\")\nasync def get_stats():\n    return {\n        \"workers\": await dashboard.worker_stats(),\n        \"tasks\": await dashboard.task_stats(),\n        \"throughput\": await dashboard.throughput_stats()\n    }\n\n@app.get(\"/workers\")\nasync def list_workers():\n    return await dashboard.list_workers()\n\n# Run: uvicorn dashboard:app --host 0.0.0.0 --port 8080\n</code></pre>"},{"location":"advanced/distributed/#fault-tolerance","title":"Fault Tolerance","text":"<pre><code>from xtools.distributed import FaultTolerantQueue\n\nqueue = FaultTolerantQueue(\n    redis_url=\"redis://localhost:6379\",\n    max_retries=3,\n    retry_backoff=\"exponential\",\n    dead_letter_queue=\"xtools-dlq\"\n)\n\n# Failed tasks go to dead letter queue after max retries\nasync def process_dead_letters():\n    dlq = await queue.get_dead_letters()\n    for task in dlq:\n        print(f\"Failed task: {task.id}, Error: {task.error}\")\n        # Manual intervention or different processing\n</code></pre> <p>Idempotency</p> <p>Design tasks to be idempotent\u2014safe to retry without side effects.</p>"},{"location":"advanced/docker/","title":"Docker Deployment","text":"<p>Deploy XTools in containers for consistent, reproducible environments.</p>"},{"location":"advanced/docker/#basic-dockerfile","title":"Basic Dockerfile","text":"<pre><code># Dockerfile\nFROM python:3.11-slim\n\n# Install system dependencies for Playwright\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    wget \\\n    gnupg \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install Playwright browsers\nRUN pip install playwright &amp;&amp; playwright install chromium --with-deps\n\n# Set working directory\nWORKDIR /app\n\n# Copy requirements and install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Run as non-root user\nRUN useradd -m xtools &amp;&amp; chown -R xtools:xtools /app\nUSER xtools\n\n# Default command\nCMD [\"python\", \"main.py\"]\n</code></pre>"},{"location":"advanced/docker/#docker-compose-setup","title":"Docker Compose Setup","text":"<pre><code># docker-compose.yml\nversion: '3.8'\n\nservices:\n  xtools:\n    build: .\n    environment:\n      - XTOOLS_HEADLESS=true\n      - REDIS_URL=redis://redis:6379\n    volumes:\n      - ./data:/app/data\n      - ./cookies:/app/cookies\n    depends_on:\n      - redis\n\n  redis:\n    image: redis:7-alpine\n    volumes:\n      - redis_data:/data\n\n  worker:\n    build: .\n    command: python worker.py\n    environment:\n      - XTOOLS_HEADLESS=true\n      - REDIS_URL=redis://redis:6379\n    deploy:\n      replicas: 3\n    depends_on:\n      - redis\n\nvolumes:\n  redis_data:\n</code></pre> <p>Headless Mode</p> <p>Always use headless mode in containers\u2014there's no display available.</p>"},{"location":"advanced/docker/#optimized-production-dockerfile","title":"Optimized Production Dockerfile","text":"<pre><code># Dockerfile.prod\nFROM python:3.11-slim as builder\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip wheel --no-cache-dir --no-deps --wheel-dir /app/wheels -r requirements.txt\n\n# Production image\nFROM python:3.11-slim\n\n# Install Playwright dependencies only\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\n    libnss3 libnspr4 libatk1.0-0 libatk-bridge2.0-0 \\\n    libcups2 libdrm2 libxkbcommon0 libxcomposite1 \\\n    libxdamage1 libxfixes3 libxrandr2 libgbm1 libasound2 \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nWORKDIR /app\n\n# Copy wheels and install\nCOPY --from=builder /app/wheels /wheels\nRUN pip install --no-cache /wheels/*\n\n# Install Playwright browsers\nRUN playwright install chromium\n\n# Copy application\nCOPY . .\n\n# Security: non-root user\nRUN useradd -m -u 1000 xtools &amp;&amp; chown -R xtools:xtools /app\nUSER xtools\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n    CMD python -c \"import xtools; print('healthy')\" || exit 1\n\nCMD [\"python\", \"main.py\"]\n</code></pre>"},{"location":"advanced/docker/#multi-architecture-build","title":"Multi-Architecture Build","text":"<pre><code># Build for multiple architectures\ndocker buildx build --platform linux/amd64,linux/arm64 \\\n    -t myregistry/xtools:latest \\\n    --push .\n</code></pre>"},{"location":"advanced/docker/#environment-configuration","title":"Environment Configuration","text":"<pre><code># config.py\nimport os\nfrom xtools import XTools\n\nasync def create_xtools():\n    \"\"\"Create XTools instance from environment.\"\"\"\n    return XTools(\n        headless=os.getenv(\"XTOOLS_HEADLESS\", \"true\").lower() == \"true\",\n        proxy=os.getenv(\"XTOOLS_PROXY\"),\n        cookies_path=os.getenv(\"XTOOLS_COOKIES\", \"/app/cookies/session.json\"),\n    )\n\n# main.py\nasync def main():\n    async with await create_xtools() as x:\n        await x.auth.load_cookies(os.getenv(\"XTOOLS_COOKIES\"))\n        # Your scraping logic here\n</code></pre>"},{"location":"advanced/docker/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<pre><code># k8s/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: xtools-worker\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: xtools-worker\n  template:\n    metadata:\n      labels:\n        app: xtools-worker\n    spec:\n      containers:\n      - name: worker\n        image: myregistry/xtools:latest\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n        env:\n        - name: REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: xtools-secrets\n              key: redis-url\n        volumeMounts:\n        - name: cookies\n          mountPath: /app/cookies\n      volumes:\n      - name: cookies\n        secret:\n          secretName: xtools-cookies\n</code></pre> <p>Resource Limits</p> <p>Playwright/Chromium is memory-intensive. Allocate at least 512Mi per container.</p>"},{"location":"advanced/docker/#docker-volume-management","title":"Docker Volume Management","text":"<pre><code># Persist cookies and data across container restarts\nasync def setup_persistence():\n    \"\"\"Setup persistent storage in Docker.\"\"\"\n    from pathlib import Path\n\n    data_dir = Path(\"/app/data\")\n    cookies_dir = Path(\"/app/cookies\")\n\n    data_dir.mkdir(exist_ok=True)\n    cookies_dir.mkdir(exist_ok=True)\n\n    async with XTools() as x:\n        cookies_path = cookies_dir / \"session.json\"\n\n        if cookies_path.exists():\n            await x.auth.load_cookies(str(cookies_path))\n\n        # Perform operations...\n\n        # Save session for next run\n        await x.auth.save_cookies(str(cookies_path))\n</code></pre>"},{"location":"advanced/docker/#logging-configuration","title":"Logging Configuration","text":"<pre><code># logging_config.py\nimport logging\nimport sys\n\ndef setup_docker_logging():\n    \"\"\"Configure logging for Docker (stdout/stderr).\"\"\"\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.StreamHandler(sys.stdout)\n        ]\n    )\n\n    # JSON logging for log aggregation\n    try:\n        import json_logging\n        json_logging.init_non_web()\n    except ImportError:\n        pass\n\nsetup_docker_logging()\n</code></pre>"},{"location":"advanced/docker/#cicd-pipeline","title":"CI/CD Pipeline","text":"<pre><code># .github/workflows/docker.yml\nname: Docker Build\n\non:\n  push:\n    branches: [main]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: .\n          push: true\n          tags: ghcr.io/${{ github.repository }}:latest\n          cache-from: type=gha\n          cache-to: type=gha,mode=max\n</code></pre>"},{"location":"advanced/docker/#health-checks-and-monitoring","title":"Health Checks and Monitoring","text":"<pre><code># health.py\nfrom fastapi import FastAPI\nfrom xtools import XTools\n\napp = FastAPI()\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Docker health check endpoint.\"\"\"\n    try:\n        async with XTools(headless=True) as x:\n            # Quick connectivity test\n            await x.browser.test_connection()\n        return {\"status\": \"healthy\"}\n    except Exception as e:\n        return {\"status\": \"unhealthy\", \"error\": str(e)}\n\n@app.get(\"/ready\")\nasync def readiness_check():\n    \"\"\"Kubernetes readiness probe.\"\"\"\n    return {\"status\": \"ready\"}\n</code></pre> <p>Graceful Shutdown</p> <p>Handle SIGTERM for clean browser shutdown in containers.</p>"},{"location":"advanced/errors/","title":"Error Handling and Recovery","text":"<p>Learn how to handle errors gracefully and implement recovery patterns in XTools automation.</p>"},{"location":"advanced/errors/#exception-hierarchy","title":"Exception Hierarchy","text":"<p>XTools provides structured exceptions for different failure scenarios:</p> <pre><code>from xtools.core.exceptions import (\n    XToolsError,           # Base exception\n    AuthenticationError,   # Login/session failures\n    RateLimitError,        # Rate limit exceeded\n    NetworkError,          # Connection issues\n    ElementNotFoundError,  # DOM element missing\n    AccountSuspendedError, # Account restrictions\n)\n</code></pre>"},{"location":"advanced/errors/#basic-error-handling","title":"Basic Error Handling","text":"<pre><code>from xtools import XTools\nfrom xtools.core.exceptions import RateLimitError, AuthenticationError\n\nasync def safe_scrape(tweet_url: str):\n    \"\"\"Scrape with proper error handling.\"\"\"\n    try:\n        async with XTools() as x:\n            await x.auth.load_cookies(\"session.json\")\n            replies = await x.scrape.replies(tweet_url)\n            return replies\n\n    except AuthenticationError:\n        print(\"Session expired - please re-authenticate\")\n        return None\n\n    except RateLimitError as e:\n        print(f\"Rate limited. Retry after {e.retry_after} seconds\")\n        return None\n</code></pre> <p>Always Handle Authentication Errors</p> <p>Session cookies can expire unexpectedly. Always wrap authenticated operations in try/except blocks.</p>"},{"location":"advanced/errors/#retry-pattern-with-exponential-backoff","title":"Retry Pattern with Exponential Backoff","text":"<pre><code>import asyncio\nfrom xtools.core.exceptions import NetworkError, RateLimitError\n\nasync def retry_with_backoff(coro_func, max_retries: int = 3, base_delay: float = 1.0):\n    \"\"\"Execute coroutine with exponential backoff on failure.\"\"\"\n    last_exception = None\n\n    for attempt in range(max_retries):\n        try:\n            return await coro_func()\n\n        except RateLimitError as e:\n            delay = e.retry_after or (base_delay * (2 ** attempt))\n            print(f\"Rate limited. Waiting {delay}s...\")\n            await asyncio.sleep(delay)\n            last_exception = e\n\n        except NetworkError as e:\n            delay = base_delay * (2 ** attempt)\n            print(f\"Network error. Retry {attempt + 1}/{max_retries} in {delay}s\")\n            await asyncio.sleep(delay)\n            last_exception = e\n\n    raise last_exception\n\n# Usage\nasync def main():\n    async with XTools() as x:\n        replies = await retry_with_backoff(\n            lambda: x.scrape.replies(\"https://x.com/user/status/123\")\n        )\n</code></pre> <p>Use tenacity Library</p> <p>For production, consider the <code>tenacity</code> library for more sophisticated retry logic: <pre><code>from tenacity import retry, stop_after_attempt, wait_exponential\n</code></pre></p>"},{"location":"advanced/errors/#circuit-breaker-pattern","title":"Circuit Breaker Pattern","text":"<pre><code>import time\nfrom dataclasses import dataclass\n\n@dataclass\nclass CircuitBreaker:\n    failure_threshold: int = 5\n    recovery_timeout: float = 60.0\n    _failures: int = 0\n    _last_failure: float = 0\n    _state: str = \"closed\"\n\n    async def call(self, coro):\n        \"\"\"Execute with circuit breaker protection.\"\"\"\n        if self._state == \"open\":\n            if time.time() - self._last_failure &gt; self.recovery_timeout:\n                self._state = \"half-open\"\n            else:\n                raise Exception(\"Circuit breaker is open\")\n\n        try:\n            result = await coro\n            self._failures = 0\n            self._state = \"closed\"\n            return result\n        except Exception as e:\n            self._failures += 1\n            self._last_failure = time.time()\n            if self._failures &gt;= self.failure_threshold:\n                self._state = \"open\"\n            raise e\n</code></pre> <p>Log All Errors</p> <p>Always log errors for debugging, even when handled gracefully: <pre><code>import logging\nlogger = logging.getLogger(__name__)\nlogger.exception(\"Operation failed\", exc_info=e)\n</code></pre></p>"},{"location":"advanced/errors/#graceful-degradation","title":"Graceful Degradation","text":"<pre><code>async def scrape_with_fallback(username: str):\n    \"\"\"Try multiple methods, gracefully degrade on failure.\"\"\"\n    async with XTools() as x:\n        try:\n            return await x.api.graphql.get_user(username)\n        except Exception:\n            pass\n\n        try:\n            return await x.scrape.profile(username)\n        except Exception:\n            pass\n\n        return await x.storage.get_cached_profile(username)\n</code></pre>"},{"location":"advanced/multi-account/","title":"Multi-Account Management","text":"<p>Manage multiple X/Twitter accounts safely with XTools session isolation.</p>"},{"location":"advanced/multi-account/#account-manager-setup","title":"Account Manager Setup","text":"<pre><code>from dataclasses import dataclass\nfrom xtools import XTools\n\n@dataclass\nclass Account:\n    username: str\n    cookies_path: str\n    proxy: str = None\n    active: bool = True\n\nclass AccountManager:\n    def __init__(self, accounts: list[Account]):\n        self.accounts = {acc.username: acc for acc in accounts}\n        self.clients: dict[str, XTools] = {}\n\n    async def get_client(self, username: str) -&gt; XTools:\n        if username not in self.clients:\n            account = self.accounts[username]\n            client = XTools(headless=True, proxy=account.proxy)\n            await client.auth.load_cookies(account.cookies_path)\n            self.clients[username] = client\n        return self.clients[username]\n\n    async def close_all(self):\n        for client in self.clients.values():\n            await client.close()\n        self.clients.clear()\n</code></pre> <p>Use Separate Proxies</p> <p>Each account should use a different proxy/IP to avoid association and mass bans.</p>"},{"location":"advanced/multi-account/#configuration-file","title":"Configuration File","text":"<pre><code># accounts.yaml\naccounts:\n  - username: \"account1\"\n    cookies_path: \"sessions/account1.json\"\n    proxy: \"http://proxy1:8080\"\n    tags: [\"main\", \"engagement\"]\n  - username: \"account2\"\n    cookies_path: \"sessions/account2.json\"\n    proxy: \"http://proxy2:8080\"\n    tags: [\"scraping\"]\n</code></pre> <pre><code>import yaml\n\ndef load_accounts(config_path: str) -&gt; list[Account]:\n    with open(config_path) as f:\n        config = yaml.safe_load(f)\n    return [Account(**acc) for acc in config[\"accounts\"]]\n\naccounts = load_accounts(\"accounts.yaml\")\nmanager = AccountManager(accounts)\n</code></pre>"},{"location":"advanced/multi-account/#round-robin-operations","title":"Round-Robin Operations","text":"<pre><code>from itertools import cycle\n\nasync def distributed_scrape(usernames: list[str]):\n    \"\"\"Distribute scraping across multiple accounts.\"\"\"\n    accounts = load_accounts(\"accounts.yaml\")\n    scraping_accounts = [a for a in accounts if \"scraping\" in a.get(\"tags\", [])]\n    account_cycle = cycle(scraping_accounts)\n\n    results = []\n    for username in usernames:\n        account = next(account_cycle)\n        async with XTools(proxy=account.proxy) as x:\n            await x.auth.load_cookies(account.cookies_path)\n            profile = await x.scrape.profile(username)\n            results.append(profile)\n        await asyncio.sleep(2)\n    return results\n</code></pre> <p>Tag Accounts by Purpose</p> <p>Use tags to separate accounts for different tasks (scraping, engagement, posting).</p>"},{"location":"advanced/multi-account/#account-health-monitoring","title":"Account Health Monitoring","text":"<pre><code>from datetime import datetime\n\n@dataclass\nclass AccountHealth:\n    username: str\n    is_active: bool\n    is_suspended: bool\n    last_checked: datetime\n\nasync def check_account_health(account: Account) -&gt; AccountHealth:\n    try:\n        async with XTools(proxy=account.proxy) as x:\n            await x.auth.load_cookies(account.cookies_path)\n            await x.scrape.profile(account.username)\n            return AccountHealth(account.username, True, False, datetime.now())\n    except Exception as e:\n        return AccountHealth(\n            account.username, False, \"suspended\" in str(e).lower(), datetime.now()\n        )\n\nasync def monitor_all_accounts():\n    accounts = load_accounts(\"accounts.yaml\")\n    health_reports = await asyncio.gather(\n        *[check_account_health(acc) for acc in accounts]\n    )\n    for report in health_reports:\n        status = \"\ud83d\udfe2\" if report.is_active else \"\ud83d\udd34\"\n        print(f\"{status} @{report.username}\")\n</code></pre> <p>Suspended Account Detection</p> <p>Regularly check account health and remove suspended accounts from rotation.</p>"},{"location":"advanced/multi-account/#concurrent-multi-account-operations","title":"Concurrent Multi-Account Operations","text":"<pre><code>async def parallel_engagement(tweet_url: str, accounts: list[Account]):\n    \"\"\"Like a tweet from multiple accounts with staggered timing.\"\"\"\n    async def like_from_account(account: Account, delay: int):\n        await asyncio.sleep(delay)\n        async with XTools(proxy=account.proxy) as x:\n            await x.auth.load_cookies(account.cookies_path)\n            await x.engage.like(tweet_url)\n        return account.username\n\n    tasks = [like_from_account(acc, i * 5) for i, acc in enumerate(accounts)]\n    return await asyncio.gather(*tasks, return_exceptions=True)\n</code></pre>"},{"location":"advanced/multi-account/#session-persistence","title":"Session Persistence","text":"<pre><code>async def refresh_all_sessions():\n    \"\"\"Refresh and save sessions for all accounts.\"\"\"\n    accounts = load_accounts(\"accounts.yaml\")\n    for account in accounts:\n        try:\n            async with XTools(proxy=account.proxy) as x:\n                await x.auth.load_cookies(account.cookies_path)\n                await x.scrape.profile(account.username)\n                await x.auth.save_cookies(account.cookies_path)\n                print(f\"\u2713 Refreshed: @{account.username}\")\n        except Exception as e:\n            print(f\"\u2717 Failed: @{account.username} - {e}\")\n</code></pre>"},{"location":"advanced/performance/","title":"Performance Tuning","text":"<p>Optimize XTools for maximum throughput while respecting rate limits.</p>"},{"location":"advanced/performance/#caching-strategy","title":"Caching Strategy","text":"<p>XTools includes a multi-level caching system to reduce redundant requests.</p> <pre><code>from xtools.storage.cache import Cache, CacheConfig\n\n# Configure caching\ncache_config = CacheConfig(\n    backend=\"sqlite\",  # or \"redis\", \"memory\"\n    ttl_seconds=3600,  # 1 hour default TTL\n    max_size_mb=100,\n    compression=True\n)\n\ncache = Cache(cache_config)\n\n# Manual cache usage\nasync def cached_profile_fetch(username: str):\n    \"\"\"Fetch profile with caching.\"\"\"\n    cache_key = f\"profile:{username}\"\n\n    # Check cache first\n    cached = await cache.get(cache_key)\n    if cached:\n        return cached\n\n    # Fetch fresh data\n    async with XTools() as x:\n        profile = await x.scrape.profile(username)\n\n    # Store in cache\n    await cache.set(cache_key, profile, ttl=7200)\n    return profile\n</code></pre> <p>Cache Invalidation</p> <p>Cache is automatically invalidated when actions modify data (follow, like, etc).</p>"},{"location":"advanced/performance/#parallel-execution","title":"Parallel Execution","text":"<p>Execute independent operations concurrently using <code>asyncio.gather</code>:</p> <pre><code>import asyncio\nfrom xtools import XTools\n\nasync def parallel_scraping():\n    \"\"\"Scrape multiple profiles in parallel.\"\"\"\n    usernames = [\"user1\", \"user2\", \"user3\", \"user4\", \"user5\"]\n\n    async with XTools() as x:\n        # Parallel profile scraping\n        tasks = [\n            x.scrape.profile(username)\n            for username in usernames\n        ]\n        profiles = await asyncio.gather(*tasks)\n\n        return profiles\n\nasync def parallel_with_semaphore():\n    \"\"\"Limit concurrency with semaphore.\"\"\"\n    usernames = [\"user\" + str(i) for i in range(100)]\n    semaphore = asyncio.Semaphore(5)  # Max 5 concurrent\n\n    async def fetch_with_limit(username):\n        async with semaphore:\n            async with XTools() as x:\n                return await x.scrape.profile(username)\n\n    tasks = [fetch_with_limit(u) for u in usernames]\n    return await asyncio.gather(*tasks)\n</code></pre> <p>Concurrency Limits</p> <p>Too many parallel requests can trigger rate limits. Use semaphores wisely.</p>"},{"location":"advanced/performance/#connection-pooling","title":"Connection Pooling","text":"<p>Reuse browser contexts for better performance:</p> <pre><code>from xtools.core.browser import BrowserPool\n\nasync def connection_pooling():\n    \"\"\"Use browser connection pool.\"\"\"\n    pool = BrowserPool(\n        size=5,           # Pool size\n        headless=True,\n        recycle_after=100  # Recycle after N uses\n    )\n\n    async with pool:\n        # Get context from pool\n        async with pool.acquire() as browser:\n            page = await browser.new_page()\n            # Use page...\n        # Context returned to pool\n\n# With XTools\nasync def xtools_with_pool():\n    pool = BrowserPool(size=3)\n\n    async with XTools(browser_pool=pool) as x:\n        # XTools uses pool internally\n        results = await x.scrape.replies(url)\n</code></pre>"},{"location":"advanced/performance/#batch-operations","title":"Batch Operations","text":"<p>Process items in batches for efficiency:</p> <pre><code>from xtools import XTools\nfrom xtools.utils import batch_processor\n\nasync def batch_follow_users():\n    \"\"\"Follow users in batches.\"\"\"\n    users_to_follow = [\"user\" + str(i) for i in range(500)]\n\n    async with XTools() as x:\n        results = await batch_processor(\n            items=users_to_follow,\n            processor=x.follow.user,\n            batch_size=10,\n            delay_between_batches=30,  # seconds\n            on_progress=lambda done, total: print(f\"{done}/{total}\")\n        )\n\n        print(f\"Followed: {results.success_count}\")\n        print(f\"Failed: {results.failure_count}\")\n</code></pre>"},{"location":"advanced/performance/#memory-optimization","title":"Memory Optimization","text":"<p>Handle large datasets without exhausting memory:</p> <pre><code>from xtools import XTools\n\nasync def stream_large_dataset():\n    \"\"\"Stream results instead of loading all in memory.\"\"\"\n    async with XTools() as x:\n        # Use async generator for large result sets\n        async for tweet in x.scrape.tweets_stream(\n            \"username\",\n            limit=10000\n        ):\n            # Process one at a time\n            process_tweet(tweet)\n\n            # Optionally write to file incrementally\n            append_to_csv(tweet, \"output.csv\")\n\nasync def chunked_export():\n    \"\"\"Export large datasets in chunks.\"\"\"\n    async with XTools() as x:\n        chunk_size = 1000\n        offset = 0\n\n        while True:\n            tweets = await x.scrape.tweets(\n                \"username\",\n                limit=chunk_size,\n                offset=offset\n            )\n\n            if not tweets:\n                break\n\n            x.export.to_csv(\n                tweets,\n                f\"tweets_{offset}.csv\"\n            )\n            offset += chunk_size\n</code></pre> <p>Generator Pattern</p> <p>Use async generators (<code>async for</code>) when processing thousands of items.</p>"},{"location":"advanced/performance/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>from xtools.monitoring import PerformanceMonitor\nimport time\n\nasync def monitored_scraping():\n    \"\"\"Monitor scraping performance.\"\"\"\n    monitor = PerformanceMonitor()\n\n    async with XTools() as x:\n        with monitor.track(\"profile_scrape\"):\n            profile = await x.scrape.profile(\"username\")\n\n        with monitor.track(\"followers_scrape\"):\n            followers = await x.scrape.followers(\"username\", limit=1000)\n\n    # Get metrics\n    stats = monitor.get_stats()\n    print(f\"Profile scrape: {stats['profile_scrape']['avg_ms']}ms avg\")\n    print(f\"Followers scrape: {stats['followers_scrape']['avg_ms']}ms avg\")\n\n    # Export metrics\n    monitor.export_prometheus(\"metrics.txt\")\n</code></pre>"},{"location":"advanced/performance/#configuration-presets","title":"Configuration Presets","text":"<pre><code>from xtools import XTools\nfrom xtools.config import PerformancePreset\n\n# High-throughput preset\nasync with XTools(preset=PerformancePreset.HIGH_THROUGHPUT) as x:\n    # Aggressive caching, parallel execution\n    pass\n\n# Conservative preset (safer for accounts)\nasync with XTools(preset=PerformancePreset.CONSERVATIVE) as x:\n    # Slower, more human-like behavior\n    pass\n\n# Custom configuration\nconfig = {\n    \"cache_ttl\": 1800,\n    \"max_concurrent\": 3,\n    \"request_delay\": (1, 3),\n    \"retry_attempts\": 3\n}\nasync with XTools(config=config) as x:\n    pass\n</code></pre>"},{"location":"advanced/plugins/","title":"Plugin System","text":"<p>XTools provides a powerful plugin system for extending functionality without modifying core code.</p>"},{"location":"advanced/plugins/#plugin-architecture","title":"Plugin Architecture","text":"<p>Plugins can hook into various lifecycle events and extend core functionality.</p> <pre><code>from xtools.plugins import Plugin, hook\n\nclass MyPlugin(Plugin):\n    \"\"\"Custom plugin example.\"\"\"\n\n    name = \"my-plugin\"\n    version = \"1.0.0\"\n\n    async def on_load(self, xtools):\n        \"\"\"Called when plugin is loaded.\"\"\"\n        self.xtools = xtools\n        print(f\"Plugin {self.name} loaded\")\n\n    async def on_unload(self):\n        \"\"\"Called when plugin is unloaded.\"\"\"\n        print(f\"Plugin {self.name} unloaded\")\n\n    @hook(\"before_scrape\")\n    async def before_scrape(self, url: str, **kwargs):\n        \"\"\"Hook called before any scrape operation.\"\"\"\n        print(f\"About to scrape: {url}\")\n        return url, kwargs  # Can modify parameters\n\n    @hook(\"after_scrape\")\n    async def after_scrape(self, results: list):\n        \"\"\"Hook called after scrape completes.\"\"\"\n        print(f\"Scraped {len(results)} items\")\n        return results  # Can modify results\n</code></pre>"},{"location":"advanced/plugins/#registering-plugins","title":"Registering Plugins","text":"<pre><code>from xtools import XTools\nfrom my_plugins import MyPlugin, AnalyticsPlugin\n\nasync def register_plugins():\n    \"\"\"Register plugins with XTools.\"\"\"\n    async with XTools() as x:\n        # Register single plugin\n        await x.plugins.register(MyPlugin())\n\n        # Register multiple plugins\n        await x.plugins.register_all([\n            MyPlugin(),\n            AnalyticsPlugin(),\n        ])\n\n        # List loaded plugins\n        for plugin in x.plugins.list():\n            print(f\"{plugin.name} v{plugin.version}\")\n</code></pre> <p>Plugin Loading Order</p> <p>Plugins are loaded in registration order. Hooks execute in the same order.</p>"},{"location":"advanced/plugins/#available-hooks","title":"Available Hooks","text":"Hook Name Arguments Description <code>before_scrape</code> <code>url, **kwargs</code> Before any scrape operation <code>after_scrape</code> <code>results</code> After scrape completes <code>before_action</code> <code>action_name, **kwargs</code> Before follow/like/etc <code>after_action</code> <code>action_name, result</code> After action completes <code>on_error</code> <code>error, context</code> When an error occurs <code>on_rate_limit</code> <code>endpoint, wait_time</code> When rate limited"},{"location":"advanced/plugins/#building-a-metrics-plugin","title":"Building a Metrics Plugin","text":"<pre><code>from xtools.plugins import Plugin, hook\nfrom datetime import datetime\nimport json\n\nclass MetricsPlugin(Plugin):\n    \"\"\"Collect and export scraping metrics.\"\"\"\n\n    name = \"metrics\"\n    version = \"1.0.0\"\n\n    def __init__(self):\n        self.metrics = {\n            \"scrapes\": 0,\n            \"items_collected\": 0,\n            \"errors\": 0,\n            \"start_time\": None\n        }\n\n    async def on_load(self, xtools):\n        self.metrics[\"start_time\"] = datetime.now().isoformat()\n\n    @hook(\"after_scrape\")\n    async def track_scrape(self, results: list):\n        self.metrics[\"scrapes\"] += 1\n        self.metrics[\"items_collected\"] += len(results)\n        return results\n\n    @hook(\"on_error\")\n    async def track_error(self, error, context):\n        self.metrics[\"errors\"] += 1\n\n    async def export_metrics(self, filepath: str):\n        \"\"\"Export metrics to JSON file.\"\"\"\n        with open(filepath, \"w\") as f:\n            json.dump(self.metrics, f, indent=2)\n</code></pre>"},{"location":"advanced/plugins/#plugin-configuration","title":"Plugin Configuration","text":"<pre><code>from xtools.plugins import Plugin, PluginConfig\n\nclass ConfigurablePlugin(Plugin):\n    \"\"\"Plugin with configuration.\"\"\"\n\n    name = \"configurable\"\n\n    class Config(PluginConfig):\n        enabled: bool = True\n        log_level: str = \"INFO\"\n        custom_setting: int = 100\n\n    def __init__(self, config: Config = None):\n        self.config = config or self.Config()\n\n    @hook(\"before_scrape\")\n    async def log_scrape(self, url, **kwargs):\n        if self.config.enabled:\n            print(f\"[{self.config.log_level}] Scraping {url}\")\n        return url, kwargs\n\n# Usage\nplugin = ConfigurablePlugin(\n    ConfigurablePlugin.Config(log_level=\"DEBUG\")\n)\n</code></pre> <p>Plugin Best Practices</p> <ul> <li>Keep plugins focused on single responsibility</li> <li>Handle errors gracefully in hooks</li> <li>Document all configuration options</li> <li>Use async/await for all I/O operations</li> </ul>"},{"location":"advanced/plugins/#notification-plugin-example","title":"Notification Plugin Example","text":"<pre><code>from xtools.plugins import Plugin, hook\nimport aiohttp\n\nclass DiscordNotifyPlugin(Plugin):\n    \"\"\"Send notifications to Discord webhook.\"\"\"\n\n    name = \"discord-notify\"\n\n    def __init__(self, webhook_url: str):\n        self.webhook_url = webhook_url\n\n    @hook(\"after_scrape\")\n    async def notify_scrape(self, results):\n        if len(results) &gt; 0:\n            await self._send(f\"\u2705 Scraped {len(results)} items\")\n        return results\n\n    @hook(\"on_error\")\n    async def notify_error(self, error, context):\n        await self._send(f\"\u274c Error: {error}\")\n\n    async def _send(self, message: str):\n        async with aiohttp.ClientSession() as session:\n            await session.post(\n                self.webhook_url,\n                json={\"content\": message}\n            )\n</code></pre> <p>Webhook Security</p> <p>Never commit webhook URLs to version control. Use environment variables.</p>"},{"location":"advanced/proxies/","title":"Proxy Configuration","text":"<p>Configure proxies for anonymity, geo-targeting, and avoiding IP-based rate limits.</p>"},{"location":"advanced/proxies/#basic-proxy-setup","title":"Basic Proxy Setup","text":"<pre><code>from xtools import XTools\n\n# HTTP/HTTPS proxy\nasync with XTools(proxy=\"http://proxy.example.com:8080\") as x:\n    await x.scrape.profile(\"username\")\n\n# Authenticated proxy\nasync with XTools(proxy=\"http://user:pass@proxy.example.com:8080\") as x:\n    await x.scrape.profile(\"username\")\n\n# SOCKS5 proxy\nasync with XTools(proxy=\"socks5://proxy.example.com:1080\") as x:\n    await x.scrape.profile(\"username\")\n</code></pre>"},{"location":"advanced/proxies/#proxy-rotation","title":"Proxy Rotation","text":"<p>Rotate through multiple proxies to distribute requests:</p> <pre><code>from xtools import XTools\nfrom xtools.core.proxy import ProxyRotator, ProxyConfig\n\n# Simple list rotation\nproxies = [\n    \"http://proxy1.example.com:8080\",\n    \"http://proxy2.example.com:8080\",\n    \"http://proxy3.example.com:8080\",\n]\n\nrotator = ProxyRotator(proxies, strategy=\"round_robin\")\n\nasync with XTools(proxy_rotator=rotator) as x:\n    # Each request uses next proxy in rotation\n    for username in usernames:\n        await x.scrape.profile(username)\n\n# Weighted rotation (use some proxies more than others)\nweighted_proxies = [\n    {\"url\": \"http://fast-proxy:8080\", \"weight\": 3},\n    {\"url\": \"http://slow-proxy:8080\", \"weight\": 1},\n]\n\nrotator = ProxyRotator(weighted_proxies, strategy=\"weighted\")\n</code></pre> <p>Sticky Sessions</p> <p>Use sticky sessions to keep the same proxy for related requests (login, scrape).</p>"},{"location":"advanced/proxies/#proxy-health-checking","title":"Proxy Health Checking","text":"<p>Automatically remove failing proxies:</p> <pre><code>from xtools.core.proxy import ProxyPool\n\nasync def managed_proxy_pool():\n    \"\"\"Proxy pool with health checks.\"\"\"\n    pool = ProxyPool(\n        proxies=[\n            \"http://proxy1:8080\",\n            \"http://proxy2:8080\",\n            \"http://proxy3:8080\",\n        ],\n        health_check_interval=60,  # seconds\n        max_failures=3,            # remove after 3 failures\n        test_url=\"https://x.com\"\n    )\n\n    await pool.start()\n\n    async with XTools(proxy_pool=pool) as x:\n        # Pool automatically rotates and health-checks\n        results = await x.scrape.followers(\"username\")\n\n    await pool.stop()\n\n    # Get pool statistics\n    stats = pool.get_stats()\n    print(f\"Active proxies: {stats['active']}\")\n    print(f\"Failed proxies: {stats['failed']}\")\n</code></pre>"},{"location":"advanced/proxies/#geo-targeted-proxies","title":"Geo-Targeted Proxies","text":"<p>Use location-specific proxies for regional content:</p> <pre><code>from xtools.core.proxy import GeoProxyManager\n\ngeo_proxies = GeoProxyManager({\n    \"US\": [\"http://us-proxy1:8080\", \"http://us-proxy2:8080\"],\n    \"UK\": [\"http://uk-proxy1:8080\"],\n    \"JP\": [\"http://jp-proxy1:8080\"],\n})\n\nasync with XTools(proxy_manager=geo_proxies) as x:\n    # Scrape US trends\n    x.set_geo(\"US\")\n    us_trends = await x.scrape.trends()\n\n    # Switch to UK\n    x.set_geo(\"UK\")\n    uk_trends = await x.scrape.trends()\n</code></pre>"},{"location":"advanced/proxies/#residential-proxy-integration","title":"Residential Proxy Integration","text":"<pre><code>from xtools import XTools\nfrom xtools.core.proxy import ResidentialProxy\n\n# BrightData/Luminati integration\nbright_data = ResidentialProxy(\n    provider=\"brightdata\",\n    username=\"your_username\",\n    password=\"your_password\",\n    country=\"US\",\n    session_type=\"rotating\"  # or \"sticky\"\n)\n\nasync with XTools(proxy=bright_data) as x:\n    await x.scrape.profile(\"username\")\n\n# Oxylabs integration\noxylabs = ResidentialProxy(\n    provider=\"oxylabs\",\n    username=\"customer-user\",\n    password=\"password\",\n    country=\"DE\"\n)\n</code></pre> <p>Residential Proxies</p> <p>Residential proxies are expensive. Use datacenter proxies for testing.</p>"},{"location":"advanced/proxies/#proxy-configuration-file","title":"Proxy Configuration File","text":"<pre><code># proxies.yaml\npools:\n  default:\n    strategy: round_robin\n    proxies:\n      - url: http://proxy1:8080\n        weight: 1\n      - url: http://proxy2:8080\n        weight: 2\n\n  residential:\n    provider: brightdata\n    username: ${BRIGHTDATA_USER}\n    password: ${BRIGHTDATA_PASS}\n    country: US\n\nhealth_check:\n  enabled: true\n  interval: 60\n  timeout: 10\n  max_failures: 3\n</code></pre> <pre><code>from xtools import XTools\nfrom xtools.core.proxy import load_proxy_config\n\nconfig = load_proxy_config(\"proxies.yaml\")\n\nasync with XTools(proxy_config=config) as x:\n    # Use default pool\n    await x.scrape.profile(\"user1\")\n\n    # Switch to residential pool\n    x.use_proxy_pool(\"residential\")\n    await x.scrape.profile(\"user2\")\n</code></pre>"},{"location":"advanced/proxies/#proxy-aware-rate-limiting","title":"Proxy-Aware Rate Limiting","text":"<pre><code>from xtools.core.rate_limiter import ProxyAwareRateLimiter\n\n# Rate limit per proxy IP\nlimiter = ProxyAwareRateLimiter(\n    requests_per_minute_per_proxy=20,\n    global_requests_per_minute=100\n)\n\nasync with XTools(\n    proxy_rotator=rotator,\n    rate_limiter=limiter\n) as x:\n    # Each proxy gets its own rate limit bucket\n    for user in users:\n        await x.scrape.profile(user)\n</code></pre> <p>Proxy Rate Limits</p> <p>Per-proxy rate limiting helps avoid bans when using shared proxy pools.</p>"},{"location":"advanced/proxies/#testing-proxy-configuration","title":"Testing Proxy Configuration","text":"<pre><code>from xtools.core.proxy import test_proxy\n\nasync def verify_proxies():\n    \"\"\"Test proxy connectivity and speed.\"\"\"\n    proxies = [\n        \"http://proxy1:8080\",\n        \"http://proxy2:8080\",\n    ]\n\n    for proxy in proxies:\n        result = await test_proxy(\n            proxy,\n            test_url=\"https://x.com\",\n            timeout=10\n        )\n\n        if result.success:\n            print(f\"\u2705 {proxy}: {result.latency_ms}ms\")\n        else:\n            print(f\"\u274c {proxy}: {result.error}\")\n</code></pre>"},{"location":"advanced/rate-limiting/","title":"Rate Limiting Configuration","text":"<p>Configure rate limiting to protect your accounts and avoid detection when using XTools.</p>"},{"location":"advanced/rate-limiting/#built-in-rate-limiter","title":"Built-in Rate Limiter","text":"<p>XTools includes an adaptive rate limiter that respects X/Twitter's limits:</p> <pre><code>from xtools import XTools\nfrom xtools.core.rate_limiter import RateLimitConfig\n\nconfig = RateLimitConfig(\n    follows_per_hour=20,\n    unfollows_per_hour=20,\n    likes_per_hour=50,\n    tweets_per_hour=25,\n    dms_per_day=500,\n    scrape_requests_per_minute=30,\n)\n\nasync with XTools(rate_limit_config=config) as x:\n    await x.follow.user(\"username\")\n</code></pre> <p>Default Limits Are Conservative</p> <p>XTools uses conservative defaults to protect your account. Increasing limits may trigger suspensions.</p>"},{"location":"advanced/rate-limiting/#rate-limiter-implementation","title":"Rate Limiter Implementation","text":"<pre><code>import asyncio\nfrom dataclasses import dataclass, field\nfrom collections import deque\nfrom time import time\n\n@dataclass\nclass RateLimiter:\n    max_requests: int\n    window_seconds: float\n    _timestamps: deque = field(default_factory=deque)\n    _lock: asyncio.Lock = field(default_factory=asyncio.Lock)\n\n    async def acquire(self):\n        \"\"\"Wait until a request slot is available.\"\"\"\n        async with self._lock:\n            now = time()\n            while self._timestamps and now - self._timestamps[0] &gt; self.window_seconds:\n                self._timestamps.popleft()\n\n            if len(self._timestamps) &gt;= self.max_requests:\n                wait_time = self.window_seconds - (now - self._timestamps[0])\n                await asyncio.sleep(wait_time)\n                self._timestamps.popleft()\n\n            self._timestamps.append(time())\n\n    @property\n    def remaining(self) -&gt; int:\n        now = time()\n        valid = sum(1 for t in self._timestamps if now - t &lt;= self.window_seconds)\n        return max(0, self.max_requests - valid)\n</code></pre>"},{"location":"advanced/rate-limiting/#adaptive-rate-limiting","title":"Adaptive Rate Limiting","text":"<pre><code>async def adaptive_follow(usernames: list[str]):\n    \"\"\"Follow users with adaptive rate limiting.\"\"\"\n    async with XTools() as x:\n        for username in usernames:\n            remaining = x.rate_limiter.remaining(\"follow\")\n\n            if remaining &lt; 5:\n                print(\"Approaching limit, cooling down...\")\n                await asyncio.sleep(300)\n\n            await x.follow.user(username)\n            base_delay = 3\n            delay = base_delay * (1 + (20 - remaining) / 20)\n            await asyncio.sleep(delay)\n</code></pre> <p>Check Limits Before Bulk Operations</p> <pre><code>status = await x.get_limits_status()\nif status[\"follow\"][\"remaining_hour\"] &lt; 10:\n    print(\"Low follow quota - consider waiting\")\n</code></pre>"},{"location":"advanced/rate-limiting/#random-delays-for-stealth","title":"Random Delays for Stealth","text":"<pre><code>import random\n\nasync def stealthy_operation(usernames: list[str]):\n    \"\"\"Add human-like random delays.\"\"\"\n    async with XTools() as x:\n        for username in usernames:\n            await x.follow.user(username)\n\n            delay = random.uniform(5, 15)\n            if random.random() &lt; 0.1:\n                delay += random.uniform(30, 60)\n            await asyncio.sleep(delay)\n</code></pre> <p>Avoid Predictable Patterns</p> <p>X/Twitter detects automation through consistent timing. Always add randomization.</p>"},{"location":"advanced/rate-limiting/#monitoring-rate-limit-status","title":"Monitoring Rate Limit Status","text":"<pre><code>async def monitor_limits():\n    \"\"\"Log rate limit status periodically.\"\"\"\n    async with XTools() as x:\n        while True:\n            status = await x.rate_limiter.status()\n            print(f\"Rate limits: {status}\")\n\n            if status[\"warning\"]:\n                await x.notifications.send(\"Rate limit warning\")\n            await asyncio.sleep(60)\n</code></pre>"},{"location":"advanced/rate-limiting/#action-specific-limits","title":"Action-Specific Limits","text":"<pre><code>class XToolsWithLimits:\n    def __init__(self):\n        self.limiters = {\n            \"follow\": RateLimiter(max_requests=20, window_seconds=3600),\n            \"like\": RateLimiter(max_requests=50, window_seconds=3600),\n            \"tweet\": RateLimiter(max_requests=25, window_seconds=3600),\n        }\n\n    async def follow(self, username: str):\n        await self.limiters[\"follow\"].acquire()\n        # Perform follow action\n</code></pre>"},{"location":"advanced/scheduling/","title":"Task Scheduling and Automation","text":"<p>Schedule XTools tasks for automated execution with precise timing control.</p>"},{"location":"advanced/scheduling/#basic-scheduling-with-apscheduler","title":"Basic Scheduling with APScheduler","text":"<pre><code>from apscheduler.schedulers.asyncio import AsyncIOScheduler\nfrom apscheduler.triggers.cron import CronTrigger\nfrom xtools import XTools\n\nscheduler = AsyncIOScheduler()\n\nasync def check_unfollowers():\n    \"\"\"Scheduled task to check unfollowers.\"\"\"\n    async with XTools() as x:\n        await x.auth.load_cookies(\"session.json\")\n        report = await x.monitor.unfollowers()\n        if report.unfollowers:\n            print(f\"New unfollowers: {[u.username for u in report.unfollowers]}\")\n\nscheduler.add_job(check_unfollowers, CronTrigger(hour=\"*/6\"), id=\"unfollower_check\")\nscheduler.start()\n</code></pre> <p>Install APScheduler</p> <pre><code>pip install apscheduler\n</code></pre>"},{"location":"advanced/scheduling/#scheduling-tweet-posts","title":"Scheduling Tweet Posts","text":"<pre><code>from datetime import datetime, timedelta\n\nasync def schedule_tweet(text: str, scheduled_time: datetime):\n    \"\"\"Schedule a tweet for future posting.\"\"\"\n    async with XTools() as x:\n        await x.auth.load_cookies(\"session.json\")\n        await x.schedule.tweet(text, scheduled_time)\n\nasync def schedule_content_calendar():\n    tweets = [\n        (\"Good morning! \u2600\ufe0f\", datetime.now() + timedelta(hours=8)),\n        (\"Lunch break thoughts \ud83c\udf55\", datetime.now() + timedelta(hours=12)),\n        (\"Evening reflection \ud83c\udf19\", datetime.now() + timedelta(hours=20)),\n    ]\n    for text, time in tweets:\n        await schedule_tweet(text, time)\n</code></pre>"},{"location":"advanced/scheduling/#recurring-engagement-jobs","title":"Recurring Engagement Jobs","text":"<pre><code>import random\n\nasync def auto_engage_job():\n    \"\"\"Automated engagement with keyword-based content.\"\"\"\n    async with XTools() as x:\n        await x.auth.load_cookies(\"session.json\")\n        keywords = [\"#Python\", \"#AsyncIO\", \"#WebDev\"]\n        keyword = random.choice(keywords)\n        await x.engage.auto_like(keywords=[keyword], limit=10, delay_range=(5, 15))\n\nfor hour in [9, 14, 20]:\n    scheduler.add_job(\n        auto_engage_job,\n        CronTrigger(hour=hour, minute=random.randint(0, 59)),\n        id=f\"engage_{hour}\"\n    )\n</code></pre> <p>Vary Scheduling Times</p> <p>Predictable automation patterns can trigger detection. Add randomization to schedules.</p>"},{"location":"advanced/scheduling/#job-queue-with-redis","title":"Job Queue with Redis","text":"<pre><code>from redis import asyncio as aioredis\nimport json\nfrom datetime import datetime\n\nclass JobQueue:\n    def __init__(self, redis_url: str = \"redis://localhost\"):\n        self.redis = aioredis.from_url(redis_url)\n        self.queue_name = \"xtools:jobs\"\n\n    async def enqueue(self, job_type: str, params: dict):\n        job = {\"type\": job_type, \"params\": params, \"created_at\": datetime.now().isoformat()}\n        await self.redis.rpush(self.queue_name, json.dumps(job))\n\n    async def process_jobs(self):\n        while True:\n            job_data = await self.redis.blpop(self.queue_name, timeout=5)\n            if job_data:\n                job = json.loads(job_data[1])\n                await self.execute_job(job)\n\n    async def execute_job(self, job: dict):\n        async with XTools() as x:\n            await x.auth.load_cookies(\"session.json\")\n            if job[\"type\"] == \"follow\":\n                await x.follow.user(job[\"params\"][\"username\"])\n            elif job[\"type\"] == \"like\":\n                await x.engage.like(job[\"params\"][\"url\"])\n</code></pre>"},{"location":"advanced/scheduling/#time-window-operations","title":"Time-Window Operations","text":"<pre><code>from datetime import time\n\ndef is_active_hours() -&gt; bool:\n    now = datetime.now().time()\n    return time(8, 0) &lt;= now &lt;= time(22, 0)\n\nasync def smart_scheduler():\n    async with XTools() as x:\n        await x.auth.load_cookies(\"session.json\")\n        while True:\n            if is_active_hours():\n                await x.engage.auto_like(keywords=[\"#tech\"], limit=5)\n                await asyncio.sleep(1800)\n            else:\n                await asyncio.sleep(3600)\n</code></pre> <p>Respect User Time Zones</p> <p>When scheduling engagement, consider your target audience's active hours.</p>"},{"location":"advanced/scheduling/#graceful-shutdown","title":"Graceful Shutdown","text":"<pre><code>import signal\nimport asyncio\n\nasync def main():\n    scheduler = AsyncIOScheduler()\n    scheduler.add_job(check_unfollowers, CronTrigger(hour=\"*/6\"))\n    scheduler.start()\n\n    loop = asyncio.get_event_loop()\n    for sig in (signal.SIGTERM, signal.SIGINT):\n        loop.add_signal_handler(sig, lambda: asyncio.create_task(shutdown(scheduler)))\n\n    await asyncio.Event().wait()\n\nasync def shutdown(scheduler):\n    scheduler.shutdown(wait=True)\n    print(\"Scheduler stopped gracefully\")\n</code></pre>"},{"location":"advanced/stealth/","title":"Anti-Detection Techniques","text":"<p>XTools includes stealth features to avoid bot detection and account restrictions.</p>"},{"location":"advanced/stealth/#browser-fingerprinting","title":"Browser Fingerprinting","text":"<p>Evade fingerprint-based detection with randomized browser profiles:</p> <pre><code>from xtools import XTools\nfrom xtools.core.stealth import StealthConfig, BrowserProfile\n\n# Use stealth mode\nstealth = StealthConfig(\n    randomize_fingerprint=True,\n    mask_webdriver=True,\n    mask_automation=True,\n    randomize_viewport=True\n)\n\nasync with XTools(stealth=stealth) as x:\n    await x.scrape.profile(\"username\")\n\n# Custom browser profile\nprofile = BrowserProfile(\n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64)...\",\n    viewport={\"width\": 1920, \"height\": 1080},\n    timezone=\"America/New_York\",\n    locale=\"en-US\",\n    geolocation={\"latitude\": 40.7128, \"longitude\": -74.0060}\n)\n\nasync with XTools(browser_profile=profile) as x:\n    await x.scrape.profile(\"username\")\n</code></pre>"},{"location":"advanced/stealth/#webdriver-detection-bypass","title":"WebDriver Detection Bypass","text":"<pre><code>from xtools.core.stealth import apply_stealth_scripts\n\nasync def stealth_browser_setup(page):\n    \"\"\"Apply stealth patches to browser page.\"\"\"\n    await apply_stealth_scripts(page)\n\n    # These properties are patched:\n    # - navigator.webdriver = undefined\n    # - navigator.plugins (populated)\n    # - navigator.languages (realistic)\n    # - window.chrome (present)\n    # - permissions API (realistic)\n\n# Built into XTools\nasync with XTools(stealth=True) as x:\n    # Stealth scripts applied automatically\n    pass\n</code></pre> <p>Detection Vectors</p> <p>X checks for: WebDriver flags, automation APIs, plugin presence, and behavior patterns.</p>"},{"location":"advanced/stealth/#human-like-behavior","title":"Human-Like Behavior","text":"<p>Simulate realistic user interactions:</p> <pre><code>from xtools.core.stealth import HumanBehavior\nimport random\n\nbehavior = HumanBehavior(\n    typing_speed=(50, 150),    # WPM range\n    mouse_movement=True,       # Realistic mouse paths\n    scroll_behavior=\"natural\", # Varied scroll patterns\n    click_variation=True,      # Slight position variance\n    think_time=(1, 5)          # Pause between actions\n)\n\nasync with XTools(behavior=behavior) as x:\n    # Actions include human-like delays and movements\n    await x.engage.like(tweet_url)\n    await x.engage.comment(tweet_url, \"Great post!\")\n\n# Manual behavior simulation\nasync def human_like_typing(page, selector: str, text: str):\n    \"\"\"Type with human-like timing.\"\"\"\n    element = await page.query_selector(selector)\n    await element.click()\n\n    for char in text:\n        await element.type(char)\n        # Random delay between keystrokes\n        await asyncio.sleep(random.uniform(0.05, 0.2))\n</code></pre>"},{"location":"advanced/stealth/#request-pattern-randomization","title":"Request Pattern Randomization","text":"<pre><code>from xtools.core.stealth import RequestRandomizer\n\nrandomizer = RequestRandomizer(\n    delay_range=(2, 8),        # Random delay between requests\n    burst_probability=0.1,    # Occasional quick bursts\n    long_pause_probability=0.05,  # Occasional long pauses\n    session_length_range=(10, 30)  # Minutes per session\n)\n\nasync with XTools(request_randomizer=randomizer) as x:\n    # Requests have randomized timing\n    for user in users:\n        await x.follow.user(user)\n</code></pre> <p>Behavioral Analysis</p> <p>Consistent timing patterns are a red flag. Always randomize delays.</p>"},{"location":"advanced/stealth/#session-management","title":"Session Management","text":"<p>Maintain realistic session patterns:</p> <pre><code>from xtools.core.stealth import SessionManager\n\nsession = SessionManager(\n    warm_up_period=30,         # Seconds of light activity first\n    cooldown_between_sessions=300,  # 5 min between sessions\n    max_session_duration=1800,  # 30 min max\n    actions_per_session=50     # Max actions\n)\n\nasync with XTools(session_manager=session) as x:\n    async with session.start():\n        # Session automatically handles timing\n        await session.warm_up()  # Light browsing first\n\n        for i in range(100):\n            if session.should_pause():\n                await session.take_break()\n\n            await x.follow.user(f\"user{i}\")\n</code></pre>"},{"location":"advanced/stealth/#cookie-and-storage-management","title":"Cookie and Storage Management","text":"<pre><code>from xtools.core.stealth import StorageManager\n\nasync def manage_browser_storage():\n    \"\"\"Realistic cookie and storage patterns.\"\"\"\n    storage = StorageManager()\n\n    async with XTools() as x:\n        # Load existing storage state\n        await storage.load(x.browser, \"state.json\")\n\n        # Perform actions...\n        await x.scrape.profile(\"username\")\n\n        # Save updated state\n        await storage.save(x.browser, \"state.json\")\n\n        # Selective clearing (like real users)\n        await storage.clear_old_items(\n            x.browser,\n            max_age_days=30\n        )\n</code></pre>"},{"location":"advanced/stealth/#canvas-fingerprint-spoofing","title":"Canvas Fingerprint Spoofing","text":"<pre><code>from xtools.core.stealth import CanvasSpoofing\n\n# Randomize canvas fingerprint\ncanvas_spoof = CanvasSpoofing(\n    mode=\"noise\",  # Add noise to canvas operations\n    noise_level=0.1\n)\n\nasync with XTools(canvas_spoofing=canvas_spoof) as x:\n    # Canvas-based fingerprinting is thwarted\n    pass\n</code></pre>"},{"location":"advanced/stealth/#full-stealth-configuration","title":"Full Stealth Configuration","text":"<pre><code>from xtools import XTools\nfrom xtools.core.stealth import (\n    StealthConfig,\n    HumanBehavior,\n    RequestRandomizer,\n    SessionManager\n)\n\n# Maximum stealth configuration\nstealth_config = StealthConfig(\n    # Browser stealth\n    mask_webdriver=True,\n    mask_automation=True,\n    randomize_fingerprint=True,\n    spoof_canvas=True,\n    spoof_webgl=True,\n\n    # Behavior simulation\n    behavior=HumanBehavior(\n        typing_speed=(60, 120),\n        mouse_movement=True,\n        scroll_behavior=\"natural\"\n    ),\n\n    # Request patterns\n    randomizer=RequestRandomizer(\n        delay_range=(3, 10),\n        burst_probability=0.05\n    ),\n\n    # Session management\n    session=SessionManager(\n        warm_up_period=60,\n        max_session_duration=1200\n    )\n)\n\nasync with XTools(stealth=stealth_config) as x:\n    # Full stealth mode enabled\n    await x.scrape.followers(\"username\", limit=500)\n</code></pre> <p>Testing Detection</p> <p>Use sites like bot.sannysoft.com to verify stealth configuration.</p>"},{"location":"advanced/stealth/#rate-limit-recovery","title":"Rate Limit Recovery","text":"<pre><code>async def graceful_rate_limit_handling():\n    \"\"\"Handle rate limits gracefully.\"\"\"\n    async with XTools(stealth=True) as x:\n        try:\n            await x.scrape.followers(\"username\", limit=1000)\n        except RateLimitError as e:\n            # Exponential backoff\n            wait_time = e.retry_after or 900  # 15 min default\n            print(f\"Rate limited. Waiting {wait_time}s...\")\n            await asyncio.sleep(wait_time)\n\n            # Resume with fresh session\n            await x.auth.refresh_session()\n</code></pre>"},{"location":"advanced/testing/","title":"Testing XTools Integrations","text":"<p>Learn how to test your XTools automation scripts using pytest and async testing patterns.</p>"},{"location":"advanced/testing/#setup-testing-environment","title":"Setup Testing Environment","text":"<p>Install testing dependencies:</p> <pre><code>pip install pytest pytest-asyncio pytest-mock aioresponses\n</code></pre> <p>Configure pytest for async tests in <code>pyproject.toml</code>:</p> <pre><code>[tool.pytest.ini_options]\nasyncio_mode = \"auto\"\ntestpaths = [\"tests\"]\n</code></pre>"},{"location":"advanced/testing/#basic-test-structure","title":"Basic Test Structure","text":"<pre><code>import pytest\nfrom unittest.mock import AsyncMock, MagicMock\nfrom xtools import XTools\n\n@pytest.fixture\nasync def xtools_client():\n    \"\"\"Create a mocked XTools client for testing.\"\"\"\n    client = XTools(headless=True)\n    client.browser = AsyncMock()\n    yield client\n    await client.close()\n\n@pytest.mark.asyncio\nasync def test_scrape_replies(xtools_client):\n    \"\"\"Test reply scraping returns expected structure.\"\"\"\n    xtools_client.scrape.replies = AsyncMock(return_value=[\n        {\"id\": \"123\", \"text\": \"Test reply\", \"author\": \"user1\"}\n    ])\n\n    replies = await xtools_client.scrape.replies(\"https://x.com/user/status/123\")\n\n    assert len(replies) == 1\n    assert replies[0][\"text\"] == \"Test reply\"\n</code></pre> <p>Use Fixtures for Reusability</p> <p>Create pytest fixtures for common setup tasks like authentication and browser initialization to keep tests DRY.</p>"},{"location":"advanced/testing/#mocking-browser-interactions","title":"Mocking Browser Interactions","text":"<pre><code>import pytest\nfrom unittest.mock import AsyncMock, patch\n\n@pytest.mark.asyncio\nasync def test_follow_user():\n    \"\"\"Test follow action with mocked browser.\"\"\"\n    with patch(\"xtools.core.browser.BrowserManager\") as mock_browser:\n        mock_page = AsyncMock()\n        mock_browser.return_value.get_page.return_value = mock_page\n\n        from xtools.actions.follow import FollowActions\n        follow = FollowActions(mock_browser.return_value)\n\n        await follow.user(\"testuser\")\n\n        mock_page.goto.assert_called()\n        mock_page.click.assert_called()\n</code></pre>"},{"location":"advanced/testing/#testing-rate-limiting","title":"Testing Rate Limiting","text":"<pre><code>import pytest\nimport time\nfrom xtools.core.rate_limiter import RateLimiter\n\n@pytest.mark.asyncio\nasync def test_rate_limiter_blocks_excess_requests():\n    \"\"\"Verify rate limiter enforces request limits.\"\"\"\n    limiter = RateLimiter(max_requests=2, window_seconds=1)\n\n    await limiter.acquire()  # Request 1\n    await limiter.acquire()  # Request 2\n\n    start = time.time()\n    await limiter.acquire()  # Request 3 - should wait\n    elapsed = time.time() - start\n\n    assert elapsed &gt;= 0.9  # Should have waited ~1 second\n</code></pre> <p>Async Test Timeouts</p> <p>Always set reasonable timeouts for async tests to prevent hanging: <pre><code>@pytest.mark.asyncio\n@pytest.mark.timeout(30)\nasync def test_with_timeout():\n    ...\n</code></pre></p>"},{"location":"advanced/testing/#integration-test-example","title":"Integration Test Example","text":"<pre><code>import pytest\nimport os\n\n@pytest.mark.integration\n@pytest.mark.skipif(\n    not os.getenv(\"XTOOLS_TEST_COOKIES\"),\n    reason=\"Integration tests require authentication\"\n)\n@pytest.mark.asyncio\nasync def test_real_profile_scrape():\n    \"\"\"Integration test with real X/Twitter.\"\"\"\n    from xtools import XTools\n\n    async with XTools() as x:\n        await x.auth.load_cookies(os.getenv(\"XTOOLS_TEST_COOKIES\"))\n        profile = await x.scrape.profile(\"twitter\")\n\n        assert profile.username == \"twitter\"\n        assert profile.followers_count &gt; 0\n</code></pre>"},{"location":"advanced/testing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=xtools --cov-report=html\n\n# Run only unit tests (skip integration)\npytest -m \"not integration\"\n\n# Run specific test file\npytest tests/test_scrapers.py -v\n</code></pre> <p>CI/CD Integration</p> <p>For GitHub Actions, use the <code>pytest-github-actions-annotate-failures</code> plugin to display test failures directly in PR annotations.</p>"},{"location":"advanced/webhooks/","title":"Webhooks and Event-Driven Integrations","text":"<p>Build event-driven automation with XTools webhooks and real-time notifications.</p>"},{"location":"advanced/webhooks/#webhook-server-setup","title":"Webhook Server Setup","text":"<p>Create a FastAPI webhook receiver for XTools events:</p> <pre><code>from fastapi import FastAPI, Request, HTTPException\nfrom pydantic import BaseModel\nimport hmac\nimport hashlib\n\napp = FastAPI()\nWEBHOOK_SECRET = \"your-secret-key\"\n\nclass WebhookPayload(BaseModel):\n    event: str\n    data: dict\n    timestamp: int\n\ndef verify_signature(payload: bytes, signature: str) -&gt; bool:\n    \"\"\"Verify webhook signature for security.\"\"\"\n    expected = hmac.new(\n        WEBHOOK_SECRET.encode(), payload, hashlib.sha256\n    ).hexdigest()\n    return hmac.compare_digest(f\"sha256={expected}\", signature)\n\n@app.post(\"/webhook/xtools\")\nasync def receive_webhook(request: Request):\n    payload = await request.body()\n    signature = request.headers.get(\"X-Signature\", \"\")\n\n    if not verify_signature(payload, signature):\n        raise HTTPException(status_code=401, detail=\"Invalid signature\")\n\n    data = WebhookPayload.parse_raw(payload)\n    await process_event(data)\n    return {\"status\": \"received\"}\n</code></pre> <p>Always Verify Signatures</p> <p>Never process webhooks without signature verification to prevent spoofing attacks.</p>"},{"location":"advanced/webhooks/#event-processing","title":"Event Processing","text":"<pre><code>from xtools import XTools\n\nasync def process_event(payload: WebhookPayload):\n    \"\"\"Process incoming webhook events.\"\"\"\n    handlers = {\n        \"new_follower\": handle_new_follower,\n        \"mention\": handle_mention,\n        \"dm_received\": handle_dm,\n    }\n    handler = handlers.get(payload.event)\n    if handler:\n        await handler(payload.data)\n\nasync def handle_new_follower(data: dict):\n    \"\"\"Auto-follow back new followers.\"\"\"\n    async with XTools() as x:\n        await x.auth.load_cookies(\"session.json\")\n        await x.follow.user(data[\"username\"])\n\nasync def handle_mention(data: dict):\n    \"\"\"Auto-reply to mentions with AI.\"\"\"\n    async with XTools() as x:\n        await x.auth.load_cookies(\"session.json\")\n        from xtools.ai import ContentGenerator\n        ai = ContentGenerator(provider=\"openai\")\n        reply = await ai.generate_reply(data[\"text\"], style=\"friendly\")\n        await x.engage.reply(data[\"tweet_url\"], reply)\n</code></pre>"},{"location":"advanced/webhooks/#outgoing-webhooks","title":"Outgoing Webhooks","text":"<pre><code>import aiohttp\nimport json\nfrom datetime import datetime\n\nclass WebhookDispatcher:\n    def __init__(self, endpoints: list[str], secret: str):\n        self.endpoints = endpoints\n        self.secret = secret\n\n    async def dispatch(self, event: str, data: dict):\n        \"\"\"Send webhook to all registered endpoints.\"\"\"\n        payload = {\n            \"event\": event,\n            \"data\": data,\n            \"timestamp\": int(datetime.now().timestamp())\n        }\n        payload_bytes = json.dumps(payload).encode()\n        signature = hmac.new(\n            self.secret.encode(), payload_bytes, hashlib.sha256\n        ).hexdigest()\n\n        async with aiohttp.ClientSession() as session:\n            for endpoint in self.endpoints:\n                await session.post(\n                    endpoint, json=payload,\n                    headers={\"X-Signature\": f\"sha256={signature}\"}\n                )\n</code></pre> <p>Use Message Queues for Reliability</p> <p>For production, consider Redis or RabbitMQ to queue webhook deliveries with retries.</p>"},{"location":"advanced/webhooks/#discord-integration","title":"Discord Integration","text":"<pre><code>from xtools.notifications import DiscordWebhook\n\nasync def setup_discord_alerts():\n    webhook = DiscordWebhook(url=\"https://discord.com/api/webhooks/123/abc\")\n\n    async with XTools() as x:\n        report = await x.monitor.unfollowers()\n        if report.unfollowers:\n            await webhook.send(\n                title=\"\ud83d\udc4b Unfollower Alert\",\n                description=f\"{len(report.unfollowers)} users unfollowed\",\n                color=\"orange\"\n            )\n</code></pre>"},{"location":"advanced/webhooks/#telegram-bot-integration","title":"Telegram Bot Integration","text":"<pre><code>from xtools.notifications import TelegramBot\n\nbot = TelegramBot(token=\"BOT_TOKEN\", chat_id=\"CHAT_ID\")\n\nasync def telegram_alerts():\n    async with XTools() as x:\n        mentions = await x.scrape.mentions(\"myusername\", limit=10)\n        for mention in mentions:\n            await bot.send_message(\n                f\"\ud83d\udd14 New mention from @{mention.author}:\\n{mention.text}\"\n            )\n</code></pre> <p>Rate Limit Notifications</p> <p>Telegram has rate limits. Batch messages or add delays between sends.</p>"},{"location":"api/","title":"API Reference","text":"<p>Complete API documentation for Xeepy modules.</p>"},{"location":"api/#module-overview","title":"Module Overview","text":"<pre><code>xeepy/\n\u251c\u2500\u2500 __init__.py          # Xeepy main class\n\u251c\u2500\u2500 core/                # Core functionality\n\u2502   \u251c\u2500\u2500 browser.py       # Browser management\n\u2502   \u251c\u2500\u2500 auth.py          # Authentication\n\u2502   \u251c\u2500\u2500 rate_limiter.py  # Rate limiting\n\u2502   \u2514\u2500\u2500 config.py        # Configuration\n\u251c\u2500\u2500 scrapers/            # Data scraping\n\u251c\u2500\u2500 actions/             # User actions\n\u251c\u2500\u2500 monitoring/          # Account monitoring\n\u251c\u2500\u2500 analytics/           # Analytics &amp; insights\n\u251c\u2500\u2500 ai/                  # AI features\n\u251c\u2500\u2500 api/                 # REST &amp; GraphQL\n\u251c\u2500\u2500 models/              # Data models\n\u251c\u2500\u2500 storage/             # Data persistence\n\u2514\u2500\u2500 notifications/       # Alerts &amp; notifications\n</code></pre>"},{"location":"api/#quick-reference","title":"Quick Reference","text":""},{"location":"api/#main-class","title":"Main Class","text":"<pre><code>from xeepy import Xeepy\n\nasync with Xeepy(\n    headless: bool = True,\n    timeout: int = 30000,\n    config_file: str = None,\n    profile: str = \"default\"\n) as x:\n    # x.scrape - Scraping operations\n    # x.follow - Follow operations  \n    # x.unfollow - Unfollow operations\n    # x.engage - Engagement actions\n    # x.monitor - Monitoring\n    # x.analytics - Analytics\n    # x.ai - AI features\n    # x.export - Data export\n    # x.auth - Authentication\n    # x.config - Configuration\n</code></pre>"},{"location":"api/#core-modules","title":"Core Modules","text":""},{"location":"api/#xeepy","title":"Xeepy","text":"<p>Main entry point and orchestrator.</p> <pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    profile = await x.scrape.profile(\"username\")\n</code></pre>"},{"location":"api/#browser-manager","title":"Browser Manager","text":"<p>Playwright browser management.</p> <pre><code>from xeepy.core.browser import BrowserManager\n\nbrowser = BrowserManager(headless=True)\nawait browser.start()\npage = await browser.new_page()\n</code></pre>"},{"location":"api/#auth-manager","title":"Auth Manager","text":"<p>Authentication and session handling.</p> <pre><code>from xeepy.core.auth import AuthManager\n\nauth = AuthManager(browser_manager)\nawait auth.login()\nawait auth.save_cookies(\"session.json\")\n</code></pre>"},{"location":"api/#rate-limiter","title":"Rate Limiter","text":"<p>Rate limiting to protect accounts.</p> <pre><code>from xeepy.core.rate_limiter import RateLimiter\n\nlimiter = RateLimiter(requests_per_minute=20)\nawait limiter.wait()\n</code></pre>"},{"location":"api/#config","title":"Config","text":"<p>Configuration management.</p> <pre><code>from xeepy.core.config import Config\n\nconfig = Config.load(\"xeepy.toml\")\nconfig.rate_limit.requests_per_minute = 15\n</code></pre>"},{"location":"api/#scrapers","title":"Scrapers","text":"Module Description replies Tweet replies profile User profiles followers Followers list following Following list tweets User tweets thread Thread unroller search Search results hashtag Hashtag tweets media Media posts likes Tweet likes lists List members mentions User mentions spaces Twitter Spaces downloads Media downloader recommendations Trends &amp; recommendations"},{"location":"api/#example-scraping","title":"Example: Scraping","text":"<pre><code># Replies\nreplies = await x.scrape.replies(tweet_url, limit=100)\n\n# Profile\nprofile = await x.scrape.profile(\"username\")\n\n# Followers with pagination\nasync for batch in x.scrape.followers_batched(\"username\", batch_size=100):\n    process(batch)\n</code></pre>"},{"location":"api/#actions","title":"Actions","text":"Module Description follow Follow operations unfollow Unfollow operations engage Like, retweet, reply messaging Direct messages scheduling Scheduled tweets polls Poll creation settings Account settings"},{"location":"api/#example-actions","title":"Example: Actions","text":"<pre><code># Follow\nawait x.follow.user(\"username\")\nawait x.follow.by_hashtag(\"#python\", limit=20)\n\n# Unfollow\nresult = await x.unfollow.non_followers(max_unfollows=50)\n\n# Engage\nawait x.engage.like(tweet_url)\nawait x.engage.auto_like(keywords=[\"python\"], limit=20)\n</code></pre>"},{"location":"api/#monitoring","title":"Monitoring","text":"Module Description unfollowers Detect unfollowers growth Track growth keywords Keyword monitoring account Account changes"},{"location":"api/#example-monitoring","title":"Example: Monitoring","text":"<pre><code># Check unfollowers\nreport = await x.monitor.unfollowers()\nprint(f\"Lost: {len(report.unfollowers)}\")\nprint(f\"Gained: {len(report.new_followers)}\")\n\n# Continuous monitoring\nasync for event in x.monitor.watch():\n    if event.type == \"unfollower\":\n        notify(f\"@{event.username} unfollowed you\")\n</code></pre>"},{"location":"api/#analytics","title":"Analytics","text":"Module Description growth Growth analytics engagement Engagement metrics audience Audience insights competitors Competitor analysis content Content performance"},{"location":"api/#example-analytics","title":"Example: Analytics","text":"<pre><code># Growth report\ngrowth = await x.analytics.growth(period=\"30d\")\nprint(f\"Net change: {growth.net_change}\")\n\n# Best posting times\nbest_times = await x.analytics.best_posting_times()\nprint(f\"Best day: {best_times.best_day}\")\nprint(f\"Best hour: {best_times.best_hour}\")\n</code></pre>"},{"location":"api/#ai","title":"AI","text":"Module Description providers Provider abstraction content Content generation sentiment Sentiment analysis detection Bot detection"},{"location":"api/#example-ai","title":"Example: AI","text":"<pre><code>from xeepy.ai import ContentGenerator\n\nai = ContentGenerator(provider=\"openai\", api_key=\"...\")\n\n# Generate reply\nreply = await ai.generate_reply(\n    tweet_text=\"Just launched my startup!\",\n    style=\"supportive\"\n)\n\n# Sentiment analysis\nsentiment = await ai.analyze_sentiment(tweets)\n</code></pre>"},{"location":"api/#api","title":"API","text":"Module Description graphql GraphQL client server REST API server"},{"location":"api/#example-graphql","title":"Example: GraphQL","text":"<pre><code>from xeepy.api.graphql import GraphQLClient\n\ngql = GraphQLClient(cookies=\"session.json\")\n\n# Batch operations\ntweets = await gql.tweets_by_ids([\"id1\", \"id2\", \"id3\"])\nusers = await gql.users_by_ids([\"id1\", \"id2\"])\n\nawait gql.close()\n</code></pre>"},{"location":"api/#data-models","title":"Data Models","text":"Model Description Tweet Tweet data User User data Engagement Engagement data"},{"location":"api/#example-models","title":"Example: Models","text":"<pre><code>from xeepy.models import Tweet, User\n\n# Models are dataclasses with type hints\ntweet = Tweet(\n    id=\"123\",\n    text=\"Hello world\",\n    author=User(username=\"example\"),\n    created_at=datetime.now()\n)\n</code></pre>"},{"location":"api/#storage","title":"Storage","text":"Module Description database SQLite caching export Data export"},{"location":"api/#example-export","title":"Example: Export","text":"<pre><code># Export to various formats\nx.export.to_csv(data, \"output.csv\")\nx.export.to_json(data, \"output.json\")\nx.export.to_excel(data, \"output.xlsx\")\nx.export.to_database(data, \"sqlite:///data.db\")\n</code></pre>"},{"location":"api/#notifications","title":"Notifications","text":"Module Description discord Discord webhooks telegram Telegram bot email Email notifications"},{"location":"api/#example-notifications","title":"Example: Notifications","text":"<pre><code>from xeepy.notifications import NotificationManager\n\nnotifier = NotificationManager()\nnotifier.add_discord(\"https://discord.com/api/webhooks/...\")\nnotifier.add_telegram(bot_token=\"...\", chat_id=\"...\")\n\nawait notifier.send(\"\ud83d\udea8 New unfollower detected!\")\n</code></pre>"},{"location":"api/#type-reference","title":"Type Reference","text":""},{"location":"api/#common-types","title":"Common Types","text":"<pre><code>from typing import List, Optional, AsyncIterator\nfrom datetime import datetime\nfrom dataclasses import dataclass\n\n# Tweet URL types\nTweetURL = str  # https://x.com/user/status/123\n\n# Username types\nUsername = str  # Without @\n\n# Time periods\nPeriod = str  # \"7d\", \"30d\", \"1y\"\n\n# Result types\n@dataclass\nclass ScrapeResult:\n    items: List[Any]\n    cursor: Optional[str]\n    has_more: bool\n\n@dataclass\nclass ActionResult:\n    success: bool\n    affected: List[str]\n    errors: List[str]\n</code></pre>"},{"location":"api/#exception-reference","title":"Exception Reference","text":"<pre><code>from xeepy.exceptions import (\n    XeepyError,           # Base exception\n    AuthenticationError,   # Auth failures\n    RateLimitError,        # Rate limited\n    NotFoundError,         # Resource not found\n    NetworkError,          # Network issues\n    TimeoutError,          # Operation timeout\n    ConfigError,           # Configuration error\n)\n</code></pre>"},{"location":"api/#see-also","title":"See Also","text":"<ul> <li>Getting Started</li> <li>Guides</li> <li>CLI Reference</li> <li>Examples</li> </ul>"},{"location":"api/graphql/","title":"GraphQLClient","text":"<p>Direct access to X/Twitter GraphQL API for higher rate limits and batch operations.</p>"},{"location":"api/graphql/#import","title":"Import","text":"<pre><code>from xeepy.api.graphql import GraphQLClient\n</code></pre>"},{"location":"api/graphql/#class-signature","title":"Class Signature","text":"<pre><code>class GraphQLClient:\n    def __init__(\n        self,\n        cookies: Union[Dict[str, str], str],\n        proxy: Optional[str] = None\n    )\n</code></pre>"},{"location":"api/graphql/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>cookies</code> <code>Union[Dict, str]</code> Required Auth cookies or path to cookies file <code>proxy</code> <code>Optional[str]</code> <code>None</code> Proxy URL"},{"location":"api/graphql/#required-cookies","title":"Required Cookies","text":"Cookie Description <code>ct0</code> CSRF token <code>auth_token</code> Authentication token"},{"location":"api/graphql/#methods","title":"Methods","text":"Method Returns Description <code>tweets_by_ids(ids)</code> <code>List[Tweet]</code> Batch fetch tweets <code>users_by_ids(ids)</code> <code>List[User]</code> Batch fetch users <code>get_user(username)</code> <code>User</code> Get single user <code>get_tweet(id)</code> <code>Tweet</code> Get single tweet <code>get_user_tweets(user_id)</code> <code>Tuple[List[Tweet], str]</code> Get user timeline <code>like(tweet_id)</code> <code>bool</code> Like a tweet <code>retweet(tweet_id)</code> <code>bool</code> Retweet <code>bookmark(tweet_id)</code> <code>bool</code> Bookmark tweet <code>follow(user_id)</code> <code>bool</code> Follow user <code>tweet(text, media_ids)</code> <code>Tweet</code> Post tweet <code>search(query, search_type)</code> <code>Tuple[List[Tweet], str]</code> Search tweets <code>close()</code> <code>None</code> Close client"},{"location":"api/graphql/#tweets_by_ids","title":"<code>tweets_by_ids</code>","text":"<pre><code>async def tweets_by_ids(\n    self,\n    tweet_ids: List[str],\n    batch_size: int = 220\n) -&gt; List[Tweet]\n</code></pre> <p>Batch fetch up to 220 tweets per request.</p> <p>Rate Limit: 500 requests / 15 minutes (vs 50 for single)</p>"},{"location":"api/graphql/#users_by_ids","title":"<code>users_by_ids</code>","text":"<pre><code>async def users_by_ids(\n    self,\n    user_ids: List[str],\n    batch_size: int = 100\n) -&gt; List[User]\n</code></pre> <p>Batch fetch up to 100 users per request.</p>"},{"location":"api/graphql/#search","title":"<code>search</code>","text":"<pre><code>async def search(\n    self,\n    query: str,\n    search_type: str = \"Latest\",\n    cursor: Optional[str] = None,\n    limit: int = 20\n) -&gt; Tuple[List[Tweet], Optional[str]]\n</code></pre> <p>Search tweets.</p> <p>Parameters: - <code>query</code>: Search query - <code>search_type</code>: <code>Latest</code>, <code>Top</code>, <code>People</code>, <code>Photos</code>, <code>Videos</code> - <code>cursor</code>: Pagination cursor - <code>limit</code>: Results per page</p>"},{"location":"api/graphql/#usage-examples","title":"Usage Examples","text":""},{"location":"api/graphql/#initialize-with-cookies","title":"Initialize with Cookies","text":"<pre><code>from xeepy.api.graphql import GraphQLClient\n\n# From dictionary\ngql = GraphQLClient(cookies={\n    \"ct0\": \"your-csrf-token\",\n    \"auth_token\": \"your-auth-token\"\n})\n\n# From file\ngql = GraphQLClient(cookies=\"cookies.json\")\n</code></pre>"},{"location":"api/graphql/#batch-fetch-tweets","title":"Batch Fetch Tweets","text":"<pre><code>from xeepy.api.graphql import GraphQLClient\n\nasync def main():\n    gql = GraphQLClient(cookies=\"cookies.json\")\n\n    tweet_ids = [\"123456\", \"789012\", \"345678\", ...]  # Up to 220\n\n    tweets = await gql.tweets_by_ids(tweet_ids)\n\n    for tweet in tweets:\n        print(f\"@{tweet.author.username}: {tweet.text[:50]}...\")\n        print(f\"  Likes: {tweet.like_count:,}\")\n\n    await gql.close()\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/graphql/#batch-fetch-users","title":"Batch Fetch Users","text":"<pre><code>from xeepy.api.graphql import GraphQLClient\n\nasync def main():\n    gql = GraphQLClient(cookies=\"cookies.json\")\n\n    user_ids = [\"123\", \"456\", \"789\", ...]  # Up to 100\n\n    users = await gql.users_by_ids(user_ids)\n\n    for user in users:\n        print(f\"@{user.username}: {user.followers_count:,} followers\")\n\n    await gql.close()\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/graphql/#get-single-user","title":"Get Single User","text":"<pre><code>from xeepy.api.graphql import GraphQLClient\n\nasync def main():\n    gql = GraphQLClient(cookies=\"cookies.json\")\n\n    user = await gql.get_user(\"elonmusk\")\n\n    print(f\"@{user.username}\")\n    print(f\"Name: {user.name}\")\n    print(f\"Followers: {user.followers_count:,}\")\n    print(f\"Following: {user.following_count:,}\")\n\n    await gql.close()\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/graphql/#get-user-timeline","title":"Get User Timeline","text":"<pre><code>from xeepy.api.graphql import GraphQLClient\n\nasync def main():\n    gql = GraphQLClient(cookies=\"cookies.json\")\n\n    user = await gql.get_user(\"username\")\n\n    # Get first page\n    tweets, cursor = await gql.get_user_tweets(user.id, limit=20)\n\n    for tweet in tweets:\n        print(f\"{tweet.created_at}: {tweet.text[:50]}...\")\n\n    # Get next page\n    if cursor:\n        more_tweets, next_cursor = await gql.get_user_tweets(\n            user.id, limit=20, cursor=cursor\n        )\n\n    await gql.close()\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/graphql/#search-tweets","title":"Search Tweets","text":"<pre><code>from xeepy.api.graphql import GraphQLClient\n\nasync def main():\n    gql = GraphQLClient(cookies=\"cookies.json\")\n\n    # Search latest tweets\n    tweets, cursor = await gql.search(\"Python programming\", search_type=\"Latest\")\n\n    print(f\"Found {len(tweets)} tweets:\")\n    for tweet in tweets:\n        print(f\"  @{tweet.author.username}: {tweet.text[:50]}...\")\n\n    # Get more results\n    if cursor:\n        more, next_cursor = await gql.search(\n            \"Python programming\",\n            search_type=\"Latest\",\n            cursor=cursor\n        )\n\n    await gql.close()\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/graphql/#engagement-actions","title":"Engagement Actions","text":"<pre><code>from xeepy.api.graphql import GraphQLClient\n\nasync def main():\n    gql = GraphQLClient(cookies=\"cookies.json\")\n\n    tweet_id = \"123456789\"\n\n    # Like\n    await gql.like(tweet_id)\n    print(\"Liked!\")\n\n    # Retweet\n    await gql.retweet(tweet_id)\n    print(\"Retweeted!\")\n\n    # Bookmark\n    await gql.bookmark(tweet_id)\n    print(\"Bookmarked!\")\n\n    await gql.close()\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/graphql/#follow-user","title":"Follow User","text":"<pre><code>from xeepy.api.graphql import GraphQLClient\n\nasync def main():\n    gql = GraphQLClient(cookies=\"cookies.json\")\n\n    # Get user ID first\n    user = await gql.get_user(\"username\")\n\n    # Follow\n    await gql.follow(user.id)\n    print(f\"Followed @{user.username}\")\n\n    await gql.close()\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/graphql/#post-tweet","title":"Post Tweet","text":"<pre><code>from xeepy.api.graphql import GraphQLClient\n\nasync def main():\n    gql = GraphQLClient(cookies=\"cookies.json\")\n\n    # Simple tweet\n    tweet = await gql.tweet(\"Hello from Xeepy! \ud83d\ude80\")\n    print(f\"Posted: {tweet.url}\")\n\n    # With media (upload media first)\n    # tweet = await gql.tweet(\"Check this out!\", media_ids=[\"123456\"])\n\n    await gql.close()\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/graphql/#context-manager","title":"Context Manager","text":"<pre><code>from xeepy.api.graphql import GraphQLClient\n\nasync def main():\n    async with GraphQLClient(cookies=\"cookies.json\") as gql:\n        user = await gql.get_user(\"username\")\n        print(f\"@{user.username}: {user.followers_count:,}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/graphql/#high-volume-scraping","title":"High-Volume Scraping","text":"<pre><code>from xeepy.api.graphql import GraphQLClient\n\nasync def scrape_large_dataset(tweet_ids: list):\n    gql = GraphQLClient(cookies=\"cookies.json\")\n\n    all_tweets = []\n\n    # Process in batches of 220\n    for i in range(0, len(tweet_ids), 220):\n        batch = tweet_ids[i:i+220]\n        tweets = await gql.tweets_by_ids(batch)\n        all_tweets.extend(tweets)\n        print(f\"Fetched {len(all_tweets)}/{len(tweet_ids)}\")\n\n    await gql.close()\n    return all_tweets\n\n# Can fetch 110,000 tweets in 15 minutes (500 requests \u00d7 220 tweets)\nasyncio.run(scrape_large_dataset(tweet_ids))\n</code></pre>"},{"location":"api/graphql/#with-proxy","title":"With Proxy","text":"<pre><code>from xeepy.api.graphql import GraphQLClient\n\ngql = GraphQLClient(\n    cookies=\"cookies.json\",\n    proxy=\"http://user:pass@proxy:8080\"\n)\n</code></pre>"},{"location":"api/graphql/#error-handling","title":"Error Handling","text":"<pre><code>from xeepy.api.graphql import GraphQLClient, GraphQLError\n\nasync def safe_fetch():\n    gql = GraphQLClient(cookies=\"cookies.json\")\n\n    try:\n        tweets = await gql.tweets_by_ids([\"123\", \"456\"])\n        return tweets\n    except GraphQLError as e:\n        print(f\"GraphQL error: {e}\")\n        return []\n    finally:\n        await gql.close()\n\nasyncio.run(safe_fetch())\n</code></pre>"},{"location":"api/graphql/#extract-cookies-from-browser","title":"Extract Cookies from Browser","text":"<pre><code>from xeepy.core.auth import AuthManager\nfrom xeepy.api.graphql import GraphQLClient\n\nasync def main():\n    # Extract from Chrome\n    auth = AuthManager()\n    await auth.import_cookies_from_browser(\"chrome\")\n    cookies = auth.get_auth_tokens()\n\n    # Use with GraphQL\n    gql = GraphQLClient(cookies=cookies)\n    user = await gql.get_user(\"username\")\n\n    await gql.close()\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/graphql/#rate-limits","title":"Rate Limits","text":"Endpoint Rate Limit Notes Batch tweets 500 / 15min 220 tweets per request Batch users 500 / 15min 100 users per request Single tweet 50 / 15min Single user 95 / 15min Search 50 / 15min Like/Retweet 50 / 15min"},{"location":"api/graphql/#see-also","title":"See Also","text":"<ul> <li>APIServer - REST API server</li> <li>AuthManager - Cookie management</li> <li>RateLimiter - Rate limiting</li> </ul>"},{"location":"api/server/","title":"APIServer","text":"<p>FastAPI-based REST API server for Xeepy.</p>"},{"location":"api/server/#import","title":"Import","text":"<pre><code>from xeepy.api.server import APIServer, create_app\n</code></pre>"},{"location":"api/server/#class-signature","title":"Class Signature","text":"<pre><code>class APIServer:\n    def __init__(\n        self,\n        host: str = \"0.0.0.0\",\n        port: int = 8000,\n        cookies: Optional[Union[Dict, str]] = None,\n        debug: bool = False\n    )\n</code></pre>"},{"location":"api/server/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>host</code> <code>str</code> <code>\"0.0.0.0\"</code> Server host <code>port</code> <code>int</code> <code>8000</code> Server port <code>cookies</code> <code>Optional[Union[Dict, str]]</code> <code>None</code> Auth cookies <code>debug</code> <code>bool</code> <code>False</code> Enable debug mode"},{"location":"api/server/#methods","title":"Methods","text":"Method Returns Description <code>start()</code> <code>None</code> Start the server <code>stop()</code> <code>None</code> Stop the server"},{"location":"api/server/#factory-function","title":"Factory Function","text":"<pre><code>def create_app(\n    cookies: Optional[Union[Dict, str]] = None\n) -&gt; FastAPI\n</code></pre> <p>Create FastAPI application instance.</p>"},{"location":"api/server/#api-endpoints","title":"API Endpoints","text":""},{"location":"api/server/#scraping","title":"Scraping","text":"Endpoint Method Description <code>/api/v1/profile/{username}</code> GET Get user profile <code>/api/v1/tweets/{username}</code> GET Get user tweets <code>/api/v1/followers/{username}</code> GET Get followers <code>/api/v1/following/{username}</code> GET Get following <code>/api/v1/replies/{tweet_id}</code> GET Get tweet replies <code>/api/v1/search</code> GET Search tweets"},{"location":"api/server/#actions","title":"Actions","text":"Endpoint Method Description <code>/api/v1/follow/{username}</code> POST Follow user <code>/api/v1/unfollow/{username}</code> POST Unfollow user <code>/api/v1/like/{tweet_id}</code> POST Like tweet <code>/api/v1/retweet/{tweet_id}</code> POST Retweet <code>/api/v1/tweet</code> POST Post tweet"},{"location":"api/server/#monitoring","title":"Monitoring","text":"Endpoint Method Description <code>/api/v1/unfollowers</code> GET Get unfollower report <code>/api/v1/analytics/{username}</code> GET Get analytics"},{"location":"api/server/#usage-examples","title":"Usage Examples","text":""},{"location":"api/server/#start-server","title":"Start Server","text":"<pre><code>from xeepy.api.server import APIServer\n\nserver = APIServer(\n    host=\"0.0.0.0\",\n    port=8000,\n    cookies=\"cookies.json\"\n)\n\nserver.start()\n</code></pre>"},{"location":"api/server/#cli-start","title":"CLI Start","text":"<pre><code>xeepy api start --port 8000 --cookies cookies.json\n</code></pre>"},{"location":"api/server/#create-custom-app","title":"Create Custom App","text":"<pre><code>from xeepy.api.server import create_app\nimport uvicorn\n\napp = create_app(cookies=\"cookies.json\")\n\n# Add custom routes\n@app.get(\"/custom\")\nasync def custom_endpoint():\n    return {\"message\": \"Custom endpoint\"}\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"api/server/#api-client-examples","title":"API Client Examples","text":""},{"location":"api/server/#get-profile","title":"Get Profile","text":"<pre><code>curl http://localhost:8000/api/v1/profile/elonmusk\n</code></pre> <pre><code>import httpx\n\nasync def get_profile(username: str):\n    async with httpx.AsyncClient() as client:\n        response = await client.get(\n            f\"http://localhost:8000/api/v1/profile/{username}\"\n        )\n        return response.json()\n</code></pre> <p>Response: <pre><code>{\n  \"username\": \"elonmusk\",\n  \"name\": \"Elon Musk\",\n  \"bio\": \"...\",\n  \"followers_count\": 150000000,\n  \"following_count\": 500,\n  \"tweet_count\": 25000\n}\n</code></pre></p>"},{"location":"api/server/#get-tweets","title":"Get Tweets","text":"<pre><code>curl \"http://localhost:8000/api/v1/tweets/elonmusk?limit=10\"\n</code></pre> <pre><code>async def get_tweets(username: str, limit: int = 10):\n    async with httpx.AsyncClient() as client:\n        response = await client.get(\n            f\"http://localhost:8000/api/v1/tweets/{username}\",\n            params={\"limit\": limit}\n        )\n        return response.json()\n</code></pre>"},{"location":"api/server/#get-followers","title":"Get Followers","text":"<pre><code>curl \"http://localhost:8000/api/v1/followers/username?limit=100\"\n</code></pre>"},{"location":"api/server/#search-tweets","title":"Search Tweets","text":"<pre><code>curl \"http://localhost:8000/api/v1/search?q=python&amp;limit=20\"\n</code></pre> <pre><code>async def search_tweets(query: str, limit: int = 20):\n    async with httpx.AsyncClient() as client:\n        response = await client.get(\n            \"http://localhost:8000/api/v1/search\",\n            params={\"q\": query, \"limit\": limit}\n        )\n        return response.json()\n</code></pre>"},{"location":"api/server/#follow-user","title":"Follow User","text":"<pre><code>curl -X POST http://localhost:8000/api/v1/follow/username\n</code></pre> <pre><code>async def follow_user(username: str):\n    async with httpx.AsyncClient() as client:\n        response = await client.post(\n            f\"http://localhost:8000/api/v1/follow/{username}\"\n        )\n        return response.json()\n</code></pre>"},{"location":"api/server/#post-tweet","title":"Post Tweet","text":"<pre><code>curl -X POST http://localhost:8000/api/v1/tweet \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"text\": \"Hello from Xeepy API!\"}'\n</code></pre> <pre><code>async def post_tweet(text: str):\n    async with httpx.AsyncClient() as client:\n        response = await client.post(\n            \"http://localhost:8000/api/v1/tweet\",\n            json={\"text\": text}\n        )\n        return response.json()\n</code></pre>"},{"location":"api/server/#like-tweet","title":"Like Tweet","text":"<pre><code>curl -X POST http://localhost:8000/api/v1/like/123456789\n</code></pre>"},{"location":"api/server/#authentication","title":"Authentication","text":""},{"location":"api/server/#api-key-authentication","title":"API Key Authentication","text":"<pre><code>from xeepy.api.server import create_app\nfrom fastapi import Depends, HTTPException, Header\n\napp = create_app(cookies=\"cookies.json\")\n\nAPI_KEY = \"your-secret-api-key\"\n\nasync def verify_api_key(x_api_key: str = Header(...)):\n    if x_api_key != API_KEY:\n        raise HTTPException(status_code=401, detail=\"Invalid API key\")\n    return x_api_key\n\n# Protected endpoint\n@app.get(\"/api/v1/protected\")\nasync def protected_endpoint(api_key: str = Depends(verify_api_key)):\n    return {\"message\": \"Authenticated!\"}\n</code></pre> <pre><code>curl -H \"X-API-Key: your-secret-api-key\" \\\n  http://localhost:8000/api/v1/protected\n</code></pre>"},{"location":"api/server/#rate-limiting","title":"Rate Limiting","text":"<pre><code>from xeepy.api.server import create_app\nfrom slowapi import Limiter\nfrom slowapi.util import get_remote_address\n\nlimiter = Limiter(key_func=get_remote_address)\napp = create_app(cookies=\"cookies.json\")\napp.state.limiter = limiter\n\n@app.get(\"/api/v1/rate-limited\")\n@limiter.limit(\"10/minute\")\nasync def rate_limited_endpoint():\n    return {\"message\": \"Rate limited endpoint\"}\n</code></pre>"},{"location":"api/server/#docker-deployment","title":"Docker Deployment","text":"<pre><code>FROM python:3.10-slim\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nCMD [\"uvicorn\", \"xeepy.api.server:create_app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre> <pre><code>docker build -t xeepy-api .\ndocker run -p 8000:8000 -v ./cookies.json:/app/cookies.json xeepy-api\n</code></pre>"},{"location":"api/server/#openapi-documentation","title":"OpenAPI Documentation","text":"<p>Access Swagger UI at: <code>http://localhost:8000/docs</code></p> <p>Access ReDoc at: <code>http://localhost:8000/redoc</code></p>"},{"location":"api/server/#error-handling","title":"Error Handling","text":"<pre><code>from xeepy.api.server import create_app\nfrom fastapi import HTTPException\n\napp = create_app(cookies=\"cookies.json\")\n\n@app.exception_handler(Exception)\nasync def global_exception_handler(request, exc):\n    return JSONResponse(\n        status_code=500,\n        content={\"error\": str(exc)}\n    )\n</code></pre>"},{"location":"api/server/#health-check","title":"Health Check","text":"<pre><code>curl http://localhost:8000/health\n</code></pre> <p>Response: <pre><code>{\n  \"status\": \"healthy\",\n  \"version\": \"1.0.0\"\n}\n</code></pre></p>"},{"location":"api/server/#cors-configuration","title":"CORS Configuration","text":"<pre><code>from xeepy.api.server import create_app\nfrom fastapi.middleware.cors import CORSMiddleware\n\napp = create_app(cookies=\"cookies.json\")\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n</code></pre>"},{"location":"api/server/#websocket-support","title":"WebSocket Support","text":"<pre><code>from xeepy.api.server import create_app\nfrom fastapi import WebSocket\n\napp = create_app(cookies=\"cookies.json\")\n\n@app.websocket(\"/ws/tweets/{username}\")\nasync def tweet_stream(websocket: WebSocket, username: str):\n    await websocket.accept()\n\n    # Stream new tweets\n    while True:\n        # Fetch new tweets\n        # await websocket.send_json(tweet.to_dict())\n        pass\n</code></pre>"},{"location":"api/server/#see-also","title":"See Also","text":"<ul> <li>GraphQLClient - GraphQL API</li> <li>AuthManager - Authentication</li> <li>Xeepy - Main class</li> </ul>"},{"location":"api/actions/engage/","title":"EngageActions","text":"<p>Actions for engaging with tweets: like, retweet, reply, bookmark, and quote.</p>"},{"location":"api/actions/engage/#import","title":"Import","text":"<pre><code>from xeepy.actions.engage import EngageActions\n</code></pre>"},{"location":"api/actions/engage/#class-signature","title":"Class Signature","text":"<pre><code>class EngageActions:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/actions/engage/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/actions/engage/#methods","title":"Methods","text":"Method Returns Description <code>like(tweet_url)</code> <code>bool</code> Like a tweet <code>unlike(tweet_url)</code> <code>bool</code> Unlike a tweet <code>retweet(tweet_url)</code> <code>bool</code> Retweet a tweet <code>unretweet(tweet_url)</code> <code>bool</code> Undo retweet <code>reply(tweet_url, text)</code> <code>Tweet</code> Reply to a tweet <code>quote(tweet_url, text)</code> <code>Tweet</code> Quote tweet <code>bookmark(tweet_url)</code> <code>bool</code> Bookmark a tweet <code>unbookmark(tweet_url)</code> <code>bool</code> Remove bookmark <code>auto_like(keywords, limit)</code> <code>EngageResult</code> Auto-like by keywords"},{"location":"api/actions/engage/#like","title":"<code>like</code>","text":"<pre><code>async def like(\n    self,\n    tweet_url: str,\n    check_existing: bool = True\n) -&gt; bool\n</code></pre> <p>Like a specific tweet.</p> <p>Parameters: - <code>tweet_url</code>: URL of the tweet to like - <code>check_existing</code>: Skip if already liked</p>"},{"location":"api/actions/engage/#reply","title":"<code>reply</code>","text":"<pre><code>async def reply(\n    self,\n    tweet_url: str,\n    text: str,\n    media: Optional[List[str]] = None\n) -&gt; Tweet\n</code></pre> <p>Reply to a tweet.</p> <p>Parameters: - <code>tweet_url</code>: URL of the tweet to reply to - <code>text</code>: Reply text content - <code>media</code>: Optional media file paths</p>"},{"location":"api/actions/engage/#quote","title":"<code>quote</code>","text":"<pre><code>async def quote(\n    self,\n    tweet_url: str,\n    text: str,\n    media: Optional[List[str]] = None\n) -&gt; Tweet\n</code></pre> <p>Quote tweet with comment.</p>"},{"location":"api/actions/engage/#auto_like","title":"<code>auto_like</code>","text":"<pre><code>async def auto_like(\n    self,\n    keywords: List[str],\n    limit: int = 50,\n    search_type: str = \"latest\",\n    delay_range: Tuple[float, float] = (2.0, 5.0),\n    min_followers: int = 100\n) -&gt; EngageResult\n</code></pre> <p>Automatically like tweets matching keywords.</p> <p>Parameters: - <code>keywords</code>: Keywords to search for - <code>limit</code>: Maximum tweets to like - <code>search_type</code>: Search type (<code>latest</code>, <code>top</code>) - <code>delay_range</code>: Delay between likes - <code>min_followers</code>: Min author followers</p>"},{"location":"api/actions/engage/#engageresult-object","title":"EngageResult Object","text":"<pre><code>@dataclass\nclass EngageResult:\n    liked: List[str]                 # Successfully liked tweet IDs\n    retweeted: List[str]             # Successfully retweeted\n    replied: List[str]               # Successfully replied\n    failed: List[Dict]               # Failed actions with errors\n    total_engagement: int            # Total successful actions\n</code></pre>"},{"location":"api/actions/engage/#usage-examples","title":"Usage Examples","text":""},{"location":"api/actions/engage/#like-a-tweet","title":"Like a Tweet","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        success = await x.engage.like(\n            \"https://x.com/user/status/123456789\"\n        )\n\n        if success:\n            print(\"Tweet liked!\")\n        else:\n            print(\"Failed to like or already liked\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/engage/#retweet","title":"Retweet","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        success = await x.engage.retweet(\n            \"https://x.com/user/status/123456789\"\n        )\n\n        print(\"Retweeted!\" if success else \"Failed to retweet\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/engage/#reply-to-tweet","title":"Reply to Tweet","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        reply = await x.engage.reply(\n            \"https://x.com/user/status/123456789\",\n            \"Great tweet! Thanks for sharing \ud83d\ude4f\"\n        )\n\n        print(f\"Reply posted: {reply.id}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/engage/#reply-with-media","title":"Reply with Media","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        reply = await x.engage.reply(\n            \"https://x.com/user/status/123456789\",\n            \"Check out this image!\",\n            media=[\"screenshot.png\"]\n        )\n\n        print(f\"Reply with media posted: {reply.id}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/engage/#quote-tweet","title":"Quote Tweet","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        quote = await x.engage.quote(\n            \"https://x.com/user/status/123456789\",\n            \"This is so important! Everyone should read this thread \ud83d\udc47\"\n        )\n\n        print(f\"Quote tweet posted: {quote.id}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/engage/#bookmark-tweet","title":"Bookmark Tweet","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        await x.engage.bookmark(\"https://x.com/user/status/123\")\n        print(\"Tweet bookmarked!\")\n\n        # Remove bookmark later\n        await x.engage.unbookmark(\"https://x.com/user/status/123\")\n        print(\"Bookmark removed!\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/engage/#auto-like-by-keywords","title":"Auto-Like by Keywords","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.engage.auto_like(\n            keywords=[\"python\", \"programming\"],\n            limit=50,\n            search_type=\"latest\",\n            delay_range=(3.0, 8.0),\n            min_followers=500\n        )\n\n        print(f\"Liked {len(result.liked)} tweets\")\n        print(f\"Failed: {len(result.failed)}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/engage/#batch-engagement","title":"Batch Engagement","text":"<pre><code>from xeepy import Xeepy\n\nasync def engage_with_tweets(tweet_urls: list):\n    async with Xeepy() as x:\n        results = {\"liked\": 0, \"retweeted\": 0, \"failed\": 0}\n\n        for url in tweet_urls:\n            try:\n                await x.engage.like(url)\n                results[\"liked\"] += 1\n\n                await x.engage.retweet(url)\n                results[\"retweeted\"] += 1\n\n                await asyncio.sleep(random.uniform(5, 15))\n            except Exception as e:\n                results[\"failed\"] += 1\n                print(f\"Failed {url}: {e}\")\n\n        return results\n\nasyncio.run(engage_with_tweets(urls))\n</code></pre>"},{"location":"api/actions/engage/#smart-auto-engagement","title":"Smart Auto-Engagement","text":"<pre><code>from xeepy import Xeepy\n\nasync def smart_engage(keywords: list, daily_limit: int = 100):\n    \"\"\"Engage smartly with content matching keywords.\"\"\"\n    async with Xeepy() as x:\n        engaged = 0\n\n        for keyword in keywords:\n            if engaged &gt;= daily_limit:\n                break\n\n            # Search for tweets\n            results = await x.scrape.search(\n                keyword,\n                search_type=\"latest\",\n                limit=30\n            )\n\n            for tweet in results.items:\n                if engaged &gt;= daily_limit:\n                    break\n\n                # Skip low-quality accounts\n                if tweet.author.followers_count &lt; 100:\n                    continue\n\n                # Like the tweet\n                url = f\"https://x.com/{tweet.author.username}/status/{tweet.id}\"\n                await x.engage.like(url)\n                engaged += 1\n\n                # Random delay\n                await asyncio.sleep(random.uniform(10, 30))\n\n        print(f\"Engaged with {engaged} tweets\")\n\nasyncio.run(smart_engage([\"#python\", \"#datascience\"]))\n</code></pre>"},{"location":"api/actions/engage/#see-also","title":"See Also","text":"<ul> <li>FollowActions - Follow operations</li> <li>SearchScraper - Search functionality</li> <li>Tweet Model - Tweet data structure</li> </ul>"},{"location":"api/actions/follow/","title":"FollowActions","text":"<p>Actions for following users on X/Twitter.</p>"},{"location":"api/actions/follow/#import","title":"Import","text":"<pre><code>from xeepy.actions.follow import FollowActions\n</code></pre>"},{"location":"api/actions/follow/#class-signature","title":"Class Signature","text":"<pre><code>class FollowActions:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/actions/follow/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/actions/follow/#methods","title":"Methods","text":"Method Returns Description <code>user(username)</code> <code>bool</code> Follow a single user <code>users(usernames)</code> <code>FollowResult</code> Follow multiple users <code>by_hashtag(hashtag, limit)</code> <code>FollowResult</code> Follow users by hashtag <code>by_keyword(keyword, limit)</code> <code>FollowResult</code> Follow users by keyword <code>followers_of(username, limit)</code> <code>FollowResult</code> Follow user's followers <code>likers_of(tweet_url, limit)</code> <code>FollowResult</code> Follow tweet likers"},{"location":"api/actions/follow/#user","title":"<code>user</code>","text":"<pre><code>async def user(\n    self,\n    username: str,\n    check_existing: bool = True\n) -&gt; bool\n</code></pre> <p>Follow a single user.</p> <p>Parameters: - <code>username</code>: Username to follow (without @) - <code>check_existing</code>: Skip if already following</p> <p>Returns: <code>True</code> if followed successfully</p>"},{"location":"api/actions/follow/#users","title":"<code>users</code>","text":"<pre><code>async def users(\n    self,\n    usernames: List[str],\n    delay_range: Tuple[float, float] = (2.0, 5.0),\n    skip_existing: bool = True\n) -&gt; FollowResult\n</code></pre> <p>Follow multiple users with delays.</p> <p>Parameters: - <code>usernames</code>: List of usernames to follow - <code>delay_range</code>: Random delay range between follows (seconds) - <code>skip_existing</code>: Skip already followed users</p>"},{"location":"api/actions/follow/#by_hashtag","title":"<code>by_hashtag</code>","text":"<pre><code>async def by_hashtag(\n    self,\n    hashtag: str,\n    limit: int = 50,\n    min_followers: int = 100,\n    delay_range: Tuple[float, float] = (3.0, 8.0)\n) -&gt; FollowResult\n</code></pre> <p>Follow users who post with a specific hashtag.</p> <p>Parameters: - <code>hashtag</code>: Target hashtag (with or without #) - <code>limit</code>: Maximum users to follow - <code>min_followers</code>: Minimum follower count filter - <code>delay_range</code>: Delay between follows</p>"},{"location":"api/actions/follow/#followers_of","title":"<code>followers_of</code>","text":"<pre><code>async def followers_of(\n    self,\n    username: str,\n    limit: int = 50,\n    min_followers: int = 100\n) -&gt; FollowResult\n</code></pre> <p>Follow followers of another user.</p>"},{"location":"api/actions/follow/#followresult-object","title":"FollowResult Object","text":"<pre><code>@dataclass\nclass FollowResult:\n    followed: List[str]              # Successfully followed usernames\n    failed: List[Dict]               # Failed follows with errors\n    skipped: List[str]               # Skipped (already following)\n    total_attempted: int             # Total attempts\n\n    @property\n    def success_rate(self) -&gt; float:\n        return len(self.followed) / self.total_attempted if self.total_attempted else 0\n</code></pre>"},{"location":"api/actions/follow/#usage-examples","title":"Usage Examples","text":""},{"location":"api/actions/follow/#follow-single-user","title":"Follow Single User","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        success = await x.follow.user(\"username\")\n\n        if success:\n            print(\"Followed successfully!\")\n        else:\n            print(\"Follow failed or already following\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/follow/#follow-multiple-users","title":"Follow Multiple Users","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        usernames = [\"user1\", \"user2\", \"user3\", \"user4\"]\n\n        result = await x.follow.users(\n            usernames,\n            delay_range=(3.0, 7.0)\n        )\n\n        print(f\"Followed: {len(result.followed)}\")\n        print(f\"Failed: {len(result.failed)}\")\n        print(f\"Skipped: {len(result.skipped)}\")\n        print(f\"Success rate: {result.success_rate:.1%}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/follow/#follow-by-hashtag","title":"Follow by Hashtag","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.follow.by_hashtag(\n            \"#python\",\n            limit=30,\n            min_followers=500,\n            delay_range=(5.0, 10.0)\n        )\n\n        print(f\"Followed {len(result.followed)} Python users\")\n\n        for username in result.followed:\n            print(f\"  \u2713 @{username}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/follow/#follow-followers-of-influencer","title":"Follow Followers of Influencer","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.follow.followers_of(\n            \"elonmusk\",\n            limit=50,\n            min_followers=1000\n        )\n\n        print(f\"Followed {len(result.followed)} accounts\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/follow/#follow-tweet-likers","title":"Follow Tweet Likers","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.follow.likers_of(\n            \"https://x.com/user/status/123456789\",\n            limit=30\n        )\n\n        print(f\"Followed {len(result.followed)} users who liked the tweet\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/follow/#follow-by-keyword-search","title":"Follow by Keyword Search","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.follow.by_keyword(\n            \"data scientist\",\n            limit=40,\n            min_followers=500\n        )\n\n        print(f\"Followed {len(result.followed)} data scientists\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/follow/#follow-with-whitelist-check","title":"Follow with Whitelist Check","text":"<pre><code>from xeepy import Xeepy\n\nasync def follow_with_filter(usernames: list, blacklist: set):\n    async with Xeepy() as x:\n        filtered = [u for u in usernames if u not in blacklist]\n\n        result = await x.follow.users(filtered)\n\n        return result\n\nblacklist = {\"spammer1\", \"spammer2\", \"bot_account\"}\nasyncio.run(follow_with_filter([\"user1\", \"user2\"], blacklist))\n</code></pre>"},{"location":"api/actions/follow/#follow-with-daily-limit","title":"Follow with Daily Limit","text":"<pre><code>from xeepy import Xeepy\nimport asyncio\n\nasync def safe_follow(usernames: list, daily_limit: int = 100):\n    \"\"\"Follow with daily limit protection.\"\"\"\n    async with Xeepy() as x:\n        followed_today = 0\n\n        for username in usernames:\n            if followed_today &gt;= daily_limit:\n                print(f\"Daily limit ({daily_limit}) reached\")\n                break\n\n            success = await x.follow.user(username)\n            if success:\n                followed_today += 1\n                print(f\"Followed @{username} ({followed_today}/{daily_limit})\")\n\n            # Extra delay to stay safe\n            await asyncio.sleep(random.uniform(10, 30))\n\n        return followed_today\n\nasyncio.run(safe_follow(usernames, daily_limit=50))\n</code></pre>"},{"location":"api/actions/follow/#see-also","title":"See Also","text":"<ul> <li>UnfollowActions - Unfollow operations</li> <li>EngageActions - Engagement actions</li> <li>FollowersScraper - Scrape followers</li> </ul>"},{"location":"api/actions/messaging/","title":"DirectMessageActions","text":"<p>Actions for sending and managing Direct Messages on X/Twitter.</p>"},{"location":"api/actions/messaging/#import","title":"Import","text":"<pre><code>from xeepy.actions.messaging import DirectMessageActions\n</code></pre>"},{"location":"api/actions/messaging/#class-signature","title":"Class Signature","text":"<pre><code>class DirectMessageActions:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/actions/messaging/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/actions/messaging/#methods","title":"Methods","text":"Method Returns Description <code>send(message, recipients)</code> <code>DMResult</code> Send DM to users <code>inbox()</code> <code>InboxResult</code> Get inbox conversations <code>history(conversation_ids)</code> <code>List[Conversation]</code> Get conversation history <code>search(query)</code> <code>List[Message]</code> Search DMs <code>delete(conversation_id)</code> <code>bool</code> Delete conversation <code>mark_read(conversation_id)</code> <code>bool</code> Mark as read"},{"location":"api/actions/messaging/#send","title":"<code>send</code>","text":"<pre><code>async def send(\n    self,\n    message: str,\n    recipients: List[str],\n    media: Optional[str] = None,\n    delay_range: Tuple[float, float] = (5.0, 15.0)\n) -&gt; DMResult\n</code></pre> <p>Send a direct message to one or more users.</p> <p>Parameters: - <code>message</code>: Message text - <code>recipients</code>: List of usernames to message - <code>media</code>: Optional media file path - <code>delay_range</code>: Delay between messages (seconds)</p>"},{"location":"api/actions/messaging/#inbox","title":"<code>inbox</code>","text":"<pre><code>async def inbox(\n    self,\n    limit: int = 50,\n    unread_only: bool = False\n) -&gt; InboxResult\n</code></pre> <p>Get inbox conversations.</p> <p>Parameters: - <code>limit</code>: Maximum conversations to fetch - <code>unread_only</code>: Only return unread conversations</p>"},{"location":"api/actions/messaging/#history","title":"<code>history</code>","text":"<pre><code>async def history(\n    self,\n    conversation_ids: List[str],\n    limit: int = 100\n) -&gt; List[Conversation]\n</code></pre> <p>Get message history for conversations.</p>"},{"location":"api/actions/messaging/#search","title":"<code>search</code>","text":"<pre><code>async def search(\n    self,\n    query: str,\n    limit: int = 50\n) -&gt; List[Message]\n</code></pre> <p>Search through DM history.</p>"},{"location":"api/actions/messaging/#delete","title":"<code>delete</code>","text":"<pre><code>async def delete(\n    self,\n    conversation_id: str,\n    message_id: Optional[str] = None\n) -&gt; bool\n</code></pre> <p>Delete a conversation or specific message.</p>"},{"location":"api/actions/messaging/#dmresult-object","title":"DMResult Object","text":"<pre><code>@dataclass\nclass DMResult:\n    sent: List[str]                  # Successfully messaged usernames\n    failed: List[Dict]               # Failed with errors\n    blocked: List[str]               # Users who block DMs\n</code></pre>"},{"location":"api/actions/messaging/#inboxresult-object","title":"InboxResult Object","text":"<pre><code>@dataclass\nclass InboxResult:\n    conversations: List[Conversation]\n    total_unread: int\n    cursor: Optional[str]\n</code></pre>"},{"location":"api/actions/messaging/#conversation-object","title":"Conversation Object","text":"<pre><code>@dataclass\nclass Conversation:\n    id: str                          # Conversation ID\n    participant_usernames: List[str] # Participants\n    last_message: Message            # Last message\n    unread_count: int                # Unread messages\n    created_at: datetime             # Conversation start\n    updated_at: datetime             # Last activity\n</code></pre>"},{"location":"api/actions/messaging/#message-object","title":"Message Object","text":"<pre><code>@dataclass\nclass Message:\n    id: str                          # Message ID\n    text: str                        # Message content\n    sender: str                      # Sender username\n    created_at: datetime             # Sent time\n    media: Optional[List[Media]]     # Attached media\n    is_read: bool                    # Read status\n</code></pre>"},{"location":"api/actions/messaging/#usage-examples","title":"Usage Examples","text":""},{"location":"api/actions/messaging/#send-single-dm","title":"Send Single DM","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.dm.send(\n            \"Hey! Thanks for following. Let me know if you have any questions!\",\n            recipients=[\"username\"]\n        )\n\n        if result.sent:\n            print(\"Message sent successfully!\")\n        else:\n            print(f\"Failed: {result.failed}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/messaging/#send-dm-with-media","title":"Send DM with Media","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.dm.send(\n            \"Check out this image!\",\n            recipients=[\"username\"],\n            media=\"screenshot.png\"\n        )\n\n        print(f\"Sent to: {result.sent}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/messaging/#send-to-multiple-recipients","title":"Send to Multiple Recipients","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.dm.send(\n            \"Hello! Thanks for your interest in our product.\",\n            recipients=[\"user1\", \"user2\", \"user3\"],\n            delay_range=(10.0, 30.0)  # Longer delays for safety\n        )\n\n        print(f\"Successfully sent: {len(result.sent)}\")\n        print(f\"Failed: {len(result.failed)}\")\n        print(f\"Blocked DMs: {len(result.blocked)}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/messaging/#get-inbox","title":"Get Inbox","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        inbox = await x.dm.inbox(limit=20)\n\n        print(f\"Total unread: {inbox.total_unread}\")\n        print(\"\\nConversations:\")\n\n        for conv in inbox.conversations:\n            participants = \", \".join(conv.participant_usernames)\n            unread = f\"({conv.unread_count} unread)\" if conv.unread_count else \"\"\n            print(f\"  {participants} {unread}\")\n            print(f\"    Last: {conv.last_message.text[:50]}...\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/messaging/#get-conversation-history","title":"Get Conversation History","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        conversations = await x.dm.history(\n            conversation_ids=[\"123456-789012\"],\n            limit=100\n        )\n\n        for conv in conversations:\n            print(f\"Conversation with {conv.participant_usernames}:\")\n            for msg in conv.messages:\n                print(f\"  [{msg.sender}] {msg.text}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/messaging/#search-dms","title":"Search DMs","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        messages = await x.dm.search(\"meeting tomorrow\")\n\n        print(f\"Found {len(messages)} messages:\")\n        for msg in messages:\n            print(f\"  [{msg.sender}]: {msg.text[:60]}...\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/messaging/#delete-conversation","title":"Delete Conversation","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Delete entire conversation\n        success = await x.dm.delete(conversation_id=\"123456-789012\")\n\n        # Delete specific message\n        success = await x.dm.delete(\n            conversation_id=\"123456-789012\",\n            message_id=\"msg123\"\n        )\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/messaging/#welcome-new-followers","title":"Welcome New Followers","text":"<pre><code>from xeepy import Xeepy\n\nasync def welcome_new_followers(previous_followers: set):\n    async with Xeepy() as x:\n        # Get current followers\n        result = await x.scrape.followers(\"myusername\", limit=1000)\n        current = {f.username for f in result.items}\n\n        # Find new followers\n        new_followers = current - previous_followers\n\n        if new_followers:\n            message = \"Thanks for following! Feel free to reach out if you have questions.\"\n\n            result = await x.dm.send(\n                message,\n                recipients=list(new_followers),\n                delay_range=(30.0, 60.0)  # Be careful with mass DMs\n            )\n\n            print(f\"Welcomed {len(result.sent)} new followers\")\n\n        return current\n\nasyncio.run(welcome_new_followers(set()))\n</code></pre>"},{"location":"api/actions/messaging/#see-also","title":"See Also","text":"<ul> <li>FollowActions - Follow operations</li> <li>FollowersScraper - Get followers</li> <li>User Model - User data structure</li> </ul>"},{"location":"api/actions/polls/","title":"PollActions","text":"<p>Actions for creating and managing polls on X/Twitter.</p>"},{"location":"api/actions/polls/#import","title":"Import","text":"<pre><code>from xeepy.actions.polls import PollActions\n</code></pre>"},{"location":"api/actions/polls/#class-signature","title":"Class Signature","text":"<pre><code>class PollActions:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/actions/polls/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/actions/polls/#methods","title":"Methods","text":"Method Returns Description <code>create_poll(question, options, duration)</code> <code>Tweet</code> Create a poll <code>vote(tweet_url, option_index)</code> <code>bool</code> Vote on a poll <code>get_poll_results(tweet_url)</code> <code>PollResults</code> Get poll results"},{"location":"api/actions/polls/#create_poll","title":"<code>create_poll</code>","text":"<pre><code>async def create_poll(\n    self,\n    question: str,\n    options: List[str],\n    duration_minutes: int = 1440\n) -&gt; Tweet\n</code></pre> <p>Create a new poll tweet.</p> <p>Parameters: - <code>question</code>: Poll question (tweet text) - <code>options</code>: List of 2-4 poll options - <code>duration_minutes</code>: Poll duration (5 min to 7 days, default 24h)</p> <p>Duration Limits: - Minimum: 5 minutes - Maximum: 10080 minutes (7 days) - Default: 1440 minutes (24 hours)</p>"},{"location":"api/actions/polls/#vote","title":"<code>vote</code>","text":"<pre><code>async def vote(\n    self,\n    tweet_url: str,\n    option_index: int\n) -&gt; bool\n</code></pre> <p>Vote on a poll.</p> <p>Parameters: - <code>tweet_url</code>: URL of the poll tweet - <code>option_index</code>: 0-based index of the option to vote for</p>"},{"location":"api/actions/polls/#get_poll_results","title":"<code>get_poll_results</code>","text":"<pre><code>async def get_poll_results(\n    self,\n    tweet_url: str\n) -&gt; PollResults\n</code></pre> <p>Get current poll results.</p>"},{"location":"api/actions/polls/#pollresults-object","title":"PollResults Object","text":"<pre><code>@dataclass\nclass PollResults:\n    question: str                    # Poll question\n    options: List[PollOption]        # Options with votes\n    total_votes: int                 # Total vote count\n    end_time: datetime               # When poll ends\n    is_final: bool                   # Whether poll has ended\n    voted_option: Optional[int]      # User's vote (if voted)\n</code></pre>"},{"location":"api/actions/polls/#polloption-object","title":"PollOption Object","text":"<pre><code>@dataclass\nclass PollOption:\n    text: str                        # Option text\n    votes: int                       # Vote count\n    percentage: float                # Vote percentage\n</code></pre>"},{"location":"api/actions/polls/#usage-examples","title":"Usage Examples","text":""},{"location":"api/actions/polls/#create-a-simple-poll","title":"Create a Simple Poll","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        poll = await x.poll.create_poll(\n            \"What's your favorite programming language?\",\n            [\"Python\", \"JavaScript\", \"Rust\", \"Go\"],\n            duration_minutes=1440  # 24 hours\n        )\n\n        print(f\"Poll created: https://x.com/i/status/{poll.id}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/polls/#create-quick-poll-5-minutes","title":"Create Quick Poll (5 minutes)","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        poll = await x.poll.create_poll(\n            \"Quick vote: Coffee or Tea?\",\n            [\"\u2615 Coffee\", \"\ud83c\udf75 Tea\"],\n            duration_minutes=5\n        )\n\n        print(f\"Quick poll created! Ends in 5 minutes.\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/polls/#create-week-long-poll","title":"Create Week-Long Poll","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        poll = await x.poll.create_poll(\n            \"What feature should we build next?\",\n            [\"Dark mode\", \"Mobile app\", \"API access\", \"Integrations\"],\n            duration_minutes=10080  # 7 days\n        )\n\n        print(f\"Poll running for 7 days!\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/polls/#vote-on-a-poll","title":"Vote on a Poll","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        success = await x.poll.vote(\n            \"https://x.com/user/status/123456789\",\n            option_index=0  # Vote for first option\n        )\n\n        print(\"Vote recorded!\" if success else \"Failed to vote\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/polls/#get-poll-results","title":"Get Poll Results","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        results = await x.poll.get_poll_results(\n            \"https://x.com/user/status/123456789\"\n        )\n\n        print(f\"Question: {results.question}\")\n        print(f\"Total votes: {results.total_votes}\")\n        print(f\"Status: {'Ended' if results.is_final else 'Active'}\")\n        print()\n\n        for i, option in enumerate(results.options):\n            bar = \"\u2588\" * int(option.percentage / 5) + \"\u2591\" * (20 - int(option.percentage / 5))\n            print(f\"  {option.text}\")\n            print(f\"    {bar} {option.percentage:.1f}% ({option.votes} votes)\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/polls/#monitor-poll-progress","title":"Monitor Poll Progress","text":"<pre><code>from xeepy import Xeepy\nimport asyncio\n\nasync def monitor_poll(tweet_url: str, interval: int = 300):\n    \"\"\"Monitor poll results in real-time.\"\"\"\n    async with Xeepy() as x:\n        while True:\n            results = await x.poll.get_poll_results(tweet_url)\n\n            print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Poll Update\")\n            print(f\"Total votes: {results.total_votes}\")\n\n            for option in results.options:\n                print(f\"  {option.text}: {option.percentage:.1f}%\")\n\n            if results.is_final:\n                print(\"\\nPoll has ended!\")\n                break\n\n            await asyncio.sleep(interval)\n\nasyncio.run(monitor_poll(\"https://x.com/user/status/123\"))\n</code></pre>"},{"location":"api/actions/polls/#create-poll-with-emojis","title":"Create Poll with Emojis","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        poll = await x.poll.create_poll(\n            \"Rate today's presentation:\",\n            [\"\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f Excellent\", \"\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f Great\", \"\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f Good\", \"\ud83c\udf1f\ud83c\udf1f Needs work\"],\n            duration_minutes=60\n        )\n\n        print(\"Rating poll created!\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/polls/#ab-testing-poll","title":"A/B Testing Poll","text":"<pre><code>from xeepy import Xeepy\n\nasync def ab_test(options: list, duration_hours: int = 24):\n    \"\"\"Run A/B test with a poll.\"\"\"\n    async with Xeepy() as x:\n        poll = await x.poll.create_poll(\n            \"Which headline do you prefer?\",\n            options,\n            duration_minutes=duration_hours * 60\n        )\n\n        print(f\"A/B test started. Poll ID: {poll.id}\")\n        print(f\"Check results in {duration_hours} hours.\")\n\n        return poll.id\n\noptions = [\n    \"A: 10 Tips to Boost Productivity\",\n    \"B: How I 10x'd My Productivity\"\n]\nasyncio.run(ab_test(options))\n</code></pre>"},{"location":"api/actions/polls/#export-poll-results","title":"Export Poll Results","text":"<pre><code>from xeepy import Xeepy\nimport json\n\nasync def export_poll_results(tweet_url: str, output_file: str):\n    async with Xeepy() as x:\n        results = await x.poll.get_poll_results(tweet_url)\n\n        data = {\n            \"question\": results.question,\n            \"total_votes\": results.total_votes,\n            \"is_final\": results.is_final,\n            \"end_time\": results.end_time.isoformat(),\n            \"options\": [\n                {\n                    \"text\": opt.text,\n                    \"votes\": opt.votes,\n                    \"percentage\": opt.percentage\n                }\n                for opt in results.options\n            ]\n        }\n\n        with open(output_file, \"w\") as f:\n            json.dump(data, f, indent=2)\n\n        print(f\"Results exported to {output_file}\")\n\nasyncio.run(export_poll_results(\"https://x.com/user/status/123\", \"poll_results.json\"))\n</code></pre>"},{"location":"api/actions/polls/#see-also","title":"See Also","text":"<ul> <li>SchedulingActions - Schedule polls</li> <li>EngageActions - Tweet engagement</li> <li>Tweet Model - Tweet data structure</li> </ul>"},{"location":"api/actions/scheduling/","title":"SchedulingActions","text":"<p>Actions for scheduling tweets and managing drafts on X/Twitter.</p>"},{"location":"api/actions/scheduling/#import","title":"Import","text":"<pre><code>from xeepy.actions.scheduling import SchedulingActions\n</code></pre>"},{"location":"api/actions/scheduling/#class-signature","title":"Class Signature","text":"<pre><code>class SchedulingActions:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/actions/scheduling/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/actions/scheduling/#methods","title":"Methods","text":"Method Returns Description <code>schedule_tweet(text, time)</code> <code>ScheduledTweet</code> Schedule a tweet <code>schedule_reply(text, tweet_id, time)</code> <code>ScheduledTweet</code> Schedule a reply <code>scheduled_tweets()</code> <code>List[ScheduledTweet]</code> Get scheduled tweets <code>delete_scheduled_tweet(id)</code> <code>bool</code> Delete scheduled tweet <code>clear_scheduled_tweets()</code> <code>int</code> Clear all scheduled <code>draft_tweets()</code> <code>List[Draft]</code> Get drafts <code>delete_draft_tweet(id)</code> <code>bool</code> Delete draft <code>clear_draft_tweets()</code> <code>int</code> Clear all drafts"},{"location":"api/actions/scheduling/#schedule_tweet","title":"<code>schedule_tweet</code>","text":"<pre><code>async def schedule_tweet(\n    self,\n    text: str,\n    scheduled_time: Union[datetime, str],\n    media: Optional[List[str]] = None,\n    poll: Optional[Dict] = None\n) -&gt; ScheduledTweet\n</code></pre> <p>Schedule a tweet for future posting.</p> <p>Parameters: - <code>text</code>: Tweet content - <code>scheduled_time</code>: When to post (datetime or \"YYYY-MM-DD HH:MM\") - <code>media</code>: Optional media file paths - <code>poll</code>: Optional poll configuration</p>"},{"location":"api/actions/scheduling/#schedule_reply","title":"<code>schedule_reply</code>","text":"<pre><code>async def schedule_reply(\n    self,\n    text: str,\n    tweet_id: str,\n    scheduled_time: Union[datetime, str],\n    media: Optional[List[str]] = None\n) -&gt; ScheduledTweet\n</code></pre> <p>Schedule a reply to a specific tweet.</p>"},{"location":"api/actions/scheduling/#scheduled_tweets","title":"<code>scheduled_tweets</code>","text":"<pre><code>async def scheduled_tweets(self) -&gt; List[ScheduledTweet]\n</code></pre> <p>Get all scheduled tweets.</p>"},{"location":"api/actions/scheduling/#draft_tweets","title":"<code>draft_tweets</code>","text":"<pre><code>async def draft_tweets(self) -&gt; List[Draft]\n</code></pre> <p>Get all saved draft tweets.</p>"},{"location":"api/actions/scheduling/#scheduledtweet-object","title":"ScheduledTweet Object","text":"<pre><code>@dataclass\nclass ScheduledTweet:\n    id: str                          # Scheduled tweet ID\n    text: str                        # Tweet content\n    scheduled_time: datetime         # When it will post\n    media: List[str]                 # Attached media\n    poll: Optional[Dict]             # Poll if any\n    reply_to_id: Optional[str]       # If it's a reply\n    created_at: datetime             # When scheduled\n</code></pre>"},{"location":"api/actions/scheduling/#draft-object","title":"Draft Object","text":"<pre><code>@dataclass\nclass Draft:\n    id: str                          # Draft ID\n    text: str                        # Draft content\n    media: List[str]                 # Attached media\n    created_at: datetime             # When saved\n    updated_at: datetime             # Last modified\n</code></pre>"},{"location":"api/actions/scheduling/#usage-examples","title":"Usage Examples","text":""},{"location":"api/actions/scheduling/#schedule-a-tweet","title":"Schedule a Tweet","text":"<pre><code>from xeepy import Xeepy\nfrom datetime import datetime, timedelta\n\nasync def main():\n    async with Xeepy() as x:\n        # Schedule for 2 hours from now\n        scheduled_time = datetime.now() + timedelta(hours=2)\n\n        scheduled = await x.schedule.tweet(\n            \"This tweet was scheduled using Xeepy! \ud83d\ude80\",\n            scheduled_time\n        )\n\n        print(f\"Scheduled for: {scheduled.scheduled_time}\")\n        print(f\"ID: {scheduled.id}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/scheduling/#schedule-with-string-time","title":"Schedule with String Time","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        scheduled = await x.schedule.tweet(\n            \"Happy New Year! \ud83c\udf89\",\n            \"2025-01-01 00:00\"\n        )\n\n        print(f\"Tweet scheduled for: {scheduled.scheduled_time}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/scheduling/#schedule-with-media","title":"Schedule with Media","text":"<pre><code>from xeepy import Xeepy\nfrom datetime import datetime, timedelta\n\nasync def main():\n    async with Xeepy() as x:\n        scheduled = await x.schedule.tweet(\n            \"Check out this amazing photo!\",\n            datetime.now() + timedelta(days=1),\n            media=[\"photo.jpg\"]\n        )\n\n        print(f\"Scheduled tweet with media: {scheduled.id}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/scheduling/#schedule-a-reply","title":"Schedule a Reply","text":"<pre><code>from xeepy import Xeepy\nfrom datetime import datetime, timedelta\n\nasync def main():\n    async with Xeepy() as x:\n        scheduled = await x.schedule.reply(\n            \"Thanks for sharing! Great insights.\",\n            tweet_id=\"123456789\",\n            scheduled_time=datetime.now() + timedelta(hours=1)\n        )\n\n        print(f\"Reply scheduled: {scheduled.id}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/scheduling/#list-scheduled-tweets","title":"List Scheduled Tweets","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        scheduled = await x.schedule.scheduled_tweets()\n\n        print(f\"You have {len(scheduled)} scheduled tweets:\")\n        for tweet in scheduled:\n            print(f\"  [{tweet.scheduled_time}] {tweet.text[:50]}...\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/scheduling/#delete-scheduled-tweet","title":"Delete Scheduled Tweet","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Get scheduled tweets\n        scheduled = await x.schedule.scheduled_tweets()\n\n        if scheduled:\n            # Delete the first one\n            success = await x.schedule.delete_scheduled_tweet(scheduled[0].id)\n            print(\"Deleted!\" if success else \"Failed to delete\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/scheduling/#clear-all-scheduled","title":"Clear All Scheduled","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        count = await x.schedule.clear_scheduled_tweets()\n        print(f\"Cleared {count} scheduled tweets\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/scheduling/#manage-drafts","title":"Manage Drafts","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Get all drafts\n        drafts = await x.schedule.draft_tweets()\n\n        print(f\"You have {len(drafts)} drafts:\")\n        for draft in drafts:\n            print(f\"  [{draft.updated_at}] {draft.text[:50]}...\")\n\n        # Delete a specific draft\n        if drafts:\n            await x.schedule.delete_draft_tweet(drafts[0].id)\n\n        # Or clear all drafts\n        # await x.schedule.clear_draft_tweets()\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/scheduling/#content-calendar","title":"Content Calendar","text":"<pre><code>from xeepy import Xeepy\nfrom datetime import datetime, timedelta\n\nasync def schedule_week(content: list):\n    \"\"\"Schedule a week's worth of content.\"\"\"\n    async with Xeepy() as x:\n        base_time = datetime.now().replace(hour=10, minute=0, second=0)\n\n        for i, tweet_text in enumerate(content):\n            scheduled_time = base_time + timedelta(days=i)\n\n            # Skip weekends\n            while scheduled_time.weekday() &gt;= 5:\n                scheduled_time += timedelta(days=1)\n\n            scheduled = await x.schedule.tweet(tweet_text, scheduled_time)\n            print(f\"Scheduled for {scheduled_time.strftime('%A, %B %d')}\")\n\ncontent = [\n    \"Monday motivation! \ud83d\udcaa\",\n    \"Tech tip Tuesday: Always backup your data! \ud83d\udcbe\",\n    \"Wisdom Wednesday: Keep learning, keep growing \ud83d\udcda\",\n    \"Throwback Thursday to our first product launch \ud83d\ude80\",\n    \"Feature Friday: Check out our new dashboard!\"\n]\n\nasyncio.run(schedule_week(content))\n</code></pre>"},{"location":"api/actions/scheduling/#optimal-time-scheduling","title":"Optimal Time Scheduling","text":"<pre><code>from xeepy import Xeepy\nfrom datetime import datetime, timedelta\n\nasync def schedule_at_optimal_times(tweets: list, optimal_hours: list = [9, 12, 17]):\n    \"\"\"Schedule tweets at optimal engagement times.\"\"\"\n    async with Xeepy() as x:\n        base_date = datetime.now().date()\n\n        for i, tweet_text in enumerate(tweets):\n            day_offset = i // len(optimal_hours)\n            hour_index = i % len(optimal_hours)\n\n            scheduled_time = datetime.combine(\n                base_date + timedelta(days=day_offset),\n                datetime.min.time()\n            ).replace(hour=optimal_hours[hour_index])\n\n            await x.schedule.tweet(tweet_text, scheduled_time)\n            print(f\"Scheduled: {scheduled_time}\")\n\nasyncio.run(schedule_at_optimal_times(my_tweets))\n</code></pre>"},{"location":"api/actions/scheduling/#see-also","title":"See Also","text":"<ul> <li>PollActions - Create polls</li> <li>EngageActions - Tweet engagement</li> <li>Tweet Model - Tweet data structure</li> </ul>"},{"location":"api/actions/settings/","title":"SettingsActions","text":"<p>Actions for managing account settings and profile on X/Twitter.</p>"},{"location":"api/actions/settings/#import","title":"Import","text":"<pre><code>from xeepy.actions.settings import SettingsActions\n</code></pre>"},{"location":"api/actions/settings/#class-signature","title":"Class Signature","text":"<pre><code>class SettingsActions:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/actions/settings/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/actions/settings/#methods","title":"Methods","text":"Method Returns Description <code>get_settings()</code> <code>AccountSettings</code> Get current settings <code>update_settings(settings)</code> <code>bool</code> Update account settings <code>get_notifications()</code> <code>List[Notification]</code> Get notifications <code>change_password(old, new)</code> <code>bool</code> Change password <code>update_profile(...)</code> <code>bool</code> Update profile info <code>update_profile_image(path)</code> <code>bool</code> Update avatar <code>update_profile_banner(path)</code> <code>bool</code> Update banner"},{"location":"api/actions/settings/#get_settings","title":"<code>get_settings</code>","text":"<pre><code>async def get_settings(self) -&gt; AccountSettings\n</code></pre> <p>Get current account settings.</p>"},{"location":"api/actions/settings/#update_settings","title":"<code>update_settings</code>","text":"<pre><code>async def update_settings(\n    self,\n    settings: Dict[str, Any]\n) -&gt; bool\n</code></pre> <p>Update account settings.</p> <p>Available Settings: - <code>protected</code>: <code>bool</code> - Private account - <code>allow_dm_from</code>: <code>str</code> - <code>\"everyone\"</code>, <code>\"following\"</code>, <code>\"verified\"</code> - <code>sensitive_media</code>: <code>bool</code> - Mark media as sensitive - <code>hide_sensitive_content</code>: <code>bool</code> - Hide sensitive content - <code>quality_filter</code>: <code>bool</code> - Enable quality filter - <code>personalization</code>: <code>bool</code> - Allow personalization</p>"},{"location":"api/actions/settings/#update_profile","title":"<code>update_profile</code>","text":"<pre><code>async def update_profile(\n    self,\n    name: Optional[str] = None,\n    bio: Optional[str] = None,\n    location: Optional[str] = None,\n    website: Optional[str] = None,\n    birth_date: Optional[str] = None\n) -&gt; bool\n</code></pre> <p>Update profile information.</p>"},{"location":"api/actions/settings/#update_profile_image","title":"<code>update_profile_image</code>","text":"<pre><code>async def update_profile_image(\n    self,\n    image_path: str\n) -&gt; bool\n</code></pre> <p>Update profile avatar image.</p>"},{"location":"api/actions/settings/#update_profile_banner","title":"<code>update_profile_banner</code>","text":"<pre><code>async def update_profile_banner(\n    self,\n    image_path: str\n) -&gt; bool\n</code></pre> <p>Update profile banner image.</p>"},{"location":"api/actions/settings/#change_password","title":"<code>change_password</code>","text":"<pre><code>async def change_password(\n    self,\n    old_password: str,\n    new_password: str\n) -&gt; bool\n</code></pre> <p>Change account password.</p>"},{"location":"api/actions/settings/#accountsettings-object","title":"AccountSettings Object","text":"<pre><code>@dataclass\nclass AccountSettings:\n    username: str                    # Current username\n    email: str                       # Account email\n    phone: Optional[str]             # Phone number\n    protected: bool                  # Private account\n    allow_dm_from: str               # DM settings\n    sensitive_media: bool            # Sensitive media setting\n    hide_sensitive_content: bool     # Hide sensitive\n    quality_filter: bool             # Quality filter enabled\n    personalization: bool            # Personalization enabled\n    language: str                    # Interface language\n    country: str                     # Country setting\n</code></pre>"},{"location":"api/actions/settings/#notification-object","title":"Notification Object","text":"<pre><code>@dataclass\nclass Notification:\n    id: str                          # Notification ID\n    type: str                        # like, retweet, follow, mention, etc.\n    text: str                        # Notification text\n    user: Optional[User]             # Related user\n    tweet: Optional[Tweet]           # Related tweet\n    created_at: datetime             # Notification time\n    is_read: bool                    # Read status\n</code></pre>"},{"location":"api/actions/settings/#usage-examples","title":"Usage Examples","text":""},{"location":"api/actions/settings/#get-current-settings","title":"Get Current Settings","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        settings = await x.settings.get_settings()\n\n        print(f\"Username: @{settings.username}\")\n        print(f\"Protected: {settings.protected}\")\n        print(f\"DM from: {settings.allow_dm_from}\")\n        print(f\"Quality filter: {settings.quality_filter}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/settings/#make-account-private","title":"Make Account Private","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        success = await x.settings.update_settings({\n            \"protected\": True\n        })\n\n        print(\"Account is now private!\" if success else \"Failed\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/settings/#update-dm-settings","title":"Update DM Settings","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Only allow DMs from people you follow\n        success = await x.settings.update_settings({\n            \"allow_dm_from\": \"following\"\n        })\n\n        # Options: \"everyone\", \"following\", \"verified\"\n        print(\"DM settings updated!\" if success else \"Failed\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/settings/#update-multiple-settings","title":"Update Multiple Settings","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        success = await x.settings.update_settings({\n            \"protected\": False,\n            \"allow_dm_from\": \"everyone\",\n            \"sensitive_media\": False,\n            \"quality_filter\": True\n        })\n\n        print(\"Settings updated!\" if success else \"Failed\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/settings/#update-profile-information","title":"Update Profile Information","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        success = await x.settings.update_profile(\n            name=\"John Doe\",\n            bio=\"Software developer | Python enthusiast | Building cool stuff\",\n            location=\"San Francisco, CA\",\n            website=\"https://johndoe.dev\"\n        )\n\n        print(\"Profile updated!\" if success else \"Failed\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/settings/#update-profile-picture","title":"Update Profile Picture","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Update avatar\n        success = await x.settings.update_profile_image(\"new_avatar.jpg\")\n        print(\"Avatar updated!\" if success else \"Failed\")\n\n        # Update banner\n        success = await x.settings.update_profile_banner(\"new_banner.jpg\")\n        print(\"Banner updated!\" if success else \"Failed\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/settings/#change-password","title":"Change Password","text":"<pre><code>from xeepy import Xeepy\nimport getpass\n\nasync def main():\n    async with Xeepy() as x:\n        old_pass = getpass.getpass(\"Current password: \")\n        new_pass = getpass.getpass(\"New password: \")\n        confirm = getpass.getpass(\"Confirm new password: \")\n\n        if new_pass != confirm:\n            print(\"Passwords don't match!\")\n            return\n\n        success = await x.settings.change_password(old_pass, new_pass)\n        print(\"Password changed!\" if success else \"Failed\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/settings/#get-notifications","title":"Get Notifications","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        notifications = await x.settings.get_notifications()\n\n        print(f\"You have {len(notifications)} notifications:\")\n\n        for notif in notifications[:10]:\n            status = \"\ud83d\udd35\" if not notif.is_read else \"\u26aa\"\n            print(f\"{status} [{notif.type}] {notif.text[:60]}...\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/settings/#profile-refresh-automation","title":"Profile Refresh Automation","text":"<pre><code>from xeepy import Xeepy\nfrom datetime import datetime\n\nasync def seasonal_profile_update():\n    \"\"\"Update profile based on current season.\"\"\"\n    async with Xeepy() as x:\n        month = datetime.now().month\n\n        if month in [12, 1, 2]:\n            bio = \"\u2744\ufe0f Winter mode | Building cool stuff\"\n            banner = \"winter_banner.jpg\"\n        elif month in [3, 4, 5]:\n            bio = \"\ud83c\udf38 Spring vibes | Building cool stuff\"\n            banner = \"spring_banner.jpg\"\n        elif month in [6, 7, 8]:\n            bio = \"\u2600\ufe0f Summer edition | Building cool stuff\"\n            banner = \"summer_banner.jpg\"\n        else:\n            bio = \"\ud83c\udf42 Fall season | Building cool stuff\"\n            banner = \"fall_banner.jpg\"\n\n        await x.settings.update_profile(bio=bio)\n        await x.settings.update_profile_banner(banner)\n\n        print(\"Profile updated for the season!\")\n\nasyncio.run(seasonal_profile_update())\n</code></pre>"},{"location":"api/actions/settings/#backup-settings","title":"Backup Settings","text":"<pre><code>from xeepy import Xeepy\nimport json\n\nasync def backup_settings(output_file: str):\n    async with Xeepy() as x:\n        settings = await x.settings.get_settings()\n\n        backup = {\n            \"username\": settings.username,\n            \"protected\": settings.protected,\n            \"allow_dm_from\": settings.allow_dm_from,\n            \"sensitive_media\": settings.sensitive_media,\n            \"quality_filter\": settings.quality_filter,\n            \"backed_up_at\": datetime.now().isoformat()\n        }\n\n        with open(output_file, \"w\") as f:\n            json.dump(backup, f, indent=2)\n\n        print(f\"Settings backed up to {output_file}\")\n\nasyncio.run(backup_settings(\"settings_backup.json\"))\n</code></pre>"},{"location":"api/actions/settings/#see-also","title":"See Also","text":"<ul> <li>AuthManager - Authentication</li> <li>ProfileScraper - Profile scraping</li> <li>User Model - User data structure</li> </ul>"},{"location":"api/actions/unfollow/","title":"UnfollowActions","text":"<p>Actions for unfollowing users on X/Twitter, including smart unfollow strategies.</p>"},{"location":"api/actions/unfollow/#import","title":"Import","text":"<pre><code>from xeepy.actions.unfollow import UnfollowActions\n</code></pre>"},{"location":"api/actions/unfollow/#class-signature","title":"Class Signature","text":"<pre><code>class UnfollowActions:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/actions/unfollow/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/actions/unfollow/#methods","title":"Methods","text":"Method Returns Description <code>user(username)</code> <code>bool</code> Unfollow single user <code>users(usernames)</code> <code>UnfollowResult</code> Unfollow multiple users <code>non_followers(max_unfollows)</code> <code>UnfollowResult</code> Unfollow non-followers <code>everyone(max_unfollows)</code> <code>UnfollowResult</code> Unfollow all users <code>inactive(days, max_unfollows)</code> <code>UnfollowResult</code> Unfollow inactive users <code>by_criteria(criteria)</code> <code>UnfollowResult</code> Unfollow by custom criteria"},{"location":"api/actions/unfollow/#user","title":"<code>user</code>","text":"<pre><code>async def user(self, username: str) -&gt; bool\n</code></pre> <p>Unfollow a single user.</p>"},{"location":"api/actions/unfollow/#non_followers","title":"<code>non_followers</code>","text":"<pre><code>async def non_followers(\n    self,\n    max_unfollows: int = 100,\n    whitelist: List[str] = None,\n    delay_range: Tuple[float, float] = (3.0, 8.0),\n    dry_run: bool = False\n) -&gt; UnfollowResult\n</code></pre> <p>Unfollow users who don't follow back.</p> <p>Parameters: - <code>max_unfollows</code>: Maximum users to unfollow - <code>whitelist</code>: Usernames to never unfollow - <code>delay_range</code>: Random delay between unfollows (seconds) - <code>dry_run</code>: Preview without actually unfollowing</p>"},{"location":"api/actions/unfollow/#everyone","title":"<code>everyone</code>","text":"<pre><code>async def everyone(\n    self,\n    max_unfollows: int = None,\n    whitelist: List[str] = None,\n    delay_range: Tuple[float, float] = (3.0, 8.0),\n    dry_run: bool = False\n) -&gt; UnfollowResult\n</code></pre> <p>Unfollow all users (use with caution).</p>"},{"location":"api/actions/unfollow/#inactive","title":"<code>inactive</code>","text":"<pre><code>async def inactive(\n    self,\n    days: int = 90,\n    max_unfollows: int = 100,\n    whitelist: List[str] = None,\n    dry_run: bool = False\n) -&gt; UnfollowResult\n</code></pre> <p>Unfollow users who haven't posted recently.</p>"},{"location":"api/actions/unfollow/#by_criteria","title":"<code>by_criteria</code>","text":"<pre><code>async def by_criteria(\n    self,\n    criteria: UnfollowCriteria,\n    max_unfollows: int = 100,\n    dry_run: bool = False\n) -&gt; UnfollowResult\n</code></pre> <p>Unfollow based on custom criteria.</p>"},{"location":"api/actions/unfollow/#unfollowresult-object","title":"UnfollowResult Object","text":"<pre><code>@dataclass\nclass UnfollowResult:\n    unfollowed: List[str]            # Successfully unfollowed\n    failed: List[Dict]               # Failed with errors\n    skipped: List[str]               # Skipped (whitelisted)\n    would_unfollow: List[str]        # Preview list (dry_run)\n    total_attempted: int             # Total attempts\n</code></pre>"},{"location":"api/actions/unfollow/#unfollowcriteria-object","title":"UnfollowCriteria Object","text":"<pre><code>@dataclass\nclass UnfollowCriteria:\n    min_followers: Optional[int] = None    # Min followers to keep\n    max_followers: Optional[int] = None    # Max followers to keep\n    min_following: Optional[int] = None    # Min following to keep\n    following_ratio_min: Optional[float] = None  # Min followers/following\n    must_be_verified: bool = False         # Keep only verified\n    must_follow_back: bool = False         # Keep only followers\n    inactive_days: Optional[int] = None    # Max days since tweet\n    bio_keywords: Optional[List[str]] = None  # Keep if bio contains\n    exclude_keywords: Optional[List[str]] = None  # Remove if bio contains\n</code></pre>"},{"location":"api/actions/unfollow/#usage-examples","title":"Usage Examples","text":""},{"location":"api/actions/unfollow/#unfollow-non-followers","title":"Unfollow Non-Followers","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.unfollow.non_followers(\n            max_unfollows=100,\n            whitelist=[\"friend1\", \"friend2\", \"important_account\"]\n        )\n\n        print(f\"Unfollowed: {len(result.unfollowed)} users\")\n        print(f\"Failed: {len(result.failed)}\")\n        print(f\"Skipped (whitelisted): {len(result.skipped)}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/unfollow/#preview-before-unfollowing-dry-run","title":"Preview Before Unfollowing (Dry Run)","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Preview who would be unfollowed\n        result = await x.unfollow.non_followers(\n            max_unfollows=200,\n            dry_run=True\n        )\n\n        print(f\"Would unfollow {len(result.would_unfollow)} users:\")\n        for username in result.would_unfollow[:20]:\n            print(f\"  - @{username}\")\n\n        # Confirm and execute\n        confirm = input(\"Proceed? (y/n): \")\n        if confirm.lower() == \"y\":\n            result = await x.unfollow.non_followers(\n                max_unfollows=200,\n                dry_run=False\n            )\n            print(f\"Unfollowed {len(result.unfollowed)} users\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/unfollow/#unfollow-everyone","title":"Unfollow Everyone","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Keep some important accounts\n        whitelist = [\"official_support\", \"best_friend\", \"spouse\"]\n\n        result = await x.unfollow.everyone(\n            whitelist=whitelist,\n            delay_range=(5.0, 15.0),\n            dry_run=True  # Preview first!\n        )\n\n        print(f\"Would unfollow: {len(result.would_unfollow)}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/unfollow/#unfollow-inactive-users","title":"Unfollow Inactive Users","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.unfollow.inactive(\n            days=90,  # No tweets in 90 days\n            max_unfollows=50\n        )\n\n        print(f\"Unfollowed {len(result.unfollowed)} inactive accounts\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/unfollow/#unfollow-by-custom-criteria","title":"Unfollow by Custom Criteria","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.actions.unfollow import UnfollowCriteria\n\nasync def main():\n    async with Xeepy() as x:\n        # Define custom criteria\n        criteria = UnfollowCriteria(\n            max_followers=100,           # Low follower accounts\n            must_follow_back=False,      # Doesn't follow back\n            inactive_days=60,            # Inactive for 60+ days\n            exclude_keywords=[\"crypto\", \"nft\", \"forex\"]  # Spam-like bios\n        )\n\n        result = await x.unfollow.by_criteria(\n            criteria=criteria,\n            max_unfollows=100,\n            dry_run=True\n        )\n\n        print(f\"Matching accounts: {len(result.would_unfollow)}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/unfollow/#keep-only-verified-users","title":"Keep Only Verified Users","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.actions.unfollow import UnfollowCriteria\n\nasync def main():\n    async with Xeepy() as x:\n        criteria = UnfollowCriteria(\n            must_be_verified=True  # Keep only verified\n        )\n\n        result = await x.unfollow.by_criteria(\n            criteria=criteria,\n            max_unfollows=500,\n            dry_run=True\n        )\n\n        print(f\"Non-verified to unfollow: {len(result.would_unfollow)}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/actions/unfollow/#gradual-unfollow-over-time","title":"Gradual Unfollow Over Time","text":"<pre><code>from xeepy import Xeepy\nimport asyncio\n\nasync def gradual_unfollow(daily_limit: int = 50, days: int = 7):\n    \"\"\"Spread unfollows over multiple days.\"\"\"\n    async with Xeepy() as x:\n        for day in range(days):\n            result = await x.unfollow.non_followers(\n                max_unfollows=daily_limit,\n                delay_range=(10.0, 30.0)\n            )\n\n            print(f\"Day {day + 1}: Unfollowed {len(result.unfollowed)}\")\n\n            if day &lt; days - 1:\n                print(\"Waiting 24 hours...\")\n                await asyncio.sleep(86400)\n\nasyncio.run(gradual_unfollow())\n</code></pre>"},{"location":"api/actions/unfollow/#see-also","title":"See Also","text":"<ul> <li>FollowActions - Follow operations</li> <li>FollowingScraper - Get following list</li> <li>FollowersScraper - Get followers list</li> </ul>"},{"location":"api/ai/content/","title":"ContentGenerator","text":"<p>AI-powered content generation for X/Twitter.</p>"},{"location":"api/ai/content/#import","title":"Import","text":"<pre><code>from xeepy.ai import ContentGenerator\n</code></pre>"},{"location":"api/ai/content/#class-signature","title":"Class Signature","text":"<pre><code>class ContentGenerator:\n    def __init__(\n        self,\n        provider: str = \"openai\",\n        api_key: Optional[str] = None,\n        model: Optional[str] = None\n    )\n</code></pre>"},{"location":"api/ai/content/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>provider</code> <code>str</code> <code>\"openai\"</code> AI provider name <code>api_key</code> <code>Optional[str]</code> <code>None</code> API key (env var fallback) <code>model</code> <code>Optional[str]</code> <code>None</code> Model name (provider default)"},{"location":"api/ai/content/#methods","title":"Methods","text":"Method Returns Description <code>generate_reply(tweet, style)</code> <code>str</code> Generate reply to tweet <code>generate_tweet(topic, style)</code> <code>str</code> Generate original tweet <code>generate_thread(topic, length)</code> <code>List[str]</code> Generate tweet thread <code>rewrite(text, style)</code> <code>str</code> Rewrite content <code>hashtags(text)</code> <code>List[str]</code> Suggest hashtags <code>hook(topic)</code> <code>str</code> Generate engaging hook"},{"location":"api/ai/content/#generate_reply","title":"<code>generate_reply</code>","text":"<pre><code>async def generate_reply(\n    self,\n    tweet_text: str,\n    style: str = \"professional\",\n    context: Optional[str] = None,\n    max_length: int = 280\n) -&gt; str\n</code></pre> <p>Generate a contextual reply.</p> <p>Parameters: - <code>tweet_text</code>: Text of tweet to reply to - <code>style</code>: Reply style (<code>professional</code>, <code>witty</code>, <code>supportive</code>, <code>informative</code>, <code>crypto</code>) - <code>context</code>: Additional context about the account - <code>max_length</code>: Maximum reply length</p>"},{"location":"api/ai/content/#generate_thread","title":"<code>generate_thread</code>","text":"<pre><code>async def generate_thread(\n    self,\n    topic: str,\n    length: int = 5,\n    style: str = \"educational\"\n) -&gt; List[str]\n</code></pre> <p>Generate a tweet thread.</p> <p>Parameters: - <code>topic</code>: Thread topic - <code>length</code>: Number of tweets (max 25) - <code>style</code>: Thread style</p>"},{"location":"api/ai/content/#hook","title":"<code>hook</code>","text":"<pre><code>async def hook(\n    self,\n    topic: str,\n    style: str = \"curiosity\"\n) -&gt; str\n</code></pre> <p>Generate an engaging hook/opener.</p> <p>Parameters: - <code>topic</code>: Topic to create hook for - <code>style</code>: Hook style (<code>curiosity</code>, <code>contrarian</code>, <code>statistic</code>, <code>story</code>, <code>question</code>)</p>"},{"location":"api/ai/content/#styles","title":"Styles","text":"Style Description <code>professional</code> Business-appropriate, thoughtful <code>witty</code> Clever, humorous <code>supportive</code> Encouraging, positive <code>informative</code> Educational, factual <code>crypto</code> Crypto/Web3 terminology <code>tech</code> Technical, developer-focused <code>casual</code> Friendly, conversational"},{"location":"api/ai/content/#usage-examples","title":"Usage Examples","text":""},{"location":"api/ai/content/#generate-reply","title":"Generate Reply","text":"<pre><code>from xeepy.ai import ContentGenerator\n\nasync def main():\n    ai = ContentGenerator(provider=\"openai\", api_key=\"sk-...\")\n\n    tweet = \"Just launched my first SaaS product after 6 months of work!\"\n\n    reply = await ai.generate_reply(\n        tweet_text=tweet,\n        style=\"supportive\"\n    )\n\n    print(f\"Reply: {reply}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/ai/content/#generate-original-tweet","title":"Generate Original Tweet","text":"<pre><code>from xeepy.ai import ContentGenerator\n\nasync def main():\n    ai = ContentGenerator(provider=\"anthropic\", api_key=\"sk-ant-...\")\n\n    tweet = await ai.generate_tweet(\n        topic=\"Python async programming tips\",\n        style=\"educational\"\n    )\n\n    print(f\"Generated tweet: {tweet}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/ai/content/#generate-thread","title":"Generate Thread","text":"<pre><code>from xeepy.ai import ContentGenerator\n\nasync def main():\n    ai = ContentGenerator(provider=\"openai\", api_key=\"sk-...\")\n\n    thread = await ai.generate_thread(\n        topic=\"How I grew my Twitter to 10K followers\",\n        length=7,\n        style=\"story\"\n    )\n\n    print(\"Generated thread:\")\n    for i, tweet in enumerate(thread, 1):\n        print(f\"\\n{i}/ {tweet}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/ai/content/#generate-hook","title":"Generate Hook","text":"<pre><code>from xeepy.ai import ContentGenerator\n\nasync def main():\n    ai = ContentGenerator(provider=\"openai\", api_key=\"sk-...\")\n\n    hooks = []\n    for style in [\"curiosity\", \"contrarian\", \"statistic\", \"question\"]:\n        hook = await ai.hook(\n            topic=\"productivity for developers\",\n            style=style\n        )\n        hooks.append((style, hook))\n\n    print(\"Hook variations:\")\n    for style, hook in hooks:\n        print(f\"\\n{style.upper()}: {hook}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/ai/content/#rewrite-content","title":"Rewrite Content","text":"<pre><code>from xeepy.ai import ContentGenerator\n\nasync def main():\n    ai = ContentGenerator(provider=\"openai\", api_key=\"sk-...\")\n\n    original = \"We are pleased to announce the release of version 2.0 of our software product.\"\n\n    rewritten = await ai.rewrite(\n        text=original,\n        style=\"casual\"\n    )\n\n    print(f\"Original: {original}\")\n    print(f\"Rewritten: {rewritten}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/ai/content/#suggest-hashtags","title":"Suggest Hashtags","text":"<pre><code>from xeepy.ai import ContentGenerator\n\nasync def main():\n    ai = ContentGenerator(provider=\"openai\", api_key=\"sk-...\")\n\n    tweet = \"Just built a Python script that automates my email responses using GPT-4\"\n\n    hashtags = await ai.hashtags(tweet)\n\n    print(f\"Suggested hashtags: {' '.join(hashtags)}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/ai/content/#auto-reply-to-mentions","title":"Auto-Reply to Mentions","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import ContentGenerator\n\nasync def auto_reply_mentions():\n    ai = ContentGenerator(provider=\"openai\", api_key=\"sk-...\")\n\n    async with Xeepy() as x:\n        mentions = await x.scrape.mentions(limit=10)\n\n        for tweet in mentions.items:\n            # Skip already replied\n            if tweet.replied:\n                continue\n\n            # Generate contextual reply\n            reply = await ai.generate_reply(\n                tweet_text=tweet.text,\n                style=\"supportive\",\n                context=\"I'm a Python developer who helps others learn coding\"\n            )\n\n            # Post reply\n            await x.engage.reply(tweet.url, reply)\n            print(f\"Replied to @{tweet.author.username}\")\n\nasyncio.run(auto_reply_mentions())\n</code></pre>"},{"location":"api/ai/content/#content-calendar-generation","title":"Content Calendar Generation","text":"<pre><code>from xeepy.ai import ContentGenerator\n\nasync def generate_week_content():\n    ai = ContentGenerator(provider=\"openai\", api_key=\"sk-...\")\n\n    topics = [\n        \"Python tip\",\n        \"Career advice for developers\",\n        \"Productivity hack\",\n        \"Book recommendation\",\n        \"Industry hot take\"\n    ]\n\n    calendar = []\n    for day, topic in enumerate(topics):\n        tweet = await ai.generate_tweet(topic, style=\"educational\")\n        calendar.append({\n            \"day\": day + 1,\n            \"topic\": topic,\n            \"tweet\": tweet\n        })\n\n    print(\"Weekly content calendar:\")\n    for item in calendar:\n        print(f\"\\nDay {item['day']} - {item['topic']}:\")\n        print(f\"  {item['tweet']}\")\n\nasyncio.run(generate_week_content())\n</code></pre>"},{"location":"api/ai/content/#different-provider-example","title":"Different Provider Example","text":"<pre><code>from xeepy.ai import ContentGenerator\n\nasync def compare_providers(tweet: str):\n    providers = [\n        (\"openai\", \"sk-...\"),\n        (\"anthropic\", \"sk-ant-...\"),\n    ]\n\n    print(f\"Original: {tweet}\\n\")\n\n    for provider, key in providers:\n        ai = ContentGenerator(provider=provider, api_key=key)\n        reply = await ai.generate_reply(tweet, style=\"witty\")\n        print(f\"{provider.upper()}: {reply}\\n\")\n\nasyncio.run(compare_providers(\"Python is the best language!\"))\n</code></pre>"},{"location":"api/ai/content/#local-ollama","title":"Local Ollama","text":"<pre><code>from xeepy.ai import ContentGenerator\n\nasync def main():\n    # Use local Ollama (no API key needed)\n    ai = ContentGenerator(provider=\"ollama\", model=\"llama2\")\n\n    tweet = await ai.generate_tweet(\n        topic=\"Benefits of open source\",\n        style=\"professional\"\n    )\n\n    print(tweet)\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/ai/content/#see-also","title":"See Also","text":"<ul> <li>AIProvider - AI provider setup</li> <li>SentimentAnalyzer - Sentiment analysis</li> <li>ContentAnalytics - Content analytics</li> </ul>"},{"location":"api/ai/detection/","title":"BotDetector","text":"<p>AI-powered bot and spam account detection.</p>"},{"location":"api/ai/detection/#import","title":"Import","text":"<pre><code>from xeepy.ai import BotDetector\n</code></pre>"},{"location":"api/ai/detection/#class-signature","title":"Class Signature","text":"<pre><code>class BotDetector:\n    def __init__(\n        self,\n        provider: str = \"openai\",\n        api_key: Optional[str] = None,\n        model: Optional[str] = None,\n        threshold: float = 0.7\n    )\n</code></pre>"},{"location":"api/ai/detection/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>provider</code> <code>str</code> <code>\"openai\"</code> AI provider name <code>api_key</code> <code>Optional[str]</code> <code>None</code> API key <code>model</code> <code>Optional[str]</code> <code>None</code> Model name <code>threshold</code> <code>float</code> <code>0.7</code> Bot probability threshold"},{"location":"api/ai/detection/#methods","title":"Methods","text":"Method Returns Description <code>analyze(user)</code> <code>BotAnalysis</code> Analyze single user <code>analyze_batch(users)</code> <code>List[BotAnalysis]</code> Analyze multiple users <code>filter_bots(users)</code> <code>FilterResult</code> Filter out bots <code>analyze_followers(username)</code> <code>FollowerAnalysis</code> Analyze follower quality <code>spam_check(tweet)</code> <code>SpamResult</code> Check if tweet is spam"},{"location":"api/ai/detection/#analyze","title":"<code>analyze</code>","text":"<pre><code>async def analyze(\n    self,\n    user: User\n) -&gt; BotAnalysis\n</code></pre> <p>Analyze if a user is a bot.</p>"},{"location":"api/ai/detection/#filter_bots","title":"<code>filter_bots</code>","text":"<pre><code>async def filter_bots(\n    self,\n    users: List[User],\n    threshold: Optional[float] = None\n) -&gt; FilterResult\n</code></pre> <p>Separate real users from bots.</p> <p>Parameters: - <code>users</code>: Users to filter - <code>threshold</code>: Custom threshold (default: instance threshold)</p>"},{"location":"api/ai/detection/#analyze_followers","title":"<code>analyze_followers</code>","text":"<pre><code>async def analyze_followers(\n    self,\n    username: str,\n    sample_size: int = 200\n) -&gt; FollowerAnalysis\n</code></pre> <p>Analyze bot percentage among followers.</p>"},{"location":"api/ai/detection/#botanalysis-object","title":"BotAnalysis Object","text":"<pre><code>@dataclass\nclass BotAnalysis:\n    user: User                       # Analyzed user\n    is_bot: bool                     # Bot classification\n    bot_probability: float           # 0.0 to 1.0\n    confidence: float                # Analysis confidence\n    signals: List[str]               # Bot indicators found\n    human_signals: List[str]         # Human indicators found\n    category: str                    # bot, human, suspicious\n</code></pre>"},{"location":"api/ai/detection/#filterresult-object","title":"FilterResult Object","text":"<pre><code>@dataclass\nclass FilterResult:\n    real_users: List[User]           # Likely human users\n    bots: List[User]                 # Likely bots\n    suspicious: List[User]           # Uncertain cases\n    bot_percentage: float            # % identified as bots\n</code></pre>"},{"location":"api/ai/detection/#followeranalysis-object","title":"FollowerAnalysis Object","text":"<pre><code>@dataclass\nclass FollowerAnalysis:\n    username: str                    # Account analyzed\n    total_sampled: int               # Followers sampled\n    bot_percentage: float            # % bots\n    suspicious_percentage: float     # % suspicious\n    quality_score: float             # 0-100 quality\n    common_bot_patterns: List[str]   # Patterns found\n</code></pre>"},{"location":"api/ai/detection/#spamresult-object","title":"SpamResult Object","text":"<pre><code>@dataclass\nclass SpamResult:\n    is_spam: bool                    # Spam classification\n    spam_probability: float          # 0.0 to 1.0\n    spam_type: Optional[str]         # Type if spam\n    signals: List[str]               # Spam indicators\n</code></pre>"},{"location":"api/ai/detection/#bot-signals","title":"Bot Signals","text":"<p>Common bot indicators detected:</p> Signal Description <code>default_profile_image</code> Using default avatar <code>no_bio</code> Empty or generic bio <code>suspicious_username</code> Random characters/numbers <code>high_following_ratio</code> Follows many, few followers <code>low_tweet_count</code> Very few tweets <code>repetitive_content</code> Duplicate/similar tweets <code>rapid_posting</code> Unnaturally fast posting <code>new_account</code> Recently created"},{"location":"api/ai/detection/#usage-examples","title":"Usage Examples","text":""},{"location":"api/ai/detection/#analyze-single-user","title":"Analyze Single User","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import BotDetector\n\nasync def main():\n    detector = BotDetector(provider=\"openai\", api_key=\"sk-...\")\n\n    async with Xeepy() as x:\n        user = await x.scrape.profile(\"suspicious_account\")\n        analysis = await detector.analyze(user)\n\n        print(f\"=== Bot Analysis: @{user.username} ===\")\n        print(f\"Classification: {analysis.category}\")\n        print(f\"Bot probability: {analysis.bot_probability:.0%}\")\n        print(f\"Confidence: {analysis.confidence:.0%}\")\n\n        if analysis.signals:\n            print(f\"\\n\ud83d\udea9 Bot signals:\")\n            for signal in analysis.signals:\n                print(f\"  - {signal}\")\n\n        if analysis.human_signals:\n            print(f\"\\n\u2713 Human signals:\")\n            for signal in analysis.human_signals:\n                print(f\"  - {signal}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/ai/detection/#filter-bots-from-followers","title":"Filter Bots from Followers","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import BotDetector\n\nasync def clean_followers(username: str):\n    detector = BotDetector(provider=\"openai\", api_key=\"sk-...\", threshold=0.7)\n\n    async with Xeepy() as x:\n        followers = await x.scrape.followers(username, limit=200)\n\n        result = await detector.filter_bots(followers.items)\n\n        print(f\"=== Follower Analysis ===\")\n        print(f\"Total analyzed: {len(followers.items)}\")\n        print(f\"Real users: {len(result.real_users)}\")\n        print(f\"Bots: {len(result.bots)}\")\n        print(f\"Suspicious: {len(result.suspicious)}\")\n        print(f\"Bot percentage: {result.bot_percentage:.1f}%\")\n\n        if result.bots:\n            print(f\"\\nIdentified bots:\")\n            for bot in result.bots[:10]:\n                print(f\"  - @{bot.username}\")\n\nasyncio.run(clean_followers(\"myaccount\"))\n</code></pre>"},{"location":"api/ai/detection/#analyze-follower-quality","title":"Analyze Follower Quality","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import BotDetector\n\nasync def follower_quality(username: str):\n    detector = BotDetector(provider=\"openai\", api_key=\"sk-...\")\n\n    async with Xeepy() as x:\n        analysis = await detector.analyze_followers(username, sample_size=300)\n\n        print(f\"=== Follower Quality: @{username} ===\")\n        print(f\"Sampled: {analysis.total_sampled}\")\n        print(f\"Bot percentage: {analysis.bot_percentage:.1f}%\")\n        print(f\"Suspicious: {analysis.suspicious_percentage:.1f}%\")\n        print(f\"Quality score: {analysis.quality_score:.0f}/100\")\n\n        if analysis.common_bot_patterns:\n            print(f\"\\nCommon bot patterns found:\")\n            for pattern in analysis.common_bot_patterns:\n                print(f\"  - {pattern}\")\n\n        if analysis.quality_score &gt;= 80:\n            print(\"\\n\u2713 High quality followers!\")\n        elif analysis.quality_score &gt;= 60:\n            print(\"\\n\u26a0\ufe0f Moderate quality\")\n        else:\n            print(\"\\n\u274c Low quality - consider cleaning\")\n\nasyncio.run(follower_quality(\"myaccount\"))\n</code></pre>"},{"location":"api/ai/detection/#check-tweet-for-spam","title":"Check Tweet for Spam","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import BotDetector\n\nasync def check_replies_for_spam(tweet_url: str):\n    detector = BotDetector(provider=\"openai\", api_key=\"sk-...\")\n\n    async with Xeepy() as x:\n        replies = await x.scrape.replies(tweet_url, limit=50)\n\n        spam_count = 0\n        for reply in replies.items:\n            result = await detector.spam_check(reply)\n\n            if result.is_spam:\n                spam_count += 1\n                print(f\"\ud83d\udeab Spam detected ({result.spam_type}):\")\n                print(f\"   @{reply.author.username}: {reply.text[:50]}...\")\n\n        print(f\"\\nTotal spam replies: {spam_count}/{len(replies.items)}\")\n\nasyncio.run(check_replies_for_spam(\"https://x.com/user/status/123\"))\n</code></pre>"},{"location":"api/ai/detection/#batch-user-analysis","title":"Batch User Analysis","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import BotDetector\n\nasync def analyze_new_followers():\n    detector = BotDetector(provider=\"openai\", api_key=\"sk-...\")\n\n    async with Xeepy() as x:\n        # Get recent followers\n        followers = await x.scrape.followers(\"myaccount\", limit=50)\n\n        # Analyze all\n        results = await detector.analyze_batch(followers.items)\n\n        for user, analysis in zip(followers.items, results):\n            if analysis.is_bot:\n                print(f\"\ud83e\udd16 @{user.username} - {analysis.bot_probability:.0%} bot\")\n            elif analysis.category == \"suspicious\":\n                print(f\"\u26a0\ufe0f @{user.username} - Suspicious\")\n            else:\n                print(f\"\u2713 @{user.username} - Human\")\n\nasyncio.run(analyze_new_followers())\n</code></pre>"},{"location":"api/ai/detection/#export-bot-report","title":"Export Bot Report","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import BotDetector\n\nasync def export_bot_report(username: str):\n    detector = BotDetector(provider=\"openai\", api_key=\"sk-...\")\n\n    async with Xeepy() as x:\n        followers = await x.scrape.followers(username, limit=500)\n        results = await detector.analyze_batch(followers.items)\n\n        data = []\n        for user, analysis in zip(followers.items, results):\n            data.append({\n                \"username\": user.username,\n                \"followers\": user.followers_count,\n                \"following\": user.following_count,\n                \"tweets\": user.tweet_count,\n                \"category\": analysis.category,\n                \"bot_probability\": analysis.bot_probability,\n                \"signals\": \", \".join(analysis.signals)\n            })\n\n        x.export.to_csv(data, f\"bot_report_{username}.csv\")\n        print(f\"Report exported with {len(data)} users\")\n\nasyncio.run(export_bot_report(\"myaccount\"))\n</code></pre>"},{"location":"api/ai/detection/#custom-threshold","title":"Custom Threshold","text":"<pre><code>from xeepy.ai import BotDetector\n\n# Strict detection (fewer false negatives)\nstrict = BotDetector(provider=\"openai\", api_key=\"sk-...\", threshold=0.5)\n\n# Lenient detection (fewer false positives)\nlenient = BotDetector(provider=\"openai\", api_key=\"sk-...\", threshold=0.9)\n\n# Per-call override\nresult = await lenient.filter_bots(users, threshold=0.6)\n</code></pre>"},{"location":"api/ai/detection/#block-detected-bots","title":"Block Detected Bots","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import BotDetector\n\nasync def block_bots(username: str, dry_run: bool = True):\n    detector = BotDetector(provider=\"openai\", api_key=\"sk-...\", threshold=0.85)\n\n    async with Xeepy() as x:\n        followers = await x.scrape.followers(username, limit=200)\n        result = await detector.filter_bots(followers.items)\n\n        print(f\"Found {len(result.bots)} bots\")\n\n        if dry_run:\n            print(\"Dry run - would block:\")\n            for bot in result.bots:\n                print(f\"  - @{bot.username}\")\n        else:\n            for bot in result.bots:\n                await x.engage.block(bot.username)\n                print(f\"Blocked @{bot.username}\")\n\nasyncio.run(block_bots(\"myaccount\", dry_run=True))\n</code></pre>"},{"location":"api/ai/detection/#see-also","title":"See Also","text":"<ul> <li>SentimentAnalyzer - Sentiment analysis</li> <li>ContentGenerator - Content generation</li> <li>AudienceAnalytics - Audience quality</li> </ul>"},{"location":"api/ai/providers/","title":"AIProvider","text":"<p>Abstract base class and factory for AI provider integrations.</p>"},{"location":"api/ai/providers/#import","title":"Import","text":"<pre><code>from xeepy.ai.providers import AIProvider, get_provider\n</code></pre>"},{"location":"api/ai/providers/#factory-function","title":"Factory Function","text":"<pre><code>def get_provider(\n    provider: str,\n    api_key: Optional[str] = None,\n    model: Optional[str] = None,\n    **kwargs\n) -&gt; AIProvider\n</code></pre> <p>Get an AI provider instance.</p> <p>Parameters: - <code>provider</code>: Provider name (<code>openai</code>, <code>anthropic</code>, <code>ollama</code>) - <code>api_key</code>: API key (not required for Ollama) - <code>model</code>: Model name (defaults to provider's best model) - <code>**kwargs</code>: Additional provider-specific options</p>"},{"location":"api/ai/providers/#supported-providers","title":"Supported Providers","text":"Provider Models API Key Required <code>openai</code> <code>gpt-4</code>, <code>gpt-4-turbo</code>, <code>gpt-3.5-turbo</code> Yes <code>anthropic</code> <code>claude-3-opus</code>, <code>claude-3-sonnet</code>, <code>claude-3-haiku</code> Yes <code>ollama</code> <code>llama2</code>, <code>mistral</code>, <code>codellama</code> No"},{"location":"api/ai/providers/#base-class","title":"Base Class","text":"<pre><code>class AIProvider(ABC):\n    @abstractmethod\n    async def generate(\n        self,\n        prompt: str,\n        system_prompt: Optional[str] = None,\n        max_tokens: int = 500,\n        temperature: float = 0.7\n    ) -&gt; str:\n        \"\"\"Generate text completion.\"\"\"\n        pass\n\n    @abstractmethod\n    async def analyze(\n        self,\n        text: str,\n        analysis_type: str\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Analyze text (sentiment, topics, etc.).\"\"\"\n        pass\n</code></pre>"},{"location":"api/ai/providers/#openaiprovider","title":"OpenAIProvider","text":"<pre><code>class OpenAIProvider(AIProvider):\n    def __init__(\n        self,\n        api_key: str,\n        model: str = \"gpt-4\",\n        organization: Optional[str] = None\n    )\n</code></pre>"},{"location":"api/ai/providers/#anthropicprovider","title":"AnthropicProvider","text":"<pre><code>class AnthropicProvider(AIProvider):\n    def __init__(\n        self,\n        api_key: str,\n        model: str = \"claude-3-opus-20240229\"\n    )\n</code></pre>"},{"location":"api/ai/providers/#ollamaprovider","title":"OllamaProvider","text":"<pre><code>class OllamaProvider(AIProvider):\n    def __init__(\n        self,\n        model: str = \"llama2\",\n        host: str = \"http://localhost:11434\"\n    )\n</code></pre>"},{"location":"api/ai/providers/#usage-examples","title":"Usage Examples","text":""},{"location":"api/ai/providers/#basic-provider-setup","title":"Basic Provider Setup","text":"<pre><code>from xeepy.ai.providers import get_provider\n\n# OpenAI\nopenai = get_provider(\"openai\", api_key=\"sk-...\", model=\"gpt-4\")\n\n# Anthropic\nclaude = get_provider(\"anthropic\", api_key=\"sk-ant-...\", model=\"claude-3-opus\")\n\n# Ollama (local)\nollama = get_provider(\"ollama\", model=\"llama2\")\n</code></pre>"},{"location":"api/ai/providers/#generate-text","title":"Generate Text","text":"<pre><code>from xeepy.ai.providers import get_provider\n\nasync def main():\n    ai = get_provider(\"openai\", api_key=\"sk-...\")\n\n    response = await ai.generate(\n        prompt=\"Write a tweet about Python programming\",\n        system_prompt=\"You are a tech influencer on Twitter\",\n        max_tokens=280,\n        temperature=0.8\n    )\n\n    print(response)\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/ai/providers/#analyze-text","title":"Analyze Text","text":"<pre><code>from xeepy.ai.providers import get_provider\n\nasync def main():\n    ai = get_provider(\"anthropic\", api_key=\"sk-ant-...\")\n\n    analysis = await ai.analyze(\n        text=\"I love this new feature! Amazing work!\",\n        analysis_type=\"sentiment\"\n    )\n\n    print(f\"Sentiment: {analysis['sentiment']}\")\n    print(f\"Score: {analysis['score']}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/ai/providers/#use-with-xeepy","title":"Use with Xeepy","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai.providers import get_provider\n\nasync def main():\n    ai = get_provider(\"openai\", api_key=\"sk-...\")\n\n    async with Xeepy() as x:\n        # Get tweets to reply to\n        tweets = await x.scrape.search(\"Python tips\", limit=5)\n\n        for tweet in tweets.items:\n            # Generate AI reply\n            reply = await ai.generate(\n                prompt=f\"Write a helpful reply to: {tweet.text}\",\n                system_prompt=\"Be helpful and friendly. Max 280 chars.\",\n                max_tokens=100\n            )\n            print(f\"Tweet: {tweet.text[:50]}...\")\n            print(f\"Reply: {reply}\\n\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/ai/providers/#local-ollama-setup","title":"Local Ollama Setup","text":"<pre><code>from xeepy.ai.providers import get_provider\n\nasync def main():\n    # Make sure Ollama is running: ollama serve\n    ai = get_provider(\n        \"ollama\",\n        model=\"llama2\",\n        host=\"http://localhost:11434\"\n    )\n\n    response = await ai.generate(\n        prompt=\"Explain async/await in Python\",\n        max_tokens=200\n    )\n    print(response)\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/ai/providers/#provider-with-custom-settings","title":"Provider with Custom Settings","text":"<pre><code>from xeepy.ai.providers import OpenAIProvider\n\nasync def main():\n    ai = OpenAIProvider(\n        api_key=\"sk-...\",\n        model=\"gpt-4-turbo\",\n        organization=\"org-...\"\n    )\n\n    # Configure generation\n    response = await ai.generate(\n        prompt=\"Tweet ideas for developers\",\n        system_prompt=\"You create viral tech content\",\n        max_tokens=500,\n        temperature=0.9  # More creative\n    )\n\n    print(response)\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/ai/providers/#multiple-providers","title":"Multiple Providers","text":"<pre><code>from xeepy.ai.providers import get_provider\n\nasync def compare_providers(prompt: str):\n    providers = {\n        \"gpt-4\": get_provider(\"openai\", api_key=\"sk-...\"),\n        \"claude\": get_provider(\"anthropic\", api_key=\"sk-ant-...\"),\n        \"llama\": get_provider(\"ollama\", model=\"llama2\")\n    }\n\n    results = {}\n    for name, ai in providers.items():\n        try:\n            results[name] = await ai.generate(prompt, max_tokens=100)\n        except Exception as e:\n            results[name] = f\"Error: {e}\"\n\n    return results\n\nasyncio.run(compare_providers(\"Write a Python tip\"))\n</code></pre>"},{"location":"api/ai/providers/#environment-variables","title":"Environment Variables","text":"<pre><code>import os\nfrom xeepy.ai.providers import get_provider\n\n# API keys from environment\nopenai = get_provider(\n    \"openai\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n)\n\nanthropic = get_provider(\n    \"anthropic\",\n    api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n)\n</code></pre>"},{"location":"api/ai/providers/#error-handling","title":"Error Handling","text":"<pre><code>from xeepy.ai.providers import get_provider, AIProviderError\n\nasync def safe_generate(prompt: str):\n    ai = get_provider(\"openai\", api_key=\"sk-...\")\n\n    try:\n        return await ai.generate(prompt)\n    except AIProviderError as e:\n        print(f\"AI error: {e}\")\n        return None\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return None\n\nasyncio.run(safe_generate(\"Hello world\"))\n</code></pre>"},{"location":"api/ai/providers/#streaming-responses","title":"Streaming Responses","text":"<pre><code>from xeepy.ai.providers import get_provider\n\nasync def stream_response():\n    ai = get_provider(\"openai\", api_key=\"sk-...\")\n\n    async for chunk in ai.generate_stream(\n        prompt=\"Write a long tweet thread about AI\",\n        max_tokens=1000\n    ):\n        print(chunk, end=\"\", flush=True)\n\nasyncio.run(stream_response())\n</code></pre>"},{"location":"api/ai/providers/#see-also","title":"See Also","text":"<ul> <li>ContentGenerator - AI content generation</li> <li>SentimentAnalyzer - Sentiment analysis</li> <li>BotDetector - Bot detection</li> </ul>"},{"location":"api/ai/sentiment/","title":"SentimentAnalyzer","text":"<p>AI-powered sentiment analysis for tweets and conversations.</p>"},{"location":"api/ai/sentiment/#import","title":"Import","text":"<pre><code>from xeepy.ai import SentimentAnalyzer\n</code></pre>"},{"location":"api/ai/sentiment/#class-signature","title":"Class Signature","text":"<pre><code>class SentimentAnalyzer:\n    def __init__(\n        self,\n        provider: str = \"openai\",\n        api_key: Optional[str] = None,\n        model: Optional[str] = None\n    )\n</code></pre>"},{"location":"api/ai/sentiment/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>provider</code> <code>str</code> <code>\"openai\"</code> AI provider name <code>api_key</code> <code>Optional[str]</code> <code>None</code> API key <code>model</code> <code>Optional[str]</code> <code>None</code> Model name"},{"location":"api/ai/sentiment/#methods","title":"Methods","text":"Method Returns Description <code>analyze(text)</code> <code>SentimentResult</code> Analyze single text <code>analyze_batch(texts)</code> <code>List[SentimentResult]</code> Analyze multiple texts <code>analyze_conversation(tweets)</code> <code>ConversationSentiment</code> Analyze thread <code>track_sentiment(username)</code> <code>SentimentTrend</code> Track over time <code>brand_sentiment(brand, tweets)</code> <code>BrandSentiment</code> Brand mention analysis"},{"location":"api/ai/sentiment/#analyze","title":"<code>analyze</code>","text":"<pre><code>async def analyze(\n    self,\n    text: str,\n    detailed: bool = False\n) -&gt; SentimentResult\n</code></pre> <p>Analyze sentiment of text.</p> <p>Parameters: - <code>text</code>: Text to analyze - <code>detailed</code>: Include emotion breakdown</p>"},{"location":"api/ai/sentiment/#analyze_conversation","title":"<code>analyze_conversation</code>","text":"<pre><code>async def analyze_conversation(\n    self,\n    tweets: List[Tweet]\n) -&gt; ConversationSentiment\n</code></pre> <p>Analyze sentiment progression in a conversation.</p>"},{"location":"api/ai/sentiment/#brand_sentiment","title":"<code>brand_sentiment</code>","text":"<pre><code>async def brand_sentiment(\n    self,\n    brand: str,\n    tweets: List[Tweet]\n) -&gt; BrandSentiment\n</code></pre> <p>Analyze sentiment toward a specific brand.</p>"},{"location":"api/ai/sentiment/#sentimentresult-object","title":"SentimentResult Object","text":"<pre><code>@dataclass\nclass SentimentResult:\n    text: str                        # Analyzed text\n    sentiment: str                   # positive, negative, neutral\n    score: float                     # -1.0 to 1.0\n    confidence: float                # 0.0 to 1.0\n    emotions: Optional[Dict[str, float]]  # Emotion breakdown\n</code></pre>"},{"location":"api/ai/sentiment/#conversationsentiment-object","title":"ConversationSentiment Object","text":"<pre><code>@dataclass\nclass ConversationSentiment:\n    overall_sentiment: str           # Overall conversation tone\n    average_score: float             # Average sentiment score\n    sentiment_flow: List[float]      # Score progression\n    turning_points: List[int]        # Where sentiment changed\n    toxic_messages: List[int]        # Indices of toxic content\n</code></pre>"},{"location":"api/ai/sentiment/#brandsentiment-object","title":"BrandSentiment Object","text":"<pre><code>@dataclass\nclass BrandSentiment:\n    brand: str                       # Brand analyzed\n    total_mentions: int              # Total mention count\n    positive_pct: float              # % positive\n    negative_pct: float              # % negative\n    neutral_pct: float               # % neutral\n    average_score: float             # Average score\n    top_positive: List[Tweet]        # Most positive mentions\n    top_negative: List[Tweet]        # Most negative mentions\n    themes: Dict[str, str]           # Common themes\n</code></pre>"},{"location":"api/ai/sentiment/#usage-examples","title":"Usage Examples","text":""},{"location":"api/ai/sentiment/#basic-sentiment-analysis","title":"Basic Sentiment Analysis","text":"<pre><code>from xeepy.ai import SentimentAnalyzer\n\nasync def main():\n    analyzer = SentimentAnalyzer(provider=\"openai\", api_key=\"sk-...\")\n\n    texts = [\n        \"I love this new feature! Amazing work!\",\n        \"This is the worst update ever. Completely broken.\",\n        \"Just updated to the new version.\"\n    ]\n\n    for text in texts:\n        result = await analyzer.analyze(text)\n        emoji = \"\ud83d\ude0a\" if result.sentiment == \"positive\" else \"\ud83d\ude20\" if result.sentiment == \"negative\" else \"\ud83d\ude10\"\n        print(f\"{emoji} {result.sentiment} ({result.score:+.2f}): {text[:50]}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/ai/sentiment/#detailed-emotion-analysis","title":"Detailed Emotion Analysis","text":"<pre><code>from xeepy.ai import SentimentAnalyzer\n\nasync def main():\n    analyzer = SentimentAnalyzer(provider=\"openai\", api_key=\"sk-...\")\n\n    text = \"I can't believe they did this! So disappointed and angry!\"\n\n    result = await analyzer.analyze(text, detailed=True)\n\n    print(f\"Sentiment: {result.sentiment} ({result.score:+.2f})\")\n    print(f\"Confidence: {result.confidence:.0%}\")\n    print(f\"\\nEmotions:\")\n    for emotion, score in sorted(result.emotions.items(), key=lambda x: -x[1]):\n        bar = \"\u2588\" * int(score * 10)\n        print(f\"  {emotion}: {bar} {score:.0%}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/ai/sentiment/#batch-analysis","title":"Batch Analysis","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import SentimentAnalyzer\n\nasync def analyze_replies():\n    analyzer = SentimentAnalyzer(provider=\"openai\", api_key=\"sk-...\")\n\n    async with Xeepy() as x:\n        replies = await x.scrape.replies(\"https://x.com/user/status/123\", limit=50)\n\n        texts = [r.text for r in replies.items]\n        results = await analyzer.analyze_batch(texts)\n\n        positive = sum(1 for r in results if r.sentiment == \"positive\")\n        negative = sum(1 for r in results if r.sentiment == \"negative\")\n        neutral = sum(1 for r in results if r.sentiment == \"neutral\")\n\n        print(f\"=== Reply Sentiment Analysis ===\")\n        print(f\"Total replies: {len(results)}\")\n        print(f\"Positive: {positive} ({positive/len(results):.0%})\")\n        print(f\"Negative: {negative} ({negative/len(results):.0%})\")\n        print(f\"Neutral: {neutral} ({neutral/len(results):.0%})\")\n\nasyncio.run(analyze_replies())\n</code></pre>"},{"location":"api/ai/sentiment/#conversation-sentiment-flow","title":"Conversation Sentiment Flow","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import SentimentAnalyzer\n\nasync def analyze_thread():\n    analyzer = SentimentAnalyzer(provider=\"openai\", api_key=\"sk-...\")\n\n    async with Xeepy() as x:\n        thread = await x.scrape.replies(\"https://x.com/user/status/123\", limit=100)\n\n        conversation = await analyzer.analyze_conversation(thread.items)\n\n        print(f\"=== Conversation Analysis ===\")\n        print(f\"Overall: {conversation.overall_sentiment}\")\n        print(f\"Average score: {conversation.average_score:+.2f}\")\n\n        print(f\"\\nSentiment flow:\")\n        for i, score in enumerate(conversation.sentiment_flow):\n            bar = \"+\" * max(0, int(score * 5)) + \"-\" * max(0, int(-score * 5))\n            print(f\"  {i+1}. [{bar:10}] {score:+.2f}\")\n\n        if conversation.turning_points:\n            print(f\"\\nTurning points at messages: {conversation.turning_points}\")\n\n        if conversation.toxic_messages:\n            print(f\"\\n\u26a0\ufe0f Toxic content at: {conversation.toxic_messages}\")\n\nasyncio.run(analyze_thread())\n</code></pre>"},{"location":"api/ai/sentiment/#brand-sentiment-analysis","title":"Brand Sentiment Analysis","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import SentimentAnalyzer\n\nasync def analyze_brand(brand: str):\n    analyzer = SentimentAnalyzer(provider=\"openai\", api_key=\"sk-...\")\n\n    async with Xeepy() as x:\n        tweets = await x.scrape.search(brand, limit=100)\n\n        report = await analyzer.brand_sentiment(brand, tweets.items)\n\n        print(f\"=== Brand Sentiment: {brand} ===\")\n        print(f\"Total mentions: {report.total_mentions}\")\n        print(f\"Positive: {report.positive_pct:.0%}\")\n        print(f\"Negative: {report.negative_pct:.0%}\")\n        print(f\"Neutral: {report.neutral_pct:.0%}\")\n        print(f\"Score: {report.average_score:+.2f}\")\n\n        print(f\"\\nTop positive mentions:\")\n        for tweet in report.top_positive[:3]:\n            print(f\"  - {tweet.text[:60]}...\")\n\n        print(f\"\\nTop negative mentions:\")\n        for tweet in report.top_negative[:3]:\n            print(f\"  - {tweet.text[:60]}...\")\n\nasyncio.run(analyze_brand(\"Python\"))\n</code></pre>"},{"location":"api/ai/sentiment/#sentiment-tracking-over-time","title":"Sentiment Tracking Over Time","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import SentimentAnalyzer\n\nasync def track_account_sentiment(username: str):\n    analyzer = SentimentAnalyzer(provider=\"openai\", api_key=\"sk-...\")\n\n    async with Xeepy() as x:\n        # Get recent tweets\n        tweets = await x.scrape.tweets(username, limit=100)\n\n        trend = await analyzer.track_sentiment(tweets.items)\n\n        print(f\"=== Sentiment Trend: @{username} ===\")\n        print(f\"Current sentiment: {trend.current}\")\n        print(f\"7-day average: {trend.avg_7d:+.2f}\")\n        print(f\"30-day average: {trend.avg_30d:+.2f}\")\n\n        if trend.avg_7d &gt; trend.avg_30d:\n            print(\"\ud83d\udcc8 Sentiment improving!\")\n        elif trend.avg_7d &lt; trend.avg_30d:\n            print(\"\ud83d\udcc9 Sentiment declining\")\n\nasyncio.run(track_account_sentiment(\"username\"))\n</code></pre>"},{"location":"api/ai/sentiment/#export-sentiment-report","title":"Export Sentiment Report","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import SentimentAnalyzer\n\nasync def export_sentiment_report(tweet_url: str):\n    analyzer = SentimentAnalyzer(provider=\"openai\", api_key=\"sk-...\")\n\n    async with Xeepy() as x:\n        replies = await x.scrape.replies(tweet_url, limit=200)\n        results = await analyzer.analyze_batch([r.text for r in replies.items])\n\n        data = []\n        for reply, sentiment in zip(replies.items, results):\n            data.append({\n                \"author\": reply.author.username,\n                \"text\": reply.text,\n                \"sentiment\": sentiment.sentiment,\n                \"score\": sentiment.score,\n                \"confidence\": sentiment.confidence\n            })\n\n        x.export.to_csv(data, \"sentiment_report.csv\")\n        print(\"Sentiment report exported\")\n\nasyncio.run(export_sentiment_report(\"https://x.com/user/status/123\"))\n</code></pre>"},{"location":"api/ai/sentiment/#filter-by-sentiment","title":"Filter by Sentiment","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import SentimentAnalyzer\n\nasync def get_positive_mentions(username: str):\n    analyzer = SentimentAnalyzer(provider=\"openai\", api_key=\"sk-...\")\n\n    async with Xeepy() as x:\n        mentions = await x.scrape.mentions(username, limit=100)\n        results = await analyzer.analyze_batch([m.text for m in mentions.items])\n\n        positive = [\n            (m, r) for m, r in zip(mentions.items, results)\n            if r.sentiment == \"positive\" and r.confidence &gt; 0.8\n        ]\n\n        print(f\"Found {len(positive)} highly positive mentions:\")\n        for mention, result in positive[:10]:\n            print(f\"  @{mention.author.username}: {mention.text[:50]}...\")\n\nasyncio.run(get_positive_mentions(\"myaccount\"))\n</code></pre>"},{"location":"api/ai/sentiment/#see-also","title":"See Also","text":"<ul> <li>ContentGenerator - AI content generation</li> <li>BotDetector - Bot detection</li> <li>AIProvider - Provider setup</li> </ul>"},{"location":"api/analytics/audience/","title":"AudienceAnalytics","text":"<p>Analyze audience demographics, interests, and behavior.</p>"},{"location":"api/analytics/audience/#import","title":"Import","text":"<pre><code>from xeepy.analytics.audience import AudienceAnalytics\n</code></pre>"},{"location":"api/analytics/audience/#class-signature","title":"Class Signature","text":"<pre><code>class AudienceAnalytics:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        storage: Optional[Storage] = None\n    )\n</code></pre>"},{"location":"api/analytics/audience/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>storage</code> <code>Optional[Storage]</code> <code>None</code> Storage for historical data"},{"location":"api/analytics/audience/#methods","title":"Methods","text":"Method Returns Description <code>analyze(username)</code> <code>AudienceAnalysis</code> Full audience analysis <code>demographics(username)</code> <code>Demographics</code> Audience demographics <code>interests(username)</code> <code>InterestAnalysis</code> Audience interests <code>quality_score(username)</code> <code>QualityReport</code> Follower quality <code>active_followers(username)</code> <code>List[User]</code> Most engaged followers"},{"location":"api/analytics/audience/#analyze","title":"<code>analyze</code>","text":"<pre><code>async def analyze(\n    self,\n    username: Optional[str] = None,\n    sample_size: int = 500\n) -&gt; AudienceAnalysis\n</code></pre> <p>Comprehensive audience analysis.</p> <p>Parameters: - <code>username</code>: Account to analyze - <code>sample_size</code>: Number of followers to sample</p>"},{"location":"api/analytics/audience/#quality_score","title":"<code>quality_score</code>","text":"<pre><code>async def quality_score(\n    self,\n    username: Optional[str] = None\n) -&gt; QualityReport\n</code></pre> <p>Calculate follower quality score.</p>"},{"location":"api/analytics/audience/#active_followers","title":"<code>active_followers</code>","text":"<pre><code>async def active_followers(\n    self,\n    username: Optional[str] = None,\n    limit: int = 100\n) -&gt; List[User]\n</code></pre> <p>Get most engaged followers.</p>"},{"location":"api/analytics/audience/#audienceanalysis-object","title":"AudienceAnalysis Object","text":"<pre><code>@dataclass\nclass AudienceAnalysis:\n    username: str                    # Account analyzed\n    total_followers: int             # Total follower count\n    sample_size: int                 # Followers sampled\n    demographics: Demographics       # Demographic data\n    interests: InterestAnalysis      # Interest data\n    quality_score: float             # 0-100 quality score\n    active_percentage: float         # % of active followers\n    influencer_followers: int        # Followers with 10K+\n    verified_followers: int          # Verified followers\n</code></pre>"},{"location":"api/analytics/audience/#demographics-object","title":"Demographics Object","text":"<pre><code>@dataclass\nclass Demographics:\n    locations: Dict[str, float]      # Location distribution\n    languages: Dict[str, float]      # Language distribution\n    account_ages: Dict[str, float]   # Account age distribution\n    follower_ranges: Dict[str, float] # Follower count ranges\n</code></pre>"},{"location":"api/analytics/audience/#interestanalysis-object","title":"InterestAnalysis Object","text":"<pre><code>@dataclass\nclass InterestAnalysis:\n    top_interests: List[str]         # Common interests\n    common_hashtags: List[str]       # Frequently used hashtags\n    common_follows: List[str]        # Commonly followed accounts\n    bio_keywords: Dict[str, int]     # Common bio keywords\n</code></pre>"},{"location":"api/analytics/audience/#qualityreport-object","title":"QualityReport Object","text":"<pre><code>@dataclass\nclass QualityReport:\n    overall_score: float             # 0-100 score\n    real_followers: float            # % real accounts\n    active_followers: float          # % active (posted recently)\n    engaged_followers: float         # % who engage\n    bot_score: float                 # Estimated bot %\n    suspicious_accounts: int         # Suspicious count\n</code></pre>"},{"location":"api/analytics/audience/#usage-examples","title":"Usage Examples","text":""},{"location":"api/analytics/audience/#full-audience-analysis","title":"Full Audience Analysis","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        analysis = await x.analytics.audience(\"username\")\n\n        print(f\"=== Audience Analysis ===\")\n        print(f\"Total followers: {analysis.total_followers:,}\")\n        print(f\"Quality score: {analysis.quality_score:.0f}/100\")\n        print(f\"Active followers: {analysis.active_percentage:.1f}%\")\n        print(f\"Influencer followers (10K+): {analysis.influencer_followers:,}\")\n        print(f\"Verified followers: {analysis.verified_followers:,}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/audience/#demographic-breakdown","title":"Demographic Breakdown","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        demo = await x.analytics.audience_demographics(\"username\")\n\n        print(f\"=== Demographics ===\")\n\n        print(f\"\\nTop locations:\")\n        for loc, pct in list(demo.locations.items())[:5]:\n            print(f\"  {loc}: {pct:.1f}%\")\n\n        print(f\"\\nLanguages:\")\n        for lang, pct in list(demo.languages.items())[:5]:\n            print(f\"  {lang}: {pct:.1f}%\")\n\n        print(f\"\\nFollower ranges:\")\n        for range_name, pct in demo.follower_ranges.items():\n            print(f\"  {range_name}: {pct:.1f}%\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/audience/#interest-analysis","title":"Interest Analysis","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        interests = await x.analytics.audience_interests(\"username\")\n\n        print(f\"=== Audience Interests ===\")\n\n        print(f\"\\nTop interests:\")\n        for interest in interests.top_interests[:10]:\n            print(f\"  - {interest}\")\n\n        print(f\"\\nCommon hashtags:\")\n        for hashtag in interests.common_hashtags[:10]:\n            print(f\"  - {hashtag}\")\n\n        print(f\"\\nAlso follow:\")\n        for account in interests.common_follows[:10]:\n            print(f\"  - @{account}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/audience/#follower-quality-score","title":"Follower Quality Score","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        quality = await x.analytics.audience_quality(\"username\")\n\n        print(f\"=== Follower Quality Report ===\")\n        print(f\"Overall score: {quality.overall_score:.0f}/100\")\n        print(f\"Real followers: {quality.real_followers:.1f}%\")\n        print(f\"Active followers: {quality.active_followers:.1f}%\")\n        print(f\"Engaged followers: {quality.engaged_followers:.1f}%\")\n        print(f\"Estimated bots: {quality.bot_score:.1f}%\")\n        print(f\"Suspicious accounts: {quality.suspicious_accounts:,}\")\n\n        if quality.overall_score &gt;= 80:\n            print(\"\\n\u2713 High quality audience!\")\n        elif quality.overall_score &gt;= 60:\n            print(\"\\n\u26a0\ufe0f Moderate quality, room for improvement\")\n        else:\n            print(\"\\n\u274c Low quality, consider cleaning followers\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/audience/#find-active-followers","title":"Find Active Followers","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        active = await x.analytics.active_followers(\"username\", limit=50)\n\n        print(f\"=== Most Active Followers ===\")\n        for user in active:\n            print(f\"@{user.username}\")\n            print(f\"  Followers: {user.followers_count:,}\")\n            print(f\"  Engagement score: {user.engagement_score:.1f}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/audience/#find-influential-followers","title":"Find Influential Followers","text":"<pre><code>from xeepy import Xeepy\n\nasync def find_influencers(username: str, min_followers: int = 10000):\n    async with Xeepy() as x:\n        analysis = await x.analytics.audience(username, sample_size=1000)\n        followers = await x.scrape.followers(username, limit=1000)\n\n        influencers = [\n            f for f in followers.items\n            if f.followers_count &gt;= min_followers\n        ]\n\n        influencers.sort(key=lambda f: f.followers_count, reverse=True)\n\n        print(f\"Found {len(influencers)} influential followers:\")\n        for user in influencers[:20]:\n            verified = \"\u2713\" if user.is_verified else \"\"\n            print(f\"  @{user.username} {verified} - {user.followers_count:,} followers\")\n\nasyncio.run(find_influencers(\"myaccount\"))\n</code></pre>"},{"location":"api/analytics/audience/#export-audience-report","title":"Export Audience Report","text":"<pre><code>from xeepy import Xeepy\n\nasync def export_audience_report(username: str):\n    async with Xeepy() as x:\n        analysis = await x.analytics.audience(username)\n\n        report = {\n            \"username\": username,\n            \"total_followers\": analysis.total_followers,\n            \"quality_score\": analysis.quality_score,\n            \"active_percentage\": analysis.active_percentage,\n            \"demographics\": {\n                \"locations\": analysis.demographics.locations,\n                \"languages\": analysis.demographics.languages\n            },\n            \"interests\": {\n                \"top_interests\": analysis.interests.top_interests[:20],\n                \"common_hashtags\": analysis.interests.common_hashtags[:20],\n                \"also_follow\": analysis.interests.common_follows[:20]\n            }\n        }\n\n        x.export.to_json([report], f\"audience_{username}.json\")\n        print(f\"Audience report exported\")\n\nasyncio.run(export_audience_report(\"myaccount\"))\n</code></pre>"},{"location":"api/analytics/audience/#compare-audience-overlap","title":"Compare Audience Overlap","text":"<pre><code>from xeepy import Xeepy\n\nasync def compare_audiences(account1: str, account2: str):\n    async with Xeepy() as x:\n        followers1 = await x.scrape.followers(account1, limit=2000)\n        followers2 = await x.scrape.followers(account2, limit=2000)\n\n        set1 = {f.username for f in followers1.items}\n        set2 = {f.username for f in followers2.items}\n\n        overlap = set1.intersection(set2)\n\n        print(f\"Audience Overlap Analysis\")\n        print(f\"@{account1} followers: {len(set1):,}\")\n        print(f\"@{account2} followers: {len(set2):,}\")\n        print(f\"Overlap: {len(overlap):,}\")\n        print(f\"Overlap %: {len(overlap) / len(set1) * 100:.1f}%\")\n\nasyncio.run(compare_audiences(\"account1\", \"account2\"))\n</code></pre>"},{"location":"api/analytics/audience/#see-also","title":"See Also","text":"<ul> <li>GrowthAnalytics - Growth analysis</li> <li>CompetitorAnalytics - Competitor analysis</li> <li>FollowersScraper - Get followers</li> </ul>"},{"location":"api/analytics/competitors/","title":"CompetitorAnalytics","text":"<p>Analyze and compare competitor accounts on X/Twitter.</p>"},{"location":"api/analytics/competitors/#import","title":"Import","text":"<pre><code>from xeepy.analytics.competitors import CompetitorAnalytics\n</code></pre>"},{"location":"api/analytics/competitors/#class-signature","title":"Class Signature","text":"<pre><code>class CompetitorAnalytics:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        storage: Optional[Storage] = None\n    )\n</code></pre>"},{"location":"api/analytics/competitors/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>storage</code> <code>Optional[Storage]</code> <code>None</code> Storage for historical data"},{"location":"api/analytics/competitors/#methods","title":"Methods","text":"Method Returns Description <code>compare(accounts)</code> <code>ComparisonReport</code> Compare multiple accounts <code>analyze(competitor)</code> <code>CompetitorAnalysis</code> Deep dive on competitor <code>content_gap(you, competitor)</code> <code>ContentGapReport</code> Find content gaps <code>best_content(competitor)</code> <code>List[Tweet]</code> Get best performing content <code>track(competitors, callback)</code> <code>None</code> Track competitors over time"},{"location":"api/analytics/competitors/#compare","title":"<code>compare</code>","text":"<pre><code>async def compare(\n    self,\n    accounts: List[str],\n    metrics: List[str] = None\n) -&gt; ComparisonReport\n</code></pre> <p>Compare multiple accounts side by side.</p> <p>Parameters: - <code>accounts</code>: Usernames to compare - <code>metrics</code>: Specific metrics to compare (default: all)</p>"},{"location":"api/analytics/competitors/#content_gap","title":"<code>content_gap</code>","text":"<pre><code>async def content_gap(\n    self,\n    your_account: str,\n    competitor: str\n) -&gt; ContentGapReport\n</code></pre> <p>Find content topics you're missing vs competitor.</p>"},{"location":"api/analytics/competitors/#best_content","title":"<code>best_content</code>","text":"<pre><code>async def best_content(\n    self,\n    competitor: str,\n    limit: int = 20,\n    period: str = \"30d\"\n) -&gt; List[Tweet]\n</code></pre> <p>Get competitor's best performing content.</p>"},{"location":"api/analytics/competitors/#comparisonreport-object","title":"ComparisonReport Object","text":"<pre><code>@dataclass\nclass ComparisonReport:\n    accounts: List[AccountMetrics]   # Metrics for each account\n    rankings: Dict[str, List[str]]   # Rankings by metric\n    insights: List[str]              # Key insights\n    generated_at: datetime           # Report timestamp\n</code></pre>"},{"location":"api/analytics/competitors/#accountmetrics-object","title":"AccountMetrics Object","text":"<pre><code>@dataclass\nclass AccountMetrics:\n    username: str                    # Account username\n    followers: int                   # Follower count\n    following: int                   # Following count\n    tweets: int                      # Tweet count\n    engagement_rate: float           # Engagement rate\n    avg_likes: float                 # Average likes\n    avg_retweets: float              # Average retweets\n    posting_frequency: float         # Tweets per day\n    growth_rate: float               # 30-day growth %\n</code></pre>"},{"location":"api/analytics/competitors/#contentgapreport-object","title":"ContentGapReport Object","text":"<pre><code>@dataclass\nclass ContentGapReport:\n    missing_topics: List[str]        # Topics you don't cover\n    missing_hashtags: List[str]      # Hashtags you don't use\n    content_type_gaps: Dict[str, float]  # Content type differences\n    timing_gaps: List[int]           # Hours you miss\n    recommendations: List[str]       # Suggested actions\n</code></pre>"},{"location":"api/analytics/competitors/#usage-examples","title":"Usage Examples","text":""},{"location":"api/analytics/competitors/#compare-accounts","title":"Compare Accounts","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        report = await x.analytics.compare_competitors([\n            \"myaccount\",\n            \"competitor1\",\n            \"competitor2\"\n        ])\n\n        print(\"=== Competitor Comparison ===\")\n        for account in report.accounts:\n            print(f\"\\n@{account.username}:\")\n            print(f\"  Followers: {account.followers:,}\")\n            print(f\"  Engagement rate: {account.engagement_rate:.2f}%\")\n            print(f\"  Avg likes: {account.avg_likes:.0f}\")\n            print(f\"  Posts/day: {account.posting_frequency:.1f}\")\n            print(f\"  30d growth: {account.growth_rate:+.2f}%\")\n\n        print(f\"\\n=== Rankings ===\")\n        for metric, ranking in report.rankings.items():\n            print(f\"  {metric}: {' &gt; '.join(ranking)}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/competitors/#deep-competitor-analysis","title":"Deep Competitor Analysis","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        analysis = await x.analytics.analyze_competitor(\"competitor\")\n\n        print(f\"=== Competitor Analysis: @{analysis.username} ===\")\n        print(f\"\\nOverview:\")\n        print(f\"  Followers: {analysis.followers:,}\")\n        print(f\"  Engagement rate: {analysis.engagement_rate:.2f}%\")\n\n        print(f\"\\nContent Strategy:\")\n        print(f\"  Posts per week: {analysis.posts_per_week:.1f}\")\n        print(f\"  Best posting times: {analysis.best_times}\")\n        print(f\"  Top content types: {analysis.top_content_types}\")\n\n        print(f\"\\nTop hashtags:\")\n        for hashtag in analysis.top_hashtags[:5]:\n            print(f\"  - {hashtag}\")\n\n        print(f\"\\nRecent growth: {analysis.growth_30d:+.2f}%\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/competitors/#find-content-gaps","title":"Find Content Gaps","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        gaps = await x.analytics.content_gap(\"myaccount\", \"competitor\")\n\n        print(\"=== Content Gap Analysis ===\")\n\n        print(f\"\\nTopics to explore:\")\n        for topic in gaps.missing_topics[:10]:\n            print(f\"  - {topic}\")\n\n        print(f\"\\nHashtags to try:\")\n        for hashtag in gaps.missing_hashtags[:10]:\n            print(f\"  - {hashtag}\")\n\n        print(f\"\\nContent type opportunities:\")\n        for content_type, diff in gaps.content_type_gaps.items():\n            if diff &gt; 0:\n                print(f\"  - More {content_type} ({diff:+.1f}%)\")\n\n        print(f\"\\nRecommendations:\")\n        for rec in gaps.recommendations:\n            print(f\"  \u2022 {rec}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/competitors/#get-competitors-best-content","title":"Get Competitor's Best Content","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        best = await x.analytics.competitor_best_content(\n            \"competitor\",\n            limit=10,\n            period=\"30d\"\n        )\n\n        print(\"=== Competitor's Top Content ===\")\n        for i, tweet in enumerate(best, 1):\n            print(f\"\\n{i}. {tweet.text[:80]}...\")\n            print(f\"   Likes: {tweet.like_count:,} | RT: {tweet.retweet_count:,}\")\n            print(f\"   Engagement: {tweet.engagement_rate:.2f}%\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/competitors/#track-competitors-over-time","title":"Track Competitors Over Time","text":"<pre><code>from xeepy import Xeepy\nimport asyncio\n\nasync def track_competitors():\n    async with Xeepy() as x:\n        async def on_update(report):\n            print(f\"\\n[{datetime.now()}] Competitor Update\")\n            for account in report.accounts:\n                change = account.followers - account.previous_followers\n                print(f\"  @{account.username}: {account.followers:,} ({change:+,})\")\n\n        await x.analytics.track_competitors(\n            [\"competitor1\", \"competitor2\", \"competitor3\"],\n            callback=on_update,\n            interval=3600  # Check hourly\n        )\n\nasyncio.run(track_competitors())\n</code></pre>"},{"location":"api/analytics/competitors/#export-comparison-report","title":"Export Comparison Report","text":"<pre><code>from xeepy import Xeepy\n\nasync def export_comparison(accounts: list):\n    async with Xeepy() as x:\n        report = await x.analytics.compare_competitors(accounts)\n\n        data = [\n            {\n                \"username\": a.username,\n                \"followers\": a.followers,\n                \"engagement_rate\": a.engagement_rate,\n                \"avg_likes\": a.avg_likes,\n                \"posting_frequency\": a.posting_frequency,\n                \"growth_rate\": a.growth_rate\n            }\n            for a in report.accounts\n        ]\n\n        x.export.to_csv(data, \"competitor_comparison.csv\")\n        print(\"Comparison exported to competitor_comparison.csv\")\n\nasyncio.run(export_comparison([\"account1\", \"account2\", \"account3\"]))\n</code></pre>"},{"location":"api/analytics/competitors/#competitive-positioning","title":"Competitive Positioning","text":"<pre><code>from xeepy import Xeepy\n\nasync def competitive_position(your_account: str, competitors: list):\n    async with Xeepy() as x:\n        all_accounts = [your_account] + competitors\n        report = await x.analytics.compare_competitors(all_accounts)\n\n        # Find your position in each ranking\n        print(f\"=== Competitive Positioning ===\")\n\n        for metric, ranking in report.rankings.items():\n            position = ranking.index(your_account) + 1\n            total = len(ranking)\n\n            if position == 1:\n                status = \"\ud83e\udd47 Leader\"\n            elif position == 2:\n                status = \"\ud83e\udd48 Strong\"\n            elif position &lt;= total // 2:\n                status = \"\ud83d\udcca Middle\"\n            else:\n                status = \"\ud83d\udcc8 Opportunity\"\n\n            print(f\"{metric}: #{position}/{total} {status}\")\n\nasyncio.run(competitive_position(\"myaccount\", [\"comp1\", \"comp2\", \"comp3\"]))\n</code></pre>"},{"location":"api/analytics/competitors/#steal-competitor-strategy","title":"Steal Competitor Strategy","text":"<pre><code>from xeepy import Xeepy\n\nasync def analyze_strategy(competitor: str):\n    async with Xeepy() as x:\n        analysis = await x.analytics.analyze_competitor(competitor)\n        best = await x.analytics.competitor_best_content(competitor, limit=50)\n\n        print(f\"=== Strategy Breakdown: @{competitor} ===\")\n\n        # Content timing\n        print(f\"\\nPosting Schedule:\")\n        print(f\"  Frequency: {analysis.posts_per_week:.1f} posts/week\")\n        print(f\"  Best times: {analysis.best_times}\")\n\n        # Content types\n        print(f\"\\nContent Mix:\")\n        for content_type, pct in analysis.content_mix.items():\n            print(f\"  {content_type}: {pct:.0f}%\")\n\n        # Engagement drivers\n        print(f\"\\nHigh-engagement patterns:\")\n        for pattern in analysis.engagement_patterns[:5]:\n            print(f\"  - {pattern}\")\n\nasyncio.run(analyze_strategy(\"successful_competitor\"))\n</code></pre>"},{"location":"api/analytics/competitors/#see-also","title":"See Also","text":"<ul> <li>GrowthAnalytics - Growth analysis</li> <li>EngagementAnalytics - Engagement metrics</li> <li>ContentAnalytics - Content analysis</li> </ul>"},{"location":"api/analytics/content/","title":"ContentAnalytics","text":"<p>Analyze content performance and optimize content strategy.</p>"},{"location":"api/analytics/content/#import","title":"Import","text":"<pre><code>from xeepy.analytics.content import ContentAnalytics\n</code></pre>"},{"location":"api/analytics/content/#class-signature","title":"Class Signature","text":"<pre><code>class ContentAnalytics:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        storage: Optional[Storage] = None\n    )\n</code></pre>"},{"location":"api/analytics/content/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>storage</code> <code>Optional[Storage]</code> <code>None</code> Storage for historical data"},{"location":"api/analytics/content/#methods","title":"Methods","text":"Method Returns Description <code>analyze(username, period)</code> <code>ContentAnalysis</code> Analyze content performance <code>top_performing(username)</code> <code>List[Tweet]</code> Get best content <code>topic_analysis(username)</code> <code>TopicAnalysis</code> Analyze content topics <code>optimize(draft)</code> <code>OptimizationReport</code> Get optimization suggestions <code>a_b_test_results(tweet_ids)</code> <code>ABTestResults</code> Analyze A/B test"},{"location":"api/analytics/content/#analyze","title":"<code>analyze</code>","text":"<pre><code>async def analyze(\n    self,\n    username: Optional[str] = None,\n    period: str = \"30d\"\n) -&gt; ContentAnalysis\n</code></pre> <p>Comprehensive content analysis.</p>"},{"location":"api/analytics/content/#topic_analysis","title":"<code>topic_analysis</code>","text":"<pre><code>async def topic_analysis(\n    self,\n    username: Optional[str] = None,\n    limit: int = 200\n) -&gt; TopicAnalysis\n</code></pre> <p>Analyze content topics and their performance.</p>"},{"location":"api/analytics/content/#optimize","title":"<code>optimize</code>","text":"<pre><code>async def optimize(\n    self,\n    draft: str,\n    context: Optional[str] = None\n) -&gt; OptimizationReport\n</code></pre> <p>Get suggestions to optimize a draft tweet.</p>"},{"location":"api/analytics/content/#contentanalysis-object","title":"ContentAnalysis Object","text":"<pre><code>@dataclass\nclass ContentAnalysis:\n    username: str                    # Account analyzed\n    period: str                      # Analysis period\n    total_posts: int                 # Posts in period\n    avg_engagement: float            # Average engagement\n    by_type: Dict[str, TypeMetrics]  # Performance by type\n    by_length: Dict[str, float]      # Performance by length\n    by_time: Dict[int, float]        # Performance by hour\n    top_hashtags: List[str]          # Best performing hashtags\n    top_topics: List[str]            # Best performing topics\n</code></pre>"},{"location":"api/analytics/content/#topicanalysis-object","title":"TopicAnalysis Object","text":"<pre><code>@dataclass\nclass TopicAnalysis:\n    topics: List[TopicMetrics]       # Topics with metrics\n    trending_topics: List[str]       # Currently trending\n    declining_topics: List[str]      # Declining in performance\n    opportunities: List[str]         # Underexplored topics\n</code></pre>"},{"location":"api/analytics/content/#optimizationreport-object","title":"OptimizationReport Object","text":"<pre><code>@dataclass\nclass OptimizationReport:\n    original: str                    # Original draft\n    suggestions: List[str]           # Improvement suggestions\n    optimized_versions: List[str]    # Alternative versions\n    predicted_engagement: float      # Predicted performance\n    best_time_to_post: datetime      # Optimal posting time\n</code></pre>"},{"location":"api/analytics/content/#usage-examples","title":"Usage Examples","text":""},{"location":"api/analytics/content/#content-performance-analysis","title":"Content Performance Analysis","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        analysis = await x.analytics.content(\"username\", period=\"30d\")\n\n        print(\"=== Content Analysis ===\")\n        print(f\"Total posts: {analysis.total_posts}\")\n        print(f\"Avg engagement: {analysis.avg_engagement:.2f}%\")\n\n        print(f\"\\nPerformance by type:\")\n        for content_type, metrics in analysis.by_type.items():\n            print(f\"  {content_type}:\")\n            print(f\"    Posts: {metrics.count}\")\n            print(f\"    Avg engagement: {metrics.avg_engagement:.2f}%\")\n\n        print(f\"\\nBest performing length:\")\n        best_length = max(analysis.by_length, key=analysis.by_length.get)\n        print(f\"  {best_length}: {analysis.by_length[best_length]:.2f}x avg\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/content/#get-top-performing-content","title":"Get Top Performing Content","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        top = await x.analytics.top_content(\"username\", limit=10)\n\n        print(\"=== Top Performing Content ===\")\n        for i, tweet in enumerate(top, 1):\n            print(f\"\\n{i}. {tweet.text[:80]}...\")\n            print(f\"   Likes: {tweet.like_count:,} | RT: {tweet.retweet_count:,}\")\n            print(f\"   Type: {tweet.content_type}\")\n            print(f\"   Posted: {tweet.created_at}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/content/#topic-analysis","title":"Topic Analysis","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        topics = await x.analytics.topic_analysis(\"username\")\n\n        print(\"=== Topic Performance ===\")\n\n        print(\"\\nTop performing topics:\")\n        for topic in topics.topics[:5]:\n            print(f\"  {topic.name}: {topic.engagement_rate:.2f}% engagement\")\n\n        print(\"\\nTrending topics:\")\n        for topic in topics.trending_topics[:5]:\n            print(f\"  \u2197\ufe0f {topic}\")\n\n        print(\"\\nDeclining topics:\")\n        for topic in topics.declining_topics[:5]:\n            print(f\"  \u2198\ufe0f {topic}\")\n\n        print(\"\\nOpportunities (underexplored):\")\n        for topic in topics.opportunities[:5]:\n            print(f\"  \ud83d\udca1 {topic}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/content/#optimize-draft-tweet","title":"Optimize Draft Tweet","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        draft = \"Just shipped a new feature for our product!\"\n\n        report = await x.analytics.optimize_content(draft)\n\n        print(\"=== Content Optimization ===\")\n        print(f\"Original: {draft}\")\n\n        print(f\"\\nSuggestions:\")\n        for suggestion in report.suggestions:\n            print(f\"  \u2022 {suggestion}\")\n\n        print(f\"\\nOptimized versions:\")\n        for i, version in enumerate(report.optimized_versions, 1):\n            print(f\"  {i}. {version}\")\n\n        print(f\"\\nPredicted engagement: {report.predicted_engagement:.2f}%\")\n        print(f\"Best time to post: {report.best_time_to_post}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/content/#content-type-comparison","title":"Content Type Comparison","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        analysis = await x.analytics.content(\"username\")\n\n        print(\"=== Content Type Comparison ===\")\n\n        types_sorted = sorted(\n            analysis.by_type.items(),\n            key=lambda x: x[1].avg_engagement,\n            reverse=True\n        )\n\n        for content_type, metrics in types_sorted:\n            bar = \"\u2588\" * int(metrics.avg_engagement / 0.5)\n            print(f\"{content_type:15} {bar} {metrics.avg_engagement:.2f}%\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/content/#hashtag-performance","title":"Hashtag Performance","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        analysis = await x.analytics.content(\"username\")\n\n        print(\"=== Hashtag Performance ===\")\n\n        for hashtag in analysis.top_hashtags[:10]:\n            print(f\"  {hashtag.tag}: {hashtag.avg_engagement:.2f}% ({hashtag.uses} uses)\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/content/#export-content-report","title":"Export Content Report","text":"<pre><code>from xeepy import Xeepy\n\nasync def export_content_report(username: str):\n    async with Xeepy() as x:\n        analysis = await x.analytics.content(username, period=\"30d\")\n        top = await x.analytics.top_content(username, limit=20)\n\n        report = {\n            \"username\": username,\n            \"total_posts\": analysis.total_posts,\n            \"avg_engagement\": analysis.avg_engagement,\n            \"by_type\": {\n                k: {\"count\": v.count, \"engagement\": v.avg_engagement}\n                for k, v in analysis.by_type.items()\n            },\n            \"top_hashtags\": analysis.top_hashtags[:10],\n            \"top_posts\": [\n                {\n                    \"text\": t.text[:100],\n                    \"likes\": t.like_count,\n                    \"retweets\": t.retweet_count\n                }\n                for t in top\n            ]\n        }\n\n        x.export.to_json([report], f\"content_report_{username}.json\")\n\nasyncio.run(export_content_report(\"myaccount\"))\n</code></pre>"},{"location":"api/analytics/content/#content-calendar-optimization","title":"Content Calendar Optimization","text":"<pre><code>from xeepy import Xeepy\n\nasync def optimize_calendar(username: str):\n    async with Xeepy() as x:\n        analysis = await x.analytics.content(username)\n\n        print(\"=== Content Calendar Recommendations ===\")\n\n        # Best hours\n        print(\"\\nBest posting hours (UTC):\")\n        sorted_hours = sorted(analysis.by_time.items(), key=lambda x: -x[1])[:5]\n        for hour, score in sorted_hours:\n            print(f\"  {hour}:00 - {score:.2f}x avg engagement\")\n\n        # Content mix recommendations\n        print(\"\\nRecommended content mix:\")\n        total_engagement = sum(m.avg_engagement for m in analysis.by_type.values())\n        for content_type, metrics in analysis.by_type.items():\n            recommended_pct = (metrics.avg_engagement / total_engagement) * 100\n            print(f\"  {content_type}: {recommended_pct:.0f}%\")\n\nasyncio.run(optimize_calendar(\"username\"))\n</code></pre>"},{"location":"api/analytics/content/#ab-test-analysis","title":"A/B Test Analysis","text":"<pre><code>from xeepy import Xeepy\n\nasync def analyze_ab_test(tweet_a: str, tweet_b: str):\n    async with Xeepy() as x:\n        results = await x.analytics.ab_test_results([tweet_a, tweet_b])\n\n        print(\"=== A/B Test Results ===\")\n        print(f\"\\nVariant A: {results.variant_a.text[:50]}...\")\n        print(f\"  Engagement: {results.variant_a.engagement:.2f}%\")\n        print(f\"  Likes: {results.variant_a.likes:,}\")\n\n        print(f\"\\nVariant B: {results.variant_b.text[:50]}...\")\n        print(f\"  Engagement: {results.variant_b.engagement:.2f}%\")\n        print(f\"  Likes: {results.variant_b.likes:,}\")\n\n        winner = \"A\" if results.variant_a.engagement &gt; results.variant_b.engagement else \"B\"\n        lift = abs(results.variant_a.engagement - results.variant_b.engagement)\n        print(f\"\\n\ud83c\udfc6 Winner: Variant {winner} (+{lift:.2f}% engagement)\")\n\nasyncio.run(analyze_ab_test(\"tweet_id_a\", \"tweet_id_b\"))\n</code></pre>"},{"location":"api/analytics/content/#see-also","title":"See Also","text":"<ul> <li>EngagementAnalytics - Engagement analysis</li> <li>CompetitorAnalytics - Competitor analysis</li> <li>ContentGenerator - AI content generation</li> </ul>"},{"location":"api/analytics/engagement/","title":"EngagementAnalytics","text":"<p>Analyze tweet engagement metrics and performance.</p>"},{"location":"api/analytics/engagement/#import","title":"Import","text":"<pre><code>from xeepy.analytics.engagement import EngagementAnalytics\n</code></pre>"},{"location":"api/analytics/engagement/#class-signature","title":"Class Signature","text":"<pre><code>class EngagementAnalytics:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        storage: Optional[Storage] = None\n    )\n</code></pre>"},{"location":"api/analytics/engagement/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>storage</code> <code>Optional[Storage]</code> <code>None</code> Storage for historical data"},{"location":"api/analytics/engagement/#methods","title":"Methods","text":"Method Returns Description <code>analyze(username, period)</code> <code>EngagementAnalysis</code> Analyze engagement <code>tweet_performance(tweet_ids)</code> <code>List[TweetPerformance]</code> Analyze specific tweets <code>best_posting_times(username)</code> <code>TimingAnalysis</code> Find optimal times <code>content_analysis(username)</code> <code>ContentAnalysis</code> Analyze content types <code>rate(username)</code> <code>float</code> Calculate engagement rate"},{"location":"api/analytics/engagement/#analyze","title":"<code>analyze</code>","text":"<pre><code>async def analyze(\n    self,\n    username: Optional[str] = None,\n    period: str = \"30d\"\n) -&gt; EngagementAnalysis\n</code></pre> <p>Comprehensive engagement analysis.</p> <p>Parameters: - <code>username</code>: Account to analyze - <code>period</code>: Analysis period</p>"},{"location":"api/analytics/engagement/#best_posting_times","title":"<code>best_posting_times</code>","text":"<pre><code>async def best_posting_times(\n    self,\n    username: Optional[str] = None\n) -&gt; TimingAnalysis\n</code></pre> <p>Find optimal posting times based on engagement data.</p>"},{"location":"api/analytics/engagement/#content_analysis","title":"<code>content_analysis</code>","text":"<pre><code>async def content_analysis(\n    self,\n    username: Optional[str] = None,\n    limit: int = 100\n) -&gt; ContentAnalysis\n</code></pre> <p>Analyze which content types perform best.</p>"},{"location":"api/analytics/engagement/#engagementanalysis-object","title":"EngagementAnalysis Object","text":"<pre><code>@dataclass\nclass EngagementAnalysis:\n    username: str                    # Account analyzed\n    period: str                      # Analysis period\n    total_tweets: int                # Tweets in period\n    total_likes: int                 # Total likes received\n    total_retweets: int              # Total retweets\n    total_replies: int               # Total replies\n    total_views: int                 # Total views\n    engagement_rate: float           # Overall rate (%)\n    avg_likes: float                 # Per tweet average\n    avg_retweets: float              # Per tweet average\n    avg_replies: float               # Per tweet average\n    top_tweets: List[Tweet]          # Best performing\n</code></pre>"},{"location":"api/analytics/engagement/#timinganalysis-object","title":"TimingAnalysis Object","text":"<pre><code>@dataclass\nclass TimingAnalysis:\n    best_hours: List[int]            # Best hours (UTC)\n    best_days: List[str]             # Best days of week\n    worst_hours: List[int]           # Worst hours\n    heatmap: Dict[str, Dict[int, float]]  # Day/hour engagement\n</code></pre>"},{"location":"api/analytics/engagement/#contentanalysis-object","title":"ContentAnalysis Object","text":"<pre><code>@dataclass\nclass ContentAnalysis:\n    by_type: Dict[str, float]        # Engagement by content type\n    by_length: Dict[str, float]      # Engagement by length\n    hashtag_performance: Dict[str, float]  # Hashtag effectiveness\n    media_vs_text: Dict[str, float]  # Media impact\n    top_topics: List[str]            # Best performing topics\n</code></pre>"},{"location":"api/analytics/engagement/#usage-examples","title":"Usage Examples","text":""},{"location":"api/analytics/engagement/#basic-engagement-analysis","title":"Basic Engagement Analysis","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        analysis = await x.analytics.engagement(\"username\", period=\"30d\")\n\n        print(f\"=== 30-Day Engagement Analysis ===\")\n        print(f\"Tweets: {analysis.total_tweets}\")\n        print(f\"Total likes: {analysis.total_likes:,}\")\n        print(f\"Total retweets: {analysis.total_retweets:,}\")\n        print(f\"Total replies: {analysis.total_replies:,}\")\n        print(f\"Engagement rate: {analysis.engagement_rate:.2f}%\")\n        print(f\"\\nAverages per tweet:\")\n        print(f\"  Likes: {analysis.avg_likes:.1f}\")\n        print(f\"  Retweets: {analysis.avg_retweets:.1f}\")\n        print(f\"  Replies: {analysis.avg_replies:.1f}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/engagement/#find-best-posting-times","title":"Find Best Posting Times","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        timing = await x.analytics.best_posting_times(\"username\")\n\n        print(f\"=== Optimal Posting Times ===\")\n        print(f\"Best hours (UTC): {timing.best_hours}\")\n        print(f\"Best days: {timing.best_days}\")\n        print(f\"Worst hours (UTC): {timing.worst_hours}\")\n\n        print(f\"\\nEngagement heatmap:\")\n        for day, hours in timing.heatmap.items():\n            best_hour = max(hours, key=hours.get)\n            print(f\"  {day}: Best at {best_hour}:00 ({hours[best_hour]:.1f}x)\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/engagement/#content-type-analysis","title":"Content Type Analysis","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        content = await x.analytics.content_analysis(\"username\")\n\n        print(f\"=== Content Performance ===\")\n        print(f\"\\nBy type:\")\n        for content_type, score in sorted(content.by_type.items(), key=lambda x: -x[1]):\n            print(f\"  {content_type}: {score:.1f}x avg engagement\")\n\n        print(f\"\\nMedia vs text:\")\n        for category, score in content.media_vs_text.items():\n            print(f\"  {category}: {score:.1f}x\")\n\n        print(f\"\\nTop topics:\")\n        for topic in content.top_topics[:5]:\n            print(f\"  - {topic}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/engagement/#analyze-specific-tweets","title":"Analyze Specific Tweets","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        tweet_ids = [\"123456789\", \"987654321\"]\n\n        performance = await x.analytics.tweet_performance(tweet_ids)\n\n        for tweet in performance:\n            print(f\"\\nTweet ID: {tweet.id}\")\n            print(f\"  Likes: {tweet.like_count:,}\")\n            print(f\"  Retweets: {tweet.retweet_count:,}\")\n            print(f\"  Replies: {tweet.reply_count:,}\")\n            print(f\"  Views: {tweet.view_count:,}\")\n            print(f\"  Engagement rate: {tweet.engagement_rate:.2f}%\")\n            print(f\"  Performance: {tweet.performance_score:.1f}x average\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/engagement/#compare-hashtag-performance","title":"Compare Hashtag Performance","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        content = await x.analytics.content_analysis(\"username\")\n\n        print(f\"=== Hashtag Performance ===\")\n        sorted_hashtags = sorted(\n            content.hashtag_performance.items(),\n            key=lambda x: -x[1]\n        )\n\n        for hashtag, score in sorted_hashtags[:10]:\n            indicator = \"\ud83d\udd25\" if score &gt; 1.5 else \"\u2713\" if score &gt; 1.0 else \"\u274c\"\n            print(f\"  {indicator} {hashtag}: {score:.2f}x avg\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/engagement/#calculate-engagement-rate","title":"Calculate Engagement Rate","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        rate = await x.analytics.engagement_rate(\"username\")\n\n        print(f\"Engagement rate: {rate:.2f}%\")\n\n        # Industry benchmarks\n        if rate &gt; 6:\n            print(\"\ud83c\udfc6 Excellent! Top 1% engagement\")\n        elif rate &gt; 3:\n            print(\"\ud83c\udf1f Great! Above average\")\n        elif rate &gt; 1:\n            print(\"\u2713 Average engagement\")\n        else:\n            print(\"\ud83d\udcc8 Room for improvement\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/engagement/#export-engagement-report","title":"Export Engagement Report","text":"<pre><code>from xeepy import Xeepy\n\nasync def generate_engagement_report(username: str):\n    async with Xeepy() as x:\n        analysis = await x.analytics.engagement(username, period=\"30d\")\n        timing = await x.analytics.best_posting_times(username)\n        content = await x.analytics.content_analysis(username)\n\n        report = {\n            \"username\": username,\n            \"metrics\": {\n                \"engagement_rate\": analysis.engagement_rate,\n                \"total_engagement\": analysis.total_likes + analysis.total_retweets + analysis.total_replies,\n                \"avg_likes\": analysis.avg_likes,\n                \"avg_retweets\": analysis.avg_retweets\n            },\n            \"timing\": {\n                \"best_hours\": timing.best_hours,\n                \"best_days\": timing.best_days\n            },\n            \"content\": {\n                \"best_type\": max(content.by_type, key=content.by_type.get),\n                \"top_topics\": content.top_topics[:5]\n            },\n            \"top_tweets\": [\n                {\"id\": t.id, \"likes\": t.like_count, \"text\": t.text[:100]}\n                for t in analysis.top_tweets[:5]\n            ]\n        }\n\n        x.export.to_json([report], f\"engagement_{username}.json\")\n\nasyncio.run(generate_engagement_report(\"myaccount\"))\n</code></pre>"},{"location":"api/analytics/engagement/#engagement-trend-over-time","title":"Engagement Trend Over Time","text":"<pre><code>from xeepy import Xeepy\n\nasync def engagement_trend(username: str):\n    async with Xeepy() as x:\n        week = await x.analytics.engagement(username, period=\"7d\")\n        month = await x.analytics.engagement(username, period=\"30d\")\n        quarter = await x.analytics.engagement(username, period=\"90d\")\n\n        print(f\"=== Engagement Trend ===\")\n        print(f\"Last 7 days:  {week.engagement_rate:.2f}%\")\n        print(f\"Last 30 days: {month.engagement_rate:.2f}%\")\n        print(f\"Last 90 days: {quarter.engagement_rate:.2f}%\")\n\n        if week.engagement_rate &gt; month.engagement_rate:\n            print(\"\ud83d\udcc8 Engagement is improving!\")\n        elif week.engagement_rate &lt; month.engagement_rate:\n            print(\"\ud83d\udcc9 Engagement is declining\")\n        else:\n            print(\"\u27a1\ufe0f Engagement is stable\")\n\nasyncio.run(engagement_trend(\"username\"))\n</code></pre>"},{"location":"api/analytics/engagement/#see-also","title":"See Also","text":"<ul> <li>GrowthAnalytics - Growth metrics</li> <li>ContentAnalytics - Content analysis</li> <li>TweetsScraper - Tweet data</li> </ul>"},{"location":"api/analytics/growth/","title":"GrowthAnalytics","text":"<p>Analyze account growth trends and patterns.</p>"},{"location":"api/analytics/growth/#import","title":"Import","text":"<pre><code>from xeepy.analytics.growth import GrowthAnalytics\n</code></pre>"},{"location":"api/analytics/growth/#class-signature","title":"Class Signature","text":"<pre><code>class GrowthAnalytics:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        storage: Optional[Storage] = None\n    )\n</code></pre>"},{"location":"api/analytics/growth/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>storage</code> <code>Optional[Storage]</code> <code>None</code> Storage for historical data"},{"location":"api/analytics/growth/#methods","title":"Methods","text":"Method Returns Description <code>analyze(username, period)</code> <code>GrowthAnalysis</code> Analyze growth metrics <code>trends(username)</code> <code>TrendAnalysis</code> Identify growth trends <code>velocity(username)</code> <code>VelocityMetrics</code> Calculate growth velocity <code>benchmark(username, peers)</code> <code>BenchmarkReport</code> Compare to peers <code>cohort_analysis(username)</code> <code>CohortAnalysis</code> Analyze follower cohorts"},{"location":"api/analytics/growth/#analyze","title":"<code>analyze</code>","text":"<pre><code>async def analyze(\n    self,\n    username: Optional[str] = None,\n    period: str = \"30d\"\n) -&gt; GrowthAnalysis\n</code></pre> <p>Comprehensive growth analysis.</p> <p>Parameters: - <code>username</code>: Account to analyze - <code>period</code>: Analysis period (<code>7d</code>, <code>30d</code>, <code>90d</code>, <code>1y</code>, <code>all</code>)</p>"},{"location":"api/analytics/growth/#trends","title":"<code>trends</code>","text":"<pre><code>async def trends(\n    self,\n    username: Optional[str] = None\n) -&gt; TrendAnalysis\n</code></pre> <p>Identify growth trends and patterns.</p>"},{"location":"api/analytics/growth/#velocity","title":"<code>velocity</code>","text":"<pre><code>async def velocity(\n    self,\n    username: Optional[str] = None\n) -&gt; VelocityMetrics\n</code></pre> <p>Calculate growth velocity and acceleration.</p>"},{"location":"api/analytics/growth/#growthanalysis-object","title":"GrowthAnalysis Object","text":"<pre><code>@dataclass\nclass GrowthAnalysis:\n    username: str                    # Account analyzed\n    period: str                      # Analysis period\n    followers_start: int             # Starting followers\n    followers_end: int               # Ending followers\n    net_growth: int                  # Net change\n    growth_rate: float               # Growth percentage\n    avg_daily_growth: float          # Average daily\n    max_daily_growth: int            # Best day\n    min_daily_growth: int            # Worst day\n    consistency_score: float         # 0-1 consistency\n    growth_trend: str                # accelerating, steady, declining\n</code></pre>"},{"location":"api/analytics/growth/#trendanalysis-object","title":"TrendAnalysis Object","text":"<pre><code>@dataclass\nclass TrendAnalysis:\n    trend_direction: str             # up, down, stable\n    trend_strength: float            # 0-1 strength\n    seasonality: Dict[str, float]    # Day/hour patterns\n    anomalies: List[datetime]        # Unusual days\n    prediction_30d: int              # 30-day forecast\n</code></pre>"},{"location":"api/analytics/growth/#usage-examples","title":"Usage Examples","text":""},{"location":"api/analytics/growth/#basic-growth-analysis","title":"Basic Growth Analysis","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        analysis = await x.analytics.growth(\"username\", period=\"30d\")\n\n        print(f\"=== 30-Day Growth Analysis ===\")\n        print(f\"Starting followers: {analysis.followers_start:,}\")\n        print(f\"Ending followers: {analysis.followers_end:,}\")\n        print(f\"Net growth: {analysis.net_growth:+,}\")\n        print(f\"Growth rate: {analysis.growth_rate:+.2f}%\")\n        print(f\"Avg daily: {analysis.avg_daily_growth:+.1f}\")\n        print(f\"Consistency: {analysis.consistency_score:.0%}\")\n        print(f\"Trend: {analysis.growth_trend}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/growth/#identify-trends","title":"Identify Trends","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        trends = await x.analytics.growth_trends(\"username\")\n\n        print(f\"=== Trend Analysis ===\")\n        print(f\"Direction: {trends.trend_direction}\")\n        print(f\"Strength: {trends.trend_strength:.0%}\")\n        print(f\"30-day prediction: {trends.prediction_30d:,}\")\n\n        print(f\"\\nBest days:\")\n        for day, score in sorted(trends.seasonality.items(), key=lambda x: -x[1])[:3]:\n            print(f\"  {day}: {score:.2f}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/growth/#calculate-growth-velocity","title":"Calculate Growth Velocity","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        velocity = await x.analytics.growth_velocity(\"username\")\n\n        print(f\"=== Growth Velocity ===\")\n        print(f\"Current velocity: {velocity.current:+.1f}/day\")\n        print(f\"Acceleration: {velocity.acceleration:+.2f}\")\n        print(f\"7-day avg: {velocity.avg_7d:+.1f}/day\")\n        print(f\"30-day avg: {velocity.avg_30d:+.1f}/day\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/growth/#benchmark-against-peers","title":"Benchmark Against Peers","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        benchmark = await x.analytics.growth_benchmark(\n            \"myaccount\",\n            peers=[\"peer1\", \"peer2\", \"peer3\"]\n        )\n\n        print(f\"=== Peer Benchmark ===\")\n        print(f\"Your growth rate: {benchmark.your_rate:+.2f}%\")\n        print(f\"Peer average: {benchmark.peer_avg:+.2f}%\")\n        print(f\"Percentile rank: {benchmark.percentile:.0f}%\")\n\n        print(f\"\\nRankings:\")\n        for i, account in enumerate(benchmark.rankings, 1):\n            print(f\"  {i}. @{account.username}: {account.growth_rate:+.2f}%\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/analytics/growth/#compare-time-periods","title":"Compare Time Periods","text":"<pre><code>from xeepy import Xeepy\n\nasync def compare_periods(username: str):\n    async with Xeepy() as x:\n        week = await x.analytics.growth(username, period=\"7d\")\n        month = await x.analytics.growth(username, period=\"30d\")\n        quarter = await x.analytics.growth(username, period=\"90d\")\n\n        print(f\"=== Period Comparison ===\")\n        print(f\"Last 7 days:  {week.growth_rate:+.2f}% ({week.avg_daily_growth:+.1f}/day)\")\n        print(f\"Last 30 days: {month.growth_rate:+.2f}% ({month.avg_daily_growth:+.1f}/day)\")\n        print(f\"Last 90 days: {quarter.growth_rate:+.2f}% ({quarter.avg_daily_growth:+.1f}/day)\")\n\nasyncio.run(compare_periods(\"username\"))\n</code></pre>"},{"location":"api/analytics/growth/#export-growth-report","title":"Export Growth Report","text":"<pre><code>from xeepy import Xeepy\nfrom datetime import datetime\n\nasync def generate_report(username: str):\n    async with Xeepy() as x:\n        analysis = await x.analytics.growth(username, period=\"30d\")\n        trends = await x.analytics.growth_trends(username)\n\n        report = {\n            \"username\": username,\n            \"generated_at\": datetime.now().isoformat(),\n            \"metrics\": {\n                \"followers_start\": analysis.followers_start,\n                \"followers_end\": analysis.followers_end,\n                \"net_growth\": analysis.net_growth,\n                \"growth_rate\": analysis.growth_rate,\n                \"avg_daily\": analysis.avg_daily_growth,\n                \"consistency\": analysis.consistency_score\n            },\n            \"trends\": {\n                \"direction\": trends.trend_direction,\n                \"strength\": trends.trend_strength,\n                \"prediction_30d\": trends.prediction_30d\n            }\n        }\n\n        x.export.to_json([report], f\"growth_report_{username}.json\")\n        print(f\"Report exported for @{username}\")\n\nasyncio.run(generate_report(\"myaccount\"))\n</code></pre>"},{"location":"api/analytics/growth/#detect-growth-anomalies","title":"Detect Growth Anomalies","text":"<pre><code>from xeepy import Xeepy\n\nasync def detect_anomalies(username: str):\n    async with Xeepy() as x:\n        trends = await x.analytics.growth_trends(username)\n\n        if trends.anomalies:\n            print(f\"Unusual growth days detected:\")\n            for date in trends.anomalies:\n                print(f\"  - {date.strftime('%Y-%m-%d')}\")\n        else:\n            print(\"No anomalies detected\")\n\nasyncio.run(detect_anomalies(\"username\"))\n</code></pre>"},{"location":"api/analytics/growth/#see-also","title":"See Also","text":"<ul> <li>EngagementAnalytics - Engagement metrics</li> <li>AudienceAnalytics - Audience analysis</li> <li>GrowthMonitor - Growth monitoring</li> </ul>"},{"location":"api/core/auth/","title":"AuthManager","text":"<p>Handles authentication, session management, and cookie operations for X/Twitter.</p>"},{"location":"api/core/auth/#import","title":"Import","text":"<pre><code>from xeepy.core.auth import AuthManager\n</code></pre>"},{"location":"api/core/auth/#class-signature","title":"Class Signature","text":"<pre><code>class AuthManager:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        cookies_path: Optional[str] = None\n    )\n</code></pre>"},{"location":"api/core/auth/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>cookies_path</code> <code>Optional[str]</code> <code>None</code> Default path for cookie storage"},{"location":"api/core/auth/#methods","title":"Methods","text":"Method Returns Description <code>login()</code> <code>bool</code> Interactive login via browser <code>login_with_credentials(username, password)</code> <code>bool</code> Automated credential login <code>save_cookies(path)</code> <code>None</code> Save session cookies to file <code>load_cookies(path)</code> <code>bool</code> Load cookies from file <code>resume_session(cookies)</code> <code>bool</code> Resume with cookie dict <code>import_cookies_from_browser(browser)</code> <code>bool</code> Import from system browser <code>is_logged_in()</code> <code>bool</code> Check login status <code>get_auth_tokens()</code> <code>Dict</code> Get ct0 and auth_token <code>logout()</code> <code>None</code> Clear session and logout"},{"location":"api/core/auth/#login","title":"<code>login</code>","text":"<pre><code>async def login(self, timeout: int = 120000) -&gt; bool\n</code></pre> <p>Opens browser for manual login. Returns <code>True</code> when login is detected.</p> <p>Parameters: - <code>timeout</code>: Maximum wait time for login completion in milliseconds</p>"},{"location":"api/core/auth/#login_with_credentials","title":"<code>login_with_credentials</code>","text":"<pre><code>async def login_with_credentials(\n    self,\n    username: str,\n    password: str,\n    email: Optional[str] = None,\n    totp_secret: Optional[str] = None\n) -&gt; bool\n</code></pre> <p>Automated login with credentials. Supports 2FA via TOTP.</p> <p>Parameters: - <code>username</code>: X/Twitter username - <code>password</code>: Account password - <code>email</code>: Email for verification challenges - <code>totp_secret</code>: TOTP secret for 2FA</p>"},{"location":"api/core/auth/#save_cookies","title":"<code>save_cookies</code>","text":"<pre><code>async def save_cookies(self, path: str) -&gt; None\n</code></pre> <p>Save current session cookies to a JSON file.</p>"},{"location":"api/core/auth/#load_cookies","title":"<code>load_cookies</code>","text":"<pre><code>async def load_cookies(self, path: str) -&gt; bool\n</code></pre> <p>Load cookies from a JSON file and restore the session.</p>"},{"location":"api/core/auth/#resume_session","title":"<code>resume_session</code>","text":"<pre><code>async def resume_session(self, cookies: Dict[str, str]) -&gt; bool\n</code></pre> <p>Resume a session using a cookies dictionary.</p> <p>Parameters: - <code>cookies</code>: Dict with <code>ct0</code> and <code>auth_token</code> keys</p>"},{"location":"api/core/auth/#import_cookies_from_browser","title":"<code>import_cookies_from_browser</code>","text":"<pre><code>async def import_cookies_from_browser(\n    self,\n    browser: str = \"chrome\"\n) -&gt; bool\n</code></pre> <p>Import cookies from an installed browser. Requires <code>browser_cookie3</code>.</p> <p>Parameters: - <code>browser</code>: Browser name (<code>chrome</code>, <code>firefox</code>, <code>edge</code>, <code>safari</code>)</p>"},{"location":"api/core/auth/#get_auth_tokens","title":"<code>get_auth_tokens</code>","text":"<pre><code>def get_auth_tokens(self) -&gt; Dict[str, str]\n</code></pre> <p>Get authentication tokens for GraphQL API usage.</p> <p>Returns: <pre><code>{\n    \"ct0\": \"csrf_token_value\",\n    \"auth_token\": \"auth_token_value\"\n}\n</code></pre></p>"},{"location":"api/core/auth/#is_logged_in","title":"<code>is_logged_in</code>","text":"<pre><code>async def is_logged_in(self) -&gt; bool\n</code></pre> <p>Check if the current session is authenticated.</p>"},{"location":"api/core/auth/#usage-examples","title":"Usage Examples","text":""},{"location":"api/core/auth/#manual-login","title":"Manual Login","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy(headless=False) as x:\n        # Opens browser for manual login\n        success = await x.auth.login()\n\n        if success:\n            # Save session for later\n            await x.auth.save_cookies(\"session.json\")\n            print(\"Login successful and saved!\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/core/auth/#resume-saved-session","title":"Resume Saved Session","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Load previously saved session\n        if await x.auth.load_cookies(\"session.json\"):\n            # Verify session is still valid\n            if await x.auth.is_logged_in():\n                print(\"Session restored!\")\n            else:\n                print(\"Session expired, need to re-login\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/core/auth/#import-from-system-browser","title":"Import from System Browser","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Import cookies from Chrome\n        success = await x.auth.import_cookies_from_browser(\"chrome\")\n\n        if success:\n            print(\"Cookies imported from Chrome!\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/core/auth/#get-tokens-for-graphql","title":"Get Tokens for GraphQL","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.api.graphql import GraphQLClient\n\nasync def main():\n    async with Xeepy() as x:\n        await x.auth.load_cookies(\"session.json\")\n\n        # Get tokens for direct GraphQL access\n        tokens = x.auth.get_auth_tokens()\n\n        gql = GraphQLClient(cookies=tokens)\n        user = await gql.get_user(\"username\")\n        await gql.close()\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/core/auth/#see-also","title":"See Also","text":"<ul> <li>Xeepy - Main entry point</li> <li>GraphQL Client - Direct API access</li> </ul>"},{"location":"api/core/browser/","title":"BrowserManager","text":"<p>Manages Playwright browser instances, pages, and browser contexts for X/Twitter automation.</p>"},{"location":"api/core/browser/#import","title":"Import","text":"<pre><code>from xeepy.core.browser import BrowserManager\n</code></pre>"},{"location":"api/core/browser/#class-signature","title":"Class Signature","text":"<pre><code>class BrowserManager:\n    def __init__(\n        self,\n        headless: bool = True,\n        proxy: Optional[str] = None,\n        user_agent: Optional[str] = None,\n        viewport: Optional[Dict[str, int]] = None,\n        timeout: int = 30000,\n        slow_mo: int = 0,\n        browser_type: str = \"chromium\"\n    )\n</code></pre>"},{"location":"api/core/browser/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>headless</code> <code>bool</code> <code>True</code> Run browser without visible UI <code>proxy</code> <code>Optional[str]</code> <code>None</code> Proxy server URL <code>user_agent</code> <code>Optional[str]</code> <code>None</code> Custom user agent string <code>viewport</code> <code>Optional[Dict[str, int]]</code> <code>None</code> Browser viewport size <code>{\"width\": 1920, \"height\": 1080}</code> <code>timeout</code> <code>int</code> <code>30000</code> Default timeout in milliseconds <code>slow_mo</code> <code>int</code> <code>0</code> Slow down Playwright operations <code>browser_type</code> <code>str</code> <code>\"chromium\"</code> Browser engine: <code>chromium</code>, <code>firefox</code>, <code>webkit</code>"},{"location":"api/core/browser/#methods","title":"Methods","text":"Method Returns Description <code>start()</code> <code>None</code> Initialize and launch browser <code>stop()</code> <code>None</code> Close browser and cleanup <code>new_page()</code> <code>Page</code> Create a new browser page <code>close_page(page)</code> <code>None</code> Close a specific page <code>goto(url, page)</code> <code>Response</code> Navigate to URL <code>wait_for_selector(selector, page)</code> <code>ElementHandle</code> Wait for element <code>screenshot(path, page)</code> <code>bytes</code> Take screenshot <code>get_cookies()</code> <code>List[Dict]</code> Get all cookies <code>set_cookies(cookies)</code> <code>None</code> Set cookies <code>clear_cookies()</code> <code>None</code> Clear all cookies"},{"location":"api/core/browser/#start","title":"<code>start</code>","text":"<pre><code>async def start(self) -&gt; None\n</code></pre> <p>Initialize Playwright and launch the browser instance.</p>"},{"location":"api/core/browser/#stop","title":"<code>stop</code>","text":"<pre><code>async def stop(self) -&gt; None\n</code></pre> <p>Close the browser and cleanup all resources.</p>"},{"location":"api/core/browser/#new_page","title":"<code>new_page</code>","text":"<pre><code>async def new_page(self, context: Optional[BrowserContext] = None) -&gt; Page\n</code></pre> <p>Create a new browser page, optionally in a specific context.</p>"},{"location":"api/core/browser/#goto","title":"<code>goto</code>","text":"<pre><code>async def goto(\n    self,\n    url: str,\n    page: Optional[Page] = None,\n    wait_until: str = \"domcontentloaded\"\n) -&gt; Response\n</code></pre> <p>Navigate to a URL and wait for the page to load.</p> <p>Parameters: - <code>url</code>: Target URL - <code>page</code>: Page instance (uses default if None) - <code>wait_until</code>: Load state to wait for (<code>domcontentloaded</code>, <code>load</code>, <code>networkidle</code>)</p>"},{"location":"api/core/browser/#wait_for_selector","title":"<code>wait_for_selector</code>","text":"<pre><code>async def wait_for_selector(\n    self,\n    selector: str,\n    page: Optional[Page] = None,\n    timeout: Optional[int] = None,\n    state: str = \"visible\"\n) -&gt; ElementHandle\n</code></pre> <p>Wait for an element matching the selector to appear.</p>"},{"location":"api/core/browser/#screenshot","title":"<code>screenshot</code>","text":"<pre><code>async def screenshot(\n    self,\n    path: Optional[str] = None,\n    page: Optional[Page] = None,\n    full_page: bool = False\n) -&gt; bytes\n</code></pre> <p>Capture a screenshot of the current page.</p>"},{"location":"api/core/browser/#usage-examples","title":"Usage Examples","text":""},{"location":"api/core/browser/#basic-browser-management","title":"Basic Browser Management","text":"<pre><code>from xeepy.core.browser import BrowserManager\n\nasync def main():\n    browser = BrowserManager(headless=False)\n    await browser.start()\n\n    try:\n        page = await browser.new_page()\n        await browser.goto(\"https://x.com\", page)\n        await browser.screenshot(\"screenshot.png\", page)\n    finally:\n        await browser.stop()\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/core/browser/#with-proxy-configuration","title":"With Proxy Configuration","text":"<pre><code>from xeepy.core.browser import BrowserManager\n\nasync def main():\n    browser = BrowserManager(\n        headless=True,\n        proxy=\"http://proxy.example.com:8080\",\n        viewport={\"width\": 1920, \"height\": 1080}\n    )\n    await browser.start()\n\n    cookies = await browser.get_cookies()\n    print(f\"Cookies: {len(cookies)}\")\n\n    await browser.stop()\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/core/browser/#see-also","title":"See Also","text":"<ul> <li>Xeepy - Main entry point</li> <li>AuthManager - Authentication handling</li> </ul>"},{"location":"api/core/config/","title":"Config","text":"<p>Configuration management for Xeepy, handling settings, defaults, and environment variables.</p>"},{"location":"api/core/config/#import","title":"Import","text":"<pre><code>from xeepy.core.config import Config\n</code></pre>"},{"location":"api/core/config/#class-signature","title":"Class Signature","text":"<pre><code>class Config:\n    def __init__(\n        self,\n        config_path: Optional[str] = None,\n        env_prefix: str = \"XEEPY_\"\n    )\n</code></pre>"},{"location":"api/core/config/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>config_path</code> <code>Optional[str]</code> <code>None</code> Path to YAML/JSON config file <code>env_prefix</code> <code>str</code> <code>\"XEEPY_\"</code> Prefix for environment variables"},{"location":"api/core/config/#configuration-options","title":"Configuration Options","text":"Option Type Default Description <code>browser.headless</code> <code>bool</code> <code>True</code> Run browser in headless mode <code>browser.timeout</code> <code>int</code> <code>30000</code> Default timeout (ms) <code>browser.slow_mo</code> <code>int</code> <code>0</code> Operation delay (ms) <code>browser.type</code> <code>str</code> <code>\"chromium\"</code> Browser engine <code>proxy.url</code> <code>str</code> <code>None</code> Proxy server URL <code>proxy.username</code> <code>str</code> <code>None</code> Proxy username <code>proxy.password</code> <code>str</code> <code>None</code> Proxy password <code>rate_limit.requests_per_minute</code> <code>int</code> <code>30</code> Rate limit per minute <code>rate_limit.min_delay</code> <code>float</code> <code>1.0</code> Min delay between requests <code>auth.cookies_path</code> <code>str</code> <code>None</code> Default cookies file path <code>storage.database_path</code> <code>str</code> <code>\"xeepy.db\"</code> SQLite database path <code>ai.provider</code> <code>str</code> <code>None</code> AI provider name <code>ai.api_key</code> <code>str</code> <code>None</code> AI provider API key"},{"location":"api/core/config/#methods","title":"Methods","text":"Method Returns Description <code>load(path)</code> <code>None</code> Load config from file <code>save(path)</code> <code>None</code> Save config to file <code>get(key, default)</code> <code>Any</code> Get configuration value <code>set(key, value)</code> <code>None</code> Set configuration value <code>from_env()</code> <code>Config</code> Load config from environment <code>to_dict()</code> <code>Dict</code> Export as dictionary <code>validate()</code> <code>bool</code> Validate configuration"},{"location":"api/core/config/#load","title":"<code>load</code>","text":"<pre><code>def load(self, path: str) -&gt; None\n</code></pre> <p>Load configuration from a YAML or JSON file.</p>"},{"location":"api/core/config/#get","title":"<code>get</code>","text":"<pre><code>def get(self, key: str, default: Any = None) -&gt; Any\n</code></pre> <p>Get a configuration value using dot notation.</p> <p>Parameters: - <code>key</code>: Configuration key (e.g., <code>browser.headless</code>) - <code>default</code>: Default value if key not found</p>"},{"location":"api/core/config/#set","title":"<code>set</code>","text":"<pre><code>def set(self, key: str, value: Any) -&gt; None\n</code></pre> <p>Set a configuration value using dot notation.</p>"},{"location":"api/core/config/#from_env","title":"<code>from_env</code>","text":"<pre><code>@classmethod\ndef from_env(cls, prefix: str = \"XEEPY_\") -&gt; \"Config\"\n</code></pre> <p>Create configuration from environment variables.</p> <p>Environment Variable Mapping: - <code>XEEPY_BROWSER_HEADLESS</code> \u2192 <code>browser.headless</code> - <code>XEEPY_PROXY_URL</code> \u2192 <code>proxy.url</code> - <code>XEEPY_AI_PROVIDER</code> \u2192 <code>ai.provider</code></p>"},{"location":"api/core/config/#configuration-file-format","title":"Configuration File Format","text":""},{"location":"api/core/config/#yaml-format","title":"YAML Format","text":"<pre><code># xeepy.yaml\nbrowser:\n  headless: true\n  timeout: 30000\n  slow_mo: 0\n  type: chromium\n\nproxy:\n  url: http://proxy.example.com:8080\n  username: user\n  password: pass\n\nrate_limit:\n  requests_per_minute: 30\n  requests_per_hour: 500\n  min_delay: 1.0\n  max_delay: 5.0\n\nauth:\n  cookies_path: ~/.xeepy/cookies.json\n\nstorage:\n  database_path: ~/.xeepy/data.db\n\nai:\n  provider: openai\n  api_key: ${OPENAI_API_KEY}\n  model: gpt-4\n\nnotifications:\n  discord_webhook: https://discord.com/api/webhooks/...\n  telegram_bot_token: ${TELEGRAM_BOT_TOKEN}\n  telegram_chat_id: \"123456789\"\n</code></pre>"},{"location":"api/core/config/#json-format","title":"JSON Format","text":"<pre><code>{\n  \"browser\": {\n    \"headless\": true,\n    \"timeout\": 30000\n  },\n  \"rate_limit\": {\n    \"requests_per_minute\": 30\n  }\n}\n</code></pre>"},{"location":"api/core/config/#usage-examples","title":"Usage Examples","text":""},{"location":"api/core/config/#load-from-file","title":"Load from File","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.core.config import Config\n\nconfig = Config(\"xeepy.yaml\")\n\nasync def main():\n    async with Xeepy(config_path=\"xeepy.yaml\") as x:\n        # Uses settings from config file\n        await x.scrape.replies(\"https://x.com/user/status/123\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/core/config/#environment-variables","title":"Environment Variables","text":"<pre><code>export XEEPY_BROWSER_HEADLESS=false\nexport XEEPY_PROXY_URL=http://proxy:8080\nexport XEEPY_AI_PROVIDER=openai\nexport XEEPY_AI_API_KEY=sk-...\n</code></pre> <pre><code>from xeepy.core.config import Config\n\nconfig = Config.from_env()\nprint(config.get(\"browser.headless\"))  # False\nprint(config.get(\"proxy.url\"))  # http://proxy:8080\n</code></pre>"},{"location":"api/core/config/#programmatic-configuration","title":"Programmatic Configuration","text":"<pre><code>from xeepy.core.config import Config\n\nconfig = Config()\nconfig.set(\"browser.headless\", False)\nconfig.set(\"rate_limit.requests_per_minute\", 20)\nconfig.set(\"proxy.url\", \"http://proxy:8080\")\n\n# Validate before use\nif config.validate():\n    print(\"Configuration is valid\")\n\n# Save for later\nconfig.save(\"my_config.yaml\")\n</code></pre>"},{"location":"api/core/config/#see-also","title":"See Also","text":"<ul> <li>Xeepy - Main entry point</li> <li>RateLimiter - Rate limiting details</li> </ul>"},{"location":"api/core/rate_limiter/","title":"RateLimiter","text":"<p>Manages rate limiting to protect accounts from suspension and respect X/Twitter's limits.</p>"},{"location":"api/core/rate_limiter/#import","title":"Import","text":"<pre><code>from xeepy.core.rate_limiter import RateLimiter\n</code></pre>"},{"location":"api/core/rate_limiter/#class-signature","title":"Class Signature","text":"<pre><code>class RateLimiter:\n    def __init__(\n        self,\n        requests_per_minute: int = 30,\n        requests_per_hour: int = 500,\n        requests_per_day: int = 5000,\n        min_delay: float = 1.0,\n        max_delay: float = 5.0,\n        burst_limit: int = 10\n    )\n</code></pre>"},{"location":"api/core/rate_limiter/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>requests_per_minute</code> <code>int</code> <code>30</code> Maximum requests per minute <code>requests_per_hour</code> <code>int</code> <code>500</code> Maximum requests per hour <code>requests_per_day</code> <code>int</code> <code>5000</code> Maximum requests per day <code>min_delay</code> <code>float</code> <code>1.0</code> Minimum delay between requests (seconds) <code>max_delay</code> <code>float</code> <code>5.0</code> Maximum delay between requests (seconds) <code>burst_limit</code> <code>int</code> <code>10</code> Maximum burst requests before delay"},{"location":"api/core/rate_limiter/#methods","title":"Methods","text":"Method Returns Description <code>acquire()</code> <code>None</code> Wait for rate limit slot <code>wait()</code> <code>None</code> Apply random delay <code>is_limited()</code> <code>bool</code> Check if currently rate limited <code>get_stats()</code> <code>Dict</code> Get rate limit statistics <code>reset()</code> <code>None</code> Reset all counters <code>set_limits(...)</code> <code>None</code> Update rate limits <code>remaining(window)</code> <code>int</code> Get remaining requests"},{"location":"api/core/rate_limiter/#acquire","title":"<code>acquire</code>","text":"<pre><code>async def acquire(self, weight: int = 1) -&gt; None\n</code></pre> <p>Acquire a rate limit slot, waiting if necessary.</p> <p>Parameters: - <code>weight</code>: Request weight (heavier operations can count as multiple)</p>"},{"location":"api/core/rate_limiter/#wait","title":"<code>wait</code>","text":"<pre><code>async def wait(self, min_delay: Optional[float] = None, max_delay: Optional[float] = None) -&gt; None\n</code></pre> <p>Apply a random delay between min and max values.</p>"},{"location":"api/core/rate_limiter/#is_limited","title":"<code>is_limited</code>","text":"<pre><code>def is_limited(self, window: str = \"minute\") -&gt; bool\n</code></pre> <p>Check if rate limit is reached for the specified window.</p> <p>Parameters: - <code>window</code>: Time window (<code>minute</code>, <code>hour</code>, <code>day</code>)</p>"},{"location":"api/core/rate_limiter/#get_stats","title":"<code>get_stats</code>","text":"<pre><code>def get_stats(self) -&gt; Dict[str, Any]\n</code></pre> <p>Get current rate limit statistics.</p> <p>Returns: <pre><code>{\n    \"requests_minute\": 15,\n    \"requests_hour\": 234,\n    \"requests_day\": 1205,\n    \"remaining_minute\": 15,\n    \"remaining_hour\": 266,\n    \"remaining_day\": 3795,\n    \"next_reset_minute\": \"2024-01-15T10:31:00\",\n    \"is_limited\": False\n}\n</code></pre></p>"},{"location":"api/core/rate_limiter/#remaining","title":"<code>remaining</code>","text":"<pre><code>def remaining(self, window: str = \"minute\") -&gt; int\n</code></pre> <p>Get remaining requests for the specified time window.</p>"},{"location":"api/core/rate_limiter/#set_limits","title":"<code>set_limits</code>","text":"<pre><code>def set_limits(\n    self,\n    requests_per_minute: Optional[int] = None,\n    requests_per_hour: Optional[int] = None,\n    requests_per_day: Optional[int] = None\n) -&gt; None\n</code></pre> <p>Update rate limit values dynamically.</p>"},{"location":"api/core/rate_limiter/#preset-configurations","title":"Preset Configurations","text":"<pre><code># Conservative (recommended for new accounts)\nRateLimiter.CONSERVATIVE = RateLimiter(\n    requests_per_minute=15,\n    requests_per_hour=200,\n    requests_per_day=2000,\n    min_delay=2.0,\n    max_delay=8.0\n)\n\n# Standard (default)\nRateLimiter.STANDARD = RateLimiter(\n    requests_per_minute=30,\n    requests_per_hour=500,\n    requests_per_day=5000\n)\n\n# Aggressive (use with caution)\nRateLimiter.AGGRESSIVE = RateLimiter(\n    requests_per_minute=60,\n    requests_per_hour=1000,\n    requests_per_day=10000,\n    min_delay=0.5,\n    max_delay=2.0\n)\n</code></pre>"},{"location":"api/core/rate_limiter/#usage-examples","title":"Usage Examples","text":""},{"location":"api/core/rate_limiter/#basic-rate-limiting","title":"Basic Rate Limiting","text":"<pre><code>from xeepy.core.rate_limiter import RateLimiter\n\nasync def main():\n    limiter = RateLimiter(\n        requests_per_minute=20,\n        min_delay=1.5,\n        max_delay=4.0\n    )\n\n    for i in range(100):\n        await limiter.acquire()\n        # Perform action\n        print(f\"Request {i + 1}\")\n\n        # Check stats periodically\n        if i % 10 == 0:\n            stats = limiter.get_stats()\n            print(f\"Remaining this minute: {stats['remaining_minute']}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/core/rate_limiter/#with-xeepy-integration","title":"With Xeepy Integration","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.core.rate_limiter import RateLimiter\n\nasync def main():\n    # Xeepy uses rate limiting internally\n    async with Xeepy() as x:\n        # Access the internal rate limiter\n        x.rate_limiter.set_limits(requests_per_minute=25)\n\n        # Operations are automatically rate limited\n        for username in usernames:\n            profile = await x.scrape.profile(username)\n            print(profile.followers_count)\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/core/rate_limiter/#conservative-mode-for-sensitive-operations","title":"Conservative Mode for Sensitive Operations","text":"<pre><code>from xeepy.core.rate_limiter import RateLimiter\n\nasync def unfollow_with_care(x, users):\n    # Use conservative limits for unfollow operations\n    limiter = RateLimiter(\n        requests_per_minute=10,\n        requests_per_hour=100,\n        min_delay=3.0,\n        max_delay=10.0\n    )\n\n    for user in users:\n        await limiter.acquire()\n        await x.unfollow.user(user)\n\n        # Add extra delay for sensitive operations\n        await limiter.wait(min_delay=5.0, max_delay=15.0)\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/core/rate_limiter/#see-also","title":"See Also","text":"<ul> <li>Xeepy - Main entry point</li> <li>Config - Configuration options</li> </ul>"},{"location":"api/core/xeepy/","title":"Xeepy","text":"<p>The main entry point for the Xeepy library, providing access to all scraping, action, and monitoring functionality.</p>"},{"location":"api/core/xeepy/#import","title":"Import","text":"<pre><code>from xeepy import Xeepy\n</code></pre>"},{"location":"api/core/xeepy/#class-signature","title":"Class Signature","text":"<pre><code>class Xeepy:\n    def __init__(\n        self,\n        headless: bool = True,\n        proxy: Optional[str] = None,\n        user_agent: Optional[str] = None,\n        timeout: int = 30000,\n        slow_mo: int = 0,\n        config_path: Optional[str] = None\n    )\n</code></pre>"},{"location":"api/core/xeepy/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>headless</code> <code>bool</code> <code>True</code> Run browser in headless mode <code>proxy</code> <code>Optional[str]</code> <code>None</code> Proxy server URL (e.g., <code>http://user:pass@host:port</code>) <code>user_agent</code> <code>Optional[str]</code> <code>None</code> Custom user agent string <code>timeout</code> <code>int</code> <code>30000</code> Default timeout in milliseconds <code>slow_mo</code> <code>int</code> <code>0</code> Slow down operations by specified milliseconds <code>config_path</code> <code>Optional[str]</code> <code>None</code> Path to configuration file"},{"location":"api/core/xeepy/#attributes","title":"Attributes","text":"Attribute Type Description <code>scrape</code> <code>ScraperManager</code> Access to all scraping operations <code>follow</code> <code>FollowActions</code> Follow-related actions <code>unfollow</code> <code>UnfollowActions</code> Unfollow-related actions <code>engage</code> <code>EngageActions</code> Engagement actions (like, retweet, etc.) <code>monitor</code> <code>MonitorManager</code> Monitoring operations <code>export</code> <code>ExportManager</code> Data export utilities <code>auth</code> <code>AuthManager</code> Authentication management <code>dm</code> <code>DirectMessageActions</code> Direct message operations <code>schedule</code> <code>SchedulingActions</code> Tweet scheduling <code>poll</code> <code>PollActions</code> Poll creation and management"},{"location":"api/core/xeepy/#methods","title":"Methods","text":""},{"location":"api/core/xeepy/#__aenter__","title":"<code>__aenter__</code>","text":"<pre><code>async def __aenter__(self) -&gt; \"Xeepy\"\n</code></pre> <p>Async context manager entry. Initializes the browser and returns the Xeepy instance.</p>"},{"location":"api/core/xeepy/#__aexit__","title":"<code>__aexit__</code>","text":"<pre><code>async def __aexit__(self, exc_type, exc_val, exc_tb) -&gt; None\n</code></pre> <p>Async context manager exit. Closes the browser and cleans up resources.</p>"},{"location":"api/core/xeepy/#close","title":"<code>close</code>","text":"<pre><code>async def close(self) -&gt; None\n</code></pre> <p>Manually close the browser and cleanup resources.</p>"},{"location":"api/core/xeepy/#usage-examples","title":"Usage Examples","text":""},{"location":"api/core/xeepy/#basic-usage","title":"Basic Usage","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Scrape replies to a tweet\n        replies = await x.scrape.replies(\"https://x.com/user/status/123\")\n\n        # Export to CSV\n        x.export.to_csv(replies, \"replies.csv\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/core/xeepy/#with-proxy-and-custom-settings","title":"With Proxy and Custom Settings","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy(\n        headless=False,\n        proxy=\"http://user:pass@proxy.example.com:8080\",\n        timeout=60000,\n        slow_mo=100\n    ) as x:\n        # Operations with visible browser and proxy\n        profile = await x.scrape.profile(\"username\")\n        print(profile)\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/core/xeepy/#manual-resource-management","title":"Manual Resource Management","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    x = Xeepy(headless=True)\n    try:\n        await x.__aenter__()\n        followers = await x.scrape.followers(\"username\", limit=500)\n    finally:\n        await x.close()\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/core/xeepy/#see-also","title":"See Also","text":"<ul> <li>BrowserManager - Browser management details</li> <li>AuthManager - Authentication methods</li> <li>Config - Configuration options</li> </ul>"},{"location":"api/models/engagement/","title":"Engagement","text":"<p>Data models for engagement metrics and interactions.</p>"},{"location":"api/models/engagement/#import","title":"Import","text":"<pre><code>from xeepy.models import Engagement, EngagementMetrics, Interaction\n</code></pre>"},{"location":"api/models/engagement/#engagement-class","title":"Engagement Class","text":"<pre><code>@dataclass\nclass Engagement:\n    tweet_id: str\n    likes: int = 0\n    retweets: int = 0\n    replies: int = 0\n    quotes: int = 0\n    views: int = 0\n    bookmarks: int = 0\n    timestamp: datetime = field(default_factory=datetime.now)\n</code></pre>"},{"location":"api/models/engagement/#engagementmetrics-class","title":"EngagementMetrics Class","text":"<pre><code>@dataclass\nclass EngagementMetrics:\n    total_engagement: int            # Sum of all interactions\n    engagement_rate: float           # Engagement / followers %\n    like_rate: float                 # Likes / views %\n    retweet_rate: float              # Retweets / views %\n    reply_rate: float                # Replies / views %\n    viral_score: float               # Virality indicator\n</code></pre>"},{"location":"api/models/engagement/#interaction-class","title":"Interaction Class","text":"<pre><code>@dataclass\nclass Interaction:\n    type: str                        # like, retweet, reply, quote, bookmark\n    user: User                       # User who interacted\n    tweet_id: str                    # Tweet interacted with\n    timestamp: datetime              # When interaction occurred\n    content: Optional[str] = None    # Reply/quote text if applicable\n</code></pre>"},{"location":"api/models/engagement/#interactionsummary-class","title":"InteractionSummary Class","text":"<pre><code>@dataclass\nclass InteractionSummary:\n    tweet_id: str\n    likers: List[User]               # Users who liked\n    retweeters: List[User]           # Users who retweeted\n    repliers: List[User]             # Users who replied\n    quoters: List[User]              # Users who quoted\n</code></pre>"},{"location":"api/models/engagement/#properties","title":"Properties","text":""},{"location":"api/models/engagement/#engagement_1","title":"Engagement","text":"Property Type Description <code>tweet_id</code> <code>str</code> Associated tweet ID <code>likes</code> <code>int</code> Like count <code>retweets</code> <code>int</code> Retweet count <code>replies</code> <code>int</code> Reply count <code>quotes</code> <code>int</code> Quote count <code>views</code> <code>int</code> View count <code>bookmarks</code> <code>int</code> Bookmark count <code>timestamp</code> <code>datetime</code> Measurement time"},{"location":"api/models/engagement/#computed-properties","title":"Computed Properties","text":"<pre><code>@property\ndef total(self) -&gt; int:\n    \"\"\"Total engagement (likes + retweets + replies + quotes).\"\"\"\n\n@property\ndef engagement_rate(self) -&gt; float:\n    \"\"\"Engagement rate (total / views * 100).\"\"\"\n</code></pre>"},{"location":"api/models/engagement/#usage-examples","title":"Usage Examples","text":""},{"location":"api/models/engagement/#track-tweet-engagement","title":"Track Tweet Engagement","text":"<pre><code>from xeepy import Xeepy\n\nasync def track_engagement(tweet_url: str):\n    async with Xeepy() as x:\n        tweet = await x.scrape.tweet(tweet_url)\n\n        engagement = Engagement(\n            tweet_id=tweet.id,\n            likes=tweet.like_count,\n            retweets=tweet.retweet_count,\n            replies=tweet.reply_count,\n            quotes=tweet.quote_count,\n            views=tweet.view_count,\n            bookmarks=tweet.bookmark_count\n        )\n\n        print(f\"=== Engagement: {tweet.id} ===\")\n        print(f\"Likes: {engagement.likes:,}\")\n        print(f\"Retweets: {engagement.retweets:,}\")\n        print(f\"Replies: {engagement.replies:,}\")\n        print(f\"Quotes: {engagement.quotes:,}\")\n        print(f\"Views: {engagement.views:,}\")\n        print(f\"Bookmarks: {engagement.bookmarks:,}\")\n        print(f\"Total: {engagement.total:,}\")\n        print(f\"Rate: {engagement.engagement_rate:.2f}%\")\n\nasyncio.run(track_engagement(\"https://x.com/user/status/123\"))\n</code></pre>"},{"location":"api/models/engagement/#calculate-metrics","title":"Calculate Metrics","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.models import EngagementMetrics\n\nasync def calculate_metrics(username: str):\n    async with Xeepy() as x:\n        user = await x.scrape.profile(username)\n        tweets = await x.scrape.tweets(username, limit=50)\n\n        total_engagement = sum(t.total_engagement for t in tweets.items)\n        total_views = sum(t.view_count for t in tweets.items)\n\n        metrics = EngagementMetrics(\n            total_engagement=total_engagement,\n            engagement_rate=(total_engagement / user.followers_count) * 100 if user.followers_count else 0,\n            like_rate=(sum(t.like_count for t in tweets.items) / total_views) * 100 if total_views else 0,\n            retweet_rate=(sum(t.retweet_count for t in tweets.items) / total_views) * 100 if total_views else 0,\n            reply_rate=(sum(t.reply_count for t in tweets.items) / total_views) * 100 if total_views else 0,\n            viral_score=total_engagement / len(tweets.items) if tweets.items else 0\n        )\n\n        print(f\"=== Metrics: @{username} ===\")\n        print(f\"Total engagement: {metrics.total_engagement:,}\")\n        print(f\"Engagement rate: {metrics.engagement_rate:.2f}%\")\n        print(f\"Like rate: {metrics.like_rate:.2f}%\")\n        print(f\"Retweet rate: {metrics.retweet_rate:.2f}%\")\n        print(f\"Reply rate: {metrics.reply_rate:.2f}%\")\n        print(f\"Viral score: {metrics.viral_score:.0f}\")\n\nasyncio.run(calculate_metrics(\"username\"))\n</code></pre>"},{"location":"api/models/engagement/#get-users-who-engaged","title":"Get Users Who Engaged","text":"<pre><code>from xeepy import Xeepy\n\nasync def get_engagers(tweet_url: str):\n    async with Xeepy() as x:\n        likers = await x.scrape.likes(tweet_url, limit=100)\n\n        summary = InteractionSummary(\n            tweet_id=tweet_url.split(\"/\")[-1],\n            likers=likers.items,\n            retweeters=[],  # Would need separate scrape\n            repliers=[],\n            quoters=[]\n        )\n\n        print(f\"=== Users who liked ===\")\n        for user in summary.likers[:20]:\n            print(f\"  @{user.username} ({user.followers_count:,} followers)\")\n\nasyncio.run(get_engagers(\"https://x.com/user/status/123\"))\n</code></pre>"},{"location":"api/models/engagement/#compare-engagement","title":"Compare Engagement","text":"<pre><code>from xeepy import Xeepy\n\nasync def compare_tweets(tweet_urls: list):\n    async with Xeepy() as x:\n        tweets = []\n        for url in tweet_urls:\n            tweet = await x.scrape.tweet(url)\n            tweets.append(tweet)\n\n        print(\"=== Engagement Comparison ===\")\n        print(f\"{'Tweet ID':20} {'Likes':&gt;10} {'RT':&gt;10} {'Replies':&gt;10} {'Rate':&gt;8}\")\n        print(\"-\" * 60)\n\n        for tweet in tweets:\n            rate = tweet.engagement_rate\n            print(f\"{tweet.id:20} {tweet.like_count:&gt;10,} {tweet.retweet_count:&gt;10,} {tweet.reply_count:&gt;10,} {rate:&gt;7.2f}%\")\n\nasyncio.run(compare_tweets([\n    \"https://x.com/user/status/123\",\n    \"https://x.com/user/status/456\"\n]))\n</code></pre>"},{"location":"api/models/engagement/#track-engagement-over-time","title":"Track Engagement Over Time","text":"<pre><code>from xeepy import Xeepy\nimport asyncio\n\nasync def track_over_time(tweet_url: str, intervals: int = 5, delay: int = 60):\n    async with Xeepy() as x:\n        history = []\n\n        for i in range(intervals):\n            tweet = await x.scrape.tweet(tweet_url)\n\n            engagement = Engagement(\n                tweet_id=tweet.id,\n                likes=tweet.like_count,\n                retweets=tweet.retweet_count,\n                replies=tweet.reply_count,\n                views=tweet.view_count\n            )\n            history.append(engagement)\n\n            print(f\"[{i+1}/{intervals}] Likes: {engagement.likes:,} | RT: {engagement.retweets:,} | Views: {engagement.views:,}\")\n\n            if i &lt; intervals - 1:\n                await asyncio.sleep(delay)\n\n        # Calculate growth\n        if len(history) &gt;= 2:\n            like_growth = history[-1].likes - history[0].likes\n            view_growth = history[-1].views - history[0].views\n            print(f\"\\nGrowth: +{like_growth:,} likes, +{view_growth:,} views\")\n\nasyncio.run(track_over_time(\"https://x.com/user/status/123\"))\n</code></pre>"},{"location":"api/models/engagement/#export-engagement-data","title":"Export Engagement Data","text":"<pre><code>from xeepy import Xeepy\n\nasync def export_engagement(username: str):\n    async with Xeepy() as x:\n        tweets = await x.scrape.tweets(username, limit=100)\n\n        data = []\n        for tweet in tweets.items:\n            data.append({\n                \"id\": tweet.id,\n                \"text\": tweet.text[:100],\n                \"likes\": tweet.like_count,\n                \"retweets\": tweet.retweet_count,\n                \"replies\": tweet.reply_count,\n                \"quotes\": tweet.quote_count,\n                \"views\": tweet.view_count,\n                \"engagement_rate\": tweet.engagement_rate,\n                \"created_at\": tweet.created_at.isoformat()\n            })\n\n        x.export.to_csv(data, f\"engagement_{username}.csv\")\n        print(f\"Exported engagement for {len(data)} tweets\")\n\nasyncio.run(export_engagement(\"username\"))\n</code></pre>"},{"location":"api/models/engagement/#find-most-engaged-followers","title":"Find Most Engaged Followers","text":"<pre><code>from xeepy import Xeepy\n\nasync def most_engaged_followers(username: str):\n    async with Xeepy() as x:\n        # Get recent tweets\n        tweets = await x.scrape.tweets(username, limit=20)\n\n        engagement_count = {}\n\n        for tweet in tweets.items:\n            replies = await x.scrape.replies(tweet.url, limit=50)\n\n            for reply in replies.items:\n                author = reply.author.username\n                if author not in engagement_count:\n                    engagement_count[author] = 0\n                engagement_count[author] += 1\n\n        sorted_engagers = sorted(\n            engagement_count.items(),\n            key=lambda x: x[1],\n            reverse=True\n        )\n\n        print(f\"Most engaged followers of @{username}:\")\n        for user, count in sorted_engagers[:20]:\n            print(f\"  @{user}: {count} replies\")\n\nasyncio.run(most_engaged_followers(\"myaccount\"))\n</code></pre>"},{"location":"api/models/engagement/#engagement-benchmarks","title":"Engagement Benchmarks","text":"<pre><code>from xeepy import Xeepy\n\nasync def benchmark_engagement(username: str):\n    async with Xeepy() as x:\n        user = await x.scrape.profile(username)\n        tweets = await x.scrape.tweets(username, limit=50)\n\n        avg_likes = sum(t.like_count for t in tweets.items) / len(tweets.items)\n        avg_rate = sum(t.engagement_rate for t in tweets.items) / len(tweets.items)\n\n        print(f\"=== Engagement Benchmarks: @{username} ===\")\n        print(f\"Followers: {user.followers_count:,}\")\n        print(f\"Avg likes/tweet: {avg_likes:.0f}\")\n        print(f\"Avg engagement rate: {avg_rate:.2f}%\")\n\n        # Industry benchmarks\n        if avg_rate &gt; 6:\n            print(\"\\n\ud83c\udfc6 Excellent! (Top 1%)\")\n        elif avg_rate &gt; 3:\n            print(\"\\n\ud83c\udf1f Great! (Top 10%)\")\n        elif avg_rate &gt; 1:\n            print(\"\\n\u2713 Good (Average)\")\n        else:\n            print(\"\\n\ud83d\udcc8 Room for improvement\")\n\nasyncio.run(benchmark_engagement(\"username\"))\n</code></pre>"},{"location":"api/models/engagement/#see-also","title":"See Also","text":"<ul> <li>Tweet - Tweet data model</li> <li>User - User data model</li> <li>EngagementAnalytics - Engagement analytics</li> </ul>"},{"location":"api/models/tweet/","title":"Tweet","text":"<p>Data model representing a tweet/post on X/Twitter.</p>"},{"location":"api/models/tweet/#import","title":"Import","text":"<pre><code>from xeepy.models import Tweet\n</code></pre>"},{"location":"api/models/tweet/#class-signature","title":"Class Signature","text":"<pre><code>@dataclass\nclass Tweet:\n    id: str\n    text: str\n    author: User\n    created_at: datetime\n    like_count: int = 0\n    retweet_count: int = 0\n    reply_count: int = 0\n    quote_count: int = 0\n    view_count: int = 0\n    bookmark_count: int = 0\n    url: str = \"\"\n    conversation_id: Optional[str] = None\n    in_reply_to_id: Optional[str] = None\n    in_reply_to_user: Optional[str] = None\n    is_retweet: bool = False\n    is_quote: bool = False\n    is_reply: bool = False\n    media: List[Media] = field(default_factory=list)\n    hashtags: List[str] = field(default_factory=list)\n    mentions: List[str] = field(default_factory=list)\n    urls: List[str] = field(default_factory=list)\n    language: Optional[str] = None\n    source: Optional[str] = None\n    is_sensitive: bool = False\n    poll: Optional[Poll] = None\n</code></pre>"},{"location":"api/models/tweet/#properties","title":"Properties","text":"Property Type Description <code>id</code> <code>str</code> Unique tweet ID <code>text</code> <code>str</code> Tweet content <code>author</code> <code>User</code> Tweet author <code>created_at</code> <code>datetime</code> Creation timestamp <code>like_count</code> <code>int</code> Number of likes <code>retweet_count</code> <code>int</code> Number of retweets <code>reply_count</code> <code>int</code> Number of replies <code>quote_count</code> <code>int</code> Number of quote tweets <code>view_count</code> <code>int</code> Number of views <code>bookmark_count</code> <code>int</code> Number of bookmarks <code>url</code> <code>str</code> Full tweet URL <code>conversation_id</code> <code>str</code> Thread conversation ID <code>in_reply_to_id</code> <code>str</code> Parent tweet ID if reply <code>in_reply_to_user</code> <code>str</code> Parent tweet author <code>is_retweet</code> <code>bool</code> Is a retweet <code>is_quote</code> <code>bool</code> Is a quote tweet <code>is_reply</code> <code>bool</code> Is a reply <code>media</code> <code>List[Media]</code> Attached media <code>hashtags</code> <code>List[str]</code> Hashtags used <code>mentions</code> <code>List[str]</code> Users mentioned <code>urls</code> <code>List[str]</code> URLs in tweet <code>language</code> <code>str</code> Detected language <code>source</code> <code>str</code> Client used to post <code>is_sensitive</code> <code>bool</code> Marked as sensitive <code>poll</code> <code>Poll</code> Poll if present"},{"location":"api/models/tweet/#computed-properties","title":"Computed Properties","text":"<pre><code>@property\ndef engagement_rate(self) -&gt; float:\n    \"\"\"Calculate engagement rate.\"\"\"\n\n@property\ndef total_engagement(self) -&gt; int:\n    \"\"\"Sum of all engagement metrics.\"\"\"\n\n@property\ndef has_media(self) -&gt; bool:\n    \"\"\"Check if tweet has media.\"\"\"\n\n@property\ndef is_thread(self) -&gt; bool:\n    \"\"\"Check if part of a thread.\"\"\"\n</code></pre>"},{"location":"api/models/tweet/#methods","title":"Methods","text":"Method Returns Description <code>to_dict()</code> <code>Dict</code> Convert to dictionary <code>from_dict(data)</code> <code>Tweet</code> Create from dictionary <code>from_api(data)</code> <code>Tweet</code> Create from API response"},{"location":"api/models/tweet/#usage-examples","title":"Usage Examples","text":""},{"location":"api/models/tweet/#access-tweet-data","title":"Access Tweet Data","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        tweets = await x.scrape.tweets(\"username\", limit=10)\n\n        for tweet in tweets.items:\n            print(f\"ID: {tweet.id}\")\n            print(f\"Text: {tweet.text}\")\n            print(f\"Author: @{tweet.author.username}\")\n            print(f\"Posted: {tweet.created_at}\")\n            print(f\"Likes: {tweet.like_count:,}\")\n            print(f\"Retweets: {tweet.retweet_count:,}\")\n            print(f\"Views: {tweet.view_count:,}\")\n            print(f\"URL: {tweet.url}\")\n            print(\"---\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/models/tweet/#check-tweet-type","title":"Check Tweet Type","text":"<pre><code>from xeepy import Xeepy\n\nasync def categorize_tweets(username: str):\n    async with Xeepy() as x:\n        tweets = await x.scrape.tweets(username, limit=100)\n\n        original = [t for t in tweets.items if not t.is_retweet and not t.is_reply]\n        replies = [t for t in tweets.items if t.is_reply]\n        retweets = [t for t in tweets.items if t.is_retweet]\n        quotes = [t for t in tweets.items if t.is_quote]\n\n        print(f\"Original: {len(original)}\")\n        print(f\"Replies: {len(replies)}\")\n        print(f\"Retweets: {len(retweets)}\")\n        print(f\"Quotes: {len(quotes)}\")\n\nasyncio.run(categorize_tweets(\"username\"))\n</code></pre>"},{"location":"api/models/tweet/#access-media","title":"Access Media","text":"<pre><code>from xeepy import Xeepy\n\nasync def get_media_tweets(username: str):\n    async with Xeepy() as x:\n        tweets = await x.scrape.tweets(username, limit=50)\n\n        for tweet in tweets.items:\n            if tweet.has_media:\n                print(f\"Tweet: {tweet.text[:50]}...\")\n                for media in tweet.media:\n                    print(f\"  - {media.type}: {media.url}\")\n\nasyncio.run(get_media_tweets(\"username\"))\n</code></pre>"},{"location":"api/models/tweet/#calculate-engagement","title":"Calculate Engagement","text":"<pre><code>from xeepy import Xeepy\n\nasync def top_engagement(username: str):\n    async with Xeepy() as x:\n        tweets = await x.scrape.tweets(username, limit=100)\n\n        # Sort by engagement\n        sorted_tweets = sorted(\n            tweets.items,\n            key=lambda t: t.total_engagement,\n            reverse=True\n        )\n\n        print(\"Top 10 by engagement:\")\n        for tweet in sorted_tweets[:10]:\n            print(f\"{tweet.total_engagement:,} - {tweet.text[:40]}...\")\n            print(f\"  Rate: {tweet.engagement_rate:.2f}%\")\n\nasyncio.run(top_engagement(\"username\"))\n</code></pre>"},{"location":"api/models/tweet/#extract-hashtags","title":"Extract Hashtags","text":"<pre><code>from xeepy import Xeepy\nfrom collections import Counter\n\nasync def popular_hashtags(username: str):\n    async with Xeepy() as x:\n        tweets = await x.scrape.tweets(username, limit=200)\n\n        all_hashtags = []\n        for tweet in tweets.items:\n            all_hashtags.extend(tweet.hashtags)\n\n        counter = Counter(all_hashtags)\n\n        print(\"Most used hashtags:\")\n        for tag, count in counter.most_common(10):\n            print(f\"  #{tag}: {count}\")\n\nasyncio.run(popular_hashtags(\"username\"))\n</code></pre>"},{"location":"api/models/tweet/#filter-by-content","title":"Filter by Content","text":"<pre><code>from xeepy import Xeepy\n\nasync def filter_tweets(username: str):\n    async with Xeepy() as x:\n        tweets = await x.scrape.tweets(username, limit=100)\n\n        # Tweets with links\n        with_links = [t for t in tweets.items if t.urls]\n        print(f\"With links: {len(with_links)}\")\n\n        # Tweets with mentions\n        with_mentions = [t for t in tweets.items if t.mentions]\n        print(f\"With mentions: {len(with_mentions)}\")\n\n        # Long tweets\n        long_tweets = [t for t in tweets.items if len(t.text) &gt; 200]\n        print(f\"Long tweets: {len(long_tweets)}\")\n\nasyncio.run(filter_tweets(\"username\"))\n</code></pre>"},{"location":"api/models/tweet/#convert-to-dictionary","title":"Convert to Dictionary","text":"<pre><code>from xeepy import Xeepy\nimport json\n\nasync def export_tweet(tweet_url: str):\n    async with Xeepy() as x:\n        # Get single tweet\n        tweet = await x.scrape.tweet(tweet_url)\n\n        # Convert to dict\n        data = tweet.to_dict()\n\n        # Save as JSON\n        with open(\"tweet.json\", \"w\") as f:\n            json.dump(data, f, indent=2, default=str)\n\nasyncio.run(export_tweet(\"https://x.com/user/status/123\"))\n</code></pre>"},{"location":"api/models/tweet/#access-poll-data","title":"Access Poll Data","text":"<pre><code>from xeepy import Xeepy\n\nasync def get_poll_tweet(tweet_url: str):\n    async with Xeepy() as x:\n        tweet = await x.scrape.tweet(tweet_url)\n\n        if tweet.poll:\n            print(f\"Poll question: {tweet.text}\")\n            print(f\"Total votes: {tweet.poll.total_votes:,}\")\n            print(f\"Status: {tweet.poll.status}\")\n\n            for option in tweet.poll.options:\n                bar = \"\u2588\" * int(option.percentage / 5)\n                print(f\"  {option.label}: {bar} {option.percentage:.1f}%\")\n\nasyncio.run(get_poll_tweet(\"https://x.com/user/status/123\"))\n</code></pre>"},{"location":"api/models/tweet/#thread-detection","title":"Thread Detection","text":"<pre><code>from xeepy import Xeepy\n\nasync def get_threads(username: str):\n    async with Xeepy() as x:\n        tweets = await x.scrape.tweets(username, limit=100)\n\n        # Group by conversation\n        conversations = {}\n        for tweet in tweets.items:\n            if tweet.conversation_id:\n                if tweet.conversation_id not in conversations:\n                    conversations[tweet.conversation_id] = []\n                conversations[tweet.conversation_id].append(tweet)\n\n        # Find threads (2+ tweets in conversation by same author)\n        threads = [\n            tweets for tweets in conversations.values()\n            if len(tweets) &gt;= 2\n        ]\n\n        print(f\"Found {len(threads)} threads\")\n\nasyncio.run(get_threads(\"username\"))\n</code></pre>"},{"location":"api/models/tweet/#see-also","title":"See Also","text":"<ul> <li>User - User data model</li> <li>Engagement - Engagement model</li> <li>TweetsScraper - Scraping tweets</li> </ul>"},{"location":"api/models/user/","title":"User","text":"<p>Data model representing a user/account on X/Twitter.</p>"},{"location":"api/models/user/#import","title":"Import","text":"<pre><code>from xeepy.models import User\n</code></pre>"},{"location":"api/models/user/#class-signature","title":"Class Signature","text":"<pre><code>@dataclass\nclass User:\n    id: str\n    username: str\n    name: str\n    bio: str = \"\"\n    location: str = \"\"\n    website: str = \"\"\n    created_at: Optional[datetime] = None\n    followers_count: int = 0\n    following_count: int = 0\n    tweet_count: int = 0\n    like_count: int = 0\n    listed_count: int = 0\n    is_verified: bool = False\n    is_blue_verified: bool = False\n    is_protected: bool = False\n    profile_image_url: str = \"\"\n    profile_banner_url: str = \"\"\n    pinned_tweet_id: Optional[str] = None\n    url: str = \"\"\n</code></pre>"},{"location":"api/models/user/#properties","title":"Properties","text":"Property Type Description <code>id</code> <code>str</code> Unique user ID <code>username</code> <code>str</code> Handle (without @) <code>name</code> <code>str</code> Display name <code>bio</code> <code>str</code> Profile description <code>location</code> <code>str</code> Profile location <code>website</code> <code>str</code> Profile website <code>created_at</code> <code>datetime</code> Account creation date <code>followers_count</code> <code>int</code> Number of followers <code>following_count</code> <code>int</code> Number following <code>tweet_count</code> <code>int</code> Total tweets <code>like_count</code> <code>int</code> Total likes given <code>listed_count</code> <code>int</code> Lists included in <code>is_verified</code> <code>bool</code> Legacy verified <code>is_blue_verified</code> <code>bool</code> Twitter Blue verified <code>is_protected</code> <code>bool</code> Private account <code>profile_image_url</code> <code>str</code> Avatar URL <code>profile_banner_url</code> <code>str</code> Banner URL <code>pinned_tweet_id</code> <code>str</code> Pinned tweet ID <code>url</code> <code>str</code> Profile URL"},{"location":"api/models/user/#computed-properties","title":"Computed Properties","text":"<pre><code>@property\ndef follower_ratio(self) -&gt; float:\n    \"\"\"Followers / Following ratio.\"\"\"\n\n@property\ndef account_age_days(self) -&gt; int:\n    \"\"\"Days since account creation.\"\"\"\n\n@property\ndef tweets_per_day(self) -&gt; float:\n    \"\"\"Average tweets per day.\"\"\"\n\n@property\ndef profile_url(self) -&gt; str:\n    \"\"\"Full profile URL.\"\"\"\n</code></pre>"},{"location":"api/models/user/#methods","title":"Methods","text":"Method Returns Description <code>to_dict()</code> <code>Dict</code> Convert to dictionary <code>from_dict(data)</code> <code>User</code> Create from dictionary <code>from_api(data)</code> <code>User</code> Create from API response"},{"location":"api/models/user/#usage-examples","title":"Usage Examples","text":""},{"location":"api/models/user/#access-user-profile","title":"Access User Profile","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        user = await x.scrape.profile(\"username\")\n\n        print(f\"=== @{user.username} ===\")\n        print(f\"Name: {user.name}\")\n        print(f\"Bio: {user.bio}\")\n        print(f\"Location: {user.location}\")\n        print(f\"Website: {user.website}\")\n        print(f\"Followers: {user.followers_count:,}\")\n        print(f\"Following: {user.following_count:,}\")\n        print(f\"Tweets: {user.tweet_count:,}\")\n        print(f\"Joined: {user.created_at}\")\n        print(f\"Verified: {'\u2713' if user.is_verified or user.is_blue_verified else '\u2717'}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/models/user/#calculate-follower-ratio","title":"Calculate Follower Ratio","text":"<pre><code>from xeepy import Xeepy\n\nasync def analyze_ratios(usernames: list):\n    async with Xeepy() as x:\n        for username in usernames:\n            user = await x.scrape.profile(username)\n\n            ratio = user.follower_ratio\n            status = \"\ud83c\udf1f\" if ratio &gt; 10 else \"\u2713\" if ratio &gt; 1 else \"\ud83d\udcca\"\n\n            print(f\"{status} @{user.username}: {ratio:.1f}x ({user.followers_count:,}/{user.following_count:,})\")\n\nasyncio.run(analyze_ratios([\"user1\", \"user2\", \"user3\"]))\n</code></pre>"},{"location":"api/models/user/#find-verified-users","title":"Find Verified Users","text":"<pre><code>from xeepy import Xeepy\n\nasync def get_verified_followers(username: str):\n    async with Xeepy() as x:\n        followers = await x.scrape.followers(username, limit=500)\n\n        verified = [\n            u for u in followers.items\n            if u.is_verified or u.is_blue_verified\n        ]\n\n        print(f\"Verified followers of @{username}:\")\n        for user in verified:\n            badge = \"\u2713\" if user.is_verified else \"\ud83d\udd35\"\n            print(f\"  {badge} @{user.username} ({user.followers_count:,} followers)\")\n\nasyncio.run(get_verified_followers(\"elonmusk\"))\n</code></pre>"},{"location":"api/models/user/#account-age-analysis","title":"Account Age Analysis","text":"<pre><code>from xeepy import Xeepy\n\nasync def analyze_account_ages(username: str):\n    async with Xeepy() as x:\n        followers = await x.scrape.followers(username, limit=200)\n\n        age_buckets = {\"&lt;30 days\": 0, \"30-90 days\": 0, \"90-365 days\": 0, \"1+ years\": 0}\n\n        for user in followers.items:\n            age = user.account_age_days\n            if age &lt; 30:\n                age_buckets[\"&lt;30 days\"] += 1\n            elif age &lt; 90:\n                age_buckets[\"30-90 days\"] += 1\n            elif age &lt; 365:\n                age_buckets[\"90-365 days\"] += 1\n            else:\n                age_buckets[\"1+ years\"] += 1\n\n        print(f\"Account age distribution:\")\n        for bucket, count in age_buckets.items():\n            pct = count / len(followers.items) * 100\n            print(f\"  {bucket}: {count} ({pct:.1f}%)\")\n\nasyncio.run(analyze_account_ages(\"username\"))\n</code></pre>"},{"location":"api/models/user/#filter-by-follower-count","title":"Filter by Follower Count","text":"<pre><code>from xeepy import Xeepy\n\nasync def find_influencers(username: str, min_followers: int = 10000):\n    async with Xeepy() as x:\n        followers = await x.scrape.followers(username, limit=500)\n\n        influencers = [\n            u for u in followers.items\n            if u.followers_count &gt;= min_followers\n        ]\n\n        influencers.sort(key=lambda u: u.followers_count, reverse=True)\n\n        print(f\"Influencers following @{username}:\")\n        for user in influencers[:20]:\n            print(f\"  @{user.username}: {user.followers_count:,}\")\n\nasyncio.run(find_influencers(\"myaccount\"))\n</code></pre>"},{"location":"api/models/user/#activity-analysis","title":"Activity Analysis","text":"<pre><code>from xeepy import Xeepy\n\nasync def analyze_activity(username: str):\n    async with Xeepy() as x:\n        user = await x.scrape.profile(username)\n\n        print(f\"=== Activity: @{username} ===\")\n        print(f\"Account age: {user.account_age_days} days\")\n        print(f\"Total tweets: {user.tweet_count:,}\")\n        print(f\"Tweets/day: {user.tweets_per_day:.2f}\")\n\n        if user.tweets_per_day &gt; 10:\n            print(\"\ud83d\udd25 Very active poster\")\n        elif user.tweets_per_day &gt; 3:\n            print(\"\u2713 Active poster\")\n        elif user.tweets_per_day &gt; 0.5:\n            print(\"\ud83d\udcca Moderate poster\")\n        else:\n            print(\"\ud83d\udca4 Infrequent poster\")\n\nasyncio.run(analyze_activity(\"username\"))\n</code></pre>"},{"location":"api/models/user/#export-user-list","title":"Export User List","text":"<pre><code>from xeepy import Xeepy\n\nasync def export_followers(username: str):\n    async with Xeepy() as x:\n        followers = await x.scrape.followers(username, limit=1000)\n\n        data = [user.to_dict() for user in followers.items]\n\n        x.export.to_csv(data, f\"followers_{username}.csv\")\n        print(f\"Exported {len(data)} followers\")\n\nasyncio.run(export_followers(\"myaccount\"))\n</code></pre>"},{"location":"api/models/user/#find-users-by-bio","title":"Find Users by Bio","text":"<pre><code>from xeepy import Xeepy\n\nasync def find_by_bio(username: str, keywords: list):\n    async with Xeepy() as x:\n        followers = await x.scrape.followers(username, limit=500)\n\n        matches = []\n        for user in followers.items:\n            bio_lower = user.bio.lower()\n            if any(kw.lower() in bio_lower for kw in keywords):\n                matches.append(user)\n\n        print(f\"Users with {keywords} in bio:\")\n        for user in matches[:20]:\n            print(f\"  @{user.username}: {user.bio[:60]}...\")\n\nasyncio.run(find_by_bio(\"pycon\", [\"python\", \"developer\", \"engineer\"]))\n</code></pre>"},{"location":"api/models/user/#compare-users","title":"Compare Users","text":"<pre><code>from xeepy import Xeepy\n\nasync def compare_users(usernames: list):\n    async with Xeepy() as x:\n        users = []\n        for username in usernames:\n            user = await x.scrape.profile(username)\n            users.append(user)\n\n        print(f\"{'Username':20} {'Followers':&gt;12} {'Following':&gt;12} {'Tweets':&gt;10} {'Ratio':&gt;8}\")\n        print(\"-\" * 65)\n\n        for user in users:\n            print(f\"@{user.username:19} {user.followers_count:&gt;12,} {user.following_count:&gt;12,} {user.tweet_count:&gt;10,} {user.follower_ratio:&gt;8.1f}\")\n\nasyncio.run(compare_users([\"user1\", \"user2\", \"user3\"]))\n</code></pre>"},{"location":"api/models/user/#private-account-check","title":"Private Account Check","text":"<pre><code>from xeepy import Xeepy\n\nasync def check_access(usernames: list):\n    async with Xeepy() as x:\n        for username in usernames:\n            user = await x.scrape.profile(username)\n\n            if user.is_protected:\n                print(f\"\ud83d\udd12 @{user.username} - Private account\")\n            else:\n                print(f\"\ud83c\udf10 @{user.username} - Public account\")\n\nasyncio.run(check_access([\"user1\", \"user2\"]))\n</code></pre>"},{"location":"api/models/user/#see-also","title":"See Also","text":"<ul> <li>Tweet - Tweet data model</li> <li>Engagement - Engagement model</li> <li>ProfileScraper - Scraping profiles</li> </ul>"},{"location":"api/monitoring/account/","title":"AccountMonitor","text":"<p>Monitor account changes, security events, and activity on X/Twitter.</p>"},{"location":"api/monitoring/account/#import","title":"Import","text":"<pre><code>from xeepy.monitoring.account import AccountMonitor\n</code></pre>"},{"location":"api/monitoring/account/#class-signature","title":"Class Signature","text":"<pre><code>class AccountMonitor:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        storage: Optional[Storage] = None\n    )\n</code></pre>"},{"location":"api/monitoring/account/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>storage</code> <code>Optional[Storage]</code> <code>None</code> Storage for tracking history"},{"location":"api/monitoring/account/#methods","title":"Methods","text":"Method Returns Description <code>track_changes(username)</code> <code>ChangeReport</code> Track profile changes <code>get_activity(username)</code> <code>ActivityReport</code> Get account activity <code>monitor(username, callback)</code> <code>None</code> Start continuous monitoring <code>get_login_history()</code> <code>List[LoginEvent]</code> Get login history <code>compare_snapshots(old, new)</code> <code>ChangeReport</code> Compare two snapshots"},{"location":"api/monitoring/account/#track_changes","title":"<code>track_changes</code>","text":"<pre><code>async def track_changes(\n    self,\n    username: Optional[str] = None\n) -&gt; ChangeReport\n</code></pre> <p>Track changes in an account since last check.</p> <p>Parameters: - <code>username</code>: Username to track (default: logged-in user)</p>"},{"location":"api/monitoring/account/#get_activity","title":"<code>get_activity</code>","text":"<pre><code>async def get_activity(\n    self,\n    username: Optional[str] = None,\n    days: int = 7\n) -&gt; ActivityReport\n</code></pre> <p>Get account activity summary.</p>"},{"location":"api/monitoring/account/#monitor","title":"<code>monitor</code>","text":"<pre><code>async def monitor(\n    self,\n    username: str,\n    callback: Callable[[ChangeReport], Awaitable[None]],\n    interval: int = 3600,\n    track: List[str] = None\n) -&gt; None\n</code></pre> <p>Start continuous monitoring for account changes.</p> <p>Parameters: - <code>username</code>: Account to monitor - <code>callback</code>: Async function called on changes - <code>interval</code>: Check interval in seconds - <code>track</code>: Specific changes to track (<code>bio</code>, <code>followers</code>, <code>name</code>, <code>avatar</code>, etc.)</p>"},{"location":"api/monitoring/account/#changereport-object","title":"ChangeReport Object","text":"<pre><code>@dataclass\nclass ChangeReport:\n    username: str                    # Account username\n    changes: List[Change]            # Detected changes\n    old_snapshot: AccountSnapshot    # Previous state\n    new_snapshot: AccountSnapshot    # Current state\n    detected_at: datetime            # When detected\n</code></pre>"},{"location":"api/monitoring/account/#change-object","title":"Change Object","text":"<pre><code>@dataclass\nclass Change:\n    field: str                       # What changed (bio, name, etc.)\n    old_value: Any                   # Previous value\n    new_value: Any                   # New value\n    changed_at: datetime             # When changed\n</code></pre>"},{"location":"api/monitoring/account/#activityreport-object","title":"ActivityReport Object","text":"<pre><code>@dataclass\nclass ActivityReport:\n    username: str                    # Account username\n    tweets_posted: int               # Tweets in period\n    replies_sent: int                # Replies sent\n    likes_given: int                 # Likes given\n    retweets_made: int               # Retweets made\n    avg_tweets_per_day: float        # Average daily tweets\n    most_active_hour: int            # Most active hour (UTC)\n    most_active_day: str             # Most active day\n    engagement_rate: float           # Engagement rate\n</code></pre>"},{"location":"api/monitoring/account/#usage-examples","title":"Usage Examples","text":""},{"location":"api/monitoring/account/#track-profile-changes","title":"Track Profile Changes","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        report = await x.monitor.track_changes(\"username\")\n\n        if report.changes:\n            print(f\"Changes detected for @{report.username}:\")\n            for change in report.changes:\n                print(f\"  {change.field}:\")\n                print(f\"    Old: {change.old_value}\")\n                print(f\"    New: {change.new_value}\")\n        else:\n            print(\"No changes detected\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/account/#monitor-account-continuously","title":"Monitor Account Continuously","text":"<pre><code>from xeepy import Xeepy\n\nasync def on_change(report):\n    if report.changes:\n        print(f\"@{report.username} changed their profile!\")\n        for change in report.changes:\n            print(f\"  - {change.field} changed\")\n\nasync def main():\n    async with Xeepy() as x:\n        await x.monitor.account(\n            \"competitor_account\",\n            callback=on_change,\n            interval=3600,  # Check every hour\n            track=[\"bio\", \"name\", \"avatar\", \"followers\"]\n        )\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/account/#get-activity-report","title":"Get Activity Report","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        activity = await x.monitor.account_activity(\"username\", days=7)\n\n        print(f\"=== 7-Day Activity Report ===\")\n        print(f\"Tweets posted: {activity.tweets_posted}\")\n        print(f\"Replies sent: {activity.replies_sent}\")\n        print(f\"Likes given: {activity.likes_given}\")\n        print(f\"Retweets: {activity.retweets_made}\")\n        print(f\"Avg tweets/day: {activity.avg_tweets_per_day:.1f}\")\n        print(f\"Most active hour: {activity.most_active_hour}:00 UTC\")\n        print(f\"Most active day: {activity.most_active_day}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/account/#monitor-bio-changes","title":"Monitor Bio Changes","text":"<pre><code>from xeepy import Xeepy\n\nasync def alert_bio_change(report):\n    for change in report.changes:\n        if change.field == \"bio\":\n            print(f\"\u26a0\ufe0f @{report.username} changed their bio!\")\n            print(f\"Old: {change.old_value}\")\n            print(f\"New: {change.new_value}\")\n\nasync def main():\n    async with Xeepy() as x:\n        await x.monitor.account(\n            \"important_account\",\n            callback=alert_bio_change,\n            interval=1800,\n            track=[\"bio\"]\n        )\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/account/#track-multiple-accounts","title":"Track Multiple Accounts","text":"<pre><code>from xeepy import Xeepy\nimport asyncio\n\nasync def monitor_multiple(accounts: list):\n    async with Xeepy() as x:\n        async def on_change(report):\n            print(f\"Change detected: @{report.username}\")\n            for change in report.changes:\n                print(f\"  {change.field}: {change.old_value} \u2192 {change.new_value}\")\n\n        tasks = []\n        for account in accounts:\n            task = asyncio.create_task(\n                x.monitor.account(\n                    account,\n                    callback=on_change,\n                    interval=3600\n                )\n            )\n            tasks.append(task)\n\n        await asyncio.gather(*tasks)\n\nasyncio.run(monitor_multiple([\"account1\", \"account2\", \"account3\"]))\n</code></pre>"},{"location":"api/monitoring/account/#export-activity-data","title":"Export Activity Data","text":"<pre><code>from xeepy import Xeepy\n\nasync def export_activity(username: str, days: int = 30):\n    async with Xeepy() as x:\n        activity = await x.monitor.account_activity(username, days=days)\n\n        data = {\n            \"username\": username,\n            \"period_days\": days,\n            \"tweets_posted\": activity.tweets_posted,\n            \"replies_sent\": activity.replies_sent,\n            \"likes_given\": activity.likes_given,\n            \"retweets_made\": activity.retweets_made,\n            \"avg_tweets_per_day\": activity.avg_tweets_per_day,\n            \"most_active_hour\": activity.most_active_hour,\n            \"most_active_day\": activity.most_active_day,\n            \"engagement_rate\": activity.engagement_rate\n        }\n\n        x.export.to_json([data], f\"activity_{username}.json\")\n        print(f\"Activity data exported\")\n\nasyncio.run(export_activity(\"myaccount\"))\n</code></pre>"},{"location":"api/monitoring/account/#detect-follower-milestones","title":"Detect Follower Milestones","text":"<pre><code>from xeepy import Xeepy\n\nasync def check_milestones(report):\n    old = report.old_snapshot.followers_count\n    new = report.new_snapshot.followers_count\n\n    milestones = [100, 500, 1000, 5000, 10000, 50000, 100000]\n\n    for milestone in milestones:\n        if old &lt; milestone &lt;= new:\n            print(f\"\ud83c\udf89 @{report.username} reached {milestone:,} followers!\")\n\nasync def main():\n    async with Xeepy() as x:\n        await x.monitor.account(\n            \"myaccount\",\n            callback=check_milestones,\n            interval=3600,\n            track=[\"followers\"]\n        )\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/account/#security-monitoring","title":"Security Monitoring","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.notifications import TelegramBot\n\nasync def security_monitor():\n    bot = TelegramBot(\"BOT_TOKEN\", \"CHAT_ID\")\n\n    async def alert(report):\n        suspicious = [\"email\", \"password\", \"phone\"]\n\n        for change in report.changes:\n            if change.field in suspicious:\n                await bot.send(\n                    f\"\ud83d\udea8 Security Alert!\\n\"\n                    f\"Account setting changed: {change.field}\\n\"\n                    f\"Time: {change.changed_at}\"\n                )\n\n    async with Xeepy() as x:\n        await x.monitor.account(\n            \"myaccount\",\n            callback=alert,\n            interval=1800\n        )\n\nasyncio.run(security_monitor())\n</code></pre>"},{"location":"api/monitoring/account/#see-also","title":"See Also","text":"<ul> <li>UnfollowersMonitor - Track unfollowers</li> <li>GrowthMonitor - Growth metrics</li> <li>SettingsActions - Account settings</li> </ul>"},{"location":"api/monitoring/growth/","title":"GrowthMonitor","text":"<p>Monitor and track account growth metrics over time.</p>"},{"location":"api/monitoring/growth/#import","title":"Import","text":"<pre><code>from xeepy.monitoring.growth import GrowthMonitor\n</code></pre>"},{"location":"api/monitoring/growth/#class-signature","title":"Class Signature","text":"<pre><code>class GrowthMonitor:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        storage: Optional[Storage] = None\n    )\n</code></pre>"},{"location":"api/monitoring/growth/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>storage</code> <code>Optional[Storage]</code> <code>None</code> Storage for tracking history"},{"location":"api/monitoring/growth/#methods","title":"Methods","text":"Method Returns Description <code>track(username)</code> <code>GrowthSnapshot</code> Record current metrics <code>get_history(username, days)</code> <code>List[GrowthSnapshot]</code> Get historical data <code>calculate_growth(username, period)</code> <code>GrowthStats</code> Calculate growth stats <code>compare(usernames)</code> <code>ComparisonReport</code> Compare multiple accounts <code>forecast(username, days)</code> <code>ForecastResult</code> Predict future growth"},{"location":"api/monitoring/growth/#track","title":"<code>track</code>","text":"<pre><code>async def track(\n    self,\n    username: Optional[str] = None\n) -&gt; GrowthSnapshot\n</code></pre> <p>Record a snapshot of current account metrics.</p>"},{"location":"api/monitoring/growth/#calculate_growth","title":"<code>calculate_growth</code>","text":"<pre><code>async def calculate_growth(\n    self,\n    username: Optional[str] = None,\n    period: str = \"7d\"\n) -&gt; GrowthStats\n</code></pre> <p>Calculate growth statistics for a period.</p> <p>Parameters: - <code>username</code>: Username to analyze - <code>period</code>: Time period (<code>1d</code>, <code>7d</code>, <code>30d</code>, <code>90d</code>, <code>1y</code>)</p>"},{"location":"api/monitoring/growth/#compare","title":"<code>compare</code>","text":"<pre><code>async def compare(\n    self,\n    usernames: List[str]\n) -&gt; ComparisonReport\n</code></pre> <p>Compare growth metrics across multiple accounts.</p>"},{"location":"api/monitoring/growth/#forecast","title":"<code>forecast</code>","text":"<pre><code>async def forecast(\n    self,\n    username: Optional[str] = None,\n    days: int = 30\n) -&gt; ForecastResult\n</code></pre> <p>Predict future growth based on historical data.</p>"},{"location":"api/monitoring/growth/#growthsnapshot-object","title":"GrowthSnapshot Object","text":"<pre><code>@dataclass\nclass GrowthSnapshot:\n    username: str                    # Account username\n    followers_count: int             # Followers at snapshot\n    following_count: int             # Following at snapshot\n    tweet_count: int                 # Tweets at snapshot\n    like_count: int                  # Likes at snapshot\n    listed_count: int                # Lists at snapshot\n    recorded_at: datetime            # Snapshot timestamp\n</code></pre>"},{"location":"api/monitoring/growth/#growthstats-object","title":"GrowthStats Object","text":"<pre><code>@dataclass\nclass GrowthStats:\n    username: str                    # Account username\n    period: str                      # Analysis period\n    start_followers: int             # Followers at start\n    end_followers: int               # Followers at end\n    followers_gained: int            # New followers\n    followers_lost: int              # Lost followers\n    net_growth: int                  # Net change\n    growth_rate: float               # Growth percentage\n    avg_daily_growth: float          # Average daily gain\n    best_day: datetime               # Day with most growth\n    worst_day: datetime              # Day with most loss\n</code></pre>"},{"location":"api/monitoring/growth/#usage-examples","title":"Usage Examples","text":""},{"location":"api/monitoring/growth/#track-current-metrics","title":"Track Current Metrics","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        snapshot = await x.monitor.track_growth()\n\n        print(f\"=== Growth Snapshot ===\")\n        print(f\"Followers: {snapshot.followers_count:,}\")\n        print(f\"Following: {snapshot.following_count:,}\")\n        print(f\"Tweets: {snapshot.tweet_count:,}\")\n        print(f\"Recorded: {snapshot.recorded_at}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/growth/#calculate-growth-stats","title":"Calculate Growth Stats","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        stats = await x.monitor.calculate_growth(period=\"30d\")\n\n        print(f\"=== 30-Day Growth Report ===\")\n        print(f\"Net growth: {stats.net_growth:+,} followers\")\n        print(f\"Growth rate: {stats.growth_rate:+.1f}%\")\n        print(f\"Avg daily: {stats.avg_daily_growth:+.1f}\")\n        print(f\"Gained: {stats.followers_gained:,}\")\n        print(f\"Lost: {stats.followers_lost:,}\")\n        print(f\"Best day: {stats.best_day}\")\n        print(f\"Worst day: {stats.worst_day}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/growth/#view-growth-history","title":"View Growth History","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        history = await x.monitor.growth_history(days=7)\n\n        print(\"Daily follower counts (last 7 days):\")\n        for snapshot in history:\n            print(f\"  {snapshot.recorded_at.date()}: {snapshot.followers_count:,}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/growth/#compare-accounts","title":"Compare Accounts","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        report = await x.monitor.compare_growth([\n            \"account1\",\n            \"account2\",\n            \"account3\"\n        ])\n\n        print(\"Account Comparison:\")\n        for account in report.accounts:\n            print(f\"\\n@{account.username}:\")\n            print(f\"  Followers: {account.followers_count:,}\")\n            print(f\"  30d growth: {account.growth_30d:+,}\")\n            print(f\"  Growth rate: {account.growth_rate:.1f}%\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/growth/#forecast-future-growth","title":"Forecast Future Growth","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        forecast = await x.monitor.forecast_growth(days=30)\n\n        print(f\"=== 30-Day Growth Forecast ===\")\n        print(f\"Current followers: {forecast.current:,}\")\n        print(f\"Predicted in 30 days: {forecast.predicted:,}\")\n        print(f\"Expected gain: {forecast.expected_gain:+,}\")\n        print(f\"Confidence: {forecast.confidence:.0%}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/growth/#weekly-growth-report","title":"Weekly Growth Report","text":"<pre><code>from xeepy import Xeepy\nfrom datetime import datetime\n\nasync def weekly_report():\n    async with Xeepy() as x:\n        stats = await x.monitor.calculate_growth(period=\"7d\")\n        history = await x.monitor.growth_history(days=7)\n\n        print(f\"=== Weekly Growth Report ({datetime.now().date()}) ===\")\n        print(f\"\\nOverview:\")\n        print(f\"  Net growth: {stats.net_growth:+,}\")\n        print(f\"  Growth rate: {stats.growth_rate:+.2f}%\")\n\n        print(f\"\\nDaily breakdown:\")\n        prev_count = None\n        for snapshot in history:\n            if prev_count is not None:\n                change = snapshot.followers_count - prev_count\n                print(f\"  {snapshot.recorded_at.date()}: {snapshot.followers_count:,} ({change:+,})\")\n            else:\n                print(f\"  {snapshot.recorded_at.date()}: {snapshot.followers_count:,}\")\n            prev_count = snapshot.followers_count\n\nasyncio.run(weekly_report())\n</code></pre>"},{"location":"api/monitoring/growth/#export-growth-data","title":"Export Growth Data","text":"<pre><code>from xeepy import Xeepy\n\nasync def export_growth_data(days: int = 90):\n    async with Xeepy() as x:\n        history = await x.monitor.growth_history(days=days)\n\n        data = [\n            {\n                \"date\": s.recorded_at.isoformat(),\n                \"followers\": s.followers_count,\n                \"following\": s.following_count,\n                \"tweets\": s.tweet_count\n            }\n            for s in history\n        ]\n\n        x.export.to_csv(data, f\"growth_data_{days}d.csv\")\n        print(f\"Exported {len(data)} data points\")\n\nasyncio.run(export_growth_data())\n</code></pre>"},{"location":"api/monitoring/growth/#milestone-tracking","title":"Milestone Tracking","text":"<pre><code>from xeepy import Xeepy\n\nasync def check_milestones(milestones: list):\n    async with Xeepy() as x:\n        snapshot = await x.monitor.track_growth()\n\n        for milestone in milestones:\n            if snapshot.followers_count &gt;= milestone:\n                print(f\"\u2713 Reached {milestone:,} followers!\")\n            else:\n                remaining = milestone - snapshot.followers_count\n                print(f\"\u25cb {milestone:,} followers ({remaining:,} to go)\")\n\nmilestones = [100, 500, 1000, 5000, 10000, 50000, 100000]\nasyncio.run(check_milestones(milestones))\n</code></pre>"},{"location":"api/monitoring/growth/#see-also","title":"See Also","text":"<ul> <li>UnfollowersMonitor - Track unfollowers</li> <li>EngagementAnalytics - Engagement metrics</li> <li>ProfileScraper - Profile data</li> </ul>"},{"location":"api/monitoring/keywords/","title":"KeywordsMonitor","text":"<p>Monitor X/Twitter for specific keywords and phrases in real-time.</p>"},{"location":"api/monitoring/keywords/#import","title":"Import","text":"<pre><code>from xeepy.monitoring.keywords import KeywordsMonitor\n</code></pre>"},{"location":"api/monitoring/keywords/#class-signature","title":"Class Signature","text":"<pre><code>class KeywordsMonitor:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/monitoring/keywords/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/monitoring/keywords/#methods","title":"Methods","text":"Method Returns Description <code>monitor(keywords, callback)</code> <code>None</code> Start keyword monitoring <code>stop()</code> <code>None</code> Stop monitoring <code>add_keyword(keyword)</code> <code>None</code> Add keyword to monitor <code>remove_keyword(keyword)</code> <code>None</code> Remove keyword <code>get_matches(keyword, limit)</code> <code>List[Tweet]</code> Get recent matches <code>get_stats()</code> <code>MonitorStats</code> Get monitoring statistics"},{"location":"api/monitoring/keywords/#monitor","title":"<code>monitor</code>","text":"<pre><code>async def monitor(\n    self,\n    keywords: List[str],\n    callback: Callable[[Tweet], Awaitable[None]],\n    interval: int = 60,\n    min_likes: int = 0,\n    language: Optional[str] = None\n) -&gt; None\n</code></pre> <p>Start monitoring for keywords.</p> <p>Parameters: - <code>keywords</code>: Keywords to monitor - <code>callback</code>: Async function called for each match - <code>interval</code>: Check interval in seconds - <code>min_likes</code>: Minimum likes filter - <code>language</code>: Language filter</p>"},{"location":"api/monitoring/keywords/#get_matches","title":"<code>get_matches</code>","text":"<pre><code>async def get_matches(\n    self,\n    keyword: str,\n    limit: int = 50,\n    since: Optional[datetime] = None\n) -&gt; List[Tweet]\n</code></pre> <p>Get recent tweets matching a keyword.</p>"},{"location":"api/monitoring/keywords/#get_stats","title":"<code>get_stats</code>","text":"<pre><code>def get_stats(self) -&gt; MonitorStats\n</code></pre> <p>Get monitoring statistics.</p>"},{"location":"api/monitoring/keywords/#monitorstats-object","title":"MonitorStats Object","text":"<pre><code>@dataclass\nclass MonitorStats:\n    keywords: List[str]              # Active keywords\n    total_matches: int               # Total matches found\n    matches_per_keyword: Dict[str, int]  # Matches by keyword\n    running_since: datetime          # When monitoring started\n    last_check: datetime             # Last check time\n</code></pre>"},{"location":"api/monitoring/keywords/#usage-examples","title":"Usage Examples","text":""},{"location":"api/monitoring/keywords/#basic-keyword-monitoring","title":"Basic Keyword Monitoring","text":"<pre><code>from xeepy import Xeepy\n\nasync def on_match(tweet):\n    print(f\"Match found!\")\n    print(f\"@{tweet.author.username}: {tweet.text[:100]}...\")\n    print(f\"Likes: {tweet.like_count}\")\n    print(\"-\" * 50)\n\nasync def main():\n    async with Xeepy() as x:\n        await x.monitor.keywords(\n            keywords=[\"#python\", \"python programming\"],\n            callback=on_match,\n            interval=60  # Check every minute\n        )\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/keywords/#monitor-brand-mentions","title":"Monitor Brand Mentions","text":"<pre><code>from xeepy import Xeepy\n\nasync def handle_mention(tweet):\n    print(f\"Brand mention detected!\")\n    print(f\"User: @{tweet.author.username}\")\n    print(f\"Text: {tweet.text}\")\n\n    # Could auto-like or respond\n    # await x.engage.like(f\"https://x.com/{tweet.author.username}/status/{tweet.id}\")\n\nasync def main():\n    async with Xeepy() as x:\n        await x.monitor.keywords(\n            keywords=[\"@mybrand\", \"MyBrand\", \"#mybrand\"],\n            callback=handle_mention,\n            interval=120\n        )\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/keywords/#filter-by-engagement","title":"Filter by Engagement","text":"<pre><code>from xeepy import Xeepy\n\nasync def on_viral_match(tweet):\n    print(f\"Viral tweet found!\")\n    print(f\"Likes: {tweet.like_count:,}\")\n    print(f\"@{tweet.author.username}: {tweet.text[:100]}...\")\n\nasync def main():\n    async with Xeepy() as x:\n        await x.monitor.keywords(\n            keywords=[\"trending topic\", \"#viral\"],\n            callback=on_viral_match,\n            interval=300,\n            min_likes=1000  # Only tweets with 1000+ likes\n        )\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/keywords/#monitor-with-language-filter","title":"Monitor with Language Filter","text":"<pre><code>from xeepy import Xeepy\n\nasync def on_match(tweet):\n    print(f\"Match: {tweet.text[:80]}...\")\n\nasync def main():\n    async with Xeepy() as x:\n        await x.monitor.keywords(\n            keywords=[\"Python\", \"Django\", \"FastAPI\"],\n            callback=on_match,\n            language=\"en\"  # English only\n        )\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/keywords/#get-recent-matches","title":"Get Recent Matches","text":"<pre><code>from xeepy import Xeepy\nfrom datetime import datetime, timedelta\n\nasync def main():\n    async with Xeepy() as x:\n        # Get matches from last hour\n        matches = await x.monitor.get_keyword_matches(\n            \"#python\",\n            limit=100,\n            since=datetime.now() - timedelta(hours=1)\n        )\n\n        print(f\"Found {len(matches)} tweets in last hour\")\n        for tweet in matches[:10]:\n            print(f\"@{tweet.author.username}: {tweet.text[:60]}...\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/keywords/#monitor-with-notifications","title":"Monitor with Notifications","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.notifications import DiscordWebhook\n\nasync def main():\n    webhook = DiscordWebhook(\"https://discord.com/api/webhooks/...\")\n\n    async def notify(tweet):\n        message = (\n            f\"**Keyword Match Found!**\\n\"\n            f\"User: @{tweet.author.username}\\n\"\n            f\"Text: {tweet.text[:200]}\\n\"\n            f\"Likes: {tweet.like_count}\"\n        )\n        await webhook.send(message)\n\n    async with Xeepy() as x:\n        await x.monitor.keywords(\n            keywords=[\"@mycompany\", \"my product name\"],\n            callback=notify,\n            interval=300\n        )\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/keywords/#dynamic-keyword-management","title":"Dynamic Keyword Management","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Start with initial keywords\n        monitor = x.monitor.keyword_monitor\n\n        await monitor.start([\"python\", \"javascript\"])\n\n        # Add more keywords later\n        monitor.add_keyword(\"rust\")\n        monitor.add_keyword(\"golang\")\n\n        # Remove a keyword\n        monitor.remove_keyword(\"javascript\")\n\n        # Get stats\n        stats = monitor.get_stats()\n        print(f\"Monitoring: {stats.keywords}\")\n        print(f\"Total matches: {stats.total_matches}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/keywords/#competition-monitoring","title":"Competition Monitoring","text":"<pre><code>from xeepy import Xeepy\nimport json\n\nasync def monitor_competitors(competitors: list):\n    matches = {}\n\n    async def track_match(tweet):\n        for comp in competitors:\n            if comp.lower() in tweet.text.lower():\n                if comp not in matches:\n                    matches[comp] = []\n                matches[comp].append({\n                    \"text\": tweet.text,\n                    \"author\": tweet.author.username,\n                    \"likes\": tweet.like_count,\n                    \"time\": tweet.created_at.isoformat()\n                })\n\n    async with Xeepy() as x:\n        # Monitor for 1 hour\n        import asyncio\n\n        task = asyncio.create_task(\n            x.monitor.keywords(\n                keywords=competitors,\n                callback=track_match,\n                interval=120\n            )\n        )\n\n        await asyncio.sleep(3600)  # 1 hour\n        task.cancel()\n\n        # Save results\n        with open(\"competitor_mentions.json\", \"w\") as f:\n            json.dump(matches, f, indent=2)\n\nasyncio.run(monitor_competitors([\"Competitor1\", \"Competitor2\"]))\n</code></pre>"},{"location":"api/monitoring/keywords/#sentiment-based-alerts","title":"Sentiment-Based Alerts","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import SentimentAnalyzer\n\nasync def main():\n    analyzer = SentimentAnalyzer()\n\n    async def check_sentiment(tweet):\n        sentiment = await analyzer.analyze(tweet.text)\n\n        if sentiment.score &lt; -0.5:  # Negative sentiment\n            print(f\"\u26a0\ufe0f Negative mention detected!\")\n            print(f\"@{tweet.author.username}: {tweet.text[:100]}...\")\n            print(f\"Sentiment: {sentiment.score:.2f}\")\n\n    async with Xeepy() as x:\n        await x.monitor.keywords(\n            keywords=[\"@mybrand\"],\n            callback=check_sentiment,\n            interval=300\n        )\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/keywords/#see-also","title":"See Also","text":"<ul> <li>SearchScraper - Search functionality</li> <li>MentionsScraper - Mentions scraping</li> <li>SentimentAnalyzer - Sentiment analysis</li> </ul>"},{"location":"api/monitoring/unfollowers/","title":"UnfollowersMonitor","text":"<p>Monitor and detect users who have unfollowed you.</p>"},{"location":"api/monitoring/unfollowers/#import","title":"Import","text":"<pre><code>from xeepy.monitoring.unfollowers import UnfollowersMonitor\n</code></pre>"},{"location":"api/monitoring/unfollowers/#class-signature","title":"Class Signature","text":"<pre><code>class UnfollowersMonitor:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        storage: Optional[Storage] = None\n    )\n</code></pre>"},{"location":"api/monitoring/unfollowers/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>storage</code> <code>Optional[Storage]</code> <code>None</code> Storage for tracking history"},{"location":"api/monitoring/unfollowers/#methods","title":"Methods","text":"Method Returns Description <code>check()</code> <code>UnfollowReport</code> Check for unfollowers <code>get_history()</code> <code>List[UnfollowEvent]</code> Get unfollow history <code>start_monitoring(interval)</code> <code>None</code> Start continuous monitoring <code>stop_monitoring()</code> <code>None</code> Stop monitoring <code>export_report(path)</code> <code>None</code> Export report to file"},{"location":"api/monitoring/unfollowers/#check","title":"<code>check</code>","text":"<pre><code>async def check(\n    self,\n    username: Optional[str] = None\n) -&gt; UnfollowReport\n</code></pre> <p>Check for new unfollowers since last check.</p> <p>Parameters: - <code>username</code>: Username to check (default: logged-in user)</p>"},{"location":"api/monitoring/unfollowers/#get_history","title":"<code>get_history</code>","text":"<pre><code>async def get_history(\n    self,\n    days: int = 30\n) -&gt; List[UnfollowEvent]\n</code></pre> <p>Get unfollow history for the specified period.</p>"},{"location":"api/monitoring/unfollowers/#start_monitoring","title":"<code>start_monitoring</code>","text":"<pre><code>async def start_monitoring(\n    self,\n    interval: int = 3600,\n    callback: Optional[Callable] = None\n) -&gt; None\n</code></pre> <p>Start continuous monitoring for unfollowers.</p> <p>Parameters: - <code>interval</code>: Check interval in seconds (default: 1 hour) - <code>callback</code>: Function to call when unfollowers detected</p>"},{"location":"api/monitoring/unfollowers/#unfollowreport-object","title":"UnfollowReport Object","text":"<pre><code>@dataclass\nclass UnfollowReport:\n    unfollowers: List[User]          # Users who unfollowed\n    new_followers: List[User]        # New followers\n    total_followers: int             # Current follower count\n    previous_followers: int          # Previous count\n    net_change: int                  # Net change\n    checked_at: datetime             # Check timestamp\n</code></pre>"},{"location":"api/monitoring/unfollowers/#unfollowevent-object","title":"UnfollowEvent Object","text":"<pre><code>@dataclass\nclass UnfollowEvent:\n    user: User                       # User who unfollowed\n    detected_at: datetime            # When detected\n    was_following_back: bool         # If you were following them\n</code></pre>"},{"location":"api/monitoring/unfollowers/#usage-examples","title":"Usage Examples","text":""},{"location":"api/monitoring/unfollowers/#basic-unfollower-check","title":"Basic Unfollower Check","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        report = await x.monitor.unfollowers()\n\n        print(f\"Current followers: {report.total_followers}\")\n        print(f\"Net change: {report.net_change:+d}\")\n\n        if report.unfollowers:\n            print(f\"\\n{len(report.unfollowers)} users unfollowed you:\")\n            for user in report.unfollowers:\n                print(f\"  - @{user.username} ({user.followers_count:,} followers)\")\n\n        if report.new_followers:\n            print(f\"\\n{len(report.new_followers)} new followers:\")\n            for user in report.new_followers:\n                print(f\"  + @{user.username}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/unfollowers/#continuous-monitoring","title":"Continuous Monitoring","text":"<pre><code>from xeepy import Xeepy\nimport asyncio\n\nasync def on_unfollow(report):\n    \"\"\"Callback when unfollowers detected.\"\"\"\n    if report.unfollowers:\n        print(f\"Alert! {len(report.unfollowers)} unfollowed you:\")\n        for user in report.unfollowers:\n            print(f\"  @{user.username}\")\n\nasync def main():\n    async with Xeepy() as x:\n        # Start monitoring (checks every hour)\n        await x.monitor.start_unfollower_monitoring(\n            interval=3600,\n            callback=on_unfollow\n        )\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/unfollowers/#get-unfollow-history","title":"Get Unfollow History","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        history = await x.monitor.unfollower_history(days=30)\n\n        print(f\"Unfollow events in last 30 days: {len(history)}\")\n\n        for event in history[-10:]:\n            print(f\"  {event.detected_at}: @{event.user.username}\")\n            if event.was_following_back:\n                print(f\"    (You were following them)\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/unfollowers/#export-report","title":"Export Report","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        report = await x.monitor.unfollowers()\n\n        # Export to CSV\n        data = [\n            {\n                \"username\": u.username,\n                \"name\": u.name,\n                \"followers\": u.followers_count,\n                \"following\": u.following_count,\n                \"verified\": u.is_verified\n            }\n            for u in report.unfollowers\n        ]\n\n        x.export.to_csv(data, \"unfollowers.csv\")\n        print(f\"Exported {len(data)} unfollowers to unfollowers.csv\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/monitoring/unfollowers/#daily-unfollower-report","title":"Daily Unfollower Report","text":"<pre><code>from xeepy import Xeepy\nfrom datetime import datetime\n\nasync def daily_report():\n    async with Xeepy() as x:\n        report = await x.monitor.unfollowers()\n\n        print(f\"=== Daily Follower Report ({datetime.now().date()}) ===\")\n        print(f\"Total followers: {report.total_followers:,}\")\n        print(f\"Change: {report.net_change:+d}\")\n        print(f\"New followers: {len(report.new_followers)}\")\n        print(f\"Unfollowers: {len(report.unfollowers)}\")\n\n        if report.unfollowers:\n            print(\"\\nUnfollowers:\")\n            for user in report.unfollowers:\n                print(f\"  @{user.username}\")\n\n        if report.new_followers:\n            print(\"\\nNew followers:\")\n            for user in report.new_followers[:5]:\n                print(f\"  @{user.username}\")\n            if len(report.new_followers) &gt; 5:\n                print(f\"  ... and {len(report.new_followers) - 5} more\")\n\nasyncio.run(daily_report())\n</code></pre>"},{"location":"api/monitoring/unfollowers/#unfollower-notification","title":"Unfollower Notification","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.notifications import DiscordWebhook\n\nasync def monitor_with_notifications():\n    webhook = DiscordWebhook(\"https://discord.com/api/webhooks/...\")\n\n    async def notify(report):\n        if report.unfollowers:\n            message = f\"\ud83d\udd14 {len(report.unfollowers)} users unfollowed you:\\n\"\n            for user in report.unfollowers[:10]:\n                message += f\"\u2022 @{user.username}\\n\"\n            await webhook.send(message)\n\n    async with Xeepy() as x:\n        await x.monitor.start_unfollower_monitoring(\n            interval=3600,\n            callback=notify\n        )\n\nasyncio.run(monitor_with_notifications())\n</code></pre>"},{"location":"api/monitoring/unfollowers/#track-influential-unfollowers","title":"Track Influential Unfollowers","text":"<pre><code>from xeepy import Xeepy\n\nasync def track_influential_unfollowers(min_followers: int = 10000):\n    async with Xeepy() as x:\n        report = await x.monitor.unfollowers()\n\n        influential = [\n            u for u in report.unfollowers\n            if u.followers_count &gt;= min_followers\n        ]\n\n        if influential:\n            print(f\"\u26a0\ufe0f Influential accounts that unfollowed you:\")\n            for user in influential:\n                print(f\"  @{user.username} - {user.followers_count:,} followers\")\n\nasyncio.run(track_influential_unfollowers())\n</code></pre>"},{"location":"api/monitoring/unfollowers/#see-also","title":"See Also","text":"<ul> <li>GrowthMonitor - Growth analytics</li> <li>FollowersScraper - Followers scraping</li> <li>UnfollowActions - Unfollow operations</li> </ul>"},{"location":"api/notifications/discord/","title":"DiscordNotifier","text":"<p>Send notifications to Discord channels via webhooks.</p>"},{"location":"api/notifications/discord/#import","title":"Import","text":"<pre><code>from xeepy.notifications import DiscordNotifier\n</code></pre>"},{"location":"api/notifications/discord/#class-signature","title":"Class Signature","text":"<pre><code>class DiscordNotifier:\n    def __init__(\n        self,\n        webhook_url: str,\n        username: str = \"Xeepy Bot\",\n        avatar_url: Optional[str] = None\n    )\n</code></pre>"},{"location":"api/notifications/discord/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>webhook_url</code> <code>str</code> Required Discord webhook URL <code>username</code> <code>str</code> <code>\"Xeepy Bot\"</code> Bot display name <code>avatar_url</code> <code>Optional[str]</code> <code>None</code> Bot avatar URL"},{"location":"api/notifications/discord/#methods","title":"Methods","text":"Method Returns Description <code>send(message)</code> <code>bool</code> Send text message <code>send_embed(embed)</code> <code>bool</code> Send rich embed <code>notify_unfollowers(users)</code> <code>bool</code> Report unfollowers <code>notify_new_followers(users)</code> <code>bool</code> Report new followers <code>notify_mention(tweet)</code> <code>bool</code> Report mention <code>notify_milestone(metric, value)</code> <code>bool</code> Report milestone"},{"location":"api/notifications/discord/#send","title":"<code>send</code>","text":"<pre><code>async def send(\n    self,\n    message: str\n) -&gt; bool\n</code></pre> <p>Send a plain text message.</p>"},{"location":"api/notifications/discord/#send_embed","title":"<code>send_embed</code>","text":"<pre><code>async def send_embed(\n    self,\n    title: str,\n    description: str = \"\",\n    color: int = 0x1DA1F2,\n    fields: List[Dict] = None,\n    thumbnail: str = None,\n    footer: str = None\n) -&gt; bool\n</code></pre> <p>Send a rich embed message.</p>"},{"location":"api/notifications/discord/#notify_unfollowers","title":"<code>notify_unfollowers</code>","text":"<pre><code>async def notify_unfollowers(\n    self,\n    users: List[User],\n    account: str = None\n) -&gt; bool\n</code></pre> <p>Send unfollower notification with user details.</p>"},{"location":"api/notifications/discord/#usage-examples","title":"Usage Examples","text":""},{"location":"api/notifications/discord/#basic-setup","title":"Basic Setup","text":"<pre><code>from xeepy.notifications import DiscordNotifier\n\nnotifier = DiscordNotifier(\n    webhook_url=\"https://discord.com/api/webhooks/...\",\n    username=\"Twitter Bot\",\n    avatar_url=\"https://example.com/avatar.png\"\n)\n</code></pre>"},{"location":"api/notifications/discord/#send-simple-message","title":"Send Simple Message","text":"<pre><code>from xeepy.notifications import DiscordNotifier\n\nasync def main():\n    notifier = DiscordNotifier(webhook_url=\"https://discord.com/api/webhooks/...\")\n\n    await notifier.send(\"Hello from Xeepy! \ud83d\udc4b\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/notifications/discord/#send-rich-embed","title":"Send Rich Embed","text":"<pre><code>from xeepy.notifications import DiscordNotifier\n\nasync def main():\n    notifier = DiscordNotifier(webhook_url=\"https://discord.com/api/webhooks/...\")\n\n    await notifier.send_embed(\n        title=\"\ud83d\udcca Daily Report\",\n        description=\"Here's your Twitter activity summary\",\n        color=0x1DA1F2,  # Twitter blue\n        fields=[\n            {\"name\": \"New Followers\", \"value\": \"+15\", \"inline\": True},\n            {\"name\": \"Unfollowers\", \"value\": \"-3\", \"inline\": True},\n            {\"name\": \"Net Change\", \"value\": \"+12\", \"inline\": True},\n            {\"name\": \"Total Followers\", \"value\": \"10,532\", \"inline\": False}\n        ],\n        footer=\"Xeepy Bot \u2022 Updated just now\"\n    )\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/notifications/discord/#notify-on-unfollowers","title":"Notify on Unfollowers","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.notifications import DiscordNotifier\n\nasync def monitor_unfollowers():\n    notifier = DiscordNotifier(webhook_url=\"https://discord.com/api/webhooks/...\")\n\n    async with Xeepy() as x:\n        report = await x.monitor.unfollowers()\n\n        if report.unfollowers:\n            await notifier.notify_unfollowers(\n                users=report.unfollowers,\n                account=\"myaccount\"\n            )\n            print(f\"Notified about {len(report.unfollowers)} unfollowers\")\n\nasyncio.run(monitor_unfollowers())\n</code></pre>"},{"location":"api/notifications/discord/#notify-on-new-followers","title":"Notify on New Followers","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.notifications import DiscordNotifier\n\nasync def notify_new_followers():\n    notifier = DiscordNotifier(webhook_url=\"https://discord.com/api/webhooks/...\")\n\n    async with Xeepy() as x:\n        report = await x.monitor.unfollowers()\n\n        if report.new_followers:\n            await notifier.notify_new_followers(\n                users=report.new_followers,\n                account=\"myaccount\"\n            )\n\nasyncio.run(notify_new_followers())\n</code></pre>"},{"location":"api/notifications/discord/#notify-on-mentions","title":"Notify on Mentions","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.notifications import DiscordNotifier\n\nasync def monitor_mentions():\n    notifier = DiscordNotifier(webhook_url=\"https://discord.com/api/webhooks/...\")\n\n    async with Xeepy() as x:\n        mentions = await x.scrape.mentions(limit=10)\n\n        for tweet in mentions.items:\n            await notifier.notify_mention(tweet)\n            print(f\"Notified about mention from @{tweet.author.username}\")\n\nasyncio.run(monitor_mentions())\n</code></pre>"},{"location":"api/notifications/discord/#milestone-notifications","title":"Milestone Notifications","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.notifications import DiscordNotifier\n\nasync def check_milestones():\n    notifier = DiscordNotifier(webhook_url=\"https://discord.com/api/webhooks/...\")\n\n    milestones = [100, 500, 1000, 5000, 10000, 50000, 100000]\n\n    async with Xeepy() as x:\n        user = await x.scrape.profile(\"myaccount\")\n\n        for milestone in milestones:\n            if user.followers_count &gt;= milestone:\n                # Check if we just crossed it\n                await notifier.notify_milestone(\n                    metric=\"followers\",\n                    value=milestone\n                )\n                break\n\nasyncio.run(check_milestones())\n</code></pre>"},{"location":"api/notifications/discord/#custom-embed-colors","title":"Custom Embed Colors","text":"<pre><code>from xeepy.notifications import DiscordNotifier\n\nasync def colored_notifications():\n    notifier = DiscordNotifier(webhook_url=\"https://discord.com/api/webhooks/...\")\n\n    # Success (green)\n    await notifier.send_embed(\n        title=\"\u2705 Success\",\n        description=\"Operation completed\",\n        color=0x00FF00\n    )\n\n    # Warning (yellow)\n    await notifier.send_embed(\n        title=\"\u26a0\ufe0f Warning\",\n        description=\"Rate limit approaching\",\n        color=0xFFFF00\n    )\n\n    # Error (red)\n    await notifier.send_embed(\n        title=\"\u274c Error\",\n        description=\"Authentication failed\",\n        color=0xFF0000\n    )\n\nasyncio.run(colored_notifications())\n</code></pre>"},{"location":"api/notifications/discord/#scheduled-daily-report","title":"Scheduled Daily Report","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.notifications import DiscordNotifier\nimport asyncio\n\nasync def daily_report():\n    notifier = DiscordNotifier(webhook_url=\"https://discord.com/api/webhooks/...\")\n\n    while True:\n        async with Xeepy() as x:\n            user = await x.scrape.profile(\"myaccount\")\n            report = await x.monitor.unfollowers()\n\n            await notifier.send_embed(\n                title=\"\ud83d\udcca Daily Twitter Report\",\n                description=f\"Stats for @{user.username}\",\n                fields=[\n                    {\"name\": \"Followers\", \"value\": f\"{user.followers_count:,}\", \"inline\": True},\n                    {\"name\": \"Following\", \"value\": f\"{user.following_count:,}\", \"inline\": True},\n                    {\"name\": \"Tweets\", \"value\": f\"{user.tweet_count:,}\", \"inline\": True},\n                    {\"name\": \"New Followers\", \"value\": f\"+{len(report.new_followers)}\", \"inline\": True},\n                    {\"name\": \"Unfollowers\", \"value\": f\"-{len(report.unfollowers)}\", \"inline\": True}\n                ]\n            )\n\n        # Wait 24 hours\n        await asyncio.sleep(86400)\n\nasyncio.run(daily_report())\n</code></pre>"},{"location":"api/notifications/discord/#integrate-with-monitoring","title":"Integrate with Monitoring","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.notifications import DiscordNotifier\n\nasync def full_monitoring():\n    notifier = DiscordNotifier(webhook_url=\"https://discord.com/api/webhooks/...\")\n\n    async with Xeepy() as x:\n        async def on_unfollow(user):\n            await notifier.send_embed(\n                title=\"\ud83d\udc4b Unfollower\",\n                description=f\"@{user.username} unfollowed you\",\n                color=0xFF6B6B,\n                fields=[\n                    {\"name\": \"Followers\", \"value\": str(user.followers_count), \"inline\": True}\n                ]\n            )\n\n        async def on_follow(user):\n            await notifier.send_embed(\n                title=\"\ud83c\udf89 New Follower\",\n                description=f\"@{user.username} followed you!\",\n                color=0x4ECDC4,\n                fields=[\n                    {\"name\": \"Followers\", \"value\": str(user.followers_count), \"inline\": True}\n                ]\n            )\n\n        await x.monitor.unfollowers(\n            on_unfollow=on_unfollow,\n            on_follow=on_follow,\n            interval=300\n        )\n\nasyncio.run(full_monitoring())\n</code></pre>"},{"location":"api/notifications/discord/#see-also","title":"See Also","text":"<ul> <li>TelegramNotifier - Telegram notifications</li> <li>EmailNotifier - Email notifications</li> <li>UnfollowersMonitor - Unfollower tracking</li> </ul>"},{"location":"api/notifications/email/","title":"EmailNotifier","text":"<p>Send email notifications via SMTP.</p>"},{"location":"api/notifications/email/#import","title":"Import","text":"<pre><code>from xeepy.notifications import EmailNotifier\n</code></pre>"},{"location":"api/notifications/email/#class-signature","title":"Class Signature","text":"<pre><code>class EmailNotifier:\n    def __init__(\n        self,\n        smtp_host: str,\n        smtp_port: int = 587,\n        username: str = None,\n        password: str = None,\n        from_email: str = None,\n        use_tls: bool = True\n    )\n</code></pre>"},{"location":"api/notifications/email/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>smtp_host</code> <code>str</code> Required SMTP server hostname <code>smtp_port</code> <code>int</code> <code>587</code> SMTP port <code>username</code> <code>str</code> <code>None</code> SMTP username <code>password</code> <code>str</code> <code>None</code> SMTP password/app password <code>from_email</code> <code>str</code> <code>None</code> Sender email address <code>use_tls</code> <code>bool</code> <code>True</code> Use TLS encryption"},{"location":"api/notifications/email/#methods","title":"Methods","text":"Method Returns Description <code>send(to, subject, body)</code> <code>bool</code> Send email <code>send_html(to, subject, html)</code> <code>bool</code> Send HTML email <code>notify_unfollowers(to, users)</code> <code>bool</code> Report unfollowers <code>notify_report(to, report)</code> <code>bool</code> Send analytics report"},{"location":"api/notifications/email/#send","title":"<code>send</code>","text":"<pre><code>async def send(\n    self,\n    to: Union[str, List[str]],\n    subject: str,\n    body: str\n) -&gt; bool\n</code></pre> <p>Send a plain text email.</p>"},{"location":"api/notifications/email/#send_html","title":"<code>send_html</code>","text":"<pre><code>async def send_html(\n    self,\n    to: Union[str, List[str]],\n    subject: str,\n    html: str,\n    text_fallback: str = None\n) -&gt; bool\n</code></pre> <p>Send an HTML email with optional text fallback.</p>"},{"location":"api/notifications/email/#common-smtp-settings","title":"Common SMTP Settings","text":"Provider Host Port Notes Gmail smtp.gmail.com 587 Requires App Password Outlook smtp.office365.com 587 Yahoo smtp.mail.yahoo.com 587 Requires App Password SendGrid smtp.sendgrid.net 587 Use API key as password"},{"location":"api/notifications/email/#usage-examples","title":"Usage Examples","text":""},{"location":"api/notifications/email/#basic-setup","title":"Basic Setup","text":"<pre><code>from xeepy.notifications import EmailNotifier\n\n# Gmail setup (requires App Password)\nnotifier = EmailNotifier(\n    smtp_host=\"smtp.gmail.com\",\n    smtp_port=587,\n    username=\"your-email@gmail.com\",\n    password=\"your-app-password\",\n    from_email=\"your-email@gmail.com\"\n)\n</code></pre>"},{"location":"api/notifications/email/#send-simple-email","title":"Send Simple Email","text":"<pre><code>from xeepy.notifications import EmailNotifier\n\nasync def main():\n    notifier = EmailNotifier(\n        smtp_host=\"smtp.gmail.com\",\n        username=\"your-email@gmail.com\",\n        password=\"your-app-password\",\n        from_email=\"your-email@gmail.com\"\n    )\n\n    await notifier.send(\n        to=\"recipient@example.com\",\n        subject=\"Test from Xeepy\",\n        body=\"Hello! This is a test email from Xeepy.\"\n    )\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/notifications/email/#send-to-multiple-recipients","title":"Send to Multiple Recipients","text":"<pre><code>from xeepy.notifications import EmailNotifier\n\nasync def main():\n    notifier = EmailNotifier(\n        smtp_host=\"smtp.gmail.com\",\n        username=\"your-email@gmail.com\",\n        password=\"your-app-password\",\n        from_email=\"your-email@gmail.com\"\n    )\n\n    await notifier.send(\n        to=[\"user1@example.com\", \"user2@example.com\"],\n        subject=\"Weekly Report\",\n        body=\"Here's your weekly Twitter analytics report.\"\n    )\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/notifications/email/#send-html-email","title":"Send HTML Email","text":"<pre><code>from xeepy.notifications import EmailNotifier\n\nasync def main():\n    notifier = EmailNotifier(\n        smtp_host=\"smtp.gmail.com\",\n        username=\"your-email@gmail.com\",\n        password=\"your-app-password\",\n        from_email=\"your-email@gmail.com\"\n    )\n\n    html = \"\"\"\n    &lt;html&gt;\n    &lt;body&gt;\n        &lt;h1&gt;\ud83d\udcca Twitter Daily Report&lt;/h1&gt;\n        &lt;table border=\"1\" cellpadding=\"10\"&gt;\n            &lt;tr&gt;\n                &lt;th&gt;Metric&lt;/th&gt;\n                &lt;th&gt;Value&lt;/th&gt;\n                &lt;th&gt;Change&lt;/th&gt;\n            &lt;/tr&gt;\n            &lt;tr&gt;\n                &lt;td&gt;Followers&lt;/td&gt;\n                &lt;td&gt;10,532&lt;/td&gt;\n                &lt;td style=\"color: green;\"&gt;+15&lt;/td&gt;\n            &lt;/tr&gt;\n            &lt;tr&gt;\n                &lt;td&gt;Following&lt;/td&gt;\n                &lt;td&gt;892&lt;/td&gt;\n                &lt;td&gt;0&lt;/td&gt;\n            &lt;/tr&gt;\n        &lt;/table&gt;\n        &lt;p&gt;&lt;i&gt;Generated by Xeepy&lt;/i&gt;&lt;/p&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n    \"\"\"\n\n    await notifier.send_html(\n        to=\"recipient@example.com\",\n        subject=\"Daily Twitter Report\",\n        html=html,\n        text_fallback=\"Followers: 10,532 (+15)\"\n    )\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/notifications/email/#notify-on-unfollowers","title":"Notify on Unfollowers","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.notifications import EmailNotifier\n\nasync def email_unfollowers():\n    notifier = EmailNotifier(\n        smtp_host=\"smtp.gmail.com\",\n        username=\"your-email@gmail.com\",\n        password=\"your-app-password\",\n        from_email=\"your-email@gmail.com\"\n    )\n\n    async with Xeepy() as x:\n        report = await x.monitor.unfollowers()\n\n        if report.unfollowers:\n            await notifier.notify_unfollowers(\n                to=\"your-email@example.com\",\n                users=report.unfollowers\n            )\n\nasyncio.run(email_unfollowers())\n</code></pre>"},{"location":"api/notifications/email/#weekly-analytics-report","title":"Weekly Analytics Report","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.notifications import EmailNotifier\n\nasync def weekly_report():\n    notifier = EmailNotifier(\n        smtp_host=\"smtp.gmail.com\",\n        username=\"your-email@gmail.com\",\n        password=\"your-app-password\",\n        from_email=\"your-email@gmail.com\"\n    )\n\n    async with Xeepy() as x:\n        user = await x.scrape.profile(\"myaccount\")\n        engagement = await x.analytics.engagement(\"myaccount\", period=\"7d\")\n\n        html = f\"\"\"\n        &lt;html&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Weekly Report: @{user.username}&lt;/h1&gt;\n\n            &lt;h2&gt;Account Stats&lt;/h2&gt;\n            &lt;ul&gt;\n                &lt;li&gt;Followers: {user.followers_count:,}&lt;/li&gt;\n                &lt;li&gt;Following: {user.following_count:,}&lt;/li&gt;\n                &lt;li&gt;Tweets: {user.tweet_count:,}&lt;/li&gt;\n            &lt;/ul&gt;\n\n            &lt;h2&gt;Engagement (7 days)&lt;/h2&gt;\n            &lt;ul&gt;\n                &lt;li&gt;Total likes: {engagement.total_likes:,}&lt;/li&gt;\n                &lt;li&gt;Total retweets: {engagement.total_retweets:,}&lt;/li&gt;\n                &lt;li&gt;Engagement rate: {engagement.engagement_rate:.2f}%&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n        \"\"\"\n\n        await notifier.send_html(\n            to=\"your-email@example.com\",\n            subject=f\"Weekly Twitter Report - @{user.username}\",\n            html=html\n        )\n\nasyncio.run(weekly_report())\n</code></pre>"},{"location":"api/notifications/email/#sendgrid-setup","title":"SendGrid Setup","text":"<pre><code>from xeepy.notifications import EmailNotifier\n\n# Using SendGrid\nnotifier = EmailNotifier(\n    smtp_host=\"smtp.sendgrid.net\",\n    smtp_port=587,\n    username=\"apikey\",  # Always \"apikey\" for SendGrid\n    password=\"SG.your-api-key-here\",\n    from_email=\"your-verified-sender@example.com\"\n)\n</code></pre>"},{"location":"api/notifications/email/#environment-variables","title":"Environment Variables","text":"<pre><code>import os\nfrom xeepy.notifications import EmailNotifier\n\nnotifier = EmailNotifier(\n    smtp_host=os.getenv(\"SMTP_HOST\"),\n    smtp_port=int(os.getenv(\"SMTP_PORT\", 587)),\n    username=os.getenv(\"SMTP_USERNAME\"),\n    password=os.getenv(\"SMTP_PASSWORD\"),\n    from_email=os.getenv(\"FROM_EMAIL\")\n)\n</code></pre>"},{"location":"api/notifications/email/#scheduled-daily-email","title":"Scheduled Daily Email","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.notifications import EmailNotifier\nimport asyncio\n\nasync def daily_email_report():\n    notifier = EmailNotifier(\n        smtp_host=\"smtp.gmail.com\",\n        username=\"your-email@gmail.com\",\n        password=\"your-app-password\",\n        from_email=\"your-email@gmail.com\"\n    )\n\n    while True:\n        async with Xeepy() as x:\n            user = await x.scrape.profile(\"myaccount\")\n            report = await x.monitor.unfollowers()\n\n            body = f\"\"\"\nDaily Twitter Report for @{user.username}\n\nStats:\n- Followers: {user.followers_count:,}\n- Following: {user.following_count:,}\n- Tweets: {user.tweet_count:,}\n\nChanges:\n- New followers: +{len(report.new_followers)}\n- Unfollowers: -{len(report.unfollowers)}\n\nGenerated by Xeepy\n\"\"\"\n\n            await notifier.send(\n                to=\"your-email@example.com\",\n                subject=f\"Daily Report: @{user.username}\",\n                body=body\n            )\n\n        # Wait 24 hours\n        await asyncio.sleep(86400)\n\nasyncio.run(daily_email_report())\n</code></pre>"},{"location":"api/notifications/email/#error-handling","title":"Error Handling","text":"<pre><code>from xeepy.notifications import EmailNotifier\n\nasync def safe_send():\n    notifier = EmailNotifier(\n        smtp_host=\"smtp.gmail.com\",\n        username=\"your-email@gmail.com\",\n        password=\"your-app-password\",\n        from_email=\"your-email@gmail.com\"\n    )\n\n    try:\n        success = await notifier.send(\n            to=\"recipient@example.com\",\n            subject=\"Test\",\n            body=\"Test message\"\n        )\n        if success:\n            print(\"Email sent!\")\n        else:\n            print(\"Failed to send\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\nasyncio.run(safe_send())\n</code></pre>"},{"location":"api/notifications/email/#see-also","title":"See Also","text":"<ul> <li>DiscordNotifier - Discord notifications</li> <li>TelegramNotifier - Telegram notifications</li> <li>UnfollowersMonitor - Unfollower tracking</li> </ul>"},{"location":"api/notifications/telegram/","title":"TelegramNotifier","text":"<p>Send notifications to Telegram chats via bot API.</p>"},{"location":"api/notifications/telegram/#import","title":"Import","text":"<pre><code>from xeepy.notifications import TelegramNotifier\n</code></pre>"},{"location":"api/notifications/telegram/#class-signature","title":"Class Signature","text":"<pre><code>class TelegramNotifier:\n    def __init__(\n        self,\n        bot_token: str,\n        chat_id: str,\n        parse_mode: str = \"HTML\"\n    )\n</code></pre>"},{"location":"api/notifications/telegram/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>bot_token</code> <code>str</code> Required Telegram bot token <code>chat_id</code> <code>str</code> Required Chat/channel ID <code>parse_mode</code> <code>str</code> <code>\"HTML\"</code> Message format (<code>HTML</code>, <code>Markdown</code>)"},{"location":"api/notifications/telegram/#methods","title":"Methods","text":"Method Returns Description <code>send(message)</code> <code>bool</code> Send text message <code>send_photo(url, caption)</code> <code>bool</code> Send photo with caption <code>notify_unfollowers(users)</code> <code>bool</code> Report unfollowers <code>notify_new_followers(users)</code> <code>bool</code> Report new followers <code>notify_mention(tweet)</code> <code>bool</code> Report mention <code>notify_milestone(metric, value)</code> <code>bool</code> Report milestone"},{"location":"api/notifications/telegram/#send","title":"<code>send</code>","text":"<pre><code>async def send(\n    self,\n    message: str,\n    disable_preview: bool = False,\n    silent: bool = False\n) -&gt; bool\n</code></pre> <p>Send a text message.</p> <p>Parameters: - <code>message</code>: Message text (supports HTML/Markdown) - <code>disable_preview</code>: Disable link previews - <code>silent</code>: Send without notification sound</p>"},{"location":"api/notifications/telegram/#send_photo","title":"<code>send_photo</code>","text":"<pre><code>async def send_photo(\n    self,\n    photo_url: str,\n    caption: str = \"\"\n) -&gt; bool\n</code></pre> <p>Send a photo with optional caption.</p>"},{"location":"api/notifications/telegram/#setup-bot","title":"Setup Bot","text":"<ol> <li>Message @BotFather on Telegram</li> <li>Create new bot: <code>/newbot</code></li> <li>Get bot token</li> <li>Get chat ID by messaging bot and checking <code>/getUpdates</code></li> </ol>"},{"location":"api/notifications/telegram/#usage-examples","title":"Usage Examples","text":""},{"location":"api/notifications/telegram/#basic-setup","title":"Basic Setup","text":"<pre><code>from xeepy.notifications import TelegramNotifier\n\nnotifier = TelegramNotifier(\n    bot_token=\"123456:ABC-DEF...\",\n    chat_id=\"987654321\"\n)\n</code></pre>"},{"location":"api/notifications/telegram/#send-simple-message","title":"Send Simple Message","text":"<pre><code>from xeepy.notifications import TelegramNotifier\n\nasync def main():\n    notifier = TelegramNotifier(\n        bot_token=\"123456:ABC-DEF...\",\n        chat_id=\"987654321\"\n    )\n\n    await notifier.send(\"Hello from Xeepy! \ud83d\udc4b\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/notifications/telegram/#send-formatted-message","title":"Send Formatted Message","text":"<pre><code>from xeepy.notifications import TelegramNotifier\n\nasync def main():\n    notifier = TelegramNotifier(\n        bot_token=\"123456:ABC-DEF...\",\n        chat_id=\"987654321\",\n        parse_mode=\"HTML\"\n    )\n\n    message = \"\"\"\n&lt;b&gt;\ud83d\udcca Daily Report&lt;/b&gt;\n\n&lt;b&gt;Followers:&lt;/b&gt; 10,532 (+15)\n&lt;b&gt;Following:&lt;/b&gt; 892\n&lt;b&gt;Tweets:&lt;/b&gt; 2,341\n\n&lt;i&gt;Updated just now&lt;/i&gt;\n\"\"\"\n\n    await notifier.send(message)\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/notifications/telegram/#markdown-format","title":"Markdown Format","text":"<pre><code>from xeepy.notifications import TelegramNotifier\n\nasync def main():\n    notifier = TelegramNotifier(\n        bot_token=\"123456:ABC-DEF...\",\n        chat_id=\"987654321\",\n        parse_mode=\"Markdown\"\n    )\n\n    message = \"\"\"\n*\ud83d\udcca Daily Report*\n\n*Followers:* 10,532 (+15)\n*Following:* 892\n*Tweets:* 2,341\n\n_Updated just now_\n\"\"\"\n\n    await notifier.send(message)\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/notifications/telegram/#notify-on-unfollowers","title":"Notify on Unfollowers","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.notifications import TelegramNotifier\n\nasync def monitor_unfollowers():\n    notifier = TelegramNotifier(\n        bot_token=\"123456:ABC-DEF...\",\n        chat_id=\"987654321\"\n    )\n\n    async with Xeepy() as x:\n        report = await x.monitor.unfollowers()\n\n        if report.unfollowers:\n            await notifier.notify_unfollowers(report.unfollowers)\n            print(f\"Notified about {len(report.unfollowers)} unfollowers\")\n\nasyncio.run(monitor_unfollowers())\n</code></pre>"},{"location":"api/notifications/telegram/#notify-on-new-followers","title":"Notify on New Followers","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.notifications import TelegramNotifier\n\nasync def notify_new_followers():\n    notifier = TelegramNotifier(\n        bot_token=\"123456:ABC-DEF...\",\n        chat_id=\"987654321\"\n    )\n\n    async with Xeepy() as x:\n        report = await x.monitor.unfollowers()\n\n        if report.new_followers:\n            await notifier.notify_new_followers(report.new_followers)\n\nasyncio.run(notify_new_followers())\n</code></pre>"},{"location":"api/notifications/telegram/#notify-on-mentions","title":"Notify on Mentions","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.notifications import TelegramNotifier\n\nasync def monitor_mentions():\n    notifier = TelegramNotifier(\n        bot_token=\"123456:ABC-DEF...\",\n        chat_id=\"987654321\"\n    )\n\n    async with Xeepy() as x:\n        mentions = await x.scrape.mentions(limit=10)\n\n        for tweet in mentions.items:\n            await notifier.notify_mention(tweet)\n\nasyncio.run(monitor_mentions())\n</code></pre>"},{"location":"api/notifications/telegram/#send-profile-photo","title":"Send Profile Photo","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.notifications import TelegramNotifier\n\nasync def send_profile():\n    notifier = TelegramNotifier(\n        bot_token=\"123456:ABC-DEF...\",\n        chat_id=\"987654321\"\n    )\n\n    async with Xeepy() as x:\n        user = await x.scrape.profile(\"username\")\n\n        await notifier.send_photo(\n            photo_url=user.profile_image_url,\n            caption=f\"@{user.username}\\n{user.followers_count:,} followers\"\n        )\n\nasyncio.run(send_profile())\n</code></pre>"},{"location":"api/notifications/telegram/#silent-notifications","title":"Silent Notifications","text":"<pre><code>from xeepy.notifications import TelegramNotifier\n\nasync def silent_update():\n    notifier = TelegramNotifier(\n        bot_token=\"123456:ABC-DEF...\",\n        chat_id=\"987654321\"\n    )\n\n    # Won't make notification sound\n    await notifier.send(\"Background update completed\", silent=True)\n\nasyncio.run(silent_update())\n</code></pre>"},{"location":"api/notifications/telegram/#scheduled-reports","title":"Scheduled Reports","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.notifications import TelegramNotifier\nimport asyncio\n\nasync def hourly_report():\n    notifier = TelegramNotifier(\n        bot_token=\"123456:ABC-DEF...\",\n        chat_id=\"987654321\"\n    )\n\n    while True:\n        async with Xeepy() as x:\n            user = await x.scrape.profile(\"myaccount\")\n\n            message = f\"\"\"\n&lt;b&gt;\u23f0 Hourly Update&lt;/b&gt;\n\n&lt;b&gt;Followers:&lt;/b&gt; {user.followers_count:,}\n&lt;b&gt;Following:&lt;/b&gt; {user.following_count:,}\n\"\"\"\n\n            await notifier.send(message, silent=True)\n\n        await asyncio.sleep(3600)\n\nasyncio.run(hourly_report())\n</code></pre>"},{"location":"api/notifications/telegram/#send-to-channel","title":"Send to Channel","text":"<pre><code>from xeepy.notifications import TelegramNotifier\n\n# For channels, use @channelname or channel ID\nnotifier = TelegramNotifier(\n    bot_token=\"123456:ABC-DEF...\",\n    chat_id=\"@mychannelname\"  # or \"-1001234567890\"\n)\n</code></pre>"},{"location":"api/notifications/telegram/#integrate-with-monitoring","title":"Integrate with Monitoring","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.notifications import TelegramNotifier\n\nasync def full_monitoring():\n    notifier = TelegramNotifier(\n        bot_token=\"123456:ABC-DEF...\",\n        chat_id=\"987654321\"\n    )\n\n    async with Xeepy() as x:\n        async def on_unfollow(user):\n            await notifier.send(\n                f\"\ud83d\udc4b &lt;b&gt;Unfollower:&lt;/b&gt; @{user.username}\\n\"\n                f\"They had {user.followers_count:,} followers\"\n            )\n\n        async def on_follow(user):\n            await notifier.send(\n                f\"\ud83c\udf89 &lt;b&gt;New follower:&lt;/b&gt; @{user.username}\\n\"\n                f\"They have {user.followers_count:,} followers\"\n            )\n\n        await x.monitor.unfollowers(\n            on_unfollow=on_unfollow,\n            on_follow=on_follow,\n            interval=300\n        )\n\nasyncio.run(full_monitoring())\n</code></pre>"},{"location":"api/notifications/telegram/#error-handling","title":"Error Handling","text":"<pre><code>from xeepy.notifications import TelegramNotifier\n\nasync def safe_send():\n    notifier = TelegramNotifier(\n        bot_token=\"123456:ABC-DEF...\",\n        chat_id=\"987654321\"\n    )\n\n    try:\n        success = await notifier.send(\"Test message\")\n        if success:\n            print(\"Message sent!\")\n        else:\n            print(\"Failed to send\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\nasyncio.run(safe_send())\n</code></pre>"},{"location":"api/notifications/telegram/#see-also","title":"See Also","text":"<ul> <li>DiscordNotifier - Discord notifications</li> <li>EmailNotifier - Email notifications</li> <li>UnfollowersMonitor - Unfollower tracking</li> </ul>"},{"location":"api/scrapers/downloads/","title":"MediaDownloader","text":"<p>Downloads media files (photos, videos, GIFs) from tweets.</p>"},{"location":"api/scrapers/downloads/#import","title":"Import","text":"<pre><code>from xeepy.scrapers.downloads import MediaDownloader\n</code></pre>"},{"location":"api/scrapers/downloads/#class-signature","title":"Class Signature","text":"<pre><code>class MediaDownloader:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/scrapers/downloads/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/scrapers/downloads/#methods","title":"Methods","text":"Method Returns Description <code>scrape(tweet_ids, output_dir)</code> <code>DownloadResult</code> Download media from tweets <code>download_user_media(username)</code> <code>List[str]</code> Download all user media <code>download_photo(url, path)</code> <code>str</code> Download single photo <code>download_video(url, path)</code> <code>str</code> Download single video <code>download_hq(tweet_id)</code> <code>str</code> Download highest quality"},{"location":"api/scrapers/downloads/#scrape","title":"<code>scrape</code>","text":"<pre><code>async def scrape(\n    self,\n    tweet_ids: List[str],\n    output_dir: str = \"media\",\n    photos: bool = True,\n    videos: bool = True,\n    gifs: bool = True,\n    hq_images: bool = False,\n    concurrent: int = 3\n) -&gt; DownloadResult\n</code></pre> <p>Download media from multiple tweets.</p> <p>Parameters: - <code>tweet_ids</code>: List of tweet IDs to download from - <code>output_dir</code>: Directory to save files - <code>photos</code>: Download photos - <code>videos</code>: Download videos - <code>gifs</code>: Download GIFs - <code>hq_images</code>: Get highest quality images (4096x4096) - <code>concurrent</code>: Concurrent downloads</p>"},{"location":"api/scrapers/downloads/#download_user_media","title":"<code>download_user_media</code>","text":"<pre><code>async def download_user_media(\n    self,\n    username: str,\n    output_dir: str = \"media\",\n    limit: int = 100,\n    media_types: List[str] = [\"photo\", \"video\", \"gif\"]\n) -&gt; List[str]\n</code></pre> <p>Download all media from a user's profile.</p>"},{"location":"api/scrapers/downloads/#download_photo","title":"<code>download_photo</code>","text":"<pre><code>async def download_photo(\n    self,\n    url: str,\n    output_path: Optional[str] = None,\n    quality: str = \"large\"\n) -&gt; str\n</code></pre> <p>Download a single photo.</p> <p>Parameters: - <code>url</code>: Photo URL - <code>output_path</code>: Custom save path - <code>quality</code>: Quality level (<code>thumb</code>, <code>small</code>, <code>medium</code>, <code>large</code>, <code>orig</code>)</p>"},{"location":"api/scrapers/downloads/#download_video","title":"<code>download_video</code>","text":"<pre><code>async def download_video(\n    self,\n    url: str,\n    output_path: Optional[str] = None,\n    quality: str = \"highest\"\n) -&gt; str\n</code></pre> <p>Download a single video.</p>"},{"location":"api/scrapers/downloads/#downloadresult-object","title":"DownloadResult Object","text":"<pre><code>@dataclass\nclass DownloadResult:\n    downloaded: List[str]            # Successfully downloaded paths\n    failed: List[Dict]               # Failed downloads with errors\n    total_size: int                  # Total bytes downloaded\n    duration: float                  # Time taken in seconds\n\n    @property\n    def success_count(self) -&gt; int:\n        return len(self.downloaded)\n\n    @property\n    def failure_count(self) -&gt; int:\n        return len(self.failed)\n</code></pre>"},{"location":"api/scrapers/downloads/#usage-examples","title":"Usage Examples","text":""},{"location":"api/scrapers/downloads/#download-from-tweet-ids","title":"Download from Tweet IDs","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.media.download(\n            tweet_ids=[\"123456789\", \"987654321\"],\n            output_dir=\"downloads\",\n            photos=True,\n            videos=True\n        )\n\n        print(f\"Downloaded: {result.success_count} files\")\n        print(f\"Failed: {result.failure_count}\")\n        print(f\"Total size: {result.total_size / 1024 / 1024:.1f} MB\")\n\n        for path in result.downloaded:\n            print(f\"  - {path}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/downloads/#download-users-media","title":"Download User's Media","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        paths = await x.media.download_user_media(\n            \"username\",\n            output_dir=\"media/username\",\n            limit=200\n        )\n\n        print(f\"Downloaded {len(paths)} files\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/downloads/#high-quality-images","title":"High Quality Images","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.media.download(\n            tweet_ids=[\"123456789\"],\n            output_dir=\"hq_images\",\n            photos=True,\n            videos=False,\n            hq_images=True  # 4096x4096 quality\n        )\n\n        print(f\"Downloaded {result.success_count} HQ images\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/downloads/#download-single-photo","title":"Download Single Photo","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        photo_url = \"https://pbs.twimg.com/media/xxx.jpg\"\n\n        path = await x.media.download_photo(\n            photo_url,\n            output_path=\"my_photo.jpg\",\n            quality=\"orig\"\n        )\n\n        print(f\"Saved to: {path}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/downloads/#download-single-video","title":"Download Single Video","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        video_url = \"https://video.twimg.com/ext_tw_video/xxx.mp4\"\n\n        path = await x.media.download_video(\n            video_url,\n            output_path=\"my_video.mp4\",\n            quality=\"highest\"\n        )\n\n        print(f\"Saved to: {path}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/downloads/#batch-download-with-progress","title":"Batch Download with Progress","text":"<pre><code>from xeepy import Xeepy\n\nasync def download_with_progress(tweet_ids: list):\n    async with Xeepy() as x:\n        total = len(tweet_ids)\n        downloaded = 0\n\n        for i, tweet_id in enumerate(tweet_ids):\n            try:\n                result = await x.media.download(\n                    tweet_ids=[tweet_id],\n                    output_dir=\"batch_media\"\n                )\n                downloaded += result.success_count\n                print(f\"Progress: {i+1}/{total} tweets processed\")\n            except Exception as e:\n                print(f\"Failed {tweet_id}: {e}\")\n\n        print(f\"Total downloaded: {downloaded} files\")\n\nasyncio.run(download_with_progress([\"111\", \"222\", \"333\"]))\n</code></pre>"},{"location":"api/scrapers/downloads/#download-only-new-media","title":"Download Only New Media","text":"<pre><code>from xeepy import Xeepy\nimport os\n\nasync def download_new_only(username: str, output_dir: str):\n    \"\"\"Download only media that doesn't exist locally.\"\"\"\n    async with Xeepy() as x:\n        # Get existing files\n        existing = set()\n        if os.path.exists(output_dir):\n            existing = set(os.listdir(output_dir))\n\n        # Get user's media tweets\n        result = await x.scrape.media(username, limit=200)\n\n        new_downloads = []\n        for tweet in result.items:\n            for media in tweet.media:\n                filename = f\"{tweet.id}_{media.type}.{'jpg' if media.type == 'photo' else 'mp4'}\"\n                if filename not in existing:\n                    path = await x.media.download_photo(\n                        media.url,\n                        output_path=os.path.join(output_dir, filename)\n                    ) if media.type == \"photo\" else await x.media.download_video(\n                        media.url,\n                        output_path=os.path.join(output_dir, filename)\n                    )\n                    new_downloads.append(path)\n\n        print(f\"Downloaded {len(new_downloads)} new files\")\n\nasyncio.run(download_new_only(\"username\", \"media_archive\"))\n</code></pre>"},{"location":"api/scrapers/downloads/#see-also","title":"See Also","text":"<ul> <li>MediaScraper - Scrape media tweets</li> <li>TweetsScraper - User tweets</li> <li>Tweet Model - Tweet structure</li> </ul>"},{"location":"api/scrapers/followers/","title":"FollowersScraper","text":"<p>Scrapes the followers list of a Twitter/X user.</p>"},{"location":"api/scrapers/followers/#import","title":"Import","text":"<pre><code>from xeepy.scrapers.followers import FollowersScraper\n</code></pre>"},{"location":"api/scrapers/followers/#class-signature","title":"Class Signature","text":"<pre><code>class FollowersScraper:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/scrapers/followers/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/scrapers/followers/#methods","title":"Methods","text":"Method Returns Description <code>scrape(username, limit)</code> <code>ScrapeResult[User]</code> Get followers list <code>scrape_all(username)</code> <code>ScrapeResult[User]</code> Get all followers <code>scrape_verified(username)</code> <code>ScrapeResult[User]</code> Get verified followers only <code>count(username)</code> <code>int</code> Get follower count"},{"location":"api/scrapers/followers/#scrape","title":"<code>scrape</code>","text":"<pre><code>async def scrape(\n    self,\n    username: str,\n    limit: int = 1000,\n    cursor: Optional[str] = None\n) -&gt; ScrapeResult[User]\n</code></pre> <p>Scrape a user's followers list.</p> <p>Parameters: - <code>username</code>: Target username (without @) - <code>limit</code>: Maximum followers to fetch - <code>cursor</code>: Pagination cursor for continuing previous scrape</p> <p>Returns: <code>ScrapeResult</code> containing <code>User</code> objects</p>"},{"location":"api/scrapers/followers/#scrape_all","title":"<code>scrape_all</code>","text":"<pre><code>async def scrape_all(\n    self,\n    username: str,\n    progress_callback: Optional[Callable] = None\n) -&gt; ScrapeResult[User]\n</code></pre> <p>Scrape all followers (may take a long time for large accounts).</p> <p>Parameters: - <code>username</code>: Target username - <code>progress_callback</code>: Callback function for progress updates</p>"},{"location":"api/scrapers/followers/#scrape_verified","title":"<code>scrape_verified</code>","text":"<pre><code>async def scrape_verified(\n    self,\n    username: str,\n    limit: int = 500\n) -&gt; ScrapeResult[User]\n</code></pre> <p>Get only verified followers.</p>"},{"location":"api/scrapers/followers/#count","title":"<code>count</code>","text":"<pre><code>async def count(self, username: str) -&gt; int\n</code></pre> <p>Get the total follower count without scraping the full list.</p>"},{"location":"api/scrapers/followers/#usage-examples","title":"Usage Examples","text":""},{"location":"api/scrapers/followers/#basic-followers-scraping","title":"Basic Followers Scraping","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.followers(\"username\", limit=1000)\n\n        print(f\"Scraped {len(result.items)} followers\")\n\n        for follower in result.items:\n            print(f\"@{follower.username} - {follower.followers_count} followers\")\n\n        # Export to CSV\n        x.export.to_csv(result.items, \"followers.csv\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/followers/#scrape-all-followers-with-progress","title":"Scrape All Followers with Progress","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        def progress(current, total):\n            print(f\"Progress: {current}/{total}\")\n\n        result = await x.scrape.followers_all(\n            \"username\",\n            progress_callback=progress\n        )\n\n        print(f\"Total followers: {len(result.items)}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/followers/#pagination-support","title":"Pagination Support","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        all_followers = []\n        cursor = None\n\n        while True:\n            result = await x.scrape.followers(\n                \"username\",\n                limit=500,\n                cursor=cursor\n            )\n\n            all_followers.extend(result.items)\n\n            if not result.has_more:\n                break\n\n            cursor = result.cursor\n            print(f\"Fetched {len(all_followers)} so far...\")\n\n        print(f\"Total: {len(all_followers)} followers\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/followers/#find-mutual-followers","title":"Find Mutual Followers","text":"<pre><code>from xeepy import Xeepy\n\nasync def find_mutual_followers(user1: str, user2: str):\n    async with Xeepy() as x:\n        followers1 = await x.scrape.followers(user1, limit=5000)\n        followers2 = await x.scrape.followers(user2, limit=5000)\n\n        set1 = {f.username for f in followers1.items}\n        set2 = {f.username for f in followers2.items}\n\n        mutual = set1.intersection(set2)\n        print(f\"Mutual followers: {len(mutual)}\")\n\n        return list(mutual)\n\nasyncio.run(find_mutual_followers(\"user1\", \"user2\"))\n</code></pre>"},{"location":"api/scrapers/followers/#filter-by-follower-count","title":"Filter by Follower Count","text":"<pre><code>from xeepy import Xeepy\n\nasync def get_influential_followers(username: str, min_followers: int = 10000):\n    async with Xeepy() as x:\n        result = await x.scrape.followers(username, limit=2000)\n\n        influential = [\n            f for f in result.items\n            if f.followers_count &gt;= min_followers\n        ]\n\n        influential.sort(key=lambda f: f.followers_count, reverse=True)\n\n        print(f\"Found {len(influential)} influential followers:\")\n        for f in influential[:10]:\n            print(f\"  @{f.username}: {f.followers_count:,} followers\")\n\n        return influential\n\nasyncio.run(get_influential_followers(\"elonmusk\"))\n</code></pre>"},{"location":"api/scrapers/followers/#see-also","title":"See Also","text":"<ul> <li>User Model - User data structure</li> <li>FollowingScraper - Get following list</li> <li>ProfileScraper - Profile scraping</li> </ul>"},{"location":"api/scrapers/following/","title":"FollowingScraper","text":"<p>Scrapes the list of accounts that a user follows.</p>"},{"location":"api/scrapers/following/#import","title":"Import","text":"<pre><code>from xeepy.scrapers.following import FollowingScraper\n</code></pre>"},{"location":"api/scrapers/following/#class-signature","title":"Class Signature","text":"<pre><code>class FollowingScraper:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/scrapers/following/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/scrapers/following/#methods","title":"Methods","text":"Method Returns Description <code>scrape(username, limit)</code> <code>ScrapeResult[User]</code> Get following list <code>scrape_all(username)</code> <code>ScrapeResult[User]</code> Get complete following list <code>count(username)</code> <code>int</code> Get following count <code>is_following(username, target)</code> <code>bool</code> Check if user follows target"},{"location":"api/scrapers/following/#scrape","title":"<code>scrape</code>","text":"<pre><code>async def scrape(\n    self,\n    username: str,\n    limit: int = 1000,\n    cursor: Optional[str] = None\n) -&gt; ScrapeResult[User]\n</code></pre> <p>Scrape accounts that a user follows.</p> <p>Parameters: - <code>username</code>: Target username (without @) - <code>limit</code>: Maximum accounts to fetch - <code>cursor</code>: Pagination cursor</p> <p>Returns: <code>ScrapeResult</code> containing <code>User</code> objects</p>"},{"location":"api/scrapers/following/#scrape_all","title":"<code>scrape_all</code>","text":"<pre><code>async def scrape_all(\n    self,\n    username: str,\n    progress_callback: Optional[Callable] = None\n) -&gt; ScrapeResult[User]\n</code></pre> <p>Scrape complete following list.</p>"},{"location":"api/scrapers/following/#is_following","title":"<code>is_following</code>","text":"<pre><code>async def is_following(\n    self,\n    username: str,\n    target: str\n) -&gt; bool\n</code></pre> <p>Check if a user follows a specific account.</p>"},{"location":"api/scrapers/following/#usage-examples","title":"Usage Examples","text":""},{"location":"api/scrapers/following/#basic-following-scraping","title":"Basic Following Scraping","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.following(\"username\", limit=500)\n\n        print(f\"User follows {len(result.items)} accounts\")\n\n        for user in result.items:\n            print(f\"@{user.username} - {user.name}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/following/#find-non-followers","title":"Find Non-Followers","text":"<pre><code>from xeepy import Xeepy\n\nasync def find_non_followers(username: str):\n    \"\"\"Find accounts that don't follow back.\"\"\"\n    async with Xeepy() as x:\n        following = await x.scrape.following(username, limit=5000)\n        followers = await x.scrape.followers(username, limit=5000)\n\n        following_set = {u.username for u in following.items}\n        followers_set = {u.username for u in followers.items}\n\n        non_followers = following_set - followers_set\n\n        print(f\"Following: {len(following_set)}\")\n        print(f\"Followers: {len(followers_set)}\")\n        print(f\"Non-followers: {len(non_followers)}\")\n\n        return list(non_followers)\n\nasyncio.run(find_non_followers(\"myusername\"))\n</code></pre>"},{"location":"api/scrapers/following/#analyze-following-categories","title":"Analyze Following Categories","text":"<pre><code>from xeepy import Xeepy\nfrom collections import Counter\n\nasync def analyze_following(username: str):\n    async with Xeepy() as x:\n        result = await x.scrape.following(username, limit=1000)\n\n        # Analyze verification status\n        verified = sum(1 for u in result.items if u.is_verified)\n\n        # Analyze follower counts\n        follower_ranges = Counter()\n        for user in result.items:\n            if user.followers_count &lt; 1000:\n                follower_ranges[\"&lt;1K\"] += 1\n            elif user.followers_count &lt; 10000:\n                follower_ranges[\"1K-10K\"] += 1\n            elif user.followers_count &lt; 100000:\n                follower_ranges[\"10K-100K\"] += 1\n            else:\n                follower_ranges[\"&gt;100K\"] += 1\n\n        print(f\"Total following: {len(result.items)}\")\n        print(f\"Verified: {verified}\")\n        print(f\"Follower distribution: {dict(follower_ranges)}\")\n\nasyncio.run(analyze_following(\"username\"))\n</code></pre>"},{"location":"api/scrapers/following/#export-with-custom-fields","title":"Export with Custom Fields","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.following(\"username\", limit=2000)\n\n        # Prepare custom data\n        data = [\n            {\n                \"username\": u.username,\n                \"name\": u.name,\n                \"followers\": u.followers_count,\n                \"following\": u.following_count,\n                \"ratio\": round(u.followers_count / max(u.following_count, 1), 2),\n                \"verified\": u.is_verified\n            }\n            for u in result.items\n        ]\n\n        x.export.to_csv(data, \"following_analysis.csv\")\n        print(f\"Exported {len(data)} accounts\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/following/#check-specific-relationship","title":"Check Specific Relationship","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Check if user follows a specific account\n        is_following = await x.scrape.is_following(\n            \"myusername\",\n            \"targetuser\"\n        )\n\n        if is_following:\n            print(\"You follow @targetuser\")\n        else:\n            print(\"You don't follow @targetuser\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/following/#see-also","title":"See Also","text":"<ul> <li>User Model - User data structure</li> <li>FollowersScraper - Get followers list</li> <li>UnfollowActions - Unfollow operations</li> </ul>"},{"location":"api/scrapers/hashtag/","title":"HashtagScraper","text":"<p>Scrapes tweets containing specific hashtags.</p>"},{"location":"api/scrapers/hashtag/#import","title":"Import","text":"<pre><code>from xeepy.scrapers.hashtag import HashtagScraper\n</code></pre>"},{"location":"api/scrapers/hashtag/#class-signature","title":"Class Signature","text":"<pre><code>class HashtagScraper:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/scrapers/hashtag/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/scrapers/hashtag/#methods","title":"Methods","text":"Method Returns Description <code>scrape(hashtag, limit)</code> <code>ScrapeResult[Tweet]</code> Get tweets with hashtag <code>scrape_latest(hashtag)</code> <code>ScrapeResult[Tweet]</code> Latest hashtag tweets <code>scrape_top(hashtag)</code> <code>ScrapeResult[Tweet]</code> Top/popular tweets <code>scrape_multiple(hashtags)</code> <code>Dict[str, ScrapeResult]</code> Multiple hashtags <code>get_stats(hashtag)</code> <code>HashtagStats</code> Hashtag statistics"},{"location":"api/scrapers/hashtag/#scrape","title":"<code>scrape</code>","text":"<pre><code>async def scrape(\n    self,\n    hashtag: str,\n    limit: int = 100,\n    sort_by: str = \"latest\",\n    language: Optional[str] = None,\n    cursor: Optional[str] = None\n) -&gt; ScrapeResult[Tweet]\n</code></pre> <p>Scrape tweets containing a specific hashtag.</p> <p>Parameters: - <code>hashtag</code>: Hashtag to search (with or without #) - <code>limit</code>: Maximum tweets to fetch - <code>sort_by</code>: Sort order (<code>latest</code>, <code>top</code>) - <code>language</code>: Filter by language code - <code>cursor</code>: Pagination cursor</p>"},{"location":"api/scrapers/hashtag/#scrape_multiple","title":"<code>scrape_multiple</code>","text":"<pre><code>async def scrape_multiple(\n    self,\n    hashtags: List[str],\n    limit_per_hashtag: int = 50\n) -&gt; Dict[str, ScrapeResult[Tweet]]\n</code></pre> <p>Scrape multiple hashtags in parallel.</p>"},{"location":"api/scrapers/hashtag/#get_stats","title":"<code>get_stats</code>","text":"<pre><code>async def get_stats(\n    self,\n    hashtag: str,\n    sample_size: int = 100\n) -&gt; HashtagStats\n</code></pre> <p>Get statistics about a hashtag.</p>"},{"location":"api/scrapers/hashtag/#hashtagstats-object","title":"HashtagStats Object","text":"<pre><code>@dataclass\nclass HashtagStats:\n    hashtag: str                     # The hashtag\n    sample_size: int                 # Tweets analyzed\n    avg_likes: float                 # Average likes\n    avg_retweets: float              # Average retweets\n    avg_replies: float               # Average replies\n    top_authors: List[str]           # Most active authors\n    related_hashtags: List[str]      # Co-occurring hashtags\n    peak_hours: List[int]            # Most active hours (UTC)\n    language_distribution: Dict      # Languages breakdown\n</code></pre>"},{"location":"api/scrapers/hashtag/#usage-examples","title":"Usage Examples","text":""},{"location":"api/scrapers/hashtag/#basic-hashtag-scraping","title":"Basic Hashtag Scraping","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.hashtag(\"#python\", limit=100)\n\n        for tweet in result.items:\n            print(f\"@{tweet.author.username}: {tweet.text[:80]}...\")\n            print(f\"  \u2764\ufe0f {tweet.like_count} | \ud83d\udd04 {tweet.retweet_count}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/hashtag/#latest-vs-top-tweets","title":"Latest vs Top Tweets","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Get latest tweets\n        latest = await x.scrape.hashtag(\n            \"#AI\",\n            limit=50,\n            sort_by=\"latest\"\n        )\n\n        # Get top/popular tweets\n        top = await x.scrape.hashtag(\n            \"#AI\",\n            limit=50,\n            sort_by=\"top\"\n        )\n\n        print(f\"Latest avg likes: {sum(t.like_count for t in latest.items) / len(latest.items):.0f}\")\n        print(f\"Top avg likes: {sum(t.like_count for t in top.items) / len(top.items):.0f}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/hashtag/#multiple-hashtags","title":"Multiple Hashtags","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        hashtags = [\"#python\", \"#javascript\", \"#rust\", \"#golang\"]\n\n        results = await x.scrape.hashtags(hashtags, limit_per_hashtag=100)\n\n        for hashtag, result in results.items():\n            avg_likes = sum(t.like_count for t in result.items) / len(result.items)\n            print(f\"{hashtag}: {len(result.items)} tweets, avg {avg_likes:.0f} likes\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/hashtag/#hashtag-analytics","title":"Hashtag Analytics","text":"<pre><code>from xeepy import Xeepy\n\nasync def analyze_hashtag(hashtag: str):\n    async with Xeepy() as x:\n        stats = await x.scrape.hashtag_stats(hashtag, sample_size=200)\n\n        print(f\"Hashtag Analysis: {stats.hashtag}\")\n        print(\"=\" * 50)\n        print(f\"Sample size: {stats.sample_size} tweets\")\n        print(f\"Avg likes: {stats.avg_likes:.1f}\")\n        print(f\"Avg retweets: {stats.avg_retweets:.1f}\")\n        print(f\"Avg replies: {stats.avg_replies:.1f}\")\n        print(f\"\\nTop authors: {', '.join(stats.top_authors[:5])}\")\n        print(f\"Related hashtags: {', '.join(stats.related_hashtags[:5])}\")\n        print(f\"Peak hours (UTC): {stats.peak_hours[:5]}\")\n\nasyncio.run(analyze_hashtag(\"#MachineLearning\"))\n</code></pre>"},{"location":"api/scrapers/hashtag/#filter-by-language","title":"Filter by Language","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # English tweets only\n        result = await x.scrape.hashtag(\n            \"#news\",\n            limit=200,\n            language=\"en\"\n        )\n\n        # Spanish tweets\n        result_es = await x.scrape.hashtag(\n            \"#noticias\",\n            limit=200,\n            language=\"es\"\n        )\n\n        print(f\"English tweets: {len(result.items)}\")\n        print(f\"Spanish tweets: {len(result_es.items)}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/hashtag/#find-trending-content","title":"Find Trending Content","text":"<pre><code>from xeepy import Xeepy\nfrom datetime import datetime, timedelta\n\nasync def find_viral_hashtag_content(hashtag: str, min_likes: int = 1000):\n    async with Xeepy() as x:\n        result = await x.scrape.hashtag(hashtag, limit=500)\n\n        viral = [\n            t for t in result.items\n            if t.like_count &gt;= min_likes\n        ]\n\n        viral.sort(key=lambda t: t.like_count, reverse=True)\n\n        print(f\"Viral tweets for {hashtag} (&gt;{min_likes} likes):\")\n        for tweet in viral[:10]:\n            print(f\"\\n@{tweet.author.username} ({tweet.like_count:,} likes)\")\n            print(f\"  {tweet.text[:100]}...\")\n\nasyncio.run(find_viral_hashtag_content(\"#startup\", min_likes=500))\n</code></pre>"},{"location":"api/scrapers/hashtag/#see-also","title":"See Also","text":"<ul> <li>Tweet Model - Tweet data structure</li> <li>SearchScraper - General search</li> <li>FollowActions - Follow by hashtag</li> </ul>"},{"location":"api/scrapers/likes/","title":"LikesScraper","text":"<p>Scrapes users who liked a specific tweet.</p>"},{"location":"api/scrapers/likes/#import","title":"Import","text":"<pre><code>from xeepy.scrapers.likes import LikesScraper\n</code></pre>"},{"location":"api/scrapers/likes/#class-signature","title":"Class Signature","text":"<pre><code>class LikesScraper:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/scrapers/likes/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/scrapers/likes/#methods","title":"Methods","text":"Method Returns Description <code>scrape(tweet_url, limit)</code> <code>ScrapeResult[User]</code> Get users who liked a tweet <code>scrape_by_id(tweet_id, limit)</code> <code>ScrapeResult[User]</code> Using tweet ID <code>count(tweet_url)</code> <code>int</code> Get like count <code>scrape_verified(tweet_url)</code> <code>ScrapeResult[User]</code> Verified likers only"},{"location":"api/scrapers/likes/#scrape","title":"<code>scrape</code>","text":"<pre><code>async def scrape(\n    self,\n    tweet_url: str,\n    limit: int = 100,\n    cursor: Optional[str] = None\n) -&gt; ScrapeResult[User]\n</code></pre> <p>Scrape users who liked a specific tweet.</p> <p>Parameters: - <code>tweet_url</code>: URL of the tweet - <code>limit</code>: Maximum users to fetch - <code>cursor</code>: Pagination cursor</p> <p>Returns: <code>ScrapeResult</code> containing <code>User</code> objects</p>"},{"location":"api/scrapers/likes/#scrape_by_id","title":"<code>scrape_by_id</code>","text":"<pre><code>async def scrape_by_id(\n    self,\n    tweet_id: str,\n    limit: int = 100\n) -&gt; ScrapeResult[User]\n</code></pre> <p>Scrape likers using tweet ID instead of URL.</p>"},{"location":"api/scrapers/likes/#scrape_verified","title":"<code>scrape_verified</code>","text":"<pre><code>async def scrape_verified(\n    self,\n    tweet_url: str,\n    limit: int = 100\n) -&gt; ScrapeResult[User]\n</code></pre> <p>Get only verified users who liked the tweet.</p>"},{"location":"api/scrapers/likes/#count","title":"<code>count</code>","text":"<pre><code>async def count(self, tweet_url: str) -&gt; int\n</code></pre> <p>Get the total like count without scraping users.</p>"},{"location":"api/scrapers/likes/#usage-examples","title":"Usage Examples","text":""},{"location":"api/scrapers/likes/#basic-likes-scraping","title":"Basic Likes Scraping","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.likes(\n            \"https://x.com/user/status/123456789\",\n            limit=100\n        )\n\n        print(f\"Scraped {len(result.items)} likers\")\n\n        for user in result.items:\n            print(f\"@{user.username} - {user.followers_count:,} followers\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/likes/#find-influential-likers","title":"Find Influential Likers","text":"<pre><code>from xeepy import Xeepy\n\nasync def find_influential_likers(tweet_url: str, min_followers: int = 10000):\n    async with Xeepy() as x:\n        result = await x.scrape.likes(tweet_url, limit=500)\n\n        influential = [\n            user for user in result.items\n            if user.followers_count &gt;= min_followers\n        ]\n\n        influential.sort(key=lambda u: u.followers_count, reverse=True)\n\n        print(f\"Influential likers (&gt;{min_followers:,} followers):\")\n        for user in influential[:10]:\n            print(f\"  @{user.username}: {user.followers_count:,} followers\")\n\n        return influential\n\nasyncio.run(find_influential_likers(\"https://x.com/user/status/123\"))\n</code></pre>"},{"location":"api/scrapers/likes/#verified-likers-only","title":"Verified Likers Only","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.likes_verified(\n            \"https://x.com/viral_tweet/status/123\",\n            limit=100\n        )\n\n        print(f\"Verified users who liked this tweet:\")\n        for user in result.items:\n            print(f\"\u2713 @{user.username} - {user.name}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/likes/#export-likers-list","title":"Export Likers List","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.likes(\n            \"https://x.com/user/status/123\",\n            limit=1000\n        )\n\n        # Export to CSV\n        x.export.to_csv(result.items, \"tweet_likers.csv\")\n\n        # Export just usernames\n        usernames = [user.username for user in result.items]\n        with open(\"likers_usernames.txt\", \"w\") as f:\n            f.write(\"\\n\".join(usernames))\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/likes/#compare-engagement-across-tweets","title":"Compare Engagement Across Tweets","text":"<pre><code>from xeepy import Xeepy\n\nasync def compare_tweet_engagement(tweet_urls: list):\n    async with Xeepy() as x:\n        for url in tweet_urls:\n            result = await x.scrape.likes(url, limit=200)\n\n            total_followers = sum(u.followers_count for u in result.items)\n            avg_followers = total_followers / len(result.items) if result.items else 0\n            verified_count = sum(1 for u in result.items if u.is_verified)\n\n            print(f\"\\nTweet: {url.split('/')[-1]}\")\n            print(f\"  Likers scraped: {len(result.items)}\")\n            print(f\"  Avg follower count: {avg_followers:,.0f}\")\n            print(f\"  Verified likers: {verified_count}\")\n\nasyncio.run(compare_tweet_engagement([\n    \"https://x.com/user/status/111\",\n    \"https://x.com/user/status/222\",\n    \"https://x.com/user/status/333\"\n]))\n</code></pre>"},{"location":"api/scrapers/likes/#find-common-likers","title":"Find Common Likers","text":"<pre><code>from xeepy import Xeepy\n\nasync def find_common_likers(tweet_urls: list):\n    \"\"\"Find users who liked multiple tweets.\"\"\"\n    async with Xeepy() as x:\n        all_likers = []\n\n        for url in tweet_urls:\n            result = await x.scrape.likes(url, limit=500)\n            all_likers.append(set(u.username for u in result.items))\n\n        # Find intersection\n        common = all_likers[0]\n        for likers in all_likers[1:]:\n            common = common.intersection(likers)\n\n        print(f\"Users who liked all {len(tweet_urls)} tweets:\")\n        for username in list(common)[:20]:\n            print(f\"  @{username}\")\n\n        return list(common)\n\nasyncio.run(find_common_likers([\n    \"https://x.com/user/status/111\",\n    \"https://x.com/user/status/222\"\n]))\n</code></pre>"},{"location":"api/scrapers/likes/#see-also","title":"See Also","text":"<ul> <li>User Model - User data structure</li> <li>EngageActions - Like/unlike tweets</li> <li>RepliesScraper - Get tweet replies</li> </ul>"},{"location":"api/scrapers/lists/","title":"ListsScraper","text":"<p>Scrapes Twitter/X list members and list information.</p>"},{"location":"api/scrapers/lists/#import","title":"Import","text":"<pre><code>from xeepy.scrapers.lists import ListsScraper\n</code></pre>"},{"location":"api/scrapers/lists/#class-signature","title":"Class Signature","text":"<pre><code>class ListsScraper:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/scrapers/lists/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/scrapers/lists/#methods","title":"Methods","text":"Method Returns Description <code>scrape_members(list_url)</code> <code>ScrapeResult[User]</code> Get list members <code>scrape_subscribers(list_url)</code> <code>ScrapeResult[User]</code> Get list subscribers <code>scrape_user_lists(username)</code> <code>ScrapeResult[List]</code> User's created lists <code>scrape_user_memberships(username)</code> <code>ScrapeResult[List]</code> Lists user is member of <code>get_list_info(list_url)</code> <code>TwitterList</code> Get list details <code>scrape_list_timeline(list_url)</code> <code>ScrapeResult[Tweet]</code> Get list timeline"},{"location":"api/scrapers/lists/#scrape_members","title":"<code>scrape_members</code>","text":"<pre><code>async def scrape_members(\n    self,\n    list_url: str,\n    limit: int = 500,\n    cursor: Optional[str] = None\n) -&gt; ScrapeResult[User]\n</code></pre> <p>Scrape members of a Twitter list.</p> <p>Parameters: - <code>list_url</code>: URL of the Twitter list - <code>limit</code>: Maximum members to fetch - <code>cursor</code>: Pagination cursor</p>"},{"location":"api/scrapers/lists/#scrape_list_timeline","title":"<code>scrape_list_timeline</code>","text":"<pre><code>async def scrape_list_timeline(\n    self,\n    list_url: str,\n    limit: int = 100\n) -&gt; ScrapeResult[Tweet]\n</code></pre> <p>Scrape tweets from a list's timeline.</p>"},{"location":"api/scrapers/lists/#get_list_info","title":"<code>get_list_info</code>","text":"<pre><code>async def get_list_info(self, list_url: str) -&gt; TwitterList\n</code></pre> <p>Get detailed information about a list.</p>"},{"location":"api/scrapers/lists/#twitterlist-object","title":"TwitterList Object","text":"<pre><code>@dataclass\nclass TwitterList:\n    id: str                          # List ID\n    name: str                        # List name\n    description: Optional[str]       # List description\n    owner: User                      # List owner\n    member_count: int                # Number of members\n    subscriber_count: int            # Number of subscribers\n    is_private: bool                 # Private list flag\n    created_at: datetime             # Creation date\n    banner_url: Optional[str]        # List banner image\n</code></pre>"},{"location":"api/scrapers/lists/#usage-examples","title":"Usage Examples","text":""},{"location":"api/scrapers/lists/#scrape-list-members","title":"Scrape List Members","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.list_members(\n            \"https://x.com/i/lists/123456789\",\n            limit=500\n        )\n\n        print(f\"List has {len(result.items)} members\")\n\n        for member in result.items:\n            print(f\"@{member.username} - {member.followers_count:,} followers\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/lists/#get-list-information","title":"Get List Information","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        list_info = await x.scrape.list_info(\n            \"https://x.com/i/lists/123456789\"\n        )\n\n        print(f\"List: {list_info.name}\")\n        print(f\"Owner: @{list_info.owner.username}\")\n        print(f\"Description: {list_info.description}\")\n        print(f\"Members: {list_info.member_count}\")\n        print(f\"Subscribers: {list_info.subscriber_count}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/lists/#scrape-list-timeline","title":"Scrape List Timeline","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.list_timeline(\n            \"https://x.com/i/lists/123456789\",\n            limit=100\n        )\n\n        for tweet in result.items:\n            print(f\"@{tweet.author.username}: {tweet.text[:80]}...\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/lists/#get-users-lists","title":"Get User's Lists","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Lists created by user\n        created = await x.scrape.user_lists(\"username\")\n        print(f\"User created {len(created.items)} lists\")\n\n        # Lists user is a member of\n        memberships = await x.scrape.user_list_memberships(\"username\")\n        print(f\"User is member of {len(memberships.items)} lists\")\n\n        for lst in memberships.items:\n            print(f\"  - {lst.name} by @{lst.owner.username}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/lists/#export-list-members","title":"Export List Members","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.list_members(\n            \"https://x.com/i/lists/123456789\",\n            limit=1000\n        )\n\n        # Export to CSV with custom fields\n        data = [\n            {\n                \"username\": m.username,\n                \"name\": m.name,\n                \"followers\": m.followers_count,\n                \"following\": m.following_count,\n                \"verified\": m.is_verified\n            }\n            for m in result.items\n        ]\n\n        x.export.to_csv(data, \"list_members.csv\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/lists/#find-common-list-members","title":"Find Common List Members","text":"<pre><code>from xeepy import Xeepy\n\nasync def find_common_members(list_urls: list):\n    async with Xeepy() as x:\n        all_members = []\n\n        for url in list_urls:\n            result = await x.scrape.list_members(url, limit=500)\n            all_members.append(set(m.username for m in result.items))\n\n        common = all_members[0]\n        for members in all_members[1:]:\n            common = common.intersection(members)\n\n        print(f\"Common members across {len(list_urls)} lists: {len(common)}\")\n        return list(common)\n\nasyncio.run(find_common_members([\n    \"https://x.com/i/lists/111\",\n    \"https://x.com/i/lists/222\"\n]))\n</code></pre>"},{"location":"api/scrapers/lists/#list-member-analytics","title":"List Member Analytics","text":"<pre><code>from xeepy import Xeepy\n\nasync def analyze_list(list_url: str):\n    async with Xeepy() as x:\n        info = await x.scrape.list_info(list_url)\n        members = await x.scrape.list_members(list_url, limit=500)\n\n        total_followers = sum(m.followers_count for m in members.items)\n        avg_followers = total_followers / len(members.items)\n        verified_count = sum(1 for m in members.items if m.is_verified)\n\n        print(f\"List Analysis: {info.name}\")\n        print(\"=\" * 40)\n        print(f\"Total members: {info.member_count}\")\n        print(f\"Combined reach: {total_followers:,}\")\n        print(f\"Avg followers: {avg_followers:,.0f}\")\n        print(f\"Verified members: {verified_count}\")\n\nasyncio.run(analyze_list(\"https://x.com/i/lists/123\"))\n</code></pre>"},{"location":"api/scrapers/lists/#see-also","title":"See Also","text":"<ul> <li>User Model - User data structure</li> <li>FollowersScraper - Followers scraping</li> <li>ProfileScraper - Profile information</li> </ul>"},{"location":"api/scrapers/media/","title":"MediaScraper","text":"<p>Scrapes media posts (photos and videos) from user profiles.</p>"},{"location":"api/scrapers/media/#import","title":"Import","text":"<pre><code>from xeepy.scrapers.media import MediaScraper\n</code></pre>"},{"location":"api/scrapers/media/#class-signature","title":"Class Signature","text":"<pre><code>class MediaScraper:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/scrapers/media/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/scrapers/media/#methods","title":"Methods","text":"Method Returns Description <code>scrape(username, limit)</code> <code>ScrapeResult[Tweet]</code> Get media tweets <code>scrape_photos(username)</code> <code>ScrapeResult[Tweet]</code> Photos only <code>scrape_videos(username)</code> <code>ScrapeResult[Tweet]</code> Videos only <code>scrape_gifs(username)</code> <code>ScrapeResult[Tweet]</code> GIFs only <code>get_media_urls(username)</code> <code>List[MediaItem]</code> Extract media URLs"},{"location":"api/scrapers/media/#scrape","title":"<code>scrape</code>","text":"<pre><code>async def scrape(\n    self,\n    username: str,\n    limit: int = 100,\n    media_type: Optional[str] = None,\n    cursor: Optional[str] = None\n) -&gt; ScrapeResult[Tweet]\n</code></pre> <p>Scrape media tweets from a user's profile.</p> <p>Parameters: - <code>username</code>: Target username - <code>limit</code>: Maximum tweets to fetch - <code>media_type</code>: Filter type (<code>photo</code>, <code>video</code>, <code>gif</code>, <code>all</code>) - <code>cursor</code>: Pagination cursor</p>"},{"location":"api/scrapers/media/#get_media_urls","title":"<code>get_media_urls</code>","text":"<pre><code>async def get_media_urls(\n    self,\n    username: str,\n    limit: int = 100,\n    media_type: Optional[str] = None\n) -&gt; List[MediaItem]\n</code></pre> <p>Extract direct media URLs from user's media tweets.</p>"},{"location":"api/scrapers/media/#mediaitem-object","title":"MediaItem Object","text":"<pre><code>@dataclass\nclass MediaItem:\n    url: str                         # Direct media URL\n    type: str                        # photo, video, gif\n    tweet_id: str                    # Source tweet ID\n    width: int                       # Media width\n    height: int                      # Media height\n    duration_ms: Optional[int]       # Video duration (ms)\n    thumbnail_url: Optional[str]     # Video thumbnail\n    alt_text: Optional[str]          # Accessibility text\n    views: Optional[int]             # Video views\n</code></pre>"},{"location":"api/scrapers/media/#usage-examples","title":"Usage Examples","text":""},{"location":"api/scrapers/media/#basic-media-scraping","title":"Basic Media Scraping","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.media(\"username\", limit=100)\n\n        for tweet in result.items:\n            print(f\"Tweet: {tweet.text[:50]}...\")\n            for media in tweet.media:\n                print(f\"  - {media.type}: {media.url}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/media/#photos-only","title":"Photos Only","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.media_photos(\"username\", limit=50)\n\n        photo_urls = []\n        for tweet in result.items:\n            for media in tweet.media:\n                if media.type == \"photo\":\n                    photo_urls.append(media.url)\n\n        print(f\"Found {len(photo_urls)} photos\")\n\n        # Save URLs to file\n        with open(\"photos.txt\", \"w\") as f:\n            f.write(\"\\n\".join(photo_urls))\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/media/#videos-only","title":"Videos Only","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.media_videos(\"username\", limit=50)\n\n        for tweet in result.items:\n            for media in tweet.media:\n                if media.type == \"video\":\n                    print(f\"Video: {media.url}\")\n                    print(f\"  Duration: {media.duration_ms / 1000}s\")\n                    print(f\"  Views: {media.views:,}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/media/#extract-all-media-urls","title":"Extract All Media URLs","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        media_items = await x.scrape.media_urls(\"username\", limit=200)\n\n        # Group by type\n        photos = [m for m in media_items if m.type == \"photo\"]\n        videos = [m for m in media_items if m.type == \"video\"]\n        gifs = [m for m in media_items if m.type == \"gif\"]\n\n        print(f\"Photos: {len(photos)}\")\n        print(f\"Videos: {len(videos)}\")\n        print(f\"GIFs: {len(gifs)}\")\n\n        # Export URLs\n        data = [{\"url\": m.url, \"type\": m.type, \"tweet_id\": m.tweet_id} for m in media_items]\n        x.export.to_csv(data, \"media_urls.csv\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/media/#high-quality-image-urls","title":"High-Quality Image URLs","text":"<pre><code>from xeepy import Xeepy\n\nasync def get_hq_images(username: str):\n    \"\"\"Get highest quality image URLs.\"\"\"\n    async with Xeepy() as x:\n        media_items = await x.scrape.media_urls(\n            username,\n            limit=100,\n            media_type=\"photo\"\n        )\n\n        hq_urls = []\n        for item in media_items:\n            # Modify URL for highest quality\n            hq_url = item.url.replace(\"name=medium\", \"name=4096x4096\")\n            hq_url = hq_url.replace(\"name=small\", \"name=4096x4096\")\n            if \"?\" not in hq_url:\n                hq_url += \"?name=4096x4096\"\n            hq_urls.append(hq_url)\n\n        return hq_urls\n\nasyncio.run(get_hq_images(\"photographer\"))\n</code></pre>"},{"location":"api/scrapers/media/#media-analytics","title":"Media Analytics","text":"<pre><code>from xeepy import Xeepy\nfrom collections import Counter\n\nasync def media_analytics(username: str):\n    async with Xeepy() as x:\n        result = await x.scrape.media(username, limit=500)\n\n        media_types = Counter()\n        total_views = 0\n        total_likes = 0\n\n        for tweet in result.items:\n            total_likes += tweet.like_count\n            for media in tweet.media:\n                media_types[media.type] += 1\n                if media.views:\n                    total_views += media.views\n\n        print(f\"Media Analytics for @{username}\")\n        print(\"=\" * 40)\n        print(f\"Total media tweets: {len(result.items)}\")\n        print(f\"Media breakdown: {dict(media_types)}\")\n        print(f\"Total video views: {total_views:,}\")\n        print(f\"Avg likes per media tweet: {total_likes / len(result.items):.1f}\")\n\nasyncio.run(media_analytics(\"username\"))\n</code></pre>"},{"location":"api/scrapers/media/#see-also","title":"See Also","text":"<ul> <li>Tweet Model - Tweet data structure</li> <li>MediaDownloader - Download media files</li> <li>TweetsScraper - General tweet scraping</li> </ul>"},{"location":"api/scrapers/mentions/","title":"MentionsScraper","text":"<p>Scrapes tweets that mention a specific user.</p>"},{"location":"api/scrapers/mentions/#import","title":"Import","text":"<pre><code>from xeepy.scrapers.mentions import MentionsScraper\n</code></pre>"},{"location":"api/scrapers/mentions/#class-signature","title":"Class Signature","text":"<pre><code>class MentionsScraper:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/scrapers/mentions/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/scrapers/mentions/#methods","title":"Methods","text":"Method Returns Description <code>scrape(username, limit)</code> <code>ScrapeResult[Tweet]</code> Get mentions of user <code>scrape_latest(username)</code> <code>ScrapeResult[Tweet]</code> Latest mentions <code>scrape_from_verified(username)</code> <code>ScrapeResult[Tweet]</code> Verified user mentions <code>scrape_high_engagement(username)</code> <code>ScrapeResult[Tweet]</code> High engagement mentions"},{"location":"api/scrapers/mentions/#scrape","title":"<code>scrape</code>","text":"<pre><code>async def scrape(\n    self,\n    username: str,\n    limit: int = 100,\n    include_replies: bool = True,\n    cursor: Optional[str] = None\n) -&gt; ScrapeResult[Tweet]\n</code></pre> <p>Scrape tweets mentioning a specific user.</p> <p>Parameters: - <code>username</code>: Username to search mentions for (without @) - <code>limit</code>: Maximum mentions to fetch - <code>include_replies</code>: Include reply tweets - <code>cursor</code>: Pagination cursor</p>"},{"location":"api/scrapers/mentions/#scrape_from_verified","title":"<code>scrape_from_verified</code>","text":"<pre><code>async def scrape_from_verified(\n    self,\n    username: str,\n    limit: int = 100\n) -&gt; ScrapeResult[Tweet]\n</code></pre> <p>Get only mentions from verified accounts.</p>"},{"location":"api/scrapers/mentions/#scrape_high_engagement","title":"<code>scrape_high_engagement</code>","text":"<pre><code>async def scrape_high_engagement(\n    self,\n    username: str,\n    min_likes: int = 100,\n    limit: int = 100\n) -&gt; ScrapeResult[Tweet]\n</code></pre> <p>Get mentions with high engagement.</p>"},{"location":"api/scrapers/mentions/#usage-examples","title":"Usage Examples","text":""},{"location":"api/scrapers/mentions/#basic-mentions-scraping","title":"Basic Mentions Scraping","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.mentions(\"username\", limit=100)\n\n        print(f\"Found {len(result.items)} mentions\")\n\n        for tweet in result.items:\n            print(f\"@{tweet.author.username}: {tweet.text[:80]}...\")\n            print(f\"  \u2764\ufe0f {tweet.like_count}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/mentions/#monitor-brand-mentions","title":"Monitor Brand Mentions","text":"<pre><code>from xeepy import Xeepy\nimport asyncio\n\nasync def monitor_mentions(username: str, interval: int = 300):\n    \"\"\"Monitor mentions in real-time.\"\"\"\n    seen_ids = set()\n\n    async with Xeepy() as x:\n        while True:\n            result = await x.scrape.mentions(username, limit=20)\n\n            new_mentions = [\n                t for t in result.items\n                if t.id not in seen_ids\n            ]\n\n            for tweet in new_mentions:\n                seen_ids.add(tweet.id)\n                print(f\"NEW MENTION by @{tweet.author.username}:\")\n                print(f\"  {tweet.text[:100]}...\")\n\n            await asyncio.sleep(interval)\n\nasyncio.run(monitor_mentions(\"mybrand\"))\n</code></pre>"},{"location":"api/scrapers/mentions/#verified-mentions-only","title":"Verified Mentions Only","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.mentions_from_verified(\n            \"username\",\n            limit=50\n        )\n\n        print(\"Mentions from verified accounts:\")\n        for tweet in result.items:\n            print(f\"\u2713 @{tweet.author.username}: {tweet.text[:60]}...\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/mentions/#high-engagement-mentions","title":"High Engagement Mentions","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.mentions_high_engagement(\n            \"username\",\n            min_likes=500,\n            limit=100\n        )\n\n        result.items.sort(key=lambda t: t.like_count, reverse=True)\n\n        print(\"Top mentions by engagement:\")\n        for tweet in result.items[:10]:\n            print(f\"@{tweet.author.username} ({tweet.like_count:,} likes)\")\n            print(f\"  {tweet.text[:80]}...\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/mentions/#mention-analytics","title":"Mention Analytics","text":"<pre><code>from xeepy import Xeepy\nfrom collections import Counter\nfrom datetime import datetime\n\nasync def analyze_mentions(username: str):\n    async with Xeepy() as x:\n        result = await x.scrape.mentions(username, limit=500)\n\n        # Analyze sentiment by author\n        author_counts = Counter(t.author.username for t in result.items)\n\n        # Time distribution\n        hours = Counter(t.created_at.hour for t in result.items)\n\n        # Engagement stats\n        total_likes = sum(t.like_count for t in result.items)\n        total_retweets = sum(t.retweet_count for t in result.items)\n\n        print(f\"Mention Analytics for @{username}\")\n        print(\"=\" * 40)\n        print(f\"Total mentions: {len(result.items)}\")\n        print(f\"Unique mentioners: {len(author_counts)}\")\n        print(f\"Total engagement: {total_likes + total_retweets:,}\")\n        print(f\"\\nTop mentioners:\")\n        for author, count in author_counts.most_common(5):\n            print(f\"  @{author}: {count} mentions\")\n        print(f\"\\nPeak hours (UTC): {hours.most_common(3)}\")\n\nasyncio.run(analyze_mentions(\"elonmusk\"))\n</code></pre>"},{"location":"api/scrapers/mentions/#export-mentions-report","title":"Export Mentions Report","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.mentions(\"mybrand\", limit=1000)\n\n        # Prepare report data\n        data = [\n            {\n                \"date\": t.created_at.isoformat(),\n                \"author\": t.author.username,\n                \"author_followers\": t.author.followers_count,\n                \"text\": t.text,\n                \"likes\": t.like_count,\n                \"retweets\": t.retweet_count,\n                \"url\": f\"https://x.com/{t.author.username}/status/{t.id}\"\n            }\n            for t in result.items\n        ]\n\n        x.export.to_csv(data, \"mentions_report.csv\")\n        x.export.to_json(data, \"mentions_report.json\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/mentions/#filter-mentions-by-sentiment","title":"Filter Mentions by Sentiment","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import SentimentAnalyzer\n\nasync def analyze_mention_sentiment(username: str):\n    async with Xeepy() as x:\n        result = await x.scrape.mentions(username, limit=200)\n\n        analyzer = SentimentAnalyzer()\n\n        positive = []\n        negative = []\n        neutral = []\n\n        for tweet in result.items:\n            sentiment = await analyzer.analyze(tweet.text)\n\n            if sentiment.score &gt; 0.3:\n                positive.append(tweet)\n            elif sentiment.score &lt; -0.3:\n                negative.append(tweet)\n            else:\n                neutral.append(tweet)\n\n        print(f\"Sentiment breakdown:\")\n        print(f\"  Positive: {len(positive)} ({len(positive)/len(result.items)*100:.1f}%)\")\n        print(f\"  Neutral: {len(neutral)} ({len(neutral)/len(result.items)*100:.1f}%)\")\n        print(f\"  Negative: {len(negative)} ({len(negative)/len(result.items)*100:.1f}%)\")\n\nasyncio.run(analyze_mention_sentiment(\"mybrand\"))\n</code></pre>"},{"location":"api/scrapers/mentions/#see-also","title":"See Also","text":"<ul> <li>Tweet Model - Tweet data structure</li> <li>SearchScraper - Advanced search</li> <li>KeywordMonitor - Keyword monitoring</li> </ul>"},{"location":"api/scrapers/profile/","title":"ProfileScraper","text":"<p>Scrapes user profile information including bio, follower counts, and account details.</p>"},{"location":"api/scrapers/profile/#import","title":"Import","text":"<pre><code>from xeepy.scrapers.profile import ProfileScraper\n</code></pre>"},{"location":"api/scrapers/profile/#class-signature","title":"Class Signature","text":"<pre><code>class ProfileScraper:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/scrapers/profile/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/scrapers/profile/#methods","title":"Methods","text":"Method Returns Description <code>scrape(username)</code> <code>User</code> Get full profile for a user <code>scrape_many(usernames)</code> <code>List[User]</code> Batch scrape multiple profiles <code>get_basic_info(username)</code> <code>User</code> Get basic profile info only <code>exists(username)</code> <code>bool</code> Check if username exists"},{"location":"api/scrapers/profile/#scrape","title":"<code>scrape</code>","text":"<pre><code>async def scrape(\n    self,\n    username: str,\n    include_pinned_tweet: bool = True\n) -&gt; User\n</code></pre> <p>Scrape complete profile information for a user.</p> <p>Parameters: - <code>username</code>: Twitter/X username (without @) - <code>include_pinned_tweet</code>: Include the user's pinned tweet</p> <p>Returns: <code>User</code> object with full profile data</p>"},{"location":"api/scrapers/profile/#scrape_many","title":"<code>scrape_many</code>","text":"<pre><code>async def scrape_many(\n    self,\n    usernames: List[str],\n    concurrency: int = 3\n) -&gt; List[User]\n</code></pre> <p>Scrape multiple profiles in parallel.</p> <p>Parameters: - <code>usernames</code>: List of usernames to scrape - <code>concurrency</code>: Number of concurrent scrapes</p>"},{"location":"api/scrapers/profile/#get_basic_info","title":"<code>get_basic_info</code>","text":"<pre><code>async def get_basic_info(self, username: str) -&gt; User\n</code></pre> <p>Get basic profile info (faster, less data).</p>"},{"location":"api/scrapers/profile/#exists","title":"<code>exists</code>","text":"<pre><code>async def exists(self, username: str) -&gt; bool\n</code></pre> <p>Check if a username exists and is accessible.</p>"},{"location":"api/scrapers/profile/#user-object","title":"User Object","text":"<pre><code>@dataclass\nclass User:\n    id: str                          # User ID\n    username: str                    # @handle\n    name: str                        # Display name\n    bio: Optional[str]               # Profile bio\n    location: Optional[str]          # Location\n    website: Optional[str]           # Website URL\n    created_at: datetime             # Account creation date\n    followers_count: int             # Number of followers\n    following_count: int             # Number following\n    tweet_count: int                 # Total tweets\n    like_count: int                  # Total likes\n    listed_count: int                # Lists the user is on\n    is_verified: bool                # Blue checkmark\n    is_protected: bool               # Private account\n    is_blue_verified: bool           # Twitter Blue subscriber\n    profile_image_url: str           # Avatar URL\n    profile_banner_url: Optional[str] # Banner image URL\n    pinned_tweet: Optional[Tweet]    # Pinned tweet\n</code></pre>"},{"location":"api/scrapers/profile/#usage-examples","title":"Usage Examples","text":""},{"location":"api/scrapers/profile/#basic-profile-scraping","title":"Basic Profile Scraping","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        profile = await x.scrape.profile(\"elonmusk\")\n\n        print(f\"Name: {profile.name}\")\n        print(f\"Username: @{profile.username}\")\n        print(f\"Followers: {profile.followers_count:,}\")\n        print(f\"Following: {profile.following_count:,}\")\n        print(f\"Tweets: {profile.tweet_count:,}\")\n        print(f\"Bio: {profile.bio}\")\n        print(f\"Verified: {profile.is_verified}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/profile/#batch-profile-scraping","title":"Batch Profile Scraping","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        usernames = [\"elonmusk\", \"BillGates\", \"sundarpichai\"]\n\n        profiles = await x.scrape.profiles(usernames)\n\n        for profile in profiles:\n            print(f\"@{profile.username}: {profile.followers_count:,} followers\")\n\n        # Export to CSV\n        x.export.to_csv(profiles, \"profiles.csv\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/profile/#check-username-availability","title":"Check Username Availability","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        username = \"desired_username\"\n\n        if await x.scrape.profile_exists(username):\n            print(f\"@{username} is taken\")\n        else:\n            print(f\"@{username} is available!\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/profile/#profile-comparison","title":"Profile Comparison","text":"<pre><code>from xeepy import Xeepy\n\nasync def compare_accounts(usernames: list):\n    async with Xeepy() as x:\n        profiles = await x.scrape.profiles(usernames)\n\n        # Sort by followers\n        sorted_profiles = sorted(\n            profiles,\n            key=lambda p: p.followers_count,\n            reverse=True\n        )\n\n        print(\"Ranking by followers:\")\n        for i, p in enumerate(sorted_profiles, 1):\n            ratio = p.followers_count / max(p.following_count, 1)\n            print(f\"{i}. @{p.username}: {p.followers_count:,} (ratio: {ratio:.1f})\")\n\nasyncio.run(compare_accounts([\"user1\", \"user2\", \"user3\"]))\n</code></pre>"},{"location":"api/scrapers/profile/#see-also","title":"See Also","text":"<ul> <li>User Model - User data structure</li> <li>FollowersScraper - Get user's followers</li> <li>FollowingScraper - Get who user follows</li> </ul>"},{"location":"api/scrapers/recommendations/","title":"RecommendationsScraper","text":"<p>Scrapes trending topics and recommended users from X/Twitter.</p>"},{"location":"api/scrapers/recommendations/#import","title":"Import","text":"<pre><code>from xeepy.scrapers.recommendations import RecommendationsScraper\n</code></pre>"},{"location":"api/scrapers/recommendations/#class-signature","title":"Class Signature","text":"<pre><code>class RecommendationsScraper:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/scrapers/recommendations/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/scrapers/recommendations/#methods","title":"Methods","text":"Method Returns Description <code>trends(location)</code> <code>ScrapeResult[Trend]</code> Get trending topics <code>recommended_users(based_on)</code> <code>ScrapeResult[Recommendation]</code> Get user recommendations <code>explore(tab, limit)</code> <code>ScrapeResult[Tweet]</code> Explore page content <code>for_you()</code> <code>ScrapeResult[Tweet]</code> For You timeline <code>topics()</code> <code>ScrapeResult[Topic]</code> Get suggested topics"},{"location":"api/scrapers/recommendations/#trends","title":"<code>trends</code>","text":"<pre><code>async def trends(\n    self,\n    location: Optional[str] = None,\n    woeid: Optional[int] = None\n) -&gt; ScrapeResult[Trend]\n</code></pre> <p>Get trending topics for a location.</p> <p>Parameters: - <code>location</code>: Location name (e.g., \"United States\", \"New York\") - <code>woeid</code>: Yahoo Where On Earth ID</p>"},{"location":"api/scrapers/recommendations/#recommended_users","title":"<code>recommended_users</code>","text":"<pre><code>async def recommended_users(\n    self,\n    based_on: Optional[List[str]] = None,\n    limit: int = 20\n) -&gt; ScrapeResult[Recommendation]\n</code></pre> <p>Get recommended users to follow.</p> <p>Parameters: - <code>based_on</code>: Usernames to base recommendations on - <code>limit</code>: Maximum recommendations</p>"},{"location":"api/scrapers/recommendations/#explore","title":"<code>explore</code>","text":"<pre><code>async def explore(\n    self,\n    tab: str = \"for-you\",\n    limit: int = 50\n) -&gt; ScrapeResult[Tweet]\n</code></pre> <p>Get content from the Explore page.</p> <p>Parameters: - <code>tab</code>: Explore tab (<code>for-you</code>, <code>trending</code>, <code>news</code>, <code>sports</code>, <code>entertainment</code>) - <code>limit</code>: Maximum tweets</p>"},{"location":"api/scrapers/recommendations/#trend-object","title":"Trend Object","text":"<pre><code>@dataclass\nclass Trend:\n    name: str                        # Trend name/hashtag\n    url: str                         # Trend URL\n    tweet_count: Optional[int]       # Number of tweets\n    category: Optional[str]          # Trend category\n    description: Optional[str]       # Trend description\n    promoted: bool                   # Is promoted content\n    position: int                    # Rank position\n</code></pre>"},{"location":"api/scrapers/recommendations/#recommendation-object","title":"Recommendation Object","text":"<pre><code>@dataclass\nclass Recommendation:\n    user: User                       # Recommended user\n    reason: str                      # Why recommended\n    mutual_followers: List[str]      # Mutual followers\n    context: Optional[str]           # Additional context\n</code></pre>"},{"location":"api/scrapers/recommendations/#usage-examples","title":"Usage Examples","text":""},{"location":"api/scrapers/recommendations/#get-trending-topics","title":"Get Trending Topics","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.trends(location=\"United States\")\n\n        print(\"Trending in United States:\")\n        for trend in result.items:\n            tweet_count = f\"{trend.tweet_count:,}\" if trend.tweet_count else \"N/A\"\n            print(f\"  {trend.position}. {trend.name} ({tweet_count} tweets)\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/recommendations/#trends-by-city","title":"Trends by City","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        cities = [\"New York\", \"Los Angeles\", \"London\", \"Tokyo\"]\n\n        for city in cities:\n            result = await x.scrape.trends(location=city)\n            print(f\"\\n{city} Top 5 Trends:\")\n            for trend in result.items[:5]:\n                print(f\"  - {trend.name}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/recommendations/#get-recommended-users","title":"Get Recommended Users","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.recommended_users(\n            based_on=[\"elonmusk\", \"BillGates\"],\n            limit=20\n        )\n\n        print(\"Recommended users to follow:\")\n        for rec in result.items:\n            print(f\"  @{rec.user.username}\")\n            print(f\"    Reason: {rec.reason}\")\n            print(f\"    Followers: {rec.user.followers_count:,}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/recommendations/#explore-page-content","title":"Explore Page Content","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # For You tab\n        for_you = await x.scrape.explore(tab=\"for-you\", limit=30)\n\n        # Trending tab\n        trending = await x.scrape.explore(tab=\"trending\", limit=30)\n\n        # News tab\n        news = await x.scrape.explore(tab=\"news\", limit=30)\n\n        print(f\"For You: {len(for_you.items)} tweets\")\n        print(f\"Trending: {len(trending.items)} tweets\")\n        print(f\"News: {len(news.items)} tweets\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/recommendations/#track-trend-movement","title":"Track Trend Movement","text":"<pre><code>from xeepy import Xeepy\nimport asyncio\nfrom datetime import datetime\n\nasync def track_trends(location: str, interval: int = 3600):\n    \"\"\"Track trend positions over time.\"\"\"\n    history = {}\n\n    async with Xeepy() as x:\n        while True:\n            result = await x.scrape.trends(location=location)\n            timestamp = datetime.now().isoformat()\n\n            for trend in result.items:\n                if trend.name not in history:\n                    history[trend.name] = []\n                history[trend.name].append({\n                    \"time\": timestamp,\n                    \"position\": trend.position,\n                    \"tweets\": trend.tweet_count\n                })\n\n            # Report changes\n            print(f\"\\n[{timestamp}] Trend Update:\")\n            for trend in result.items[:10]:\n                prev = history[trend.name][-2] if len(history[trend.name]) &gt; 1 else None\n                change = \"\"\n                if prev:\n                    diff = prev[\"position\"] - trend.position\n                    if diff &gt; 0:\n                        change = f\"\u2191{diff}\"\n                    elif diff &lt; 0:\n                        change = f\"\u2193{abs(diff)}\"\n                print(f\"  {trend.position}. {trend.name} {change}\")\n\n            await asyncio.sleep(interval)\n\nasyncio.run(track_trends(\"United States\"))\n</code></pre>"},{"location":"api/scrapers/recommendations/#export-trends-data","title":"Export Trends Data","text":"<pre><code>from xeepy import Xeepy\nfrom datetime import datetime\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.trends(location=\"Worldwide\")\n\n        data = [\n            {\n                \"position\": t.position,\n                \"name\": t.name,\n                \"tweet_count\": t.tweet_count,\n                \"category\": t.category,\n                \"promoted\": t.promoted,\n                \"scraped_at\": datetime.now().isoformat()\n            }\n            for t in result.items\n        ]\n\n        x.export.to_csv(data, \"trends.csv\")\n        x.export.to_json(data, \"trends.json\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/recommendations/#get-suggested-topics","title":"Get Suggested Topics","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.topics()\n\n        print(\"Suggested Topics:\")\n        for topic in result.items:\n            print(f\"  - {topic.name}\")\n            print(f\"    Category: {topic.category}\")\n            print(f\"    Followers: {topic.follower_count:,}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/recommendations/#see-also","title":"See Also","text":"<ul> <li>Tweet Model - Tweet data structure</li> <li>HashtagScraper - Hashtag scraping</li> <li>SearchScraper - Search functionality</li> </ul>"},{"location":"api/scrapers/replies/","title":"RepliesScraper","text":"<p>Scrapes replies to a specific tweet, with support for filtering and pagination.</p>"},{"location":"api/scrapers/replies/#import","title":"Import","text":"<pre><code>from xeepy.scrapers.replies import RepliesScraper\n</code></pre>"},{"location":"api/scrapers/replies/#class-signature","title":"Class Signature","text":"<pre><code>class RepliesScraper:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/scrapers/replies/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/scrapers/replies/#methods","title":"Methods","text":"Method Returns Description <code>scrape(tweet_url, limit)</code> <code>ScrapeResult[Tweet]</code> Scrape replies to a tweet <code>scrape_all(tweet_url)</code> <code>ScrapeResult[Tweet]</code> Scrape all available replies <code>scrape_by_author(tweet_url, author)</code> <code>ScrapeResult[Tweet]</code> Filter replies by author <code>scrape_verified_only(tweet_url)</code> <code>ScrapeResult[Tweet]</code> Get only verified user replies"},{"location":"api/scrapers/replies/#scrape","title":"<code>scrape</code>","text":"<pre><code>async def scrape(\n    self,\n    tweet_url: str,\n    limit: int = 100,\n    include_author_replies: bool = True,\n    sort_by: str = \"relevance\"\n) -&gt; ScrapeResult[Tweet]\n</code></pre> <p>Scrape replies to a specific tweet.</p> <p>Parameters: - <code>tweet_url</code>: URL of the tweet to get replies for - <code>limit</code>: Maximum number of replies to fetch - <code>include_author_replies</code>: Include replies from the tweet author - <code>sort_by</code>: Sort order (<code>relevance</code>, <code>recency</code>, <code>likes</code>)</p> <p>Returns: <code>ScrapeResult</code> containing list of <code>Tweet</code> objects</p>"},{"location":"api/scrapers/replies/#scrape_all","title":"<code>scrape_all</code>","text":"<pre><code>async def scrape_all(\n    self,\n    tweet_url: str,\n    include_author_replies: bool = True\n) -&gt; ScrapeResult[Tweet]\n</code></pre> <p>Scrape all available replies without limit.</p>"},{"location":"api/scrapers/replies/#scrape_by_author","title":"<code>scrape_by_author</code>","text":"<pre><code>async def scrape_by_author(\n    self,\n    tweet_url: str,\n    author: str,\n    limit: int = 100\n) -&gt; ScrapeResult[Tweet]\n</code></pre> <p>Get replies from a specific author.</p>"},{"location":"api/scrapers/replies/#scrape_verified_only","title":"<code>scrape_verified_only</code>","text":"<pre><code>async def scrape_verified_only(\n    self,\n    tweet_url: str,\n    limit: int = 100\n) -&gt; ScrapeResult[Tweet]\n</code></pre> <p>Get replies only from verified accounts.</p>"},{"location":"api/scrapers/replies/#scraperesult-object","title":"ScrapeResult Object","text":"<pre><code>@dataclass\nclass ScrapeResult[T]:\n    items: List[T]           # List of scraped items\n    total_count: int         # Total items scraped\n    cursor: Optional[str]    # Pagination cursor\n    has_more: bool           # More items available\n    scrape_time: datetime    # When scrape completed\n    errors: List[str]        # Any errors encountered\n</code></pre>"},{"location":"api/scrapers/replies/#usage-examples","title":"Usage Examples","text":""},{"location":"api/scrapers/replies/#basic-reply-scraping","title":"Basic Reply Scraping","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Scrape up to 100 replies\n        result = await x.scrape.replies(\n            \"https://x.com/elonmusk/status/123456789\"\n        )\n\n        for reply in result.items:\n            print(f\"@{reply.author.username}: {reply.text}\")\n            print(f\"  Likes: {reply.like_count}, Retweets: {reply.retweet_count}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/replies/#with-filtering-options","title":"With Filtering Options","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Get recent replies, excluding author\n        result = await x.scrape.replies(\n            \"https://x.com/user/status/123\",\n            limit=500,\n            include_author_replies=False,\n            sort_by=\"recency\"\n        )\n\n        print(f\"Found {result.total_count} replies\")\n\n        # Export to CSV\n        x.export.to_csv(result.items, \"replies.csv\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/replies/#filter-by-specific-author","title":"Filter by Specific Author","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Get replies from a specific user\n        result = await x.scrape.replies_by_author(\n            \"https://x.com/user/status/123\",\n            author=\"specificuser\"\n        )\n\n        for reply in result.items:\n            print(f\"{reply.created_at}: {reply.text}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/replies/#verified-users-only","title":"Verified Users Only","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Only get replies from verified accounts\n        result = await x.scrape.replies_verified_only(\n            \"https://x.com/viral_tweet/status/123\"\n        )\n\n        for reply in result.items:\n            print(f\"\u2713 @{reply.author.username}: {reply.text}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/replies/#see-also","title":"See Also","text":"<ul> <li>Tweet Model - Tweet data structure</li> <li>ProfileScraper - User profile scraping</li> <li>TweetsScraper - User tweets scraping</li> </ul>"},{"location":"api/scrapers/search/","title":"SearchScraper","text":"<p>Scrapes Twitter/X search results with advanced filtering options.</p>"},{"location":"api/scrapers/search/#import","title":"Import","text":"<pre><code>from xeepy.scrapers.search import SearchScraper\n</code></pre>"},{"location":"api/scrapers/search/#class-signature","title":"Class Signature","text":"<pre><code>class SearchScraper:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/scrapers/search/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/scrapers/search/#methods","title":"Methods","text":"Method Returns Description <code>scrape(query, limit)</code> <code>ScrapeResult[Tweet]</code> Search tweets <code>scrape_users(query, limit)</code> <code>ScrapeResult[User]</code> Search users <code>scrape_advanced(query)</code> <code>ScrapeResult[Tweet]</code> Advanced search <code>scrape_latest(query)</code> <code>ScrapeResult[Tweet]</code> Latest tweets <code>scrape_top(query)</code> <code>ScrapeResult[Tweet]</code> Top/popular tweets"},{"location":"api/scrapers/search/#scrape","title":"<code>scrape</code>","text":"<pre><code>async def scrape(\n    self,\n    query: str,\n    limit: int = 100,\n    search_type: str = \"latest\",\n    language: Optional[str] = None,\n    cursor: Optional[str] = None\n) -&gt; ScrapeResult[Tweet]\n</code></pre> <p>Search for tweets matching a query.</p> <p>Parameters: - <code>query</code>: Search query (supports Twitter search operators) - <code>limit</code>: Maximum results to fetch - <code>search_type</code>: Result type (<code>latest</code>, <code>top</code>, <code>photos</code>, <code>videos</code>) - <code>language</code>: Filter by language code (e.g., <code>en</code>, <code>es</code>) - <code>cursor</code>: Pagination cursor</p>"},{"location":"api/scrapers/search/#scrape_advanced","title":"<code>scrape_advanced</code>","text":"<pre><code>async def scrape_advanced(\n    self,\n    query: str,\n    from_user: Optional[str] = None,\n    to_user: Optional[str] = None,\n    mentions: Optional[List[str]] = None,\n    min_likes: Optional[int] = None,\n    min_retweets: Optional[int] = None,\n    min_replies: Optional[int] = None,\n    start_date: Optional[datetime] = None,\n    end_date: Optional[datetime] = None,\n    has_media: Optional[bool] = None,\n    has_links: Optional[bool] = None,\n    exclude_retweets: bool = False,\n    limit: int = 100\n) -&gt; ScrapeResult[Tweet]\n</code></pre> <p>Advanced search with multiple filters.</p>"},{"location":"api/scrapers/search/#scrape_users","title":"<code>scrape_users</code>","text":"<pre><code>async def scrape_users(\n    self,\n    query: str,\n    limit: int = 50\n) -&gt; ScrapeResult[User]\n</code></pre> <p>Search for users matching a query.</p>"},{"location":"api/scrapers/search/#search-operators","title":"Search Operators","text":"Operator Example Description <code>from:</code> <code>from:elonmusk</code> Tweets from user <code>to:</code> <code>to:elonmusk</code> Replies to user <code>@</code> <code>@username</code> Mentions user <code>#</code> <code>#python</code> Contains hashtag <code>\"text\"</code> <code>\"exact phrase\"</code> Exact phrase match <code>min_faves:</code> <code>min_faves:100</code> Minimum likes <code>min_retweets:</code> <code>min_retweets:50</code> Minimum retweets <code>since:</code> <code>since:2024-01-01</code> After date <code>until:</code> <code>until:2024-12-31</code> Before date <code>filter:media</code> <code>filter:media</code> Has media <code>filter:links</code> <code>filter:links</code> Has links <code>-filter:retweets</code> <code>-filter:retweets</code> Exclude retweets <code>lang:</code> <code>lang:en</code> Language filter"},{"location":"api/scrapers/search/#usage-examples","title":"Usage Examples","text":""},{"location":"api/scrapers/search/#basic-search","title":"Basic Search","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.search(\"python programming\", limit=100)\n\n        for tweet in result.items:\n            print(f\"@{tweet.author.username}: {tweet.text[:80]}...\")\n            print(f\"  \u2764\ufe0f {tweet.like_count}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/search/#advanced-search-with-filters","title":"Advanced Search with Filters","text":"<pre><code>from xeepy import Xeepy\nfrom datetime import datetime, timedelta\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.search_advanced(\n            query=\"AI\",\n            from_user=\"OpenAI\",\n            min_likes=1000,\n            start_date=datetime.now() - timedelta(days=30),\n            exclude_retweets=True,\n            limit=50\n        )\n\n        print(f\"Found {len(result.items)} tweets\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/search/#search-using-operators","title":"Search Using Operators","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Complex search query\n        query = (\n            \"#python \"\n            \"min_faves:100 \"\n            \"-filter:retweets \"\n            \"lang:en\"\n        )\n\n        result = await x.scrape.search(query, limit=200)\n\n        for tweet in result.items:\n            print(f\"{tweet.created_at}: {tweet.text[:50]}...\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/search/#search-users","title":"Search Users","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.search_users(\"data scientist\", limit=50)\n\n        for user in result.items:\n            print(f\"@{user.username} - {user.name}\")\n            print(f\"  {user.followers_count:,} followers\")\n            print(f\"  Bio: {user.bio[:50] if user.bio else 'N/A'}...\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/search/#monitor-keywords","title":"Monitor Keywords","text":"<pre><code>from xeepy import Xeepy\nimport asyncio\n\nasync def monitor_keyword(keyword: str, interval: int = 300):\n    \"\"\"Monitor keyword and print new tweets.\"\"\"\n    seen_ids = set()\n\n    async with Xeepy() as x:\n        while True:\n            result = await x.scrape.search(\n                keyword,\n                search_type=\"latest\",\n                limit=20\n            )\n\n            for tweet in result.items:\n                if tweet.id not in seen_ids:\n                    seen_ids.add(tweet.id)\n                    print(f\"NEW: @{tweet.author.username}: {tweet.text[:80]}\")\n\n            await asyncio.sleep(interval)\n\nasyncio.run(monitor_keyword(\"#breaking\"))\n</code></pre>"},{"location":"api/scrapers/search/#export-search-results","title":"Export Search Results","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.search(\n            \"startup funding\",\n            search_type=\"top\",\n            limit=500\n        )\n\n        # Export to multiple formats\n        x.export.to_csv(result.items, \"search_results.csv\")\n        x.export.to_json(result.items, \"search_results.json\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/search/#see-also","title":"See Also","text":"<ul> <li>Tweet Model - Tweet data structure</li> <li>HashtagScraper - Hashtag-specific scraping</li> <li>KeywordMonitor - Keyword monitoring</li> </ul>"},{"location":"api/scrapers/spaces/","title":"SpacesScraper","text":"<p>Scrapes Twitter/X Spaces including audio, transcripts, and chat messages.</p>"},{"location":"api/scrapers/spaces/#import","title":"Import","text":"<pre><code>from xeepy.scrapers.spaces import SpacesScraper, SpaceCategory\n</code></pre>"},{"location":"api/scrapers/spaces/#class-signature","title":"Class Signature","text":"<pre><code>class SpacesScraper:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/scrapers/spaces/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/scrapers/spaces/#spacecategory-enum","title":"SpaceCategory Enum","text":"<pre><code>class SpaceCategory(Enum):\n    LIVE = \"live\"           # Currently live Spaces\n    UPCOMING = \"upcoming\"   # Scheduled Spaces\n    RECORDED = \"recorded\"   # Ended/recorded Spaces\n    ALL = \"all\"             # All Spaces\n</code></pre>"},{"location":"api/scrapers/spaces/#methods","title":"Methods","text":"Method Returns Description <code>scrape(room_ids, search)</code> <code>ScrapeResult[Space]</code> Scrape Spaces by ID or search <code>scrape_live()</code> <code>ScrapeResult[Space]</code> Get currently live Spaces <code>scrape_upcoming()</code> <code>ScrapeResult[Space]</code> Get scheduled Spaces <code>scrape_by_user(username)</code> <code>ScrapeResult[Space]</code> User's Spaces <code>capture_audio(room_id)</code> <code>str</code> Download Space audio <code>get_transcript(room_id)</code> <code>List[TranscriptSegment]</code> Get transcript <code>get_chat(room_id)</code> <code>List[ChatMessage]</code> Get chat messages"},{"location":"api/scrapers/spaces/#scrape","title":"<code>scrape</code>","text":"<pre><code>async def scrape(\n    self,\n    room_ids: Optional[List[str]] = None,\n    search: Optional[List[Dict]] = None,\n    audio: bool = False,\n    chat: bool = False,\n    transcript: bool = False,\n    output_dir: str = \"spaces\"\n) -&gt; ScrapeResult[Space]\n</code></pre> <p>Scrape Spaces by room IDs or search criteria.</p> <p>Parameters: - <code>room_ids</code>: List of Space room IDs - <code>search</code>: Search criteria with query and filter - <code>audio</code>: Capture audio stream - <code>chat</code>: Capture chat messages - <code>transcript</code>: Capture live transcript - <code>output_dir</code>: Directory for downloaded files</p>"},{"location":"api/scrapers/spaces/#scrape_live","title":"<code>scrape_live</code>","text":"<pre><code>async def scrape_live(\n    self,\n    limit: int = 20\n) -&gt; ScrapeResult[Space]\n</code></pre> <p>Get currently live Spaces.</p>"},{"location":"api/scrapers/spaces/#capture_audio","title":"<code>capture_audio</code>","text":"<pre><code>async def capture_audio(\n    self,\n    room_id: str,\n    output_path: Optional[str] = None,\n    duration: Optional[int] = None\n) -&gt; str\n</code></pre> <p>Capture audio from a live or recorded Space.</p>"},{"location":"api/scrapers/spaces/#space-object","title":"Space Object","text":"<pre><code>@dataclass\nclass Space:\n    id: str                          # Space ID\n    title: str                       # Space title\n    state: str                       # live, ended, scheduled\n    host: User                       # Space host\n    speakers: List[User]             # Current speakers\n    listener_count: int              # Number of listeners\n    participant_count: int           # Total participants\n    started_at: Optional[datetime]   # Start time\n    ended_at: Optional[datetime]     # End time\n    scheduled_start: Optional[datetime] # Scheduled start\n    is_recorded: bool                # Recording available\n    audio_url: Optional[str]         # Audio file path\n    chat_messages: List[ChatMessage] # Chat messages\n    transcripts: List[TranscriptSegment] # Transcript segments\n</code></pre>"},{"location":"api/scrapers/spaces/#usage-examples","title":"Usage Examples","text":""},{"location":"api/scrapers/spaces/#scrape-space-by-id","title":"Scrape Space by ID","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.spaces(\n            room_ids=[\"1eaJbrAPnBVJX\"]\n        )\n\n        for space in result.items:\n            print(f\"Title: {space.title}\")\n            print(f\"Host: @{space.host.username}\")\n            print(f\"Listeners: {space.listener_count}\")\n            print(f\"State: {space.state}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/spaces/#search-for-spaces","title":"Search for Spaces","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.scrapers.spaces import SpaceCategory\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.spaces(\n            search=[\n                {\"query\": \"crypto\", \"filter\": SpaceCategory.LIVE},\n                {\"query\": \"AI\", \"filter\": SpaceCategory.UPCOMING}\n            ]\n        )\n\n        for space in result.items:\n            print(f\"[{space.state.upper()}] {space.title}\")\n            print(f\"  Host: @{space.host.username}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/spaces/#capture-audio-and-transcript","title":"Capture Audio and Transcript","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.spaces(\n            room_ids=[\"1eaJbrAPnBVJX\"],\n            audio=True,\n            transcript=True,\n            output_dir=\"recordings\"\n        )\n\n        for space in result.items:\n            print(f\"Space: {space.title}\")\n            print(f\"Audio saved to: {space.audio_url}\")\n            print(f\"Transcript segments: {len(space.transcripts)}\")\n\n            for segment in space.transcripts[:5]:\n                print(f\"  [{segment.speaker}]: {segment.text}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/spaces/#capture-chat-messages","title":"Capture Chat Messages","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.spaces(\n            room_ids=[\"1eaJbrAPnBVJX\"],\n            chat=True\n        )\n\n        for space in result.items:\n            print(f\"Chat messages: {len(space.chat_messages)}\")\n\n            for msg in space.chat_messages[:10]:\n                print(f\"@{msg.author.username}: {msg.text}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/spaces/#get-live-spaces","title":"Get Live Spaces","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.spaces_live(limit=20)\n\n        print(\"Currently Live Spaces:\")\n        for space in result.items:\n            print(f\"  \ud83d\udd34 {space.title}\")\n            print(f\"     Host: @{space.host.username}\")\n            print(f\"     Listeners: {space.listener_count:,}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/spaces/#get-users-spaces","title":"Get User's Spaces","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.spaces_by_user(\"username\")\n\n        for space in result.items:\n            status = \"\ud83d\udd34 LIVE\" if space.state == \"live\" else \"\u23fa\ufe0f RECORDED\"\n            print(f\"{status} {space.title}\")\n            print(f\"  Started: {space.started_at}\")\n            print(f\"  Listeners: {space.listener_count}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/spaces/#export-space-data","title":"Export Space Data","text":"<pre><code>from xeepy import Xeepy\nimport json\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.spaces(\n            room_ids=[\"1eaJbrAPnBVJX\"],\n            chat=True,\n            transcript=True\n        )\n\n        for space in result.items:\n            data = {\n                \"title\": space.title,\n                \"host\": space.host.username,\n                \"listener_count\": space.listener_count,\n                \"chat\": [\n                    {\"author\": m.author.username, \"text\": m.text}\n                    for m in space.chat_messages\n                ],\n                \"transcript\": [\n                    {\"speaker\": s.speaker, \"text\": s.text, \"time\": s.timestamp}\n                    for s in space.transcripts\n                ]\n            }\n\n            with open(f\"space_{space.id}.json\", \"w\") as f:\n                json.dump(data, f, indent=2)\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/spaces/#see-also","title":"See Also","text":"<ul> <li>User Model - User data structure</li> <li>MediaDownloader - Media downloading</li> <li>SearchScraper - Search functionality</li> </ul>"},{"location":"api/scrapers/thread/","title":"ThreadScraper","text":"<p>Scrapes and unrolls Twitter/X threads into a complete conversation.</p>"},{"location":"api/scrapers/thread/#import","title":"Import","text":"<pre><code>from xeepy.scrapers.thread import ThreadScraper\n</code></pre>"},{"location":"api/scrapers/thread/#class-signature","title":"Class Signature","text":"<pre><code>class ThreadScraper:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/scrapers/thread/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/scrapers/thread/#methods","title":"Methods","text":"Method Returns Description <code>scrape(tweet_url)</code> <code>Thread</code> Unroll a thread <code>scrape_from_tweet(tweet_id)</code> <code>Thread</code> Unroll from tweet ID <code>is_thread(tweet_url)</code> <code>bool</code> Check if tweet is part of thread <code>get_thread_length(tweet_url)</code> <code>int</code> Count tweets in thread"},{"location":"api/scrapers/thread/#scrape","title":"<code>scrape</code>","text":"<pre><code>async def scrape(\n    self,\n    tweet_url: str,\n    include_media: bool = True\n) -&gt; Thread\n</code></pre> <p>Unroll a complete thread from any tweet in the thread.</p> <p>Parameters: - <code>tweet_url</code>: URL of any tweet in the thread - <code>include_media</code>: Include media attachments</p> <p>Returns: <code>Thread</code> object containing all tweets in order</p>"},{"location":"api/scrapers/thread/#scrape_from_tweet","title":"<code>scrape_from_tweet</code>","text":"<pre><code>async def scrape_from_tweet(\n    self,\n    tweet_id: str,\n    include_media: bool = True\n) -&gt; Thread\n</code></pre> <p>Unroll thread using tweet ID.</p>"},{"location":"api/scrapers/thread/#is_thread","title":"<code>is_thread</code>","text":"<pre><code>async def is_thread(self, tweet_url: str) -&gt; bool\n</code></pre> <p>Check if a tweet is part of a thread.</p>"},{"location":"api/scrapers/thread/#get_thread_length","title":"<code>get_thread_length</code>","text":"<pre><code>async def get_thread_length(self, tweet_url: str) -&gt; int\n</code></pre> <p>Get the number of tweets in a thread without full scrape.</p>"},{"location":"api/scrapers/thread/#thread-object","title":"Thread Object","text":"<pre><code>@dataclass\nclass Thread:\n    author: User                     # Thread author\n    tweets: List[Tweet]              # Ordered list of tweets\n    total_tweets: int                # Number of tweets\n    created_at: datetime             # First tweet timestamp\n    total_likes: int                 # Sum of all likes\n    total_retweets: int              # Sum of all retweets\n    total_replies: int               # Sum of all replies\n\n    def to_text(self) -&gt; str:\n        \"\"\"Convert thread to readable text.\"\"\"\n        pass\n\n    def to_markdown(self) -&gt; str:\n        \"\"\"Convert thread to markdown format.\"\"\"\n        pass\n</code></pre>"},{"location":"api/scrapers/thread/#usage-examples","title":"Usage Examples","text":""},{"location":"api/scrapers/thread/#basic-thread-unrolling","title":"Basic Thread Unrolling","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        thread = await x.scrape.thread(\n            \"https://x.com/user/status/123456789\"\n        )\n\n        print(f\"Thread by @{thread.author.username}\")\n        print(f\"Total tweets: {thread.total_tweets}\")\n        print(\"=\" * 50)\n\n        for i, tweet in enumerate(thread.tweets, 1):\n            print(f\"\\n[{i}/{thread.total_tweets}]\")\n            print(tweet.text)\n            print(f\"\u2764\ufe0f {tweet.like_count}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/thread/#export-thread-as-text","title":"Export Thread as Text","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        thread = await x.scrape.thread(\n            \"https://x.com/user/status/123456789\"\n        )\n\n        # Export as plain text\n        text = thread.to_text()\n        with open(\"thread.txt\", \"w\") as f:\n            f.write(text)\n\n        # Export as markdown\n        markdown = thread.to_markdown()\n        with open(\"thread.md\", \"w\") as f:\n            f.write(markdown)\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/thread/#check-thread-before-scraping","title":"Check Thread Before Scraping","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        url = \"https://x.com/user/status/123\"\n\n        if await x.scrape.is_thread(url):\n            length = await x.scrape.thread_length(url)\n            print(f\"This is a thread with {length} tweets\")\n\n            thread = await x.scrape.thread(url)\n            # Process thread...\n        else:\n            print(\"This is a single tweet\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/thread/#thread-analytics","title":"Thread Analytics","text":"<pre><code>from xeepy import Xeepy\n\nasync def analyze_thread(url: str):\n    async with Xeepy() as x:\n        thread = await x.scrape.thread(url)\n\n        print(f\"Thread Analytics for @{thread.author.username}\")\n        print(\"=\" * 50)\n        print(f\"Total tweets: {thread.total_tweets}\")\n        print(f\"Total likes: {thread.total_likes:,}\")\n        print(f\"Total retweets: {thread.total_retweets:,}\")\n        print(f\"Total replies: {thread.total_replies:,}\")\n\n        avg_likes = thread.total_likes / thread.total_tweets\n        print(f\"Avg likes per tweet: {avg_likes:.1f}\")\n\n        # Find best performing tweet in thread\n        best = max(thread.tweets, key=lambda t: t.like_count)\n        print(f\"\\nBest performing tweet ({best.like_count} likes):\")\n        print(f\"  {best.text[:100]}...\")\n\nasyncio.run(analyze_thread(\"https://x.com/user/status/123\"))\n</code></pre>"},{"location":"api/scrapers/thread/#batch-thread-export","title":"Batch Thread Export","text":"<pre><code>from xeepy import Xeepy\nimport json\n\nasync def export_threads(urls: list):\n    async with Xeepy() as x:\n        threads_data = []\n\n        for url in urls:\n            try:\n                thread = await x.scrape.thread(url)\n                threads_data.append({\n                    \"author\": thread.author.username,\n                    \"tweets\": [t.text for t in thread.tweets],\n                    \"total_likes\": thread.total_likes,\n                    \"url\": url\n                })\n            except Exception as e:\n                print(f\"Failed to scrape {url}: {e}\")\n\n        with open(\"threads.json\", \"w\") as f:\n            json.dump(threads_data, f, indent=2)\n\nurls = [\n    \"https://x.com/user1/status/123\",\n    \"https://x.com/user2/status/456\"\n]\nasyncio.run(export_threads(urls))\n</code></pre>"},{"location":"api/scrapers/thread/#see-also","title":"See Also","text":"<ul> <li>Tweet Model - Tweet data structure</li> <li>TweetsScraper - User timeline scraping</li> <li>RepliesScraper - Tweet replies</li> </ul>"},{"location":"api/scrapers/tweets/","title":"TweetsScraper","text":"<p>Scrapes tweets from a user's profile timeline.</p>"},{"location":"api/scrapers/tweets/#import","title":"Import","text":"<pre><code>from xeepy.scrapers.tweets import TweetsScraper\n</code></pre>"},{"location":"api/scrapers/tweets/#class-signature","title":"Class Signature","text":"<pre><code>class TweetsScraper:\n    def __init__(\n        self,\n        browser_manager: BrowserManager,\n        rate_limiter: Optional[RateLimiter] = None\n    )\n</code></pre>"},{"location":"api/scrapers/tweets/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>browser_manager</code> <code>BrowserManager</code> Required Browser manager instance <code>rate_limiter</code> <code>Optional[RateLimiter]</code> <code>None</code> Rate limiter instance"},{"location":"api/scrapers/tweets/#methods","title":"Methods","text":"Method Returns Description <code>scrape(username, limit)</code> <code>ScrapeResult[Tweet]</code> Get user's tweets <code>scrape_with_replies(username)</code> <code>ScrapeResult[Tweet]</code> Include replies <code>scrape_media_only(username)</code> <code>ScrapeResult[Tweet]</code> Media tweets only <code>scrape_by_date_range(username, start, end)</code> <code>ScrapeResult[Tweet]</code> Filter by date <code>get_tweet(tweet_id)</code> <code>Tweet</code> Get single tweet by ID"},{"location":"api/scrapers/tweets/#scrape","title":"<code>scrape</code>","text":"<pre><code>async def scrape(\n    self,\n    username: str,\n    limit: int = 100,\n    include_retweets: bool = True,\n    include_replies: bool = False,\n    cursor: Optional[str] = None\n) -&gt; ScrapeResult[Tweet]\n</code></pre> <p>Scrape tweets from a user's timeline.</p> <p>Parameters: - <code>username</code>: Target username - <code>limit</code>: Maximum tweets to fetch - <code>include_retweets</code>: Include retweets - <code>include_replies</code>: Include reply tweets - <code>cursor</code>: Pagination cursor</p> <p>Returns: <code>ScrapeResult</code> containing <code>Tweet</code> objects</p>"},{"location":"api/scrapers/tweets/#scrape_by_date_range","title":"<code>scrape_by_date_range</code>","text":"<pre><code>async def scrape_by_date_range(\n    self,\n    username: str,\n    start_date: datetime,\n    end_date: datetime,\n    limit: int = 1000\n) -&gt; ScrapeResult[Tweet]\n</code></pre> <p>Scrape tweets within a specific date range.</p>"},{"location":"api/scrapers/tweets/#get_tweet","title":"<code>get_tweet</code>","text":"<pre><code>async def get_tweet(self, tweet_id: str) -&gt; Tweet\n</code></pre> <p>Get a single tweet by its ID.</p>"},{"location":"api/scrapers/tweets/#tweet-object","title":"Tweet Object","text":"<pre><code>@dataclass\nclass Tweet:\n    id: str                          # Tweet ID\n    text: str                        # Tweet content\n    author: User                     # Author info\n    created_at: datetime             # Post time\n    like_count: int                  # Number of likes\n    retweet_count: int               # Number of retweets\n    reply_count: int                 # Number of replies\n    quote_count: int                 # Number of quotes\n    view_count: Optional[int]        # View count\n    language: str                    # Detected language\n    source: str                      # Posted from (device/app)\n    is_retweet: bool                 # Is a retweet\n    is_reply: bool                   # Is a reply\n    is_quote: bool                   # Is a quote tweet\n    retweeted_tweet: Optional[Tweet] # Original if retweet\n    quoted_tweet: Optional[Tweet]    # Quoted tweet\n    in_reply_to_id: Optional[str]    # Reply parent ID\n    media: List[Media]               # Attached media\n    urls: List[str]                  # URLs in tweet\n    hashtags: List[str]              # Hashtags used\n    mentions: List[str]              # Mentioned users\n</code></pre>"},{"location":"api/scrapers/tweets/#usage-examples","title":"Usage Examples","text":""},{"location":"api/scrapers/tweets/#basic-tweet-scraping","title":"Basic Tweet Scraping","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.tweets(\"elonmusk\", limit=100)\n\n        for tweet in result.items:\n            print(f\"[{tweet.created_at}] {tweet.text[:50]}...\")\n            print(f\"  \u2764\ufe0f {tweet.like_count} | \ud83d\udd04 {tweet.retweet_count}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/tweets/#filter-retweets-and-replies","title":"Filter Retweets and Replies","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Original tweets only\n        result = await x.scrape.tweets(\n            \"username\",\n            limit=200,\n            include_retweets=False,\n            include_replies=False\n        )\n\n        print(f\"Found {len(result.items)} original tweets\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/tweets/#scrape-by-date-range","title":"Scrape by Date Range","text":"<pre><code>from xeepy import Xeepy\nfrom datetime import datetime, timedelta\n\nasync def main():\n    async with Xeepy() as x:\n        end_date = datetime.now()\n        start_date = end_date - timedelta(days=30)\n\n        result = await x.scrape.tweets_by_date(\n            \"username\",\n            start_date=start_date,\n            end_date=end_date\n        )\n\n        print(f\"Tweets in last 30 days: {len(result.items)}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/tweets/#analyze-tweet-performance","title":"Analyze Tweet Performance","text":"<pre><code>from xeepy import Xeepy\n\nasync def analyze_tweets(username: str):\n    async with Xeepy() as x:\n        result = await x.scrape.tweets(username, limit=500)\n\n        total_likes = sum(t.like_count for t in result.items)\n        total_retweets = sum(t.retweet_count for t in result.items)\n\n        avg_likes = total_likes / len(result.items)\n        avg_retweets = total_retweets / len(result.items)\n\n        # Find best performing tweet\n        best_tweet = max(result.items, key=lambda t: t.like_count)\n\n        print(f\"Analyzed {len(result.items)} tweets\")\n        print(f\"Avg likes: {avg_likes:.1f}\")\n        print(f\"Avg retweets: {avg_retweets:.1f}\")\n        print(f\"Best tweet ({best_tweet.like_count} likes):\")\n        print(f\"  {best_tweet.text[:100]}...\")\n\nasyncio.run(analyze_tweets(\"username\"))\n</code></pre>"},{"location":"api/scrapers/tweets/#export-media-tweets","title":"Export Media Tweets","text":"<pre><code>from xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        result = await x.scrape.tweets_media_only(\"username\", limit=100)\n\n        media_data = []\n        for tweet in result.items:\n            for media in tweet.media:\n                media_data.append({\n                    \"tweet_id\": tweet.id,\n                    \"media_type\": media.type,\n                    \"media_url\": media.url,\n                    \"likes\": tweet.like_count\n                })\n\n        x.export.to_csv(media_data, \"media_tweets.csv\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/scrapers/tweets/#see-also","title":"See Also","text":"<ul> <li>Tweet Model - Tweet data structure</li> <li>RepliesScraper - Get tweet replies</li> <li>MediaScraper - Media-focused scraping</li> </ul>"},{"location":"api/storage/database/","title":"Database","text":"<p>SQLite-based storage for caching and historical data.</p>"},{"location":"api/storage/database/#import","title":"Import","text":"<pre><code>from xeepy.storage import Database\n</code></pre>"},{"location":"api/storage/database/#class-signature","title":"Class Signature","text":"<pre><code>class Database:\n    def __init__(\n        self,\n        path: str = \"xeepy.db\",\n        auto_create: bool = True\n    )\n</code></pre>"},{"location":"api/storage/database/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>path</code> <code>str</code> <code>\"xeepy.db\"</code> Database file path <code>auto_create</code> <code>bool</code> <code>True</code> Create tables automatically"},{"location":"api/storage/database/#methods","title":"Methods","text":"Method Returns Description <code>save_user(user)</code> <code>None</code> Store user data <code>save_tweet(tweet)</code> <code>None</code> Store tweet data <code>save_followers(username, users)</code> <code>None</code> Store follower list <code>get_user(username)</code> <code>Optional[User]</code> Retrieve user <code>get_tweet(id)</code> <code>Optional[Tweet]</code> Retrieve tweet <code>get_followers(username)</code> <code>List[User]</code> Retrieve followers <code>get_history(username, metric)</code> <code>List[DataPoint]</code> Get historical data <code>query(sql, params)</code> <code>List[Dict]</code> Execute raw SQL <code>close()</code> <code>None</code> Close connection"},{"location":"api/storage/database/#save_user","title":"<code>save_user</code>","text":"<pre><code>def save_user(\n    self,\n    user: User,\n    timestamp: Optional[datetime] = None\n) -&gt; None\n</code></pre> <p>Store user data with optional timestamp.</p>"},{"location":"api/storage/database/#get_history","title":"<code>get_history</code>","text":"<pre><code>def get_history(\n    self,\n    username: str,\n    metric: str,\n    start_date: Optional[datetime] = None,\n    end_date: Optional[datetime] = None\n) -&gt; List[DataPoint]\n</code></pre> <p>Retrieve historical data points.</p> <p>Parameters: - <code>username</code>: Account to get history for - <code>metric</code>: Metric name (<code>followers</code>, <code>following</code>, <code>tweets</code>) - <code>start_date</code>: Filter start - <code>end_date</code>: Filter end</p>"},{"location":"api/storage/database/#datapoint-object","title":"DataPoint Object","text":"<pre><code>@dataclass\nclass DataPoint:\n    timestamp: datetime\n    value: int\n    metric: str\n</code></pre>"},{"location":"api/storage/database/#usage-examples","title":"Usage Examples","text":""},{"location":"api/storage/database/#basic-storage","title":"Basic Storage","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.storage import Database\n\nasync def main():\n    db = Database(\"my_data.db\")\n\n    async with Xeepy() as x:\n        user = await x.scrape.profile(\"username\")\n        db.save_user(user)\n\n        print(f\"Saved @{user.username}\")\n\n    # Later retrieval\n    stored = db.get_user(\"username\")\n    print(f\"Retrieved: @{stored.username}\")\n\n    db.close()\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/storage/database/#store-followers","title":"Store Followers","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.storage import Database\n\nasync def store_followers(username: str):\n    db = Database()\n\n    async with Xeepy() as x:\n        followers = await x.scrape.followers(username, limit=1000)\n\n        db.save_followers(username, followers.items)\n        print(f\"Saved {len(followers.items)} followers\")\n\n    db.close()\n\nasyncio.run(store_followers(\"myaccount\"))\n</code></pre>"},{"location":"api/storage/database/#track-growth-over-time","title":"Track Growth Over Time","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.storage import Database\nfrom datetime import datetime, timedelta\n\nasync def track_growth(username: str):\n    db = Database()\n\n    async with Xeepy() as x:\n        user = await x.scrape.profile(username)\n        db.save_user(user)\n\n    # Get history\n    history = db.get_history(\n        username,\n        metric=\"followers\",\n        start_date=datetime.now() - timedelta(days=30)\n    )\n\n    print(f\"Follower history for @{username}:\")\n    for point in history:\n        print(f\"  {point.timestamp.date()}: {point.value:,}\")\n\n    db.close()\n\nasyncio.run(track_growth(\"myaccount\"))\n</code></pre>"},{"location":"api/storage/database/#compare-historical-data","title":"Compare Historical Data","text":"<pre><code>from xeepy.storage import Database\nfrom datetime import datetime, timedelta\n\ndef compare_periods(username: str):\n    db = Database()\n\n    now = datetime.now()\n    week_ago = now - timedelta(days=7)\n    month_ago = now - timedelta(days=30)\n\n    current = db.get_history(username, \"followers\", start_date=now - timedelta(hours=1))\n    week = db.get_history(username, \"followers\", start_date=week_ago, end_date=week_ago + timedelta(hours=1))\n    month = db.get_history(username, \"followers\", start_date=month_ago, end_date=month_ago + timedelta(hours=1))\n\n    if current and week and month:\n        print(f\"@{username} growth:\")\n        print(f\"  7-day: {current[0].value - week[0].value:+,}\")\n        print(f\"  30-day: {current[0].value - month[0].value:+,}\")\n\n    db.close()\n\ncompare_periods(\"myaccount\")\n</code></pre>"},{"location":"api/storage/database/#cache-tweet-data","title":"Cache Tweet Data","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.storage import Database\n\nasync def cache_tweets(username: str):\n    db = Database()\n\n    async with Xeepy() as x:\n        tweets = await x.scrape.tweets(username, limit=100)\n\n        for tweet in tweets.items:\n            db.save_tweet(tweet)\n\n        print(f\"Cached {len(tweets.items)} tweets\")\n\n    # Retrieve cached\n    cached = db.get_tweet(\"123456789\")\n    if cached:\n        print(f\"Found: {cached.text[:50]}...\")\n\n    db.close()\n\nasyncio.run(cache_tweets(\"username\"))\n</code></pre>"},{"location":"api/storage/database/#raw-sql-queries","title":"Raw SQL Queries","text":"<pre><code>from xeepy.storage import Database\n\ndef custom_query():\n    db = Database()\n\n    # Custom query\n    results = db.query(\"\"\"\n        SELECT username, followers_count, timestamp\n        FROM users\n        WHERE followers_count &gt; ?\n        ORDER BY followers_count DESC\n        LIMIT 10\n    \"\"\", [10000])\n\n    for row in results:\n        print(f\"@{row['username']}: {row['followers_count']:,}\")\n\n    db.close()\n\ncustom_query()\n</code></pre>"},{"location":"api/storage/database/#detect-unfollowers","title":"Detect Unfollowers","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.storage import Database\n\nasync def detect_unfollowers(username: str):\n    db = Database()\n\n    # Get previously stored followers\n    previous = set(u.username for u in db.get_followers(username))\n\n    async with Xeepy() as x:\n        current_result = await x.scrape.followers(username, limit=5000)\n        current = set(u.username for u in current_result.items)\n\n        # Save current\n        db.save_followers(username, current_result.items)\n\n    # Find differences\n    unfollowed = previous - current\n    new_followers = current - previous\n\n    print(f\"Unfollowed: {len(unfollowed)}\")\n    for user in list(unfollowed)[:10]:\n        print(f\"  - @{user}\")\n\n    print(f\"\\nNew followers: {len(new_followers)}\")\n\n    db.close()\n\nasyncio.run(detect_unfollowers(\"myaccount\"))\n</code></pre>"},{"location":"api/storage/database/#context-manager","title":"Context Manager","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.storage import Database\n\nasync def with_context_manager():\n    with Database(\"data.db\") as db:\n        async with Xeepy() as x:\n            user = await x.scrape.profile(\"username\")\n            db.save_user(user)\n\n        # Auto-closes on exit\n\nasyncio.run(with_context_manager())\n</code></pre>"},{"location":"api/storage/database/#export-from-database","title":"Export from Database","text":"<pre><code>from xeepy.storage import Database\nfrom xeepy import Xeepy\n\ndef export_stored_data(username: str):\n    db = Database()\n    x = Xeepy()\n\n    followers = db.get_followers(username)\n\n    data = [\n        {\n            \"username\": u.username,\n            \"followers\": u.followers_count,\n            \"following\": u.following_count\n        }\n        for u in followers\n    ]\n\n    x.export.to_csv(data, f\"stored_followers_{username}.csv\")\n    print(f\"Exported {len(data)} stored followers\")\n\n    db.close()\n\nexport_stored_data(\"myaccount\")\n</code></pre>"},{"location":"api/storage/database/#see-also","title":"See Also","text":"<ul> <li>Export - Export functions</li> <li>UnfollowersMonitor - Unfollower tracking</li> <li>GrowthMonitor - Growth tracking</li> </ul>"},{"location":"api/storage/export/","title":"Export","text":"<p>Export data to various file formats.</p>"},{"location":"api/storage/export/#import","title":"Import","text":"<pre><code>from xeepy.storage import Export\n# Or via Xeepy instance\nx.export\n</code></pre>"},{"location":"api/storage/export/#class-signature","title":"Class Signature","text":"<pre><code>class Export:\n    def __init__(self)\n</code></pre>"},{"location":"api/storage/export/#methods","title":"Methods","text":"Method Returns Description <code>to_csv(data, filename)</code> <code>str</code> Export to CSV <code>to_json(data, filename)</code> <code>str</code> Export to JSON <code>to_excel(data, filename)</code> <code>str</code> Export to Excel <code>to_parquet(data, filename)</code> <code>str</code> Export to Parquet <code>to_markdown(data, filename)</code> <code>str</code> Export to Markdown table"},{"location":"api/storage/export/#to_csv","title":"<code>to_csv</code>","text":"<pre><code>def to_csv(\n    self,\n    data: Union[List[Dict], List[Any], ScrapeResult],\n    filename: str,\n    columns: Optional[List[str]] = None\n) -&gt; str\n</code></pre> <p>Export to CSV file.</p> <p>Parameters: - <code>data</code>: Data to export (dicts, models, or ScrapeResult) - <code>filename</code>: Output filename - <code>columns</code>: Specific columns to include (optional)</p> <p>Returns: Path to created file</p>"},{"location":"api/storage/export/#to_json","title":"<code>to_json</code>","text":"<pre><code>def to_json(\n    self,\n    data: Union[List[Dict], List[Any], ScrapeResult],\n    filename: str,\n    indent: int = 2\n) -&gt; str\n</code></pre> <p>Export to JSON file.</p>"},{"location":"api/storage/export/#to_excel","title":"<code>to_excel</code>","text":"<pre><code>def to_excel(\n    self,\n    data: Union[List[Dict], List[Any], ScrapeResult],\n    filename: str,\n    sheet_name: str = \"Sheet1\"\n) -&gt; str\n</code></pre> <p>Export to Excel file.</p>"},{"location":"api/storage/export/#to_parquet","title":"<code>to_parquet</code>","text":"<pre><code>def to_parquet(\n    self,\n    data: Union[List[Dict], List[Any], ScrapeResult],\n    filename: str,\n    compression: str = \"snappy\"\n) -&gt; str\n</code></pre> <p>Export to Parquet file (for large datasets).</p>"},{"location":"api/storage/export/#to_markdown","title":"<code>to_markdown</code>","text":"<pre><code>def to_markdown(\n    self,\n    data: Union[List[Dict], List[Any], ScrapeResult],\n    filename: str,\n    columns: Optional[List[str]] = None\n) -&gt; str\n</code></pre> <p>Export to Markdown table.</p>"},{"location":"api/storage/export/#usage-examples","title":"Usage Examples","text":""},{"location":"api/storage/export/#export-replies-to-csv","title":"Export Replies to CSV","text":"<pre><code>from xeepy import Xeepy\n\nasync def export_replies(tweet_url: str):\n    async with Xeepy() as x:\n        replies = await x.scrape.replies(tweet_url, limit=100)\n\n        path = x.export.to_csv(replies, \"replies.csv\")\n        print(f\"Exported to {path}\")\n\nasyncio.run(export_replies(\"https://x.com/user/status/123\"))\n</code></pre>"},{"location":"api/storage/export/#export-followers-to-json","title":"Export Followers to JSON","text":"<pre><code>from xeepy import Xeepy\n\nasync def export_followers(username: str):\n    async with Xeepy() as x:\n        followers = await x.scrape.followers(username, limit=500)\n\n        path = x.export.to_json(followers, f\"followers_{username}.json\")\n        print(f\"Exported to {path}\")\n\nasyncio.run(export_followers(\"myaccount\"))\n</code></pre>"},{"location":"api/storage/export/#export-with-specific-columns","title":"Export with Specific Columns","text":"<pre><code>from xeepy import Xeepy\n\nasync def export_selective(username: str):\n    async with Xeepy() as x:\n        followers = await x.scrape.followers(username, limit=200)\n\n        path = x.export.to_csv(\n            followers,\n            \"followers_simple.csv\",\n            columns=[\"username\", \"name\", \"followers_count\", \"bio\"]\n        )\n\n        print(f\"Exported selected columns to {path}\")\n\nasyncio.run(export_selective(\"myaccount\"))\n</code></pre>"},{"location":"api/storage/export/#export-to-excel","title":"Export to Excel","text":"<pre><code>from xeepy import Xeepy\n\nasync def export_to_excel(username: str):\n    async with Xeepy() as x:\n        tweets = await x.scrape.tweets(username, limit=100)\n\n        path = x.export.to_excel(\n            tweets,\n            f\"tweets_{username}.xlsx\",\n            sheet_name=\"Tweets\"\n        )\n\n        print(f\"Exported to {path}\")\n\nasyncio.run(export_to_excel(\"username\"))\n</code></pre>"},{"location":"api/storage/export/#export-large-dataset-to-parquet","title":"Export Large Dataset to Parquet","text":"<pre><code>from xeepy import Xeepy\n\nasync def export_large_dataset(username: str):\n    async with Xeepy() as x:\n        followers = await x.scrape.followers(username, limit=10000)\n\n        path = x.export.to_parquet(\n            followers,\n            f\"followers_{username}.parquet\",\n            compression=\"snappy\"\n        )\n\n        print(f\"Exported {len(followers.items):,} rows to {path}\")\n\nasyncio.run(export_large_dataset(\"elonmusk\"))\n</code></pre>"},{"location":"api/storage/export/#export-to-markdown","title":"Export to Markdown","text":"<pre><code>from xeepy import Xeepy\n\nasync def export_markdown(username: str):\n    async with Xeepy() as x:\n        tweets = await x.scrape.tweets(username, limit=10)\n\n        path = x.export.to_markdown(\n            tweets,\n            f\"tweets_{username}.md\",\n            columns=[\"text\", \"like_count\", \"created_at\"]\n        )\n\n        print(f\"Exported to {path}\")\n\nasyncio.run(export_markdown(\"username\"))\n</code></pre>"},{"location":"api/storage/export/#export-custom-data","title":"Export Custom Data","text":"<pre><code>from xeepy import Xeepy\n\nasync def export_custom_analysis(username: str):\n    async with Xeepy() as x:\n        tweets = await x.scrape.tweets(username, limit=100)\n\n        # Create custom data structure\n        data = []\n        for tweet in tweets.items:\n            data.append({\n                \"id\": tweet.id,\n                \"text\": tweet.text[:100],\n                \"likes\": tweet.like_count,\n                \"retweets\": tweet.retweet_count,\n                \"engagement_rate\": tweet.engagement_rate,\n                \"has_media\": tweet.has_media,\n                \"hashtags\": \", \".join(tweet.hashtags),\n                \"posted_at\": tweet.created_at.isoformat()\n            })\n\n        x.export.to_csv(data, f\"analysis_{username}.csv\")\n        x.export.to_json(data, f\"analysis_{username}.json\")\n\nasyncio.run(export_custom_analysis(\"username\"))\n</code></pre>"},{"location":"api/storage/export/#export-multiple-sheets-to-excel","title":"Export Multiple Sheets to Excel","text":"<pre><code>from xeepy import Xeepy\n\nasync def export_multi_sheet(username: str):\n    async with Xeepy() as x:\n        user = await x.scrape.profile(username)\n        tweets = await x.scrape.tweets(username, limit=50)\n        followers = await x.scrape.followers(username, limit=100)\n\n        # Export each to separate Excel files\n        x.export.to_excel([user.to_dict()], f\"{username}_profile.xlsx\", sheet_name=\"Profile\")\n        x.export.to_excel(tweets, f\"{username}_tweets.xlsx\", sheet_name=\"Tweets\")\n        x.export.to_excel(followers, f\"{username}_followers.xlsx\", sheet_name=\"Followers\")\n\nasyncio.run(export_multi_sheet(\"username\"))\n</code></pre>"},{"location":"api/storage/export/#export-with-timestamp","title":"Export with Timestamp","text":"<pre><code>from xeepy import Xeepy\nfrom datetime import datetime\n\nasync def export_with_timestamp(username: str):\n    async with Xeepy() as x:\n        followers = await x.scrape.followers(username, limit=500)\n\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        path = x.export.to_csv(followers, f\"followers_{username}_{timestamp}.csv\")\n\n        print(f\"Exported to {path}\")\n\nasyncio.run(export_with_timestamp(\"myaccount\"))\n</code></pre>"},{"location":"api/storage/export/#export-search-results","title":"Export Search Results","text":"<pre><code>from xeepy import Xeepy\n\nasync def export_search(query: str):\n    async with Xeepy() as x:\n        results = await x.scrape.search(query, limit=200)\n\n        x.export.to_csv(results, f\"search_{query.replace(' ', '_')}.csv\")\n        print(f\"Exported {len(results.items)} search results\")\n\nasyncio.run(export_search(\"Python programming\"))\n</code></pre>"},{"location":"api/storage/export/#batch-export","title":"Batch Export","text":"<pre><code>from xeepy import Xeepy\n\nasync def batch_export(usernames: list):\n    async with Xeepy() as x:\n        for username in usernames:\n            try:\n                followers = await x.scrape.followers(username, limit=500)\n                x.export.to_csv(followers, f\"followers_{username}.csv\")\n                print(f\"\u2713 Exported @{username}\")\n            except Exception as e:\n                print(f\"\u2717 Failed @{username}: {e}\")\n\nasyncio.run(batch_export([\"user1\", \"user2\", \"user3\"]))\n</code></pre>"},{"location":"api/storage/export/#convert-to-dataframe","title":"Convert to DataFrame","text":"<pre><code>from xeepy import Xeepy\nimport pandas as pd\n\nasync def to_dataframe(username: str):\n    async with Xeepy() as x:\n        followers = await x.scrape.followers(username, limit=500)\n\n        # Export to CSV, then read as DataFrame\n        path = x.export.to_csv(followers, \"temp_followers.csv\")\n        df = pd.read_csv(path)\n\n        print(f\"DataFrame shape: {df.shape}\")\n        print(df.head())\n\n        return df\n\nasyncio.run(to_dataframe(\"username\"))\n</code></pre>"},{"location":"api/storage/export/#see-also","title":"See Also","text":"<ul> <li>Database - SQLite storage</li> <li>Tweet - Tweet model</li> <li>User - User model</li> </ul>"},{"location":"cli/","title":"CLI Reference","text":"<p>Xeepy provides a powerful command-line interface for common operations without writing code.</p>"},{"location":"cli/#installation","title":"Installation","text":"<p>The CLI is included with Xeepy:</p> <pre><code>pip install xeepy\n</code></pre>"},{"location":"cli/#basic-usage","title":"Basic Usage","text":"<pre><code>xeepy [COMMAND] [SUBCOMMAND] [OPTIONS]\n\n# Get help\nxeepy --help\nxeepy scrape --help\nxeepy scrape replies --help\n</code></pre>"},{"location":"cli/#commands-overview","title":"Commands Overview","text":"Command Description <code>auth</code> Authentication management <code>scrape</code> Scrape data from X/Twitter <code>follow</code> Follow users <code>unfollow</code> Unfollow users <code>engage</code> Like, retweet, reply <code>monitor</code> Monitor account changes <code>analytics</code> View analytics and reports <code>ai</code> AI-powered features <code>export</code> Export data to files <code>config</code> Configuration management"},{"location":"cli/#authentication-commands","title":"Authentication Commands","text":""},{"location":"cli/#login","title":"Login","text":"<pre><code># Interactive login (opens browser)\nxeepy auth login\n\n# Login with specific profile\nxeepy auth login --profile business\n\n# Login with browser visible\nxeepy auth login --headful\n</code></pre>"},{"location":"cli/#status","title":"Status","text":"<pre><code># Check authentication status\nxeepy auth status\n\n# Output:\n# \u2713 Authenticated as @username\n# Session age: 2 days\n</code></pre>"},{"location":"cli/#logout","title":"Logout","text":"<pre><code># Clear session\nxeepy auth logout\n\n# Clear all sessions\nxeepy auth logout --all\n</code></pre>"},{"location":"cli/#importexport","title":"Import/Export","text":"<pre><code># Export session for backup\nxeepy auth export session_backup.json\n\n# Import session\nxeepy auth import session_backup.json\n\n# Import from browser cookies\nxeepy auth import cookies.txt --format netscape\n</code></pre>"},{"location":"cli/#scrape-commands","title":"Scrape Commands","text":""},{"location":"cli/#scrape-replies","title":"Scrape Replies","text":"<pre><code># Basic usage\nxeepy scrape replies https://x.com/user/status/123456\n\n# With options\nxeepy scrape replies https://x.com/user/status/123456 \\\n    --limit 500 \\\n    --output replies.csv\n\n# Filter options\nxeepy scrape replies URL \\\n    --min-likes 10 \\\n    --verified-only \\\n    --sort top\n</code></pre>"},{"location":"cli/#scrape-profile","title":"Scrape Profile","text":"<pre><code># Get profile info\nxeepy scrape profile elonmusk\n\n# Output as JSON\nxeepy scrape profile elonmusk --format json\n\n# Multiple profiles\nxeepy scrape profile user1 user2 user3 -o profiles.csv\n</code></pre>"},{"location":"cli/#scrape-tweets","title":"Scrape Tweets","text":"<pre><code># User's tweets\nxeepy scrape tweets username --limit 100\n\n# Include retweets and replies\nxeepy scrape tweets username --include-retweets --include-replies\n\n# Date range\nxeepy scrape tweets username --since 2024-01-01 --until 2024-02-01\n</code></pre>"},{"location":"cli/#scrape-followers","title":"Scrape Followers","text":"<pre><code># Basic\nxeepy scrape followers username --limit 1000\n\n# Output to file\nxeepy scrape followers username -o followers.csv\n\n# With metadata\nxeepy scrape followers username --include-bio --include-stats\n</code></pre>"},{"location":"cli/#scrape-following","title":"Scrape Following","text":"<pre><code>xeepy scrape following username --limit 500 -o following.csv\n</code></pre>"},{"location":"cli/#scrape-search","title":"Scrape Search","text":"<pre><code># Basic search\nxeepy scrape search \"python programming\" --limit 100\n\n# Advanced search\nxeepy scrape search \"python programming\" \\\n    --min-likes 50 \\\n    --min-retweets 10 \\\n    --lang en \\\n    --since 2024-01-01\n\n# Search type\nxeepy scrape search \"keyword\" --type latest  # or \"top\", \"people\"\n</code></pre>"},{"location":"cli/#scrape-hashtag","title":"Scrape Hashtag","text":"<pre><code>xeepy scrape hashtag \"#buildinpublic\" --limit 200 -o hashtag.csv\n</code></pre>"},{"location":"cli/#scrape-thread","title":"Scrape Thread","text":"<pre><code># Unroll a thread\nxeepy scrape thread https://x.com/user/status/123456 -o thread.json\n</code></pre>"},{"location":"cli/#follow-commands","title":"Follow Commands","text":""},{"location":"cli/#follow-user","title":"Follow User","text":"<pre><code># Follow single user\nxeepy follow user naval\n\n# Follow multiple\nxeepy follow user user1 user2 user3\n</code></pre>"},{"location":"cli/#follow-by-hashtag","title":"Follow by Hashtag","text":"<pre><code>xeepy follow hashtag \"#buildinpublic\" \\\n    --limit 20 \\\n    --min-followers 100 \\\n    --max-followers 50000\n</code></pre>"},{"location":"cli/#follow-from-search","title":"Follow from Search","text":"<pre><code>xeepy follow search \"indie hacker\" \\\n    --limit 15 \\\n    --min-followers 500\n</code></pre>"},{"location":"cli/#follow-followers-of","title":"Follow Followers Of","text":"<pre><code>xeepy follow followers-of competitor_account \\\n    --limit 30 \\\n    --active-days 30\n</code></pre>"},{"location":"cli/#unfollow-commands","title":"Unfollow Commands","text":""},{"location":"cli/#unfollow-non-followers","title":"Unfollow Non-Followers","text":"<pre><code># Dry run (preview)\nxeepy unfollow non-followers --dry-run\n\n# Execute\nxeepy unfollow non-followers --max 50\n\n# With whitelist\nxeepy unfollow non-followers \\\n    --max 50 \\\n    --whitelist-file whitelist.txt\n\n# Inline whitelist\nxeepy unfollow non-followers \\\n    --max 50 \\\n    --whitelist user1,user2,user3\n</code></pre>"},{"location":"cli/#unfollow-inactive","title":"Unfollow Inactive","text":"<pre><code>xeepy unfollow inactive --days 180 --max 30\n</code></pre>"},{"location":"cli/#smart-unfollow","title":"Smart Unfollow","text":"<pre><code>xeepy unfollow smart \\\n    --criteria inactive,no-bio,not-following \\\n    --max 25\n</code></pre>"},{"location":"cli/#unfollow-everyone","title":"Unfollow Everyone","text":"<pre><code># Requires confirmation\nxeepy unfollow everyone \\\n    --whitelist-file whitelist.txt \\\n    --confirm\n</code></pre>"},{"location":"cli/#engage-commands","title":"Engage Commands","text":""},{"location":"cli/#like","title":"Like","text":"<pre><code># Like a tweet\nxeepy engage like https://x.com/user/status/123456\n\n# Like multiple\nxeepy engage like URL1 URL2 URL3\n</code></pre>"},{"location":"cli/#retweet","title":"Retweet","text":"<pre><code>xeepy engage retweet https://x.com/user/status/123456\n</code></pre>"},{"location":"cli/#reply","title":"Reply","text":"<pre><code>xeepy engage reply https://x.com/user/status/123456 \"Great thread!\"\n</code></pre>"},{"location":"cli/#auto-like","title":"Auto-Like","text":"<pre><code>xeepy engage auto-like \\\n    --keywords \"python,automation\" \\\n    --limit 20 \\\n    --min-likes 10\n</code></pre>"},{"location":"cli/#monitor-commands","title":"Monitor Commands","text":""},{"location":"cli/#check-unfollowers","title":"Check Unfollowers","text":"<pre><code># One-time check\nxeepy monitor unfollowers\n\n# With notification\nxeepy monitor unfollowers --notify discord\n\n# Continuous monitoring\nxeepy monitor unfollowers --watch --interval 3600\n</code></pre>"},{"location":"cli/#track-growth","title":"Track Growth","text":"<pre><code>xeepy monitor growth --period 7d\n</code></pre>"},{"location":"cli/#monitor-keywords","title":"Monitor Keywords","text":"<pre><code>xeepy monitor keywords \"your_brand,your_product\" \\\n    --notify telegram \\\n    --interval 300\n</code></pre>"},{"location":"cli/#start-daemon","title":"Start Daemon","text":"<pre><code># Start all monitors in background\nxeepy monitor start --config monitoring.yaml --daemon\n</code></pre>"},{"location":"cli/#analytics-commands","title":"Analytics Commands","text":""},{"location":"cli/#growth-report","title":"Growth Report","text":"<pre><code>xeepy analytics growth --period 30d\n</code></pre>"},{"location":"cli/#engagement-analysis","title":"Engagement Analysis","text":"<pre><code>xeepy analytics engagement --period 7d\n</code></pre>"},{"location":"cli/#best-time-to-post","title":"Best Time to Post","text":"<pre><code>xeepy analytics best-time\n\n# Output:\n# Best day: Tuesday\n# Best hour: 14:00\n# Top 5 slots: ...\n</code></pre>"},{"location":"cli/#audience-insights","title":"Audience Insights","text":"<pre><code>xeepy analytics audience --sample 1000\n</code></pre>"},{"location":"cli/#competitor-analysis","title":"Competitor Analysis","text":"<pre><code>xeepy analytics competitors comp1,comp2,comp3\n</code></pre>"},{"location":"cli/#generate-report","title":"Generate Report","text":"<pre><code># Markdown report\nxeepy analytics report --period 30d -o report.md\n\n# PDF report\nxeepy analytics report --period 30d --format pdf -o report.pdf\n</code></pre>"},{"location":"cli/#ai-commands","title":"AI Commands","text":""},{"location":"cli/#generate-tweet","title":"Generate Tweet","text":"<pre><code>xeepy ai tweet \"Python tips\" --style educational\n</code></pre>"},{"location":"cli/#generate-thread","title":"Generate Thread","text":"<pre><code>xeepy ai thread \"My startup journey\" --length 5\n</code></pre>"},{"location":"cli/#generate-reply","title":"Generate Reply","text":"<pre><code>xeepy ai reply https://x.com/user/status/123456 --style supportive\n</code></pre>"},{"location":"cli/#analyze-sentiment","title":"Analyze Sentiment","text":"<pre><code>xeepy ai sentiment \"This is amazing!\"\n\n# Analyze from file\nxeepy ai sentiment --file tweets.txt\n</code></pre>"},{"location":"cli/#bot-detection","title":"Bot Detection","text":"<pre><code>xeepy ai bot-check suspicious_username\n</code></pre>"},{"location":"cli/#export-commands","title":"Export Commands","text":""},{"location":"cli/#convert-formats","title":"Convert Formats","text":"<pre><code># CSV to JSON\nxeepy export convert data.csv data.json\n\n# JSON to Excel\nxeepy export convert data.json data.xlsx\n</code></pre>"},{"location":"cli/#export-to-database","title":"Export to Database","text":"<pre><code>xeepy export database data.csv sqlite:///data.db --table tweets\nxeepy export database data.csv postgresql://user:pass@host/db --table tweets\n</code></pre>"},{"location":"cli/#configuration-commands","title":"Configuration Commands","text":""},{"location":"cli/#view-config","title":"View Config","text":"<pre><code># Show current configuration\nxeepy config show\n\n# Show specific setting\nxeepy config get rate_limit.requests_per_minute\n</code></pre>"},{"location":"cli/#set-config","title":"Set Config","text":"<pre><code>xeepy config set rate_limit.requests_per_minute 25\nxeepy config set headless true\n</code></pre>"},{"location":"cli/#profiles","title":"Profiles","text":"<pre><code># List profiles\nxeepy config profiles\n\n# Create profile\nxeepy config create-profile business\n\n# Use profile\nxeepy --profile business scrape replies URL\n</code></pre>"},{"location":"cli/#global-options","title":"Global Options","text":"<p>Available for all commands:</p> Option Description <code>--profile NAME</code> Use named profile <code>--config FILE</code> Use config file <code>--headless/--no-headless</code> Browser visibility <code>--verbose/-v</code> Verbose output <code>--quiet/-q</code> Suppress output <code>--dry-run</code> Preview without executing <code>--output/-o FILE</code> Output file <code>--format FORMAT</code> Output format (csv, json, etc.)"},{"location":"cli/#output-formats","title":"Output Formats","text":"<pre><code># CSV (default)\nxeepy scrape tweets user -o tweets.csv\n\n# JSON\nxeepy scrape tweets user -o tweets.json --format json\n\n# Excel\nxeepy scrape tweets user -o tweets.xlsx --format excel\n\n# Pretty print to console\nxeepy scrape profile user --format pretty\n</code></pre>"},{"location":"cli/#environment-variables","title":"Environment Variables","text":"Variable Description <code>XEEPY_SESSION_FILE</code> Session file path <code>XEEPY_CONFIG_FILE</code> Config file path <code>XEEPY_PROFILE</code> Default profile <code>XEEPY_HEADLESS</code> Headless mode (true/false) <code>DISCORD_WEBHOOK</code> Discord notification URL <code>TELEGRAM_BOT_TOKEN</code> Telegram bot token <code>OPENAI_API_KEY</code> OpenAI API key"},{"location":"cli/#examples","title":"Examples","text":""},{"location":"cli/#daily-routine-script","title":"Daily Routine Script","text":"<pre><code>#!/bin/bash\n# daily_routine.sh\n\necho \"\ud83c\udf05 Starting daily routine...\"\n\n# Check unfollowers\nxeepy monitor unfollowers --notify discord\n\n# Unfollow non-followers\nxeepy unfollow non-followers --max 25 --whitelist-file whitelist.txt\n\n# Follow from target hashtag\nxeepy follow hashtag \"#buildinpublic\" --limit 15 --min-followers 100\n\n# Generate growth report\nxeepy analytics growth --period 24h --notify discord\n\necho \"\u2705 Daily routine complete!\"\n</code></pre>"},{"location":"cli/#data-collection-pipeline","title":"Data Collection Pipeline","text":"<pre><code>#!/bin/bash\n# collect_data.sh\n\nUSERNAME=$1\nOUTPUT_DIR=\"data/$USERNAME\"\nmkdir -p $OUTPUT_DIR\n\necho \"\ud83d\udcca Collecting data for @$USERNAME...\"\n\nxeepy scrape profile $USERNAME -o \"$OUTPUT_DIR/profile.json\" --format json\nxeepy scrape tweets $USERNAME --limit 500 -o \"$OUTPUT_DIR/tweets.csv\"\nxeepy scrape followers $USERNAME --limit 1000 -o \"$OUTPUT_DIR/followers.csv\"\n\necho \"\u2705 Data saved to $OUTPUT_DIR/\"\n</code></pre>"},{"location":"cli/#competitor-analysis_1","title":"Competitor Analysis","text":"<pre><code>#!/bin/bash\n# analyze_competitors.sh\n\nCOMPETITORS=\"comp1 comp2 comp3\"\n\nfor comp in $COMPETITORS; do\n    echo \"Analyzing @$comp...\"\n    xeepy scrape tweets $comp --limit 100 -o \"analysis/$comp_tweets.csv\"\ndone\n\nxeepy analytics competitors $COMPETITORS -o \"analysis/comparison.md\"\n</code></pre>"},{"location":"cli/commands/reference/","title":"CLI Command Reference","text":"<p>Complete reference for all Xeepy command-line commands.</p>"},{"location":"cli/commands/reference/#global-options","title":"Global Options","text":"<pre><code># All commands support these options\nxeepy [OPTIONS] COMMAND [ARGS]\n\nOptions:\n  --config PATH    Configuration file path\n  --profile NAME   Profile to use (for multiple accounts)\n  --verbose, -v    Enable verbose output\n  --quiet, -q      Suppress non-essential output\n  --json           Output in JSON format\n  --help           Show help message\n</code></pre>"},{"location":"cli/commands/reference/#authentication-commands","title":"Authentication Commands","text":""},{"location":"cli/commands/reference/#auth-login","title":"<code>auth login</code>","text":"<p>Authenticate with X/Twitter.</p> <pre><code># Interactive browser login\nxeepy auth login\n\n# With specific profile\nxeepy auth login --profile work\n\n# Headless (cookie import)\nxeepy auth login --cookies cookies.json\n</code></pre>"},{"location":"cli/commands/reference/#auth-logout","title":"<code>auth logout</code>","text":"<p>Clear authentication.</p> <pre><code>xeepy auth logout\nxeepy auth logout --profile work\n</code></pre>"},{"location":"cli/commands/reference/#auth-status","title":"<code>auth status</code>","text":"<p>Check authentication status.</p> <pre><code>xeepy auth status\n# Output: \u2713 Logged in as @username\n</code></pre>"},{"location":"cli/commands/reference/#auth-export","title":"<code>auth export</code>","text":"<p>Export session cookies.</p> <pre><code>xeepy auth export session.json\nxeepy auth export --format netscape cookies.txt\n</code></pre>"},{"location":"cli/commands/reference/#scraping-commands","title":"Scraping Commands","text":""},{"location":"cli/commands/reference/#scrape-replies","title":"<code>scrape replies</code>","text":"<p>Get replies to a tweet.</p> <pre><code># Basic usage\nxeepy scrape replies https://x.com/user/status/123\n\n# With options\nxeepy scrape replies https://x.com/user/status/123 \\\n  --limit 500 \\\n  --output replies.csv \\\n  --format csv\n\n# Include nested replies\nxeepy scrape replies https://x.com/user/status/123 --nested\n</code></pre> <p>Options:</p> Option Description Default <code>--limit, -n</code> Maximum replies 100 <code>--output, -o</code> Output file stdout <code>--format, -f</code> Output format (csv, json, table) table <code>--nested</code> Include nested replies false <code>--since</code> Only replies after date -"},{"location":"cli/commands/reference/#scrape-profile","title":"<code>scrape profile</code>","text":"<p>Get user profile information.</p> <pre><code>xeepy scrape profile elonmusk\nxeepy scrape profile elonmusk --json\nxeepy scrape profile user1 user2 user3 --output profiles.csv\n</code></pre>"},{"location":"cli/commands/reference/#scrape-followers","title":"<code>scrape followers</code>","text":"<p>Get user's followers.</p> <pre><code># Basic\nxeepy scrape followers username\n\n# With limit\nxeepy scrape followers username --limit 1000 --output followers.csv\n\n# Filter by followers count\nxeepy scrape followers username --min-followers 1000\n</code></pre>"},{"location":"cli/commands/reference/#scrape-following","title":"<code>scrape following</code>","text":"<p>Get who a user follows.</p> <pre><code>xeepy scrape following username --limit 500\nxeepy scrape following username --output following.json --format json\n</code></pre>"},{"location":"cli/commands/reference/#scrape-tweets","title":"<code>scrape tweets</code>","text":"<p>Get user's tweets.</p> <pre><code># Recent tweets\nxeepy scrape tweets username --limit 100\n\n# With date range\nxeepy scrape tweets username \\\n  --since 2024-01-01 \\\n  --until 2024-02-01 \\\n  --output tweets.csv\n\n# Include retweets\nxeepy scrape tweets username --include-retweets\n</code></pre>"},{"location":"cli/commands/reference/#scrape-search","title":"<code>scrape search</code>","text":"<p>Search for tweets.</p> <pre><code># Basic search\nxeepy scrape search \"python programming\"\n\n# Advanced search\nxeepy scrape search \"AI tools\" \\\n  --limit 200 \\\n  --type Latest \\\n  --min-likes 100 \\\n  --lang en\n\n# Search with operators\nxeepy scrape search \"from:elonmusk AI\" --limit 50\n</code></pre>"},{"location":"cli/commands/reference/#scrape-hashtag","title":"<code>scrape hashtag</code>","text":"<p>Get tweets with a hashtag.</p> <pre><code>xeepy scrape hashtag python --limit 100\nxeepy scrape hashtag machinelearning --since 2024-01-01\n</code></pre>"},{"location":"cli/commands/reference/#scrape-thread","title":"<code>scrape thread</code>","text":"<p>Unroll a Twitter thread.</p> <pre><code>xeepy scrape thread https://x.com/user/status/123\nxeepy scrape thread https://x.com/user/status/123 --output thread.md --format markdown\n</code></pre>"},{"location":"cli/commands/reference/#followunfollow-commands","title":"Follow/Unfollow Commands","text":""},{"location":"cli/commands/reference/#follow-user","title":"<code>follow user</code>","text":"<p>Follow a user.</p> <pre><code>xeepy follow user username\nxeepy follow user user1 user2 user3\n</code></pre>"},{"location":"cli/commands/reference/#follow-by-keyword","title":"<code>follow by-keyword</code>","text":"<p>Follow users tweeting about keywords.</p> <pre><code>xeepy follow by-keyword \"python\" \"machine learning\" \\\n  --limit 50 \\\n  --min-followers 100\n</code></pre>"},{"location":"cli/commands/reference/#follow-by-hashtag","title":"<code>follow by-hashtag</code>","text":"<p>Follow users using specific hashtags.</p> <pre><code>xeepy follow by-hashtag python AI \\\n  --limit 30 \\\n  --since 24h\n</code></pre>"},{"location":"cli/commands/reference/#follow-target-followers","title":"<code>follow target-followers</code>","text":"<p>Follow followers of a specific account.</p> <pre><code>xeepy follow target-followers competitor_account \\\n  --limit 100 \\\n  --min-followers 500 \\\n  --skip-verified\n</code></pre>"},{"location":"cli/commands/reference/#unfollow-non-followers","title":"<code>unfollow non-followers</code>","text":"<p>Unfollow accounts that don't follow you back.</p> <pre><code># Preview mode\nxeepy unfollow non-followers --dry-run\n\n# Execute\nxeepy unfollow non-followers --limit 100\n\n# With whitelist\nxeepy unfollow non-followers \\\n  --whitelist @friend1 @friend2 \\\n  --skip-verified \\\n  --min-following-days 30\n</code></pre>"},{"location":"cli/commands/reference/#unfollow-everyone","title":"<code>unfollow everyone</code>","text":"<p>Mass unfollow (use with caution).</p> <pre><code># ALWAYS preview first\nxeepy unfollow everyone --dry-run\n\n# With safety limits\nxeepy unfollow everyone \\\n  --limit 100 \\\n  --whitelist-file whitelist.txt \\\n  --delay 5\n</code></pre>"},{"location":"cli/commands/reference/#unfollow-smart","title":"<code>unfollow smart</code>","text":"<p>AI-powered unfollowing based on engagement.</p> <pre><code>xeepy unfollow smart \\\n  --criteria \"no_interaction_30_days\" \\\n  --limit 50 \\\n  --dry-run\n</code></pre>"},{"location":"cli/commands/reference/#engagement-commands","title":"Engagement Commands","text":""},{"location":"cli/commands/reference/#engage-like","title":"<code>engage like</code>","text":"<p>Like tweets.</p> <pre><code># Single tweet\nxeepy engage like https://x.com/user/status/123\n\n# Multiple tweets\nxeepy engage like url1 url2 url3\n\n# Auto-like by keyword\nxeepy engage like --keyword \"python tips\" --limit 20\n</code></pre>"},{"location":"cli/commands/reference/#engage-retweet","title":"<code>engage retweet</code>","text":"<p>Retweet tweets.</p> <pre><code>xeepy engage retweet https://x.com/user/status/123\nxeepy engage retweet url1 url2 --quote \"Great insight!\"\n</code></pre>"},{"location":"cli/commands/reference/#engage-comment","title":"<code>engage comment</code>","text":"<p>Reply to tweets.</p> <pre><code>xeepy engage comment https://x.com/user/status/123 \"Great post!\"\n\n# AI-generated comment\nxeepy engage comment https://x.com/user/status/123 \\\n  --ai \\\n  --style supportive\n</code></pre>"},{"location":"cli/commands/reference/#engage-auto-like","title":"<code>engage auto-like</code>","text":"<p>Automatic liking based on criteria.</p> <pre><code>xeepy engage auto-like \\\n  --keywords \"python\" \"ai\" \"programming\" \\\n  --limit 50 \\\n  --min-followers 500 \\\n  --delay 2-5\n</code></pre>"},{"location":"cli/commands/reference/#engage-bookmark","title":"<code>engage bookmark</code>","text":"<p>Bookmark tweets.</p> <pre><code>xeepy engage bookmark https://x.com/user/status/123\nxeepy engage bookmark url1 url2 url3\n</code></pre>"},{"location":"cli/commands/reference/#monitoring-commands","title":"Monitoring Commands","text":""},{"location":"cli/commands/reference/#monitor-unfollowers","title":"<code>monitor unfollowers</code>","text":"<p>Check for unfollowers.</p> <pre><code># Basic check\nxeepy monitor unfollowers\n\n# With notification\nxeepy monitor unfollowers --notify discord\n\n# Detailed report\nxeepy monitor unfollowers --detailed --output report.json\n</code></pre>"},{"location":"cli/commands/reference/#monitor-growth","title":"<code>monitor growth</code>","text":"<p>Track follower growth.</p> <pre><code>xeepy monitor growth --days 30\nxeepy monitor growth --output growth.csv --chart growth.png\n</code></pre>"},{"location":"cli/commands/reference/#monitor-keywords","title":"<code>monitor keywords</code>","text":"<p>Monitor keyword mentions.</p> <pre><code># Start monitoring\nxeepy monitor keywords \"brand\" \"product\" \\\n  --interval 5m \\\n  --notify telegram\n\n# One-time check\nxeepy monitor keywords \"brand\" --since 1h\n</code></pre>"},{"location":"cli/commands/reference/#monitor-accounts","title":"<code>monitor accounts</code>","text":"<p>Watch specific accounts for changes.</p> <pre><code>xeepy monitor accounts competitor1 competitor2 \\\n  --watch bio,followers,tweets \\\n  --notify discord\n</code></pre>"},{"location":"cli/commands/reference/#dm-commands","title":"DM Commands","text":""},{"location":"cli/commands/reference/#dm-send","title":"<code>dm send</code>","text":"<p>Send direct messages.</p> <pre><code>xeepy dm send username \"Hello!\"\nxeepy dm send user1 user2 \"Check this out!\" --media image.jpg\n</code></pre>"},{"location":"cli/commands/reference/#dm-inbox","title":"<code>dm inbox</code>","text":"<p>View DM inbox.</p> <pre><code>xeepy dm inbox\nxeepy dm inbox --unread-only --limit 20\n</code></pre>"},{"location":"cli/commands/reference/#dm-search","title":"<code>dm search</code>","text":"<p>Search DMs.</p> <pre><code>xeepy dm search \"keyword\"\n</code></pre>"},{"location":"cli/commands/reference/#scheduling-commands","title":"Scheduling Commands","text":""},{"location":"cli/commands/reference/#schedule-tweet","title":"<code>schedule tweet</code>","text":"<p>Schedule a tweet.</p> <pre><code>xeepy schedule tweet \"Hello world!\" --at \"2024-12-25 09:00\"\nxeepy schedule tweet \"With media!\" --at \"2024-12-25 12:00\" --media photo.jpg\n</code></pre>"},{"location":"cli/commands/reference/#schedule-list","title":"<code>schedule list</code>","text":"<p>List scheduled tweets.</p> <pre><code>xeepy schedule list\nxeepy schedule list --json\n</code></pre>"},{"location":"cli/commands/reference/#schedule-delete","title":"<code>schedule delete</code>","text":"<p>Delete scheduled tweet.</p> <pre><code>xeepy schedule delete TWEET_ID\nxeepy schedule delete --all\n</code></pre>"},{"location":"cli/commands/reference/#export-commands","title":"Export Commands","text":""},{"location":"cli/commands/reference/#export-csv","title":"<code>export csv</code>","text":"<p>Export data to CSV.</p> <pre><code>xeepy export csv followers.json followers.csv\nxeepy export csv --fields username,followers_count,bio\n</code></pre>"},{"location":"cli/commands/reference/#export-json","title":"<code>export json</code>","text":"<p>Export to JSON.</p> <pre><code>xeepy export json followers.csv followers.json\n</code></pre>"},{"location":"cli/commands/reference/#export-excel","title":"<code>export excel</code>","text":"<p>Export to Excel.</p> <pre><code>xeepy export excel data.json report.xlsx\nxeepy export excel --sheets followers,following\n</code></pre>"},{"location":"cli/commands/reference/#utility-commands","title":"Utility Commands","text":""},{"location":"cli/commands/reference/#trends","title":"<code>trends</code>","text":"<p>Get trending topics.</p> <pre><code>xeepy trends\nxeepy trends --location \"United States\"\nxeepy trends --json\n</code></pre>"},{"location":"cli/commands/reference/#rate-limits","title":"<code>rate-limits</code>","text":"<p>Check rate limit status.</p> <pre><code>xeepy rate-limits\n</code></pre>"},{"location":"cli/commands/reference/#config","title":"<code>config</code>","text":"<p>Manage configuration.</p> <pre><code># Show current config\nxeepy config show\n\n# Set value\nxeepy config set default_limit 100\n\n# Edit in editor\nxeepy config edit\n</code></pre>"},{"location":"cli/commands/reference/#version","title":"<code>version</code>","text":"<p>Show version information.</p> <pre><code>xeepy version\nxeepy version --check-update\n</code></pre>"},{"location":"cli/commands/reference/#environment-variables","title":"Environment Variables","text":"<pre><code># Authentication\nexport XEEPY_SESSION_PATH=~/.xeepy/session.json\n\n# Defaults\nexport XEEPY_DEFAULT_LIMIT=100\nexport XEEPY_RATE_LIMIT_DELAY=2\n\n# Notifications\nexport XEEPY_DISCORD_WEBHOOK=https://discord.com/api/webhooks/...\nexport XEEPY_TELEGRAM_BOT_TOKEN=123456:ABC...\nexport XEEPY_TELEGRAM_CHAT_ID=123456789\n\n# AI Providers\nexport XEEPY_OPENAI_API_KEY=sk-...\nexport XEEPY_ANTHROPIC_API_KEY=sk-ant-...\n</code></pre>"},{"location":"cli/commands/reference/#examples","title":"Examples","text":""},{"location":"cli/commands/reference/#daily-workflow","title":"Daily Workflow","text":"<pre><code># Morning check\nxeepy monitor unfollowers --notify discord\nxeepy monitor growth\n\n# Engagement session\nxeepy engage auto-like --keywords \"python\" --limit 30\nxeepy follow by-keyword \"developer\" --limit 20\n\n# Evening cleanup\nxeepy unfollow non-followers --limit 50 --skip-verified\n</code></pre>"},{"location":"cli/commands/reference/#research-session","title":"Research Session","text":"<pre><code># Scrape competitor data\nxeepy scrape followers competitor --limit 5000 -o comp_followers.csv\nxeepy scrape tweets competitor --limit 500 -o comp_tweets.csv\n\n# Analyze\nxeepy scrape profile comp_followers.csv --output comp_profiles.csv\n</code></pre>"},{"location":"cli/commands/reference/#content-curation","title":"Content Curation","text":"<pre><code># Find content to engage with\nxeepy scrape search \"AI tools\" --min-likes 1000 -o viral_ai.csv\n\n# Scrape threads for inspiration\nxeepy scrape thread https://x.com/user/status/123 -o thread.md\n</code></pre>"},{"location":"cli/commands/reference/#shell-completions","title":"Shell Completions","text":"<pre><code># Bash\nxeepy --install-completion bash\n\n# Zsh\nxeepy --install-completion zsh\n\n# Fish\nxeepy --install-completion fish\n</code></pre>"},{"location":"community/","title":"Community","text":"<p>Welcome to the Xeepy community! Whether you need help, want to contribute, or just want to connect with other users, you're in the right place.</p>"},{"location":"community/#get-help","title":"Get Help","text":"<ul> <li> <p> FAQ</p> <p>Find answers to common questions</p> </li> <li> <p> GitHub Issues</p> <p>Report bugs or request features</p> </li> <li> <p>:material-discord:{ .lg .middle } Discord</p> <p>Chat with the community</p> </li> <li> <p> Stack Overflow</p> <p>Ask technical questions</p> </li> </ul>"},{"location":"community/#contribute","title":"Contribute","text":"<p>We welcome contributions of all kinds!</p>"},{"location":"community/#ways-to-contribute","title":"Ways to Contribute","text":"<ul> <li>\ud83d\udc1b Report bugs - Found an issue? Open a GitHub issue</li> <li>\ud83d\udca1 Suggest features - Have an idea? Start a discussion</li> <li>\ud83d\udcd6 Improve docs - Fix typos, add examples, clarify explanations</li> <li>\ud83d\udd27 Submit PRs - Fix bugs or implement features</li> <li>\ud83c\udf10 Translate - Help translate docs to other languages</li> <li>\ud83d\udce3 Spread the word - Star us on GitHub, share on social media</li> </ul>"},{"location":"community/#getting-started","title":"Getting Started","text":"<pre><code># Clone the repo\ngit clone https://github.com/xeepy/xeepy.git\ncd xeepy\n\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate\n\n# Install dev dependencies\npip install -e \".[dev]\"\n\n# Install pre-commit hooks\npre-commit install\n\n# Run tests\npytest\n\n# Build docs locally\nmkdocs serve\n</code></pre>"},{"location":"community/#contribution-guidelines","title":"Contribution Guidelines","text":"<p>See our Contributing Guide for detailed information on:</p> <ul> <li>Code style and formatting</li> <li>Commit message conventions</li> <li>Pull request process</li> <li>Testing requirements</li> </ul>"},{"location":"community/#code-of-conduct","title":"Code of Conduct","text":"<p>We are committed to providing a welcoming and inspiring community for all.</p> <p>See our Code of Conduct for community standards.</p> <p>In short:</p> <ul> <li>\u2705 Be respectful and inclusive</li> <li>\u2705 Accept constructive criticism gracefully</li> <li>\u2705 Focus on what's best for the community</li> <li>\u274c No harassment, discrimination, or trolling</li> <li>\u274c No spam or self-promotion</li> </ul>"},{"location":"community/#showcase","title":"Showcase","text":"<p>Projects built with Xeepy:</p> <ul> <li> <p>Growth Dashboard</p> <p>Real-time Twitter analytics dashboard</p> <p>View Project \u2192</p> </li> <li> <p>Thread Unroller Bot</p> <p>Telegram bot that unrolls Twitter threads</p> <p>View Project \u2192</p> </li> <li> <p>Sentiment Tracker</p> <p>Track brand sentiment on Twitter</p> <p>View Project \u2192</p> </li> <li> <p>Auto-Engagement Engine</p> <p>Intelligent engagement automation</p> <p>View Project \u2192</p> </li> </ul> <p>Built something cool? Submit your project!</p>"},{"location":"community/#maintainers","title":"Maintainers","text":"Maintainer Role \ud83d\udc64 @maintainer1 Lead Developer \ud83d\udc64 @maintainer2 Documentation \ud83d\udc64 @maintainer3 Community"},{"location":"community/#sponsors","title":"Sponsors","text":"<p>Xeepy is free and open-source. If you find it valuable, consider supporting development:</p> <ul> <li> <p> GitHub Sponsors</p> <p>Support ongoing development</p> <p>Sponsor \u2192</p> </li> <li> <p> Buy Me a Coffee</p> <p>One-time support</p> <p>Donate \u2192</p> </li> </ul>"},{"location":"community/#sponsors_1","title":"Sponsors","text":"<p>Thank you to our generous sponsors:</p> <ul> <li>\ud83e\udd47 Gold: Your company here</li> <li>\ud83e\udd48 Silver: Your company here </li> <li>\ud83e\udd49 Bronze: Your company here</li> </ul>"},{"location":"community/#stay-updated","title":"Stay Updated","text":"<ul> <li>\u2b50 Star us on GitHub</li> <li>\ud83d\udc26 Follow @xeepy on X</li> <li>\ud83d\udce7 Newsletter</li> <li>\ud83d\udcf0 Blog</li> </ul>"},{"location":"community/#resources","title":"Resources","text":"<ul> <li>Roadmap - See what's coming next</li> <li>Changelog - Version history</li> <li>License - MIT License</li> </ul> <p>Thank you for being part of the Xeepy community! \ud83d\ude4f</p>"},{"location":"community/code-of-conduct/","title":"Code of Conduct","text":""},{"location":"community/code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"community/code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment:</p> <ul> <li>\u2705 Using welcoming and inclusive language</li> <li>\u2705 Being respectful of differing viewpoints and experiences</li> <li>\u2705 Gracefully accepting constructive criticism</li> <li>\u2705 Focusing on what is best for the community</li> <li>\u2705 Showing empathy towards other community members</li> <li>\u2705 Helping newcomers get started</li> <li>\u2705 Giving credit where credit is due</li> </ul> <p>Examples of unacceptable behavior:</p> <ul> <li>\u274c The use of sexualized language or imagery, and sexual attention or advances of any kind</li> <li>\u274c Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>\u274c Public or private harassment</li> <li>\u274c Publishing others' private information without explicit permission</li> <li>\u274c Spam or excessive self-promotion</li> <li>\u274c Other conduct which could reasonably be considered inappropriate</li> </ul>"},{"location":"community/code-of-conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"community/code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces.</p> <p>Examples include:</p> <ul> <li>Using an official project email address</li> <li>Posting via an official social media account</li> <li>Acting as an appointed representative at an event</li> <li>Participating in project GitHub discussions or Discord</li> </ul>"},{"location":"community/code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at:</p> <p>\ud83d\udce7 conduct@xeepy.dev</p> <p>All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"community/code-of-conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these guidelines in determining consequences:</p>"},{"location":"community/code-of-conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome.</p> <p>Consequence: A private, written warning, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"community/code-of-conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved for a specified period. This includes avoiding interactions in community spaces as well as external channels. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"community/code-of-conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period. No public or private interaction with the people involved is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"community/code-of-conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"community/code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p>"},{"location":"community/code-of-conduct/#questions","title":"Questions?","text":"<p>If you have questions about this Code of Conduct, please reach out:</p> <ul> <li>\ud83d\udce7 Email: conduct@xeepy.dev</li> <li>\ud83d\udcac Discord: Ask a moderator</li> <li>\ud83d\udc19 GitHub: Open a discussion</li> </ul>"},{"location":"community/contributing/","title":"Contributing to Xeepy","text":"<p>Thank you for your interest in contributing to Xeepy! This guide will help you get started.</p>"},{"location":"community/contributing/#ways-to-contribute","title":"Ways to Contribute","text":""},{"location":"community/contributing/#report-bugs","title":"\ud83d\udc1b Report Bugs","text":"<p>Found a bug? Open an issue with:</p> <ul> <li>Clear description of the problem</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Xeepy version and Python version</li> <li>OS and browser information</li> </ul>"},{"location":"community/contributing/#suggest-features","title":"\ud83d\udca1 Suggest Features","text":"<p>Have an idea? Start a discussion with:</p> <ul> <li>Clear description of the feature</li> <li>Use case and motivation</li> <li>Possible implementation approach</li> <li>Examples from similar tools (if any)</li> </ul>"},{"location":"community/contributing/#improve-documentation","title":"\ud83d\udcd6 Improve Documentation","text":"<p>Documentation improvements are always welcome:</p> <ul> <li>Fix typos and grammar</li> <li>Add code examples</li> <li>Clarify confusing sections</li> <li>Translate to other languages</li> <li>Add tutorials and guides</li> </ul>"},{"location":"community/contributing/#submit-code","title":"\ud83d\udd27 Submit Code","text":"<p>Ready to code? Follow the process below.</p>"},{"location":"community/contributing/#development-setup","title":"Development Setup","text":""},{"location":"community/contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Git</li> <li>Node.js (for docs preview)</li> </ul>"},{"location":"community/contributing/#clone-install","title":"Clone &amp; Install","text":"<pre><code># Fork the repo on GitHub, then clone your fork\ngit clone https://github.com/YOUR_USERNAME/xeepy.git\ncd xeepy\n\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # Linux/macOS\n# or: .venv\\Scripts\\activate  # Windows\n\n# Install in editable mode with dev dependencies\npip install -e \".[dev]\"\n\n# Install Playwright browser\nplaywright install chromium\n\n# Install pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"community/contributing/#verify-setup","title":"Verify Setup","text":"<pre><code># Run tests\npytest\n\n# Run linter\nruff check .\n\n# Run type checker\nmypy xeepy\n\n# Build docs\nmkdocs serve\n</code></pre>"},{"location":"community/contributing/#code-style","title":"Code Style","text":""},{"location":"community/contributing/#python-style","title":"Python Style","text":"<p>We use: - Ruff for linting and formatting - MyPy for type checking - Black style formatting (via Ruff)</p> <pre><code># Format code\nruff format .\n\n# Fix lint issues\nruff check --fix .\n</code></pre>"},{"location":"community/contributing/#type-hints","title":"Type Hints","text":"<p>All public APIs must have type hints:</p> <pre><code># Good \u2713\nasync def scrape_replies(\n    self,\n    tweet_url: str,\n    limit: int = 100,\n    *,\n    include_author: bool = True,\n) -&gt; ScrapeResult[Tweet]:\n    \"\"\"Scrape replies to a tweet.\n\n    Args:\n        tweet_url: URL of the tweet to scrape replies from.\n        limit: Maximum number of replies to return.\n        include_author: Whether to include the original author's replies.\n\n    Returns:\n        ScrapeResult containing Tweet objects.\n\n    Raises:\n        NotFoundError: If the tweet doesn't exist.\n        AuthenticationError: If not authenticated.\n    \"\"\"\n    ...\n\n# Bad \u2717\nasync def scrape_replies(self, url, limit=100, include_author=True):\n    ...\n</code></pre>"},{"location":"community/contributing/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> <pre><code>def function(arg1: str, arg2: int) -&gt; bool:\n    \"\"\"Short description of function.\n\n    Longer description if needed. Can span multiple\n    lines and include examples.\n\n    Args:\n        arg1: Description of arg1.\n        arg2: Description of arg2.\n\n    Returns:\n        Description of return value.\n\n    Raises:\n        ValueError: When arg1 is empty.\n\n    Examples:\n        &gt;&gt;&gt; function(\"hello\", 42)\n        True\n    \"\"\"\n</code></pre>"},{"location":"community/contributing/#naming-conventions","title":"Naming Conventions","text":"Type Convention Example Classes PascalCase <code>RepliesScraper</code> Functions snake_case <code>scrape_replies</code> Variables snake_case <code>tweet_count</code> Constants UPPER_CASE <code>MAX_RETRIES</code> Private _prefix <code>_internal_method</code>"},{"location":"community/contributing/#project-structure","title":"Project Structure","text":"<pre><code>xeepy/\n\u251c\u2500\u2500 __init__.py          # Public exports\n\u251c\u2500\u2500 core/                # Core functionality\n\u2502   \u251c\u2500\u2500 browser.py       # Browser management\n\u2502   \u251c\u2500\u2500 auth.py          # Authentication\n\u2502   \u251c\u2500\u2500 rate_limiter.py  # Rate limiting\n\u2502   \u2514\u2500\u2500 config.py        # Configuration\n\u251c\u2500\u2500 scrapers/            # Scraping modules\n\u2502   \u251c\u2500\u2500 base.py          # Base class\n\u2502   \u2514\u2500\u2500 *.py             # Specific scrapers\n\u251c\u2500\u2500 actions/             # Action modules\n\u2502   \u251c\u2500\u2500 base.py          # Base class\n\u2502   \u2514\u2500\u2500 *.py             # Specific actions\n\u251c\u2500\u2500 models/              # Data models\n\u251c\u2500\u2500 exceptions.py        # Custom exceptions\n\u2514\u2500\u2500 ...\n\ntests/\n\u251c\u2500\u2500 conftest.py          # Shared fixtures\n\u251c\u2500\u2500 unit/                # Unit tests\n\u251c\u2500\u2500 integration/         # Integration tests\n\u2514\u2500\u2500 e2e/                 # End-to-end tests\n\ndocs/\n\u251c\u2500\u2500 index.md             # Documentation home\n\u251c\u2500\u2500 getting-started/     # Getting started guides\n\u251c\u2500\u2500 guides/              # Feature guides\n\u2514\u2500\u2500 api/                 # API reference\n</code></pre>"},{"location":"community/contributing/#testing","title":"Testing","text":""},{"location":"community/contributing/#running-tests","title":"Running Tests","text":"<pre><code># All tests\npytest\n\n# Specific file\npytest tests/unit/test_scrapers.py\n\n# With coverage\npytest --cov=xeepy --cov-report=html\n\n# Only unit tests (fast)\npytest tests/unit\n\n# Integration tests (requires auth)\npytest tests/integration\n</code></pre>"},{"location":"community/contributing/#writing-tests","title":"Writing Tests","text":"<pre><code># tests/unit/test_scrapers/test_replies.py\nimport pytest\nfrom xeepy.scrapers.replies import RepliesScraper\n\nclass TestRepliesScraper:\n    \"\"\"Tests for RepliesScraper.\"\"\"\n\n    @pytest.fixture\n    def scraper(self, mock_browser):\n        \"\"\"Create scraper with mocked browser.\"\"\"\n        return RepliesScraper(mock_browser)\n\n    async def test_scrape_replies_success(self, scraper):\n        \"\"\"Should return replies for valid tweet.\"\"\"\n        result = await scraper.scrape(\"https://x.com/user/status/123\")\n        assert len(result.items) &gt; 0\n        assert all(isinstance(r, Tweet) for r in result.items)\n\n    async def test_scrape_replies_not_found(self, scraper):\n        \"\"\"Should raise NotFoundError for invalid tweet.\"\"\"\n        with pytest.raises(NotFoundError):\n            await scraper.scrape(\"https://x.com/user/status/invalid\")\n\n    @pytest.mark.parametrize(\"limit\", [10, 50, 100])\n    async def test_scrape_respects_limit(self, scraper, limit):\n        \"\"\"Should respect the limit parameter.\"\"\"\n        result = await scraper.scrape(\"...\", limit=limit)\n        assert len(result.items) &lt;= limit\n</code></pre>"},{"location":"community/contributing/#test-fixtures","title":"Test Fixtures","text":"<p>We provide shared fixtures in <code>conftest.py</code>:</p> <pre><code>@pytest.fixture\ndef mock_browser():\n    \"\"\"Mocked browser manager.\"\"\"\n    ...\n\n@pytest.fixture\ndef mock_page():\n    \"\"\"Mocked Playwright page.\"\"\"\n    ...\n\n@pytest.fixture\ndef sample_tweet():\n    \"\"\"Sample Tweet object.\"\"\"\n    ...\n\n@pytest.fixture\nasync def authenticated_xeepy():\n    \"\"\"Xeepy instance with authentication.\"\"\"\n    ...\n</code></pre>"},{"location":"community/contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"community/contributing/#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code># Update main\ngit checkout main\ngit pull upstream main\n\n# Create feature branch\ngit checkout -b feature/your-feature-name\n</code></pre> <p>Branch naming: - <code>feature/description</code> - New features - <code>fix/description</code> - Bug fixes - <code>docs/description</code> - Documentation - <code>refactor/description</code> - Code refactoring</p>"},{"location":"community/contributing/#2-make-changes","title":"2. Make Changes","text":"<ul> <li>Write code following our style guide</li> <li>Add/update tests</li> <li>Update documentation</li> <li>Run tests locally</li> </ul>"},{"location":"community/contributing/#3-commit","title":"3. Commit","text":"<p>Follow Conventional Commits:</p> <pre><code># Format\n&lt;type&gt;(&lt;scope&gt;): &lt;description&gt;\n\n# Examples\nfeat(scrapers): add Twitter Spaces scraper\nfix(auth): handle session expiration gracefully\ndocs(readme): add installation instructions\ntest(actions): add unfollow unit tests\nrefactor(core): simplify rate limiter logic\n</code></pre> <p>Types: - <code>feat</code> - New feature - <code>fix</code> - Bug fix - <code>docs</code> - Documentation - <code>test</code> - Tests - <code>refactor</code> - Code refactoring - <code>chore</code> - Maintenance</p>"},{"location":"community/contributing/#4-push-create-pr","title":"4. Push &amp; Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a Pull Request on GitHub with:</p> <ul> <li>Clear title and description</li> <li>Link to related issues</li> <li>Screenshots (if UI changes)</li> <li>Test results</li> </ul>"},{"location":"community/contributing/#5-review-process","title":"5. Review Process","text":"<ol> <li>Automated checks run (tests, lint, types)</li> <li>Maintainer reviews code</li> <li>Address feedback</li> <li>Get approval</li> <li>Maintainer merges</li> </ol>"},{"location":"community/contributing/#code-review-guidelines","title":"Code Review Guidelines","text":""},{"location":"community/contributing/#for-authors","title":"For Authors","text":"<ul> <li>Keep PRs small and focused</li> <li>Respond to feedback promptly</li> <li>Don't take feedback personally</li> <li>Ask questions if unclear</li> </ul>"},{"location":"community/contributing/#for-reviewers","title":"For Reviewers","text":"<ul> <li>Be constructive and kind</li> <li>Explain the \"why\" behind suggestions</li> <li>Approve when ready, not perfect</li> <li>Use suggestions feature for small fixes</li> </ul>"},{"location":"community/contributing/#release-process","title":"Release Process","text":"<p>Maintainers handle releases:</p> <ol> <li>Update <code>__version__</code> in <code>__init__.py</code></li> <li>Update <code>CHANGELOG.md</code></li> <li>Create GitHub release</li> <li>PyPI publish (automated)</li> </ol>"},{"location":"community/contributing/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcac Discord - Chat with maintainers</li> <li>\ud83d\udc1b Issues - Bug reports</li> <li>\ud83d\udca1 Discussions - Questions</li> </ul>"},{"location":"community/contributing/#recognition","title":"Recognition","text":"<p>Contributors are: - Listed in <code>CONTRIBUTORS.md</code> - Mentioned in release notes - Thanked in community channels</p> <p>Thank you for contributing to Xeepy! \ud83d\ude4f</p>"},{"location":"community/faq/","title":"Frequently Asked Questions","text":"<p>Common questions and answers about Xeepy.</p>"},{"location":"community/faq/#general","title":"General","text":""},{"location":"community/faq/#what-is-xeepy","title":"What is Xeepy?","text":"<p>Xeepy is a Python toolkit for X/Twitter automation. It uses browser automation (Playwright) instead of the Twitter API, which means:</p> <ul> <li>\u2705 No API fees ($0/month vs $100+/month)</li> <li>\u2705 No rate limit anxiety</li> <li>\u2705 No approval process</li> <li>\u2705 Access to all features</li> </ul>"},{"location":"community/faq/#is-xeepy-free","title":"Is Xeepy free?","text":"<p>Yes! Xeepy is open-source and free under the MIT license.</p>"},{"location":"community/faq/#is-xeepy-legal","title":"Is Xeepy legal?","text":"<p>Xeepy is for educational purposes only. While browser automation isn't explicitly forbidden, automated access to X/Twitter may violate their Terms of Service. Use responsibly and at your own risk.</p>"},{"location":"community/faq/#does-xeepy-require-twitter-api-keys","title":"Does Xeepy require Twitter API keys?","text":"<p>No. Xeepy uses browser automation, so you don't need:</p> <ul> <li>Twitter Developer Account</li> <li>API keys or tokens</li> <li>Elevated access approval</li> </ul> <p>You just log in with your regular X/Twitter account.</p>"},{"location":"community/faq/#installation","title":"Installation","text":""},{"location":"community/faq/#what-python-version-do-i-need","title":"What Python version do I need?","text":"<p>Python 3.10 or higher is required.</p> <pre><code>python --version  # Should be 3.10+\n</code></pre>"},{"location":"community/faq/#how-do-i-install-xeepy","title":"How do I install Xeepy?","text":"<pre><code>pip install xeepy\nplaywright install chromium\n</code></pre>"},{"location":"community/faq/#i-get-playwright-not-found-error","title":"I get \"playwright not found\" error","text":"<p>Install Playwright and its browser:</p> <pre><code>pip install playwright\nplaywright install chromium\n</code></pre>"},{"location":"community/faq/#installation-is-slow-on-linux","title":"Installation is slow on Linux","text":"<p>The browser download can be slow. Try using a CDN:</p> <pre><code>PLAYWRIGHT_DOWNLOAD_HOST=https://playwright.azureedge.net playwright install chromium\n</code></pre>"},{"location":"community/faq/#how-do-i-update-xeepy","title":"How do I update Xeepy?","text":"<pre><code>pip install --upgrade xeepy\nplaywright install chromium  # Update browser too\n</code></pre>"},{"location":"community/faq/#authentication","title":"Authentication","text":""},{"location":"community/faq/#how-do-i-authenticate","title":"How do I authenticate?","text":"<pre><code>xeepy auth login  # Opens browser for manual login\n</code></pre> <p>Or in Python:</p> <pre><code>async with Xeepy() as x:\n    await x.auth.login()\n</code></pre>"},{"location":"community/faq/#where-is-my-session-stored","title":"Where is my session stored?","text":"<p>Default locations:</p> OS Path Linux <code>~/.config/xeepy/session.json</code> macOS <code>~/Library/Application Support/xeepy/session.json</code> Windows <code>%APPDATA%\\xeepy\\session.json</code>"},{"location":"community/faq/#my-session-expired-what-do-i-do","title":"My session expired. What do I do?","text":"<p>Re-authenticate:</p> <pre><code>xeepy auth login\n</code></pre> <p>Sessions typically last 30 days.</p>"},{"location":"community/faq/#can-i-use-multiple-accounts","title":"Can I use multiple accounts?","text":"<p>Yes! Use profiles:</p> <pre><code>xeepy auth login --profile personal\nxeepy auth login --profile business\n\n# Use specific profile\nxeepy --profile business scrape replies URL\n</code></pre>"},{"location":"community/faq/#can-i-run-on-a-headless-server","title":"Can I run on a headless server?","text":"<p>Yes, but you'll need to authenticate first:</p> <ol> <li> <p>Authenticate on a machine with a display:    <pre><code>xeepy auth login\nxeepy auth export session.json\n</code></pre></p> </li> <li> <p>Copy <code>session.json</code> to your server</p> </li> <li> <p>Import on server:    <pre><code>xeepy auth import session.json\n</code></pre></p> </li> </ol>"},{"location":"community/faq/#scraping","title":"Scraping","text":""},{"location":"community/faq/#how-many-tweetsfollowers-can-i-scrape","title":"How many tweets/followers can I scrape?","text":"<p>There's no hard limit in Xeepy, but X/Twitter may impose limits:</p> <ul> <li>Start with small amounts (100-500)</li> <li>Increase gradually</li> <li>Use delays between requests</li> <li>Respect rate limits</li> </ul>"},{"location":"community/faq/#scraping-is-slow-how-can-i-speed-it-up","title":"Scraping is slow. How can I speed it up?","text":"<pre><code># Increase rate limit (use carefully)\nx.config.rate_limit.requests_per_minute = 30\n\n# Use caching\nx.config.storage.cache_enabled = True\n</code></pre>"},{"location":"community/faq/#im-getting-blockedrate-limited","title":"I'm getting blocked/rate limited","text":"<p>Reduce your request rate:</p> <pre><code>x.config.rate_limit.requests_per_minute = 10\n</code></pre> <p>Or use proxies:</p> <pre><code># xeepy.toml\n[xeepy.proxy]\nenabled = true\nurl = \"http://user:pass@proxy:8080\"\n</code></pre>"},{"location":"community/faq/#can-i-scrape-private-accounts","title":"Can I scrape private accounts?","text":"<p>Only if: 1. You follow the account 2. They follow you back (for some data)</p> <p>Xeepy respects privacy settings.</p>"},{"location":"community/faq/#how-do-i-export-data","title":"How do I export data?","text":"<pre><code>x.export.to_csv(data, \"output.csv\")\nx.export.to_json(data, \"output.json\")\nx.export.to_excel(data, \"output.xlsx\")\n</code></pre> <p>Or via CLI:</p> <pre><code>xeepy scrape tweets username --limit 100 -o tweets.csv\n</code></pre>"},{"location":"community/faq/#followunfollow","title":"Follow/Unfollow","text":""},{"location":"community/faq/#how-many-followsunfollows-per-day-is-safe","title":"How many follows/unfollows per day is safe?","text":"<p>Conservative limits:</p> Action Safe Limit Follows 30-50/day Unfollows 50-100/day Likes 100-200/day <p>Xeepy has built-in limits. Don't disable them.</p>"},{"location":"community/faq/#i-accidentally-unfollowed-someone-important","title":"I accidentally unfollowed someone important!","text":"<p>Check the result object for who was unfollowed:</p> <pre><code>result = await x.unfollow.non_followers()\nprint(result.unfollowed_users)  # List of unfollowed\n\n# Re-follow\nawait x.follow.user(\"important_person\")\n</code></pre>"},{"location":"community/faq/#how-do-i-protect-accounts-from-unfollowing","title":"How do I protect accounts from unfollowing?","text":"<p>Use a whitelist:</p> <pre><code>await x.unfollow.non_followers(\n    whitelist=[\"friend1\", \"friend2\"],\n    # or\n    whitelist_file=\"whitelist.txt\"\n)\n</code></pre>"},{"location":"community/faq/#my-follow-ratio-is-bad-how-do-i-fix-it","title":"My follow ratio is bad. How do I fix it?","text":"<pre><code># Unfollow non-followers gradually\nawait x.unfollow.non_followers(max_unfollows=25)  # Daily\n\n# Or smart unfollow\nawait x.unfollow.smart(\n    criteria={\"inactive_days\": 90, \"not_following_back\": True},\n    max_unfollows=50\n)\n</code></pre>"},{"location":"community/faq/#ai-features","title":"AI Features","text":""},{"location":"community/faq/#which-ai-providers-are-supported","title":"Which AI providers are supported?","text":"<ul> <li>OpenAI (GPT-4, GPT-3.5)</li> <li>Anthropic (Claude 3)</li> <li>Ollama (local, free)</li> </ul>"},{"location":"community/faq/#how-do-i-use-ai-without-paying","title":"How do I use AI without paying?","text":"<p>Use Ollama for free, local AI:</p> <pre><code># Install Ollama\ncurl https://ollama.ai/install.sh | sh\n\n# Pull a model\nollama pull llama3\n</code></pre> <pre><code>from xeepy.ai import ContentGenerator\n\nai = ContentGenerator(\n    provider=\"ollama\",\n    model=\"llama3\"\n)\n</code></pre>"},{"location":"community/faq/#ai-generated-content-sounds-robotic","title":"AI-generated content sounds robotic","text":"<p>Adjust the temperature and add personalization:</p> <pre><code>ai = ContentGenerator(\n    provider=\"openai\",\n    temperature=0.8,  # Higher = more creative\n)\n\n# Add your style context\nreply = await ai.generate_reply(\n    tweet_text=tweet,\n    style=\"casual\",\n    voice_sample=\"your previous tweets here\"  # Optional\n)\n</code></pre>"},{"location":"community/faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"community/faq/#browser-keeps-crashing","title":"Browser keeps crashing","text":"<pre><code># Reinstall browser\nplaywright install chromium --force\n\n# Install dependencies (Linux)\nplaywright install-deps chromium\n</code></pre>"},{"location":"community/faq/#element-not-found-errors","title":"\"Element not found\" errors","text":"<p>X/Twitter's UI changed. Update Xeepy:</p> <pre><code>pip install --upgrade xeepy\n</code></pre> <p>If still broken, report the issue.</p>"},{"location":"community/faq/#high-memory-usage","title":"High memory usage","text":"<p>Use streaming for large datasets:</p> <pre><code>async for batch in x.scrape.followers_batched(\"user\", batch_size=100):\n    x.export.append_csv(batch, \"followers.csv\")\n</code></pre>"},{"location":"community/faq/#operations-are-timing-out","title":"Operations are timing out","text":"<p>Increase timeout:</p> <pre><code>async with Xeepy(timeout=60000) as x:  # 60 seconds\n    ...\n</code></pre>"},{"location":"community/faq/#im-getting-captcha-challenges","title":"I'm getting CAPTCHA challenges","text":"<p>This means X/Twitter detected automation:</p> <ol> <li> <p>Use headful mode to solve manually:    <pre><code>async with Xeepy(headless=False) as x:\n    await x.auth.login()\n</code></pre></p> </li> <li> <p>Reduce request rate</p> </li> <li>Use residential proxies</li> <li>Take a break (24-48 hours)</li> </ol>"},{"location":"community/faq/#best-practices","title":"Best Practices","text":""},{"location":"community/faq/#whats-the-safest-way-to-use-xeepy","title":"What's the safest way to use Xeepy?","text":"<ol> <li>Start slow - Begin with low limits</li> <li>Use delays - Don't rapid-fire requests</li> <li>Be human - Mix automated and manual activity</li> <li>Monitor - Watch for warnings/blocks</li> <li>Backup - Keep whitelist and session backups</li> </ol>"},{"location":"community/faq/#how-do-i-avoid-getting-my-account-flagged","title":"How do I avoid getting my account flagged?","text":"<ul> <li>Don't follow/unfollow the same people repeatedly</li> <li>Keep following ratio reasonable (&lt; 1.5)</li> <li>Mix content types (not all automation)</li> <li>Take breaks</li> <li>Use residential proxies if needed</li> </ul>"},{"location":"community/faq/#should-i-use-a-dedicated-account","title":"Should I use a dedicated account?","text":"<p>For heavy automation, yes. Don't risk your main account.</p>"},{"location":"community/faq/#more-questions","title":"More Questions?","text":"<ul> <li>\ud83d\udcd6 Full Documentation</li> <li>\ud83d\udcac Discord Community</li> <li>\ud83d\udc1b Report Issues</li> <li>\ud83d\udce7 Email Support</li> </ul>"},{"location":"community/roadmap/","title":"Roadmap","text":"<p>Our vision for Xeepy' future development.</p>"},{"location":"community/roadmap/#current-version-100","title":"Current Version: 1.0.0","text":"<p>Released January 2024 with core functionality.</p>"},{"location":"community/roadmap/#q1-2024","title":"Q1 2024","text":""},{"location":"community/roadmap/#v110-enhanced-scraping","title":"v1.1.0 - Enhanced Scraping \ud83d\udd0d","text":"<ul> <li> Parallel scraping - Scrape multiple targets simultaneously</li> <li> Resume from checkpoint - Continue interrupted scrapes</li> <li> Smart caching - Intelligent cache with TTL</li> <li> Media download improvements - Better video quality selection</li> <li> Spaces transcription - Auto-transcribe audio Spaces</li> </ul>"},{"location":"community/roadmap/#v120-advanced-analytics","title":"v1.2.0 - Advanced Analytics \ud83d\udcca","text":"<ul> <li> Engagement prediction - ML-based engagement forecasting</li> <li> Optimal posting scheduler - AI-powered best time calculator</li> <li> Audience overlap analysis - Find shared audiences</li> <li> Content performance insights - Deep content analysis</li> <li> Export to dashboards - Grafana, Metabase integrations</li> </ul>"},{"location":"community/roadmap/#q2-2024","title":"Q2 2024","text":""},{"location":"community/roadmap/#v130-enterprise-features","title":"v1.3.0 - Enterprise Features \ud83c\udfe2","text":"<ul> <li> Multi-account management - Manage multiple accounts</li> <li> Team collaboration - Shared sessions and configs</li> <li> Audit logging - Complete action history</li> <li> Role-based access - Permission management</li> <li> SSO integration - Enterprise authentication</li> </ul>"},{"location":"community/roadmap/#v140-platform-expansion","title":"v1.4.0 - Platform Expansion \ud83c\udf10","text":"<ul> <li> Threads support - Meta's Threads platform</li> <li> Bluesky support - AT Protocol integration</li> <li> Mastodon support - Fediverse compatibility</li> <li> Cross-platform analytics - Unified metrics</li> </ul>"},{"location":"community/roadmap/#q3-2024","title":"Q3 2024","text":""},{"location":"community/roadmap/#v150-ai-first-features","title":"v1.5.0 - AI-First Features \ud83e\udd16","text":"<ul> <li> AI content calendar - Automated content planning</li> <li> Smart replies - Context-aware AI responses</li> <li> Trend prediction - Predict trending topics</li> <li> Viral content detector - Identify viral potential</li> <li> Voice-to-thread - Convert audio to threads</li> </ul>"},{"location":"community/roadmap/#v160-automation-20","title":"v1.6.0 - Automation 2.0 \u26a1","text":"<ul> <li> Visual automation builder - No-code workflows</li> <li> Conditional logic - If-then automation rules</li> <li> Webhook triggers - Event-driven automation</li> <li> Scheduled campaigns - Multi-day campaigns</li> <li> A/B testing framework - Built-in experiments</li> </ul>"},{"location":"community/roadmap/#q4-2024","title":"Q4 2024","text":""},{"location":"community/roadmap/#v200-xeepy-cloud","title":"v2.0.0 - Xeepy Cloud \u2601\ufe0f","text":"<p>Major version with cloud capabilities:</p> <ul> <li> Xeepy Cloud - Managed SaaS version</li> <li> Web dashboard - Browser-based interface</li> <li> Mobile app - iOS/Android companion</li> <li> Real-time sync - Cross-device synchronization</li> <li> API marketplace - Third-party integrations</li> </ul>"},{"location":"community/roadmap/#future-ideas","title":"Future Ideas \ud83d\udcad","text":"<p>These are being explored but not yet scheduled:</p>"},{"location":"community/roadmap/#research-analysis","title":"Research &amp; Analysis","text":"<ul> <li>Academic research tools</li> <li>Network graph visualization</li> <li>Influence mapping</li> <li>Sentiment trends over time</li> <li>Fake news detection</li> </ul>"},{"location":"community/roadmap/#engagement","title":"Engagement","text":"<ul> <li>Smart DM campaigns</li> <li>Automated customer support</li> <li>Community management tools</li> <li>Event promotion automation</li> <li>Influencer outreach</li> </ul>"},{"location":"community/roadmap/#integrations","title":"Integrations","text":"<ul> <li>CRM integrations (Salesforce, HubSpot)</li> <li>Marketing tools (Buffer, Hootsuite)</li> <li>Analytics platforms (Google Analytics)</li> <li>Automation tools (Zapier, n8n)</li> <li>Databases (PostgreSQL, MongoDB)</li> </ul>"},{"location":"community/roadmap/#developer-experience","title":"Developer Experience","text":"<ul> <li>VS Code extension</li> <li>Jupyter notebook integration</li> <li>GraphQL playground</li> <li>SDK for other languages</li> <li>Plugin marketplace</li> </ul>"},{"location":"community/roadmap/#how-to-influence-the-roadmap","title":"How to Influence the Roadmap","text":""},{"location":"community/roadmap/#vote-on-features","title":"Vote on Features","text":"<p>Star and comment on GitHub Issues to show interest.</p>"},{"location":"community/roadmap/#propose-features","title":"Propose Features","text":"<p>Open a Discussion with your idea.</p>"},{"location":"community/roadmap/#sponsor-development","title":"Sponsor Development","text":"<p>Sponsor to prioritize features you need.</p>"},{"location":"community/roadmap/#contribute","title":"Contribute","text":"<p>Implement features yourself - we love PRs!</p>"},{"location":"community/roadmap/#version-history","title":"Version History","text":"Version Date Highlights 1.0.0 Jan 2024 Initial release 0.9.0 Dec 2023 Public beta"},{"location":"community/roadmap/#release-philosophy","title":"Release Philosophy","text":"<ul> <li>Stability first - No breaking changes in minor versions</li> <li>Monthly releases - Predictable release cycle</li> <li>Community driven - Features based on user feedback</li> <li>Open development - Transparent roadmap and progress</li> </ul> <p>Last updated: January 2024</p> <p>This roadmap is subject to change based on community feedback and priorities.</p>"},{"location":"cookbook/","title":"Cookbook","text":"<p>Real-world recipes for X/Twitter success. These aren't tutorials\u2014they're battle-tested workflows.</p> <ul> <li> <p> Growth Hacking</p> <p>Proven strategies to grow your following fast</p> <p> Growth Recipes</p> </li> <li> <p> Automation Workflows</p> <p>Set-and-forget automation scripts</p> <p> Automation</p> </li> <li> <p> Data Science</p> <p>Advanced analytics and ML applications</p> <p> Data Science</p> </li> <li> <p> Business Intelligence</p> <p>Competitive intelligence and lead gen</p> <p> Business</p> </li> <li> <p> Research</p> <p>Academic and journalism applications</p> <p> Research</p> </li> </ul>"},{"location":"cookbook/#quick-wins","title":"Quick Wins","text":"<p>Copy-paste scripts for immediate results:</p>"},{"location":"cookbook/#clean-your-following-5-minutes","title":"\ud83e\uddf9 Clean Your Following (5 minutes)","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\n\nasync def quick_cleanup():\n    async with Xeepy() as x:\n        result = await x.unfollow.non_followers(\n            max_unfollows=50,\n            min_following_days=7,\n            exclude_verified=True,\n            dry_run=False\n        )\n        print(f\"\u2705 Unfollowed {result.success_count} non-followers\")\n\nasyncio.run(quick_cleanup())\n</code></pre>"},{"location":"cookbook/#daily-growth-script","title":"\ud83d\udcc8 Daily Growth Script","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\n\nasync def daily_growth():\n    async with Xeepy() as x:\n        # Follow 50 users in your niche\n        await x.follow.by_keyword(\n            [\"your\", \"niche\", \"keywords\"],\n            max_follows=50\n        )\n\n        # Like 30 tweets\n        await x.engage.auto_like(\n            keywords=[\"your niche\"],\n            max_likes=30\n        )\n\n        # Check stats\n        me = await x.scrape.profile(\"me\")\n        print(f\"Followers: {me.followers_count} | Following: {me.following_count}\")\n\nasyncio.run(daily_growth())\n</code></pre>"},{"location":"cookbook/#competitor-analysis","title":"\ud83d\udd0d Competitor Analysis","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\n\nasync def analyze_competitor(username: str):\n    async with Xeepy() as x:\n        profile = await x.scrape.profile(username)\n        tweets = await x.scrape.tweets(username, limit=100)\n\n        # Calculate engagement rate\n        total_engagement = sum(t.likes + t.retweets for t in tweets)\n        avg_engagement = total_engagement / len(tweets)\n        engagement_rate = (avg_engagement / profile.followers_count) * 100\n\n        print(f\"@{username} Analysis:\")\n        print(f\"  Followers: {profile.followers_count:,}\")\n        print(f\"  Avg Engagement: {avg_engagement:.0f}\")\n        print(f\"  Engagement Rate: {engagement_rate:.2f}%\")\n\n        # Top performing content\n        top_tweets = sorted(tweets, key=lambda t: t.likes, reverse=True)[:5]\n        print(f\"\\nTop Tweets:\")\n        for t in top_tweets:\n            print(f\"  - {t.text[:60]}... ({t.likes} likes)\")\n\nasyncio.run(analyze_competitor(\"competitor_username\"))\n</code></pre>"},{"location":"cookbook/#featured-recipes","title":"Featured Recipes","text":""},{"location":"cookbook/#the-ghost-follower-detector","title":"\ud83c\udfaf The Ghost Follower Detector","text":"<p>Find and remove followers who never engage:</p> <pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom collections import defaultdict\n\nasync def detect_ghost_followers():\n    \"\"\"Find followers who never engage with your content\"\"\"\n\n    async with Xeepy() as x:\n        # Get your recent tweets\n        my_tweets = await x.scrape.tweets(\"me\", limit=50)\n\n        # Get all engagers\n        engager_counts = defaultdict(int)\n\n        for tweet in my_tweets:\n            likers = await x.scrape.likers(tweet.url, limit=100)\n            for user in likers:\n                engager_counts[user.username] += 1\n\n        # Get your followers\n        followers = await x.scrape.followers(\"me\", limit=1000)\n        follower_usernames = {f.username for f in followers}\n\n        # Find ghosts (followers who never engage)\n        engagers = set(engager_counts.keys())\n        ghosts = follower_usernames - engagers\n\n        print(f\"\ud83d\udcca Ghost Follower Analysis\")\n        print(f\"   Total followers: {len(follower_usernames)}\")\n        print(f\"   Active engagers: {len(engagers &amp; follower_usernames)}\")\n        print(f\"   Ghost followers: {len(ghosts)} ({len(ghosts)/len(follower_usernames)*100:.1f}%)\")\n\n        return list(ghosts)\n\nasyncio.run(detect_ghost_followers())\n</code></pre>"},{"location":"cookbook/#the-sleep-schedule-optimizer","title":"\ud83c\udf19 The Sleep Schedule Optimizer","text":"<p>Post when your audience is most active:</p> <pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom collections import Counter\nfrom datetime import datetime\n\nasync def find_optimal_post_times():\n    \"\"\"Analyze when your audience engages most\"\"\"\n\n    async with Xeepy() as x:\n        # Get your followers' recent activity\n        followers = await x.scrape.followers(\"me\", limit=200)\n\n        activity_hours = Counter()\n\n        for follower in followers[:50]:  # Sample 50 followers\n            tweets = await x.scrape.tweets(follower.username, limit=20)\n\n            for tweet in tweets:\n                hour = tweet.created_at.hour\n                activity_hours[hour] += 1\n\n        # Find peak hours\n        total = sum(activity_hours.values())\n        print(\"\ud83d\udcca Audience Activity by Hour (your timezone)\")\n        print(\"=\"*50)\n\n        for hour in range(24):\n            count = activity_hours.get(hour, 0)\n            pct = (count / total * 100) if total &gt; 0 else 0\n            bar = \"\u2588\" * int(pct / 2)\n            time_str = f\"{hour:02d}:00\"\n            print(f\"{time_str} | {bar} {pct:.1f}%\")\n\n        # Top 3 hours\n        top_hours = activity_hours.most_common(3)\n        print(f\"\\n\ud83c\udfaf Best times to post: {', '.join(f'{h}:00' for h, _ in top_hours)}\")\n\nasyncio.run(find_optimal_post_times())\n</code></pre>"},{"location":"cookbook/#advanced-patterns","title":"Advanced Patterns","text":"<p>These recipes combine multiple Xeepy features:</p> <ul> <li>Viral Content Detector - Find trends before they peak</li> <li>Engagement Pod Automation - Coordinate with allies</li> <li>Sentiment Dashboard - Real-time brand monitoring</li> <li>Lead Generation Pipeline - Find and qualify prospects</li> <li>Crisis Detection System - Early warning for brand issues</li> </ul>"},{"location":"cookbook/automation/","title":"\ud83e\udd16 Automation Workflows Cookbook","text":"<p>Production-ready automation recipes for X/Twitter. These workflows run reliably, handle errors gracefully, and respect rate limits.</p>"},{"location":"cookbook/automation/#the-247-automation-stack","title":"The 24/7 Automation Stack","text":"<p>A complete automation system that runs continuously.</p> <pre><code>\"\"\"\nThe 24/7 Automation Stack\n=========================\nA production-ready automation system with:\n- Scheduled tasks\n- Error handling\n- Rate limit management\n- Logging and monitoring\n- Graceful shutdown\n\"\"\"\nimport asyncio\nimport signal\nimport logging\nfrom datetime import datetime, timedelta\nfrom typing import Callable, List\nfrom dataclasses import dataclass, field\n\nfrom xeepy import Xeepy\nfrom xeepy.notifications import DiscordNotifier\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('automation.log'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger('xeepy.automation')\n\n\n@dataclass\nclass ScheduledTask:\n    \"\"\"A task that runs on a schedule\"\"\"\n    name: str\n    func: Callable\n    interval_seconds: int\n    last_run: datetime = None\n    error_count: int = 0\n    max_errors: int = 3\n    enabled: bool = True\n\n\nclass AutomationStack:\n    \"\"\"\n    Production automation runner with scheduling,\n    error handling, and monitoring.\n    \"\"\"\n\n    def __init__(self, notify_webhook: str = None):\n        self.tasks: List[ScheduledTask] = []\n        self.running = False\n        self.xeepy: Xeepy = None\n        self.notifier = DiscordNotifier(webhook_url=notify_webhook) if notify_webhook else None\n\n        # Handle graceful shutdown\n        signal.signal(signal.SIGINT, self._shutdown)\n        signal.signal(signal.SIGTERM, self._shutdown)\n\n    def add_task(self, name: str, func: Callable, interval: str):\n        \"\"\"Add a scheduled task\"\"\"\n        # Parse interval\n        intervals = {\n            \"hourly\": 3600,\n            \"daily\": 86400,\n            \"every_6h\": 21600,\n            \"every_30m\": 1800,\n            \"every_15m\": 900,\n        }\n        seconds = intervals.get(interval, int(interval))\n\n        self.tasks.append(ScheduledTask(\n            name=name,\n            func=func,\n            interval_seconds=seconds\n        ))\n        logger.info(f\"Added task: {name} (every {seconds}s)\")\n\n    async def run(self):\n        \"\"\"Main automation loop\"\"\"\n        self.running = True\n        logger.info(\"\ud83d\ude80 Automation stack starting...\")\n\n        async with Xeepy() as x:\n            self.xeepy = x\n\n            if self.notifier:\n                await self.notifier.send(\"\ud83d\udfe2 Xeepy automation started\")\n\n            while self.running:\n                for task in self.tasks:\n                    if not task.enabled:\n                        continue\n\n                    # Check if task should run\n                    if self._should_run(task):\n                        await self._run_task(task)\n\n                # Sleep before next check\n                await asyncio.sleep(60)  # Check every minute\n\n        logger.info(\"Automation stack stopped\")\n\n    def _should_run(self, task: ScheduledTask) -&gt; bool:\n        \"\"\"Check if task is due to run\"\"\"\n        if task.last_run is None:\n            return True\n\n        elapsed = (datetime.now() - task.last_run).total_seconds()\n        return elapsed &gt;= task.interval_seconds\n\n    async def _run_task(self, task: ScheduledTask):\n        \"\"\"Execute a task with error handling\"\"\"\n        logger.info(f\"Running task: {task.name}\")\n\n        try:\n            await task.func(self.xeepy)\n            task.last_run = datetime.now()\n            task.error_count = 0\n            logger.info(f\"\u2713 Task completed: {task.name}\")\n\n        except Exception as e:\n            task.error_count += 1\n            logger.error(f\"\u2717 Task failed: {task.name} - {e}\")\n\n            if self.notifier:\n                await self.notifier.send(\n                    f\"\u26a0\ufe0f Task failed: {task.name}\\nError: {str(e)[:200]}\"\n                )\n\n            # Disable task after too many errors\n            if task.error_count &gt;= task.max_errors:\n                task.enabled = False\n                logger.warning(f\"Task disabled due to errors: {task.name}\")\n\n                if self.notifier:\n                    await self.notifier.send(\n                        f\"\ud83d\udd34 Task disabled: {task.name} (too many errors)\"\n                    )\n\n    def _shutdown(self, signum, frame):\n        \"\"\"Handle shutdown signal\"\"\"\n        logger.info(\"Shutdown signal received...\")\n        self.running = False\n\n\n# ============================================\n# AUTOMATION TASKS\n# ============================================\n\nasync def task_unfollower_check(x: Xeepy):\n    \"\"\"Check for new unfollowers\"\"\"\n    report = await x.monitor.unfollowers()\n\n    if report.unfollowers:\n        logger.info(f\"New unfollowers: {report.unfollowers}\")\n\n    logger.info(f\"Follower change: {report.net_change:+d}\")\n\n\nasync def task_engagement_routine(x: Xeepy):\n    \"\"\"Daily engagement routine\"\"\"\n    # Like tweets from your network\n    timeline = await x.scrape.home_timeline(limit=50)\n\n    liked = 0\n    for tweet in timeline:\n        if tweet.likes &gt; 10 and not tweet.is_liked:\n            await x.engage.like(tweet.url)\n            liked += 1\n\n            if liked &gt;= 20:  # Limit per run\n                break\n\n    logger.info(f\"Liked {liked} tweets\")\n\n\nasync def task_cleanup_following(x: Xeepy):\n    \"\"\"Unfollow non-followers\"\"\"\n    result = await x.unfollow.non_followers(\n        max_unfollows=25,\n        whitelist_file=\"whitelist.txt\",\n        min_days_following=7\n    )\n\n    logger.info(f\"Unfollowed {result.unfollowed_count} non-followers\")\n\n\nasync def task_growth_tracking(x: Xeepy):\n    \"\"\"Track and log growth metrics\"\"\"\n    growth = await x.analytics.track_growth(\"24h\")\n\n    logger.info(f\"\"\"\n    Growth Report (24h):\n    - Followers: {growth.followers_count:,}\n    - Net change: {growth.net_change:+d}\n    - New: {len(growth.new_followers)}\n    - Lost: {len(growth.unfollowers)}\n    \"\"\")\n\n\nasync def task_mention_monitoring(x: Xeepy):\n    \"\"\"Monitor and respond to mentions\"\"\"\n    mentions = await x.scrape.mentions(\"your_username\", limit=20, since=\"1h\")\n\n    for mention in mentions:\n        if not mention.has_my_reply:\n            # Auto-like mentions\n            await x.engage.like(mention.url)\n            logger.info(f\"Liked mention from @{mention.author.username}\")\n\n\n# ============================================\n# MAIN ENTRY POINT\n# ============================================\n\nasync def main():\n    # Create automation stack\n    stack = AutomationStack(\n        notify_webhook=\"https://discord.com/api/webhooks/...\"\n    )\n\n    # Add tasks\n    stack.add_task(\"unfollower_check\", task_unfollower_check, \"hourly\")\n    stack.add_task(\"engagement\", task_engagement_routine, \"every_6h\")\n    stack.add_task(\"cleanup\", task_cleanup_following, \"daily\")\n    stack.add_task(\"growth_tracking\", task_growth_tracking, \"every_6h\")\n    stack.add_task(\"mentions\", task_mention_monitoring, \"every_15m\")\n\n    # Run forever\n    await stack.run()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"cookbook/automation/#smart-engagement-bot","title":"Smart Engagement Bot","text":"<p>An intelligent engagement bot that adds value, not spam.</p> <pre><code>\"\"\"\nSmart Engagement Bot\n====================\nEngages thoughtfully based on content relevance and quality.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\nfrom xeepy.ai import ContentGenerator, SentimentAnalyzer\n\nclass SmartEngagementBot:\n    \"\"\"\n    Engagement bot that:\n    - Analyzes content before engaging\n    - Generates contextual comments\n    - Respects rate limits\n    - Avoids spam behavior\n    \"\"\"\n\n    def __init__(self, keywords: list, daily_limits: dict = None):\n        self.keywords = keywords\n        self.limits = daily_limits or {\n            \"likes\": 100,\n            \"replies\": 20,\n            \"follows\": 30\n        }\n        self.daily_stats = {\"likes\": 0, \"replies\": 0, \"follows\": 0}\n        self.engaged_today = set()  # Track engaged tweets\n\n    async def run(self):\n        async with Xeepy() as x:\n            ai = ContentGenerator(provider=\"openai\")\n            sentiment = SentimentAnalyzer(provider=\"openai\")\n\n            while True:\n                # Find relevant content\n                for keyword in self.keywords:\n                    tweets = await x.scrape.search(\n                        keyword,\n                        limit=20,\n                        min_likes=5,\n                        max_age_hours=6\n                    )\n\n                    for tweet in tweets:\n                        # Skip if already engaged\n                        if tweet.id in self.engaged_today:\n                            continue\n\n                        # Analyze tweet quality\n                        quality = await self._analyze_quality(tweet, sentiment)\n\n                        if quality[\"score\"] &gt; 0.7:\n                            await self._engage_with_tweet(x, ai, tweet, quality)\n\n                # Wait before next round\n                await asyncio.sleep(1800)  # 30 minutes\n\n                # Reset daily stats at midnight\n                if self._is_new_day():\n                    self._reset_daily_stats()\n\n    async def _analyze_quality(self, tweet, sentiment):\n        \"\"\"Analyze if tweet is worth engaging with\"\"\"\n        score = 0\n        reasons = []\n\n        # Sentiment analysis\n        sent_result = await sentiment.analyze(tweet.text)\n        if sent_result.label == \"positive\":\n            score += 0.3\n            reasons.append(\"positive_content\")\n\n        # Engagement indicators\n        if tweet.likes &gt; 50:\n            score += 0.2\n            reasons.append(\"high_engagement\")\n\n        # Author quality\n        if tweet.author.followers_count &gt; 1000:\n            score += 0.2\n            reasons.append(\"quality_author\")\n\n        if tweet.author.verified:\n            score += 0.1\n            reasons.append(\"verified\")\n\n        # Content quality\n        if len(tweet.text) &gt; 100:\n            score += 0.1\n            reasons.append(\"substantial_content\")\n\n        if \"?\" in tweet.text:  # Questions are engagement opportunities\n            score += 0.1\n            reasons.append(\"question\")\n\n        return {\"score\": score, \"reasons\": reasons}\n\n    async def _engage_with_tweet(self, x, ai, tweet, quality):\n        \"\"\"Engage with a quality tweet\"\"\"\n        self.engaged_today.add(tweet.id)\n\n        # Always like quality content\n        if self.daily_stats[\"likes\"] &lt; self.limits[\"likes\"]:\n            await x.engage.like(tweet.url)\n            self.daily_stats[\"likes\"] += 1\n\n        # Reply if it's a question or high-engagement opportunity\n        if \"question\" in quality[\"reasons\"] and self.daily_stats[\"replies\"] &lt; self.limits[\"replies\"]:\n            reply = await ai.generate_reply(\n                tweet_text=tweet.text,\n                style=\"helpful\",\n                add_value=True\n            )\n\n            # Only reply if we have something valuable to say\n            if len(reply) &gt; 50:\n                await x.engage.reply(tweet.url, reply)\n                self.daily_stats[\"replies\"] += 1\n\n        # Consider following if quality author\n        if \"quality_author\" in quality[\"reasons\"] and self.daily_stats[\"follows\"] &lt; self.limits[\"follows\"]:\n            if tweet.author.followers_count &lt; 50000:  # Not too big\n                await x.follow.user(tweet.author.username)\n                self.daily_stats[\"follows\"] += 1\n\n    def _is_new_day(self):\n        from datetime import datetime\n        return datetime.now().hour == 0 and datetime.now().minute &lt; 30\n\n    def _reset_daily_stats(self):\n        self.daily_stats = {\"likes\": 0, \"replies\": 0, \"follows\": 0}\n        self.engaged_today.clear()\n\n\n# Run the bot\nasync def main():\n    bot = SmartEngagementBot(\n        keywords=[\"python programming\", \"machine learning\", \"startup\"],\n        daily_limits={\"likes\": 100, \"replies\": 15, \"follows\": 20}\n    )\n    await bot.run()\n\nasyncio.run(main())\n</code></pre>"},{"location":"cookbook/automation/#content-pipeline-automation","title":"Content Pipeline Automation","text":"<p>Automate your content creation and publishing workflow.</p> <pre><code>\"\"\"\nContent Pipeline Automation\n===========================\nFrom ideation to publishing, fully automated.\n\"\"\"\nimport asyncio\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\nfrom xeepy import Xeepy\nfrom xeepy.ai import ContentGenerator\n\n\n@dataclass\nclass ContentItem:\n    \"\"\"A piece of content in the pipeline\"\"\"\n    id: str\n    content_type: str  # \"tweet\", \"thread\", \"reply\"\n    text: str\n    status: str = \"draft\"  # draft, scheduled, published, failed\n    scheduled_time: Optional[datetime] = None\n    published_url: Optional[str] = None\n    engagement_score: Optional[float] = None\n\n\nclass ContentPipeline:\n    \"\"\"\n    Automated content pipeline:\n    1. Generate content ideas based on trends\n    2. Create drafts using AI\n    3. Schedule for optimal times\n    4. Publish automatically\n    5. Track performance\n    \"\"\"\n\n    def __init__(self):\n        self.queue: List[ContentItem] = []\n        self.published: List[ContentItem] = []\n\n    async def run(self):\n        async with Xeepy() as x:\n            ai = ContentGenerator(provider=\"openai\")\n\n            while True:\n                # Phase 1: Content Generation (every 6 hours)\n                if self._should_generate():\n                    await self._generate_content(x, ai)\n\n                # Phase 2: Publishing (check every 5 minutes)\n                await self._publish_scheduled(x)\n\n                # Phase 3: Performance Tracking (hourly)\n                if self._should_track():\n                    await self._track_performance(x)\n\n                await asyncio.sleep(300)  # 5 minutes\n\n    async def _generate_content(self, x: Xeepy, ai: ContentGenerator):\n        \"\"\"Generate new content based on trends and best practices\"\"\"\n        print(\"\ud83d\udcdd Generating new content...\")\n\n        # Get trending topics in your niche\n        trending = await x.scrape.trending_topics(category=\"technology\")\n\n        # Analyze your best performing content\n        my_best = await x.analytics.top_performing_tweets(limit=20)\n        patterns = self._extract_patterns(my_best)\n\n        # Generate content matching your style + trends\n        for topic in trending[:3]:\n            # Generate tweet\n            tweet = await ai.generate_tweet(\n                topic=topic,\n                style=patterns[\"style\"],\n                hooks=patterns[\"hooks\"],\n                max_length=280\n            )\n\n            self.queue.append(ContentItem(\n                id=f\"tweet_{datetime.now().timestamp()}\",\n                content_type=\"tweet\",\n                text=tweet\n            ))\n\n        # Generate a thread\n        thread_topic = await ai.suggest_thread_topic(\n            based_on=my_best,\n            trending=trending\n        )\n\n        thread = await ai.generate_thread(\n            topic=thread_topic,\n            style=\"educational\",\n            length=8\n        )\n\n        self.queue.append(ContentItem(\n            id=f\"thread_{datetime.now().timestamp()}\",\n            content_type=\"thread\",\n            text=\"\\n---\\n\".join(thread.tweets)\n        ))\n\n        # Schedule content\n        await self._schedule_content(x)\n\n        print(f\"\u2705 Generated {len(self.queue)} new items\")\n\n    async def _schedule_content(self, x: Xeepy):\n        \"\"\"Schedule content for optimal posting times\"\"\"\n        best_times = await x.analytics.best_time_to_post()\n\n        unscheduled = [c for c in self.queue if c.status == \"draft\"]\n\n        for i, content in enumerate(unscheduled):\n            # Find next available slot\n            slot = best_times.get_next_slot(\n                after=datetime.now() + timedelta(hours=i * 4)\n            )\n\n            content.scheduled_time = slot\n            content.status = \"scheduled\"\n\n            print(f\"  Scheduled: {content.id} for {slot}\")\n\n    async def _publish_scheduled(self, x: Xeepy):\n        \"\"\"Publish content that's due\"\"\"\n        now = datetime.now()\n\n        for content in self.queue:\n            if content.status == \"scheduled\" and content.scheduled_time &lt;= now:\n                try:\n                    if content.content_type == \"tweet\":\n                        result = await x.post.tweet(content.text)\n                    elif content.content_type == \"thread\":\n                        tweets = content.text.split(\"\\n---\\n\")\n                        result = await x.post.thread(tweets)\n\n                    content.status = \"published\"\n                    content.published_url = result.url\n                    self.published.append(content)\n\n                    print(f\"\u2705 Published: {content.id}\")\n\n                except Exception as e:\n                    content.status = \"failed\"\n                    print(f\"\u274c Failed to publish {content.id}: {e}\")\n\n        # Remove published from queue\n        self.queue = [c for c in self.queue if c.status not in [\"published\", \"failed\"]]\n\n    async def _track_performance(self, x: Xeepy):\n        \"\"\"Track performance of published content\"\"\"\n        for content in self.published[-10:]:  # Last 10 items\n            if content.published_url:\n                stats = await x.scrape.tweet(content.published_url)\n                content.engagement_score = (\n                    stats.likes + stats.retweets * 2 + stats.replies * 3\n                )\n\n    def _extract_patterns(self, tweets):\n        \"\"\"Extract patterns from top performing tweets\"\"\"\n        # Simplified pattern extraction\n        return {\n            \"style\": \"educational\",\n            \"hooks\": [\"How to\", \"Here's why\", \"The truth about\"],\n            \"avg_length\": sum(len(t.text) for t in tweets) // len(tweets)\n        }\n\n    def _should_generate(self):\n        # Generate if queue is low\n        return len([c for c in self.queue if c.status == \"scheduled\"]) &lt; 5\n\n    def _should_track(self):\n        return datetime.now().minute == 0  # Top of each hour\n\n\n# Run the pipeline\nasyncio.run(ContentPipeline().run())\n</code></pre>"},{"location":"cookbook/automation/#monitoring-alerting-system","title":"Monitoring &amp; Alerting System","text":"<p>Real-time monitoring with intelligent alerts.</p> <pre><code>\"\"\"\nMonitoring &amp; Alerting System\n============================\nReal-time monitoring with smart notifications.\n\"\"\"\nimport asyncio\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom typing import List, Callable\nfrom enum import Enum\n\nfrom xeepy import Xeepy\nfrom xeepy.notifications import NotificationManager\n\n\nclass AlertLevel(Enum):\n    INFO = \"info\"\n    WARNING = \"warning\"\n    CRITICAL = \"critical\"\n\n\n@dataclass\nclass Alert:\n    level: AlertLevel\n    title: str\n    message: str\n    timestamp: datetime = None\n\n    def __post_init__(self):\n        self.timestamp = self.timestamp or datetime.now()\n\n\nclass MonitoringSystem:\n    \"\"\"\n    Comprehensive monitoring system:\n    - Real-time follower tracking\n    - Engagement anomaly detection\n    - Rate limit monitoring\n    - System health checks\n    \"\"\"\n\n    def __init__(self, config: dict):\n        self.config = config\n        self.alerts: List[Alert] = []\n        self.metrics_history = []\n        self.notifications = NotificationManager()\n\n    async def run(self):\n        async with Xeepy() as x:\n            while True:\n                # Collect metrics\n                metrics = await self._collect_metrics(x)\n                self.metrics_history.append(metrics)\n\n                # Check thresholds\n                await self._check_follower_alerts(metrics)\n                await self._check_engagement_alerts(metrics)\n                await self._check_rate_limits(x)\n                await self._check_system_health(x)\n\n                # Process alerts\n                await self._process_alerts()\n\n                # Cleanup old metrics\n                self._cleanup_history()\n\n                await asyncio.sleep(300)  # 5 minutes\n\n    async def _collect_metrics(self, x: Xeepy):\n        \"\"\"Collect all relevant metrics\"\"\"\n        profile = await x.scrape.profile(self.config[\"username\"])\n        engagement = await x.analytics.engagement_analysis(\"1h\")\n\n        return {\n            \"timestamp\": datetime.now(),\n            \"followers\": profile.followers_count,\n            \"following\": profile.following_count,\n            \"engagement_rate\": engagement.rate,\n            \"recent_likes\": engagement.total_likes,\n            \"recent_replies\": engagement.total_replies\n        }\n\n    async def _check_follower_alerts(self, metrics):\n        \"\"\"Check for unusual follower changes\"\"\"\n        if len(self.metrics_history) &lt; 2:\n            return\n\n        prev = self.metrics_history[-2]\n        change = metrics[\"followers\"] - prev[\"followers\"]\n\n        # Sudden drop alert\n        if change &lt; -self.config[\"thresholds\"][\"follower_drop\"]:\n            self.alerts.append(Alert(\n                level=AlertLevel.WARNING,\n                title=\"Sudden Follower Drop\",\n                message=f\"Lost {abs(change)} followers in last check period\"\n            ))\n\n        # Unusual gain (could be bot attack)\n        if change &gt; self.config[\"thresholds\"][\"follower_spike\"]:\n            self.alerts.append(Alert(\n                level=AlertLevel.INFO,\n                title=\"Follower Spike Detected\",\n                message=f\"Gained {change} followers in last check period\"\n            ))\n\n    async def _check_engagement_alerts(self, metrics):\n        \"\"\"Detect engagement anomalies\"\"\"\n        if len(self.metrics_history) &lt; 12:  # Need 1 hour of data\n            return\n\n        # Calculate average engagement\n        recent = self.metrics_history[-12:]\n        avg_rate = sum(m[\"engagement_rate\"] for m in recent) / len(recent)\n\n        # Alert if engagement dropped significantly\n        if metrics[\"engagement_rate\"] &lt; avg_rate * 0.5:\n            self.alerts.append(Alert(\n                level=AlertLevel.WARNING,\n                title=\"Engagement Drop\",\n                message=f\"Engagement rate dropped to {metrics['engagement_rate']:.2%} (avg: {avg_rate:.2%})\"\n            ))\n\n    async def _check_rate_limits(self, x: Xeepy):\n        \"\"\"Monitor rate limit usage\"\"\"\n        limits = await x.get_rate_limit_status()\n\n        for endpoint, status in limits.items():\n            usage_pct = status[\"used\"] / status[\"limit\"]\n\n            if usage_pct &gt; 0.9:\n                self.alerts.append(Alert(\n                    level=AlertLevel.WARNING,\n                    title=\"Rate Limit Warning\",\n                    message=f\"{endpoint}: {usage_pct:.0%} of limit used\"\n                ))\n\n    async def _check_system_health(self, x: Xeepy):\n        \"\"\"Check system health\"\"\"\n        try:\n            # Test basic functionality\n            await x.auth.is_authenticated()\n        except Exception as e:\n            self.alerts.append(Alert(\n                level=AlertLevel.CRITICAL,\n                title=\"System Health Check Failed\",\n                message=str(e)\n            ))\n\n    async def _process_alerts(self):\n        \"\"\"Send alert notifications\"\"\"\n        for alert in self.alerts:\n            if alert.level == AlertLevel.CRITICAL:\n                await self.notifications.broadcast(\n                    f\"\ud83d\udea8 {alert.title}\\n{alert.message}\"\n                )\n            elif alert.level == AlertLevel.WARNING:\n                await self.notifications.send(\n                    f\"\u26a0\ufe0f {alert.title}\\n{alert.message}\",\n                    channels=[\"discord\"]\n                )\n            else:\n                # Just log info alerts\n                print(f\"\u2139\ufe0f {alert.title}: {alert.message}\")\n\n        self.alerts.clear()\n\n    def _cleanup_history(self):\n        \"\"\"Keep only last 24 hours of metrics\"\"\"\n        cutoff = datetime.now() - timedelta(hours=24)\n        self.metrics_history = [\n            m for m in self.metrics_history \n            if m[\"timestamp\"] &gt; cutoff\n        ]\n\n\n# Configuration\nconfig = {\n    \"username\": \"your_username\",\n    \"thresholds\": {\n        \"follower_drop\": 50,     # Alert if lose 50+ followers\n        \"follower_spike\": 500,   # Alert if gain 500+ followers\n    }\n}\n\n# Run monitoring\nasyncio.run(MonitoringSystem(config).run())\n</code></pre>"},{"location":"cookbook/automation/#docker-deployment","title":"Docker Deployment","text":"<p>Deploy your automation with Docker.</p> <pre><code># Dockerfile\nFROM mcr.microsoft.com/playwright/python:v1.40.0-jammy\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\n# Copy automation code\nCOPY . .\n\n# Run automation\nCMD [\"python\", \"automation_stack.py\"]\n</code></pre> <pre><code># docker-compose.yml\nversion: '3.8'\n\nservices:\n  xeepy-automation:\n    build: .\n    restart: unless-stopped\n    environment:\n      - DISCORD_WEBHOOK=${DISCORD_WEBHOOK}\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n    volumes:\n      - ./data:/app/data          # Persist data\n      - ./session.json:/app/session.json  # Auth session\n      - ./whitelist.txt:/app/whitelist.txt\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n</code></pre> <pre><code># Deploy\ndocker-compose up -d\n\n# View logs\ndocker-compose logs -f\n\n# Stop\ndocker-compose down\n</code></pre>"},{"location":"cookbook/automation/#next-steps","title":"Next Steps","text":"<ul> <li> <p>Data Science Recipes</p> <p>Analyze your automation data</p> </li> <li> <p>Business Intelligence</p> <p>Turn automation into business value</p> </li> </ul>"},{"location":"cookbook/automation/content-calendar/","title":"AI-Powered Content Calendar","text":"<p>Build an intelligent content calendar that uses AI to generate topics, identify trends, and maintain a consistent content strategy.</p>"},{"location":"cookbook/automation/content-calendar/#overview","title":"Overview","text":"<p>This recipe creates an AI-powered content calendar with:</p> <ul> <li>Content pillars - Organize by themes</li> <li>AI topic generation - Generate ideas per pillar</li> <li>Trend integration - Incorporate trending topics</li> <li>Gap analysis - Find missing content areas</li> <li>Performance feedback - Learn from results</li> <li>30-day generator - Automated calendar creation</li> </ul>"},{"location":"cookbook/automation/content-calendar/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Content        \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  AI Topic    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Calendar       \u2502\n\u2502  Pillars        \u2502     \u2502  Generator   \u2502     \u2502  Builder        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                       \u2502                     \u2502\n        \u25bc                       \u25bc                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Trend          \u2502     \u2502  Gap         \u2502     \u2502  Performance    \u2502\n\u2502  Analyzer       \u2502     \u2502  Analyzer    \u2502     \u2502  Tracker        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cookbook/automation/content-calendar/#data-models","title":"Data Models","text":"<pre><code># calendar_models.py\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, date\nfrom typing import Optional\nfrom enum import Enum\n\nclass ContentPillar(Enum):\n    EDUCATIONAL = \"educational\"\n    PROMOTIONAL = \"promotional\"\n    ENGAGEMENT = \"engagement\"\n    PERSONAL = \"personal\"\n    CURATED = \"curated\"\n    TRENDING = \"trending\"\n\n@dataclass\nclass ContentIdea:\n    id: str\n    title: str\n    pillar: ContentPillar\n    description: str\n    keywords: list[str]\n    suggested_format: str  # tweet, thread, poll, image\n    estimated_engagement: float\n    trending_score: float = 0.0\n    generated_at: datetime = field(default_factory=datetime.now)\n\n    # Draft content\n    draft_content: Optional[str] = None\n\n    # Scheduling\n    scheduled_date: Optional[date] = None\n    published: bool = False\n\n@dataclass\nclass CalendarDay:\n    date: date\n    posts: list[ContentIdea] = field(default_factory=list)\n    pillar_distribution: dict[ContentPillar, int] = field(default_factory=dict)\n    is_holiday: bool = False\n    holiday_name: Optional[str] = None\n    notes: str = \"\"\n\n@dataclass\nclass ContentCalendar:\n    name: str\n    start_date: date\n    end_date: date\n    days: list[CalendarDay] = field(default_factory=list)\n    pillar_ratios: dict[ContentPillar, float] = field(default_factory=dict)\n    posts_per_day: int = 3\n</code></pre>"},{"location":"cookbook/automation/content-calendar/#content-pillar-framework","title":"Content Pillar Framework","text":"<pre><code># pillar_framework.py\nfrom dataclasses import dataclass\nfrom calendar_models import ContentPillar\n\n@dataclass\nclass PillarDefinition:\n    pillar: ContentPillar\n    name: str\n    description: str\n    example_topics: list[str]\n    target_ratio: float  # 0-1, portion of content\n    best_formats: list[str]\n    best_days: list[int]  # 0=Monday\n    engagement_multiplier: float = 1.0\n\nclass PillarFramework:\n    \"\"\"Define and manage content pillars.\"\"\"\n\n    DEFAULT_PILLARS = [\n        PillarDefinition(\n            pillar=ContentPillar.EDUCATIONAL,\n            name=\"Educational\",\n            description=\"Teach your audience something valuable\",\n            example_topics=[\n                \"How-to guides\",\n                \"Tips and tricks\",\n                \"Industry insights\",\n                \"Tutorials\"\n            ],\n            target_ratio=0.35,\n            best_formats=[\"thread\", \"image\"],\n            best_days=[1, 2, 3],  # Tue, Wed, Thu\n            engagement_multiplier=1.2\n        ),\n        PillarDefinition(\n            pillar=ContentPillar.PROMOTIONAL,\n            name=\"Promotional\",\n            description=\"Promote your products or services\",\n            example_topics=[\n                \"Product updates\",\n                \"Case studies\",\n                \"Testimonials\",\n                \"Launches\"\n            ],\n            target_ratio=0.15,\n            best_formats=[\"tweet\", \"image\"],\n            best_days=[2, 4],  # Wed, Fri\n            engagement_multiplier=0.8\n        ),\n        PillarDefinition(\n            pillar=ContentPillar.ENGAGEMENT,\n            name=\"Engagement\",\n            description=\"Spark conversation and interaction\",\n            example_topics=[\n                \"Questions\",\n                \"Polls\",\n                \"Hot takes\",\n                \"Debates\"\n            ],\n            target_ratio=0.25,\n            best_formats=[\"poll\", \"tweet\"],\n            best_days=[0, 4],  # Mon, Fri\n            engagement_multiplier=1.5\n        ),\n        PillarDefinition(\n            pillar=ContentPillar.PERSONAL,\n            name=\"Personal\",\n            description=\"Share personal stories and behind-the-scenes\",\n            example_topics=[\n                \"Journey updates\",\n                \"Lessons learned\",\n                \"Behind the scenes\",\n                \"Celebrations\"\n            ],\n            target_ratio=0.15,\n            best_formats=[\"tweet\", \"thread\"],\n            best_days=[0, 6],  # Mon, Sun\n            engagement_multiplier=1.3\n        ),\n        PillarDefinition(\n            pillar=ContentPillar.CURATED,\n            name=\"Curated\",\n            description=\"Share valuable content from others\",\n            example_topics=[\n                \"Industry news\",\n                \"Interesting threads\",\n                \"Tool recommendations\",\n                \"Resource roundups\"\n            ],\n            target_ratio=0.10,\n            best_formats=[\"tweet\"],\n            best_days=[1, 3, 5],  # Tue, Thu, Sat\n            engagement_multiplier=0.9\n        ),\n    ]\n\n    def __init__(self, custom_pillars: list[PillarDefinition] = None):\n        self.pillars = custom_pillars or self.DEFAULT_PILLARS\n        self.pillar_map = {p.pillar: p for p in self.pillars}\n\n    def get_pillar(self, pillar: ContentPillar) -&gt; PillarDefinition:\n        return self.pillar_map.get(pillar)\n\n    def get_ratios(self) -&gt; dict[ContentPillar, float]:\n        return {p.pillar: p.target_ratio for p in self.pillars}\n\n    def suggest_pillar_for_day(self, day_of_week: int) -&gt; ContentPillar:\n        \"\"\"Suggest best pillar for a day of week.\"\"\"\n\n        candidates = [\n            p for p in self.pillars\n            if day_of_week in p.best_days\n        ]\n\n        if not candidates:\n            # Default to educational\n            return ContentPillar.EDUCATIONAL\n\n        # Return highest engagement multiplier\n        return max(candidates, key=lambda p: p.engagement_multiplier).pillar\n</code></pre>"},{"location":"cookbook/automation/content-calendar/#ai-topic-generator","title":"AI Topic Generator","text":"<pre><code># topic_generator.py\nimport os\nfrom typing import Optional\nimport json\n\nfrom calendar_models import ContentIdea, ContentPillar\nfrom pillar_framework import PillarFramework\n\nclass AITopicGenerator:\n    \"\"\"Generate content topics using AI.\"\"\"\n\n    def __init__(\n        self,\n        provider: str = \"openai\",\n        api_key: str = None,\n        model: str = \"gpt-4\"\n    ):\n        self.provider = provider\n        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n        self.model = model\n        self.framework = PillarFramework()\n\n    async def generate_topics(\n        self,\n        pillar: ContentPillar,\n        niche: str,\n        count: int = 5,\n        recent_topics: list[str] = None\n    ) -&gt; list[ContentIdea]:\n        \"\"\"Generate topic ideas for a pillar.\"\"\"\n\n        pillar_def = self.framework.get_pillar(pillar)\n\n        prompt = f\"\"\"Generate {count} unique content ideas for Twitter/X.\n\nNiche: {niche}\nContent Pillar: {pillar_def.name}\nPillar Description: {pillar_def.description}\nExample Topics: {', '.join(pillar_def.example_topics)}\nBest Formats: {', '.join(pillar_def.best_formats)}\n\n{\"Avoid these recent topics: \" + ', '.join(recent_topics) if recent_topics else \"\"}\n\nFor each idea, provide:\n1. Title (catchy, specific)\n2. Description (2-3 sentences)\n3. Keywords (3-5 relevant keywords)\n4. Suggested format (tweet/thread/poll/image)\n5. Estimated engagement (1-10 scale)\n\nReturn as JSON array with fields: title, description, keywords, format, engagement\"\"\"\n\n        # Call AI API\n        response = await self._call_ai(prompt)\n\n        # Parse response\n        ideas = self._parse_response(response, pillar)\n\n        return ideas\n\n    async def generate_draft_content(\n        self,\n        idea: ContentIdea,\n        style: str = \"professional\",\n        max_length: int = 280\n    ) -&gt; str:\n        \"\"\"Generate draft tweet content for an idea.\"\"\"\n\n        prompt = f\"\"\"Write a Twitter/X post for this content idea:\n\nTitle: {idea.title}\nDescription: {idea.description}\nKeywords: {', '.join(idea.keywords)}\nFormat: {idea.suggested_format}\nStyle: {style}\nMax Length: {max_length} characters\n\nRequirements:\n- Be engaging and hook the reader\n- Include relevant emoji\n- End with a call to action or question\n- Stay within character limit\n\n{\"If thread format, write the first tweet that hooks readers.\" if idea.suggested_format == 'thread' else \"\"}\n{\"If poll format, include 2-4 poll options.\" if idea.suggested_format == 'poll' else \"\"}\n\nReturn just the tweet text, no explanation.\"\"\"\n\n        response = await self._call_ai(prompt)\n        return response.strip()\n\n    async def _call_ai(self, prompt: str) -&gt; str:\n        \"\"\"Call AI API.\"\"\"\n\n        if self.provider == \"openai\":\n            import openai\n\n            client = openai.AsyncOpenAI(api_key=self.api_key)\n\n            response = await client.chat.completions.create(\n                model=self.model,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.8\n            )\n\n            return response.choices[0].message.content\n\n        else:\n            raise ValueError(f\"Unknown provider: {self.provider}\")\n\n    def _parse_response(\n        self,\n        response: str,\n        pillar: ContentPillar\n    ) -&gt; list[ContentIdea]:\n        \"\"\"Parse AI response into ContentIdea objects.\"\"\"\n\n        ideas = []\n\n        try:\n            # Try to parse as JSON\n            data = json.loads(response)\n\n            for i, item in enumerate(data):\n                idea = ContentIdea(\n                    id=f\"idea_{pillar.value}_{i}\",\n                    title=item.get('title', ''),\n                    pillar=pillar,\n                    description=item.get('description', ''),\n                    keywords=item.get('keywords', []),\n                    suggested_format=item.get('format', 'tweet'),\n                    estimated_engagement=float(item.get('engagement', 5))\n                )\n                ideas.append(idea)\n\n        except json.JSONDecodeError:\n            # Fallback: create single idea from response\n            ideas.append(ContentIdea(\n                id=f\"idea_{pillar.value}_0\",\n                title=\"Generated Idea\",\n                pillar=pillar,\n                description=response[:200],\n                keywords=[],\n                suggested_format='tweet',\n                estimated_engagement=5.0\n            ))\n\n        return ideas\n</code></pre>"},{"location":"cookbook/automation/content-calendar/#trend-analyzer","title":"Trend Analyzer","text":"<pre><code># trend_analyzer.py\nfrom datetime import datetime\nfrom typing import Optional\n\nfrom xeepy import Xeepy\n\nclass TrendAnalyzer:\n    \"\"\"Analyze trending topics for content opportunities.\"\"\"\n\n    def __init__(self):\n        self.cached_trends: list[dict] = []\n        self.last_fetch: Optional[datetime] = None\n\n    async def fetch_trends(\n        self,\n        location: str = \"Worldwide\"\n    ) -&gt; list[dict]:\n        \"\"\"Fetch current trending topics.\"\"\"\n\n        async with Xeepy() as x:\n            trends = await x.trends(location=location)\n\n            self.cached_trends = [\n                {\n                    'name': t.name,\n                    'tweet_count': t.tweet_count,\n                    'url': t.url\n                }\n                for t in trends.items\n            ]\n\n            self.last_fetch = datetime.now()\n\n            return self.cached_trends\n\n    def find_relevant_trends(\n        self,\n        niche_keywords: list[str],\n        min_relevance: float = 0.3\n    ) -&gt; list[dict]:\n        \"\"\"Find trends relevant to your niche.\"\"\"\n\n        relevant = []\n\n        for trend in self.cached_trends:\n            trend_lower = trend['name'].lower()\n\n            # Check keyword match\n            relevance = 0.0\n            matched_keywords = []\n\n            for keyword in niche_keywords:\n                if keyword.lower() in trend_lower:\n                    relevance += 0.5\n                    matched_keywords.append(keyword)\n\n            if relevance &gt;= min_relevance:\n                relevant.append({\n                    **trend,\n                    'relevance': relevance,\n                    'matched_keywords': matched_keywords\n                })\n\n        return sorted(relevant, key=lambda t: t['relevance'], reverse=True)\n\n    def get_trending_content_ideas(\n        self,\n        niche_keywords: list[str]\n    ) -&gt; list[dict]:\n        \"\"\"Generate content ideas from trends.\"\"\"\n\n        relevant_trends = self.find_relevant_trends(niche_keywords)\n\n        ideas = []\n        for trend in relevant_trends[:5]:\n            ideas.append({\n                'trend': trend['name'],\n                'suggestion': f\"Create content about {trend['name']} from your perspective\",\n                'potential_reach': trend['tweet_count'],\n                'urgency': 'high'  # Trends are time-sensitive\n            })\n\n        return ideas\n</code></pre>"},{"location":"cookbook/automation/content-calendar/#gap-analyzer","title":"Gap Analyzer","text":"<pre><code># gap_analyzer.py\nfrom collections import Counter\nfrom datetime import datetime, timedelta\n\nfrom calendar_models import ContentPillar, ContentIdea\n\nclass GapAnalyzer:\n    \"\"\"Analyze content gaps and opportunities.\"\"\"\n\n    def __init__(self, framework: 'PillarFramework'):\n        self.framework = framework\n\n    def analyze_pillar_gaps(\n        self,\n        published_content: list[ContentIdea],\n        days: int = 30\n    ) -&gt; dict[ContentPillar, float]:\n        \"\"\"Find pillars that are underrepresented.\"\"\"\n\n        # Count published by pillar\n        pillar_counts = Counter()\n        for content in published_content:\n            if content.published:\n                pillar_counts[content.pillar] += 1\n\n        total = sum(pillar_counts.values()) or 1\n\n        # Calculate gaps\n        target_ratios = self.framework.get_ratios()\n        gaps = {}\n\n        for pillar, target in target_ratios.items():\n            actual = pillar_counts.get(pillar, 0) / total\n            gaps[pillar] = target - actual\n\n        return gaps\n\n    def analyze_keyword_gaps(\n        self,\n        published_content: list[ContentIdea],\n        target_keywords: list[str]\n    ) -&gt; list[str]:\n        \"\"\"Find keywords not covered recently.\"\"\"\n\n        # Get all used keywords\n        used_keywords = set()\n        for content in published_content:\n            if content.published:\n                used_keywords.update(kw.lower() for kw in content.keywords)\n\n        # Find gaps\n        gaps = [\n            kw for kw in target_keywords\n            if kw.lower() not in used_keywords\n        ]\n\n        return gaps\n\n    def analyze_format_gaps(\n        self,\n        published_content: list[ContentIdea]\n    ) -&gt; dict[str, float]:\n        \"\"\"Find content formats that are underused.\"\"\"\n\n        format_counts = Counter()\n        for content in published_content:\n            if content.published:\n                format_counts[content.suggested_format] += 1\n\n        total = sum(format_counts.values()) or 1\n\n        # Target ratios\n        target_formats = {\n            'tweet': 0.5,\n            'thread': 0.25,\n            'poll': 0.15,\n            'image': 0.10\n        }\n\n        gaps = {}\n        for fmt, target in target_formats.items():\n            actual = format_counts.get(fmt, 0) / total\n            gaps[fmt] = target - actual\n\n        return gaps\n\n    def get_recommendations(\n        self,\n        published_content: list[ContentIdea],\n        target_keywords: list[str]\n    ) -&gt; list[str]:\n        \"\"\"Get content recommendations based on gaps.\"\"\"\n\n        recommendations = []\n\n        # Pillar gaps\n        pillar_gaps = self.analyze_pillar_gaps(published_content)\n        top_pillar_gap = max(pillar_gaps.items(), key=lambda x: x[1])\n\n        if top_pillar_gap[1] &gt; 0.1:\n            pillar_def = self.framework.get_pillar(top_pillar_gap[0])\n            recommendations.append(\n                f\"Create more {pillar_def.name} content (+{top_pillar_gap[1]*100:.0f}% needed)\"\n            )\n\n        # Keyword gaps\n        keyword_gaps = self.analyze_keyword_gaps(published_content, target_keywords)\n        if keyword_gaps:\n            recommendations.append(\n                f\"Cover these keywords: {', '.join(keyword_gaps[:5])}\"\n            )\n\n        # Format gaps\n        format_gaps = self.analyze_format_gaps(published_content)\n        top_format_gap = max(format_gaps.items(), key=lambda x: x[1])\n\n        if top_format_gap[1] &gt; 0.1:\n            recommendations.append(\n                f\"Create more {top_format_gap[0]} content\"\n            )\n\n        return recommendations\n</code></pre>"},{"location":"cookbook/automation/content-calendar/#calendar-builder","title":"Calendar Builder","text":"<pre><code># calendar_builder.py\nfrom datetime import date, timedelta\nfrom typing import Optional\n\nfrom calendar_models import ContentCalendar, CalendarDay, ContentIdea, ContentPillar\nfrom pillar_framework import PillarFramework\nfrom topic_generator import AITopicGenerator\nfrom trend_analyzer import TrendAnalyzer\nfrom gap_analyzer import GapAnalyzer\n\nclass CalendarBuilder:\n    \"\"\"Build AI-powered content calendars.\"\"\"\n\n    def __init__(\n        self,\n        niche: str,\n        api_key: str = None,\n        posts_per_day: int = 3\n    ):\n        self.niche = niche\n        self.posts_per_day = posts_per_day\n        self.framework = PillarFramework()\n        self.generator = AITopicGenerator(api_key=api_key)\n        self.trend_analyzer = TrendAnalyzer()\n        self.gap_analyzer = GapAnalyzer(self.framework)\n\n    async def build_calendar(\n        self,\n        days: int = 30,\n        start_date: date = None\n    ) -&gt; ContentCalendar:\n        \"\"\"Build a content calendar for N days.\"\"\"\n\n        if start_date is None:\n            start_date = date.today()\n\n        end_date = start_date + timedelta(days=days - 1)\n\n        calendar = ContentCalendar(\n            name=f\"{self.niche} Calendar\",\n            start_date=start_date,\n            end_date=end_date,\n            pillar_ratios=self.framework.get_ratios(),\n            posts_per_day=self.posts_per_day\n        )\n\n        # Generate ideas for each pillar\n        all_ideas: dict[ContentPillar, list[ContentIdea]] = {}\n\n        for pillar in ContentPillar:\n            if pillar == ContentPillar.TRENDING:\n                continue  # Handle separately\n\n            ideas_needed = int(days * self.posts_per_day * \n                             self.framework.get_ratios().get(pillar, 0.1))\n\n            ideas = await self.generator.generate_topics(\n                pillar=pillar,\n                niche=self.niche,\n                count=max(5, ideas_needed)\n            )\n\n            all_ideas[pillar] = ideas\n\n        # Build each day\n        for day_offset in range(days):\n            current_date = start_date + timedelta(days=day_offset)\n\n            day = CalendarDay(\n                date=current_date,\n                posts=[],\n                pillar_distribution={}\n            )\n\n            # Select posts for this day\n            day_of_week = current_date.weekday()\n\n            for slot in range(self.posts_per_day):\n                # Determine pillar for this slot\n                pillar = self._select_pillar_for_slot(\n                    day_of_week, slot, day.pillar_distribution\n                )\n\n                # Get idea from pool\n                if pillar in all_ideas and all_ideas[pillar]:\n                    idea = all_ideas[pillar].pop(0)\n                    idea.scheduled_date = current_date\n                    day.posts.append(idea)\n\n                    # Track distribution\n                    day.pillar_distribution[pillar] = \\\n                        day.pillar_distribution.get(pillar, 0) + 1\n\n            calendar.days.append(day)\n\n        return calendar\n\n    def _select_pillar_for_slot(\n        self,\n        day_of_week: int,\n        slot: int,\n        current_distribution: dict[ContentPillar, int]\n    ) -&gt; ContentPillar:\n        \"\"\"Select pillar for a slot based on rules.\"\"\"\n\n        # First slot: engagement or trending\n        if slot == 0:\n            if day_of_week in [0, 4]:  # Mon, Fri\n                return ContentPillar.ENGAGEMENT\n            return ContentPillar.EDUCATIONAL\n\n        # Last slot: personal or curated\n        if slot == self.posts_per_day - 1:\n            if day_of_week in [0, 6]:  # Mon, Sun\n                return ContentPillar.PERSONAL\n            return ContentPillar.CURATED\n\n        # Middle slots: based on day and balance\n        return self.framework.suggest_pillar_for_day(day_of_week)\n\n    async def generate_drafts(\n        self,\n        calendar: ContentCalendar,\n        style: str = \"professional\"\n    ) -&gt; ContentCalendar:\n        \"\"\"Generate draft content for all ideas.\"\"\"\n\n        for day in calendar.days:\n            for idea in day.posts:\n                if idea.draft_content is None:\n                    idea.draft_content = await self.generator.generate_draft_content(\n                        idea, style=style\n                    )\n\n        return calendar\n\n    def export_calendar(\n        self,\n        calendar: ContentCalendar,\n        format: str = \"markdown\"\n    ) -&gt; str:\n        \"\"\"Export calendar to readable format.\"\"\"\n\n        if format == \"markdown\":\n            return self._export_markdown(calendar)\n        elif format == \"csv\":\n            return self._export_csv(calendar)\n        else:\n            raise ValueError(f\"Unknown format: {format}\")\n\n    def _export_markdown(self, calendar: ContentCalendar) -&gt; str:\n        \"\"\"Export as markdown.\"\"\"\n\n        days_of_week = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n\n        md = f\"# {calendar.name}\\n\\n\"\n        md += f\"**Period:** {calendar.start_date} to {calendar.end_date}\\n\"\n        md += f\"**Posts per day:** {calendar.posts_per_day}\\n\\n\"\n\n        for day in calendar.days:\n            dow = days_of_week[day.date.weekday()]\n            md += f\"## {dow}, {day.date}\\n\\n\"\n\n            for i, post in enumerate(day.posts, 1):\n                pillar_emoji = {\n                    ContentPillar.EDUCATIONAL: \"\ud83d\udcda\",\n                    ContentPillar.PROMOTIONAL: \"\ud83d\udce2\",\n                    ContentPillar.ENGAGEMENT: \"\ud83d\udcac\",\n                    ContentPillar.PERSONAL: \"\ud83d\udc64\",\n                    ContentPillar.CURATED: \"\ud83d\udd17\",\n                    ContentPillar.TRENDING: \"\ud83d\udd25\"\n                }.get(post.pillar, \"\ud83d\udcdd\")\n\n                md += f\"### {i}. {pillar_emoji} {post.title}\\n\"\n                md += f\"**Pillar:** {post.pillar.value} | \"\n                md += f\"**Format:** {post.suggested_format}\\n\\n\"\n\n                if post.draft_content:\n                    md += f\"&gt; {post.draft_content}\\n\\n\"\n                else:\n                    md += f\"*{post.description}*\\n\\n\"\n\n                md += f\"Keywords: {', '.join(post.keywords)}\\n\\n\"\n\n            md += \"---\\n\\n\"\n\n        return md\n</code></pre>"},{"location":"cookbook/automation/content-calendar/#usage-example","title":"Usage Example","text":"<pre><code># main.py\nimport asyncio\nimport os\nfrom datetime import date\n\nfrom calendar_builder import CalendarBuilder\n\nasync def main():\n    # Initialize\n    builder = CalendarBuilder(\n        niche=\"Python programming and software development\",\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        posts_per_day=3\n    )\n\n    # Build 30-day calendar\n    print(\"Building content calendar...\")\n    calendar = await builder.build_calendar(days=30)\n\n    print(f\"Generated calendar with {len(calendar.days)} days\")\n\n    # Generate draft content\n    print(\"Generating draft content...\")\n    calendar = await builder.generate_drafts(calendar, style=\"casual\")\n\n    # Export\n    markdown = builder.export_calendar(calendar, format=\"markdown\")\n\n    with open(\"content_calendar.md\", \"w\") as f:\n        f.write(markdown)\n\n    print(\"Calendar saved to content_calendar.md\")\n\n    # Print preview\n    print(\"\\n\ud83d\udcc5 First Week Preview:\\n\")\n    for day in calendar.days[:7]:\n        print(f\"\\n{day.date} ({day.date.strftime('%A')})\")\n        for post in day.posts:\n            print(f\"  \u2022 [{post.pillar.value}] {post.title}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"cookbook/automation/content-calendar/#best-practices","title":"Best Practices","text":"<p>Content Balance</p> <ul> <li>Follow 80/20 rule: 80% value, 20% promotional</li> <li>Mix formats throughout the week</li> <li>Save high-engagement content for peak days</li> </ul> <p>AI Content</p> <ul> <li>Always review and personalize AI drafts</li> <li>Add your unique voice and insights</li> <li>Fact-check any generated claims</li> </ul>"},{"location":"cookbook/automation/content-calendar/#related-recipes","title":"Related Recipes","text":"<ul> <li>Scheduled Posts - Automation</li> <li>Optimal Timing - When to post</li> <li>Hashtag Strategy - Reach optimization</li> </ul>"},{"location":"cookbook/automation/scheduled-posts/","title":"Advanced Content Scheduling System","text":"<p>Build a sophisticated content scheduling system with multi-queue management, timezone awareness, and intelligent resharing.</p>"},{"location":"cookbook/automation/scheduled-posts/#overview","title":"Overview","text":"<p>This recipe creates an advanced scheduling system with:</p> <ul> <li>Multi-queue management - Organize content by type</li> <li>Timezone awareness - Post at optimal times globally</li> <li>Evergreen rotation - Auto-reshare top content</li> <li>Calendar integration - Sync with external calendars</li> <li>Holiday awareness - Adjust for events</li> <li>Multi-account support - Coordinate across accounts</li> </ul>"},{"location":"cookbook/automation/scheduled-posts/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Content        \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Queue       \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Scheduler      \u2502\n\u2502  Queue          \u2502     \u2502  Manager     \u2502     \u2502  Engine         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                       \u2502                     \u2502\n        \u25bc                       \u25bc                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Evergreen      \u2502     \u2502  Holiday     \u2502     \u2502  Publisher      \u2502\n\u2502  Rotator        \u2502     \u2502  Detector    \u2502     \u2502  (Xeepy)        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cookbook/automation/scheduled-posts/#data-models","title":"Data Models","text":"<pre><code># scheduler_models.py\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Optional\nfrom enum import Enum\nimport uuid\n\nclass ContentType(Enum):\n    REGULAR = \"regular\"\n    THREAD = \"thread\"\n    POLL = \"poll\"\n    EVERGREEN = \"evergreen\"\n    PROMOTIONAL = \"promotional\"\n    ENGAGEMENT = \"engagement\"\n\nclass PostStatus(Enum):\n    DRAFT = \"draft\"\n    SCHEDULED = \"scheduled\"\n    PUBLISHED = \"published\"\n    FAILED = \"failed\"\n\n@dataclass\nclass ScheduledPost:\n    id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])\n    content: str = \"\"\n    content_type: ContentType = ContentType.REGULAR\n    media_paths: list[str] = field(default_factory=list)\n\n    # Scheduling\n    scheduled_time: Optional[datetime] = None\n    timezone: str = \"UTC\"\n\n    # Thread support\n    is_thread: bool = False\n    thread_tweets: list[str] = field(default_factory=list)\n\n    # Poll support\n    poll_options: list[str] = field(default_factory=list)\n    poll_duration_hours: int = 24\n\n    # Targeting\n    target_accounts: list[str] = field(default_factory=list)\n\n    # Metadata\n    status: PostStatus = PostStatus.DRAFT\n    created_at: datetime = field(default_factory=datetime.now)\n    published_at: Optional[datetime] = None\n    tweet_id: Optional[str] = None\n\n    # Evergreen settings\n    is_evergreen: bool = False\n    min_days_between_posts: int = 30\n    last_posted: Optional[datetime] = None\n    times_posted: int = 0\n\n    # Performance\n    engagement_score: float = 0.0\n\n@dataclass\nclass ContentQueue:\n    name: str\n    content_types: list[ContentType]\n    posts_per_day: int = 3\n    optimal_hours: list[int] = field(default_factory=lambda: [9, 12, 17])\n    timezone: str = \"UTC\"\n    is_active: bool = True\n</code></pre>"},{"location":"cookbook/automation/scheduled-posts/#queue-manager","title":"Queue Manager","text":"<pre><code># queue_manager.py\nfrom datetime import datetime, timedelta\nfrom typing import Optional\nimport json\n\nfrom scheduler_models import ScheduledPost, ContentQueue, ContentType, PostStatus\n\nclass QueueManager:\n    \"\"\"Manage multiple content queues.\"\"\"\n\n    def __init__(self):\n        self.queues: dict[str, ContentQueue] = {}\n        self.posts: dict[str, ScheduledPost] = {}\n\n    def create_queue(\n        self,\n        name: str,\n        content_types: list[ContentType],\n        posts_per_day: int = 3,\n        optimal_hours: list[int] = None,\n        timezone: str = \"UTC\"\n    ) -&gt; ContentQueue:\n        \"\"\"Create a new content queue.\"\"\"\n\n        queue = ContentQueue(\n            name=name,\n            content_types=content_types,\n            posts_per_day=posts_per_day,\n            optimal_hours=optimal_hours or [9, 12, 17],\n            timezone=timezone\n        )\n\n        self.queues[name] = queue\n        return queue\n\n    def add_post(\n        self,\n        queue_name: str,\n        post: ScheduledPost\n    ) -&gt; ScheduledPost:\n        \"\"\"Add post to queue.\"\"\"\n\n        if queue_name not in self.queues:\n            raise ValueError(f\"Queue '{queue_name}' not found\")\n\n        # Assign to next available slot if not scheduled\n        if post.scheduled_time is None:\n            post.scheduled_time = self._get_next_slot(queue_name)\n\n        post.status = PostStatus.SCHEDULED\n        self.posts[post.id] = post\n\n        return post\n\n    def _get_next_slot(self, queue_name: str) -&gt; datetime:\n        \"\"\"Get next available time slot for queue.\"\"\"\n\n        queue = self.queues[queue_name]\n\n        # Get existing scheduled times for this queue\n        scheduled_times = [\n            p.scheduled_time for p in self.posts.values()\n            if p.status == PostStatus.SCHEDULED\n        ]\n\n        # Start from now\n        candidate = datetime.now().replace(minute=0, second=0, microsecond=0)\n\n        # Find next optimal hour\n        for _ in range(7 * 24):  # Search up to 1 week\n            if candidate.hour in queue.optimal_hours:\n                if candidate not in scheduled_times and candidate &gt; datetime.now():\n                    return candidate\n\n            candidate += timedelta(hours=1)\n\n        return candidate\n\n    def get_pending_posts(\n        self,\n        queue_name: str = None,\n        hours_ahead: int = 24\n    ) -&gt; list[ScheduledPost]:\n        \"\"\"Get posts scheduled in the next N hours.\"\"\"\n\n        cutoff = datetime.now() + timedelta(hours=hours_ahead)\n\n        pending = [\n            p for p in self.posts.values()\n            if p.status == PostStatus.SCHEDULED\n            and p.scheduled_time\n            and p.scheduled_time &lt;= cutoff\n        ]\n\n        if queue_name:\n            queue = self.queues.get(queue_name)\n            if queue:\n                pending = [\n                    p for p in pending\n                    if p.content_type in queue.content_types\n                ]\n\n        return sorted(pending, key=lambda p: p.scheduled_time)\n\n    def reschedule_post(\n        self,\n        post_id: str,\n        new_time: datetime\n    ):\n        \"\"\"Reschedule a post.\"\"\"\n\n        if post_id not in self.posts:\n            raise ValueError(f\"Post '{post_id}' not found\")\n\n        post = self.posts[post_id]\n        post.scheduled_time = new_time\n\n    def mark_published(\n        self,\n        post_id: str,\n        tweet_id: str\n    ):\n        \"\"\"Mark post as published.\"\"\"\n\n        if post_id in self.posts:\n            post = self.posts[post_id]\n            post.status = PostStatus.PUBLISHED\n            post.published_at = datetime.now()\n            post.tweet_id = tweet_id\n\n    def save(self, filepath: str):\n        \"\"\"Save queues and posts to JSON.\"\"\"\n\n        data = {\n            'queues': {\n                name: {\n                    'name': q.name,\n                    'content_types': [ct.value for ct in q.content_types],\n                    'posts_per_day': q.posts_per_day,\n                    'optimal_hours': q.optimal_hours,\n                    'timezone': q.timezone,\n                    'is_active': q.is_active\n                }\n                for name, q in self.queues.items()\n            },\n            'posts': {\n                pid: {\n                    'id': p.id,\n                    'content': p.content,\n                    'content_type': p.content_type.value,\n                    'scheduled_time': p.scheduled_time.isoformat() if p.scheduled_time else None,\n                    'status': p.status.value,\n                    'is_evergreen': p.is_evergreen,\n                    'times_posted': p.times_posted\n                }\n                for pid, p in self.posts.items()\n            }\n        }\n\n        with open(filepath, 'w') as f:\n            json.dump(data, f, indent=2)\n\n    def load(self, filepath: str):\n        \"\"\"Load queues and posts from JSON.\"\"\"\n\n        with open(filepath) as f:\n            data = json.load(f)\n\n        # Load queues\n        for name, qdata in data.get('queues', {}).items():\n            self.queues[name] = ContentQueue(\n                name=qdata['name'],\n                content_types=[ContentType(ct) for ct in qdata['content_types']],\n                posts_per_day=qdata['posts_per_day'],\n                optimal_hours=qdata['optimal_hours'],\n                timezone=qdata['timezone'],\n                is_active=qdata['is_active']\n            )\n\n        # Load posts\n        for pid, pdata in data.get('posts', {}).items():\n            self.posts[pid] = ScheduledPost(\n                id=pdata['id'],\n                content=pdata['content'],\n                content_type=ContentType(pdata['content_type']),\n                scheduled_time=datetime.fromisoformat(pdata['scheduled_time']) if pdata['scheduled_time'] else None,\n                status=PostStatus(pdata['status']),\n                is_evergreen=pdata.get('is_evergreen', False),\n                times_posted=pdata.get('times_posted', 0)\n            )\n</code></pre>"},{"location":"cookbook/automation/scheduled-posts/#evergreen-content-rotator","title":"Evergreen Content Rotator","text":"<pre><code># evergreen_rotator.py\nfrom datetime import datetime, timedelta\nfrom typing import Optional\nimport random\n\nfrom scheduler_models import ScheduledPost, ContentType, PostStatus\n\nclass EvergreenRotator:\n    \"\"\"Manage evergreen content rotation.\"\"\"\n\n    def __init__(self, queue_manager: 'QueueManager'):\n        self.manager = queue_manager\n        self.evergreen_pool: list[ScheduledPost] = []\n\n    def add_to_pool(self, post: ScheduledPost):\n        \"\"\"Add post to evergreen pool.\"\"\"\n        post.is_evergreen = True\n        post.content_type = ContentType.EVERGREEN\n        self.evergreen_pool.append(post)\n\n    def get_eligible_posts(\n        self,\n        min_days_since_last: int = 30\n    ) -&gt; list[ScheduledPost]:\n        \"\"\"Get posts eligible for reposting.\"\"\"\n\n        cutoff = datetime.now() - timedelta(days=min_days_since_last)\n\n        eligible = []\n        for post in self.evergreen_pool:\n            if post.last_posted is None or post.last_posted &lt; cutoff:\n                eligible.append(post)\n\n        return eligible\n\n    def select_next_evergreen(\n        self,\n        eligible: list[ScheduledPost] = None\n    ) -&gt; Optional[ScheduledPost]:\n        \"\"\"Select next evergreen post to schedule.\"\"\"\n\n        if eligible is None:\n            eligible = self.get_eligible_posts()\n\n        if not eligible:\n            return None\n\n        # Weight by engagement score and time since last post\n        weighted = []\n        for post in eligible:\n            days_since = 30  # Default if never posted\n            if post.last_posted:\n                days_since = (datetime.now() - post.last_posted).days\n\n            # Higher engagement + longer time = higher weight\n            weight = (post.engagement_score + 1) * (days_since / 30)\n            weighted.append((post, weight))\n\n        # Weighted random selection\n        total_weight = sum(w for _, w in weighted)\n        if total_weight == 0:\n            return random.choice(eligible)\n\n        r = random.uniform(0, total_weight)\n        cumulative = 0\n        for post, weight in weighted:\n            cumulative += weight\n            if r &lt;= cumulative:\n                return post\n\n        return weighted[-1][0]\n\n    def schedule_evergreen_batch(\n        self,\n        queue_name: str,\n        count: int = 5\n    ) -&gt; list[ScheduledPost]:\n        \"\"\"Schedule batch of evergreen posts.\"\"\"\n\n        scheduled = []\n        eligible = self.get_eligible_posts()\n\n        for _ in range(min(count, len(eligible))):\n            post = self.select_next_evergreen(eligible)\n            if post:\n                # Create copy for scheduling\n                new_post = ScheduledPost(\n                    content=post.content,\n                    content_type=ContentType.EVERGREEN,\n                    media_paths=post.media_paths.copy(),\n                    is_evergreen=True\n                )\n\n                self.manager.add_post(queue_name, new_post)\n                scheduled.append(new_post)\n\n                # Update original post tracking\n                post.last_posted = new_post.scheduled_time\n                post.times_posted += 1\n\n                # Remove from eligible\n                eligible.remove(post)\n\n        return scheduled\n\n    def update_engagement_scores(self, performance_data: dict[str, float]):\n        \"\"\"Update engagement scores from performance data.\"\"\"\n\n        for post in self.evergreen_pool:\n            if post.tweet_id and post.tweet_id in performance_data:\n                # Weighted average with existing score\n                new_score = performance_data[post.tweet_id]\n                post.engagement_score = (\n                    post.engagement_score * 0.7 + \n                    new_score * 0.3\n                )\n</code></pre>"},{"location":"cookbook/automation/scheduled-posts/#holiday-detector","title":"Holiday Detector","text":"<pre><code># holiday_detector.py\nfrom datetime import datetime, date\nfrom dataclasses import dataclass\n\n@dataclass\nclass Holiday:\n    name: str\n    date: date\n    adjust_posting: bool = True  # Should adjust posting schedule\n    suggested_content: str = \"\"\n\nclass HolidayDetector:\n    \"\"\"Detect holidays and special events.\"\"\"\n\n    # Major holidays (add more as needed)\n    HOLIDAYS = {\n        (1, 1): Holiday(\"New Year's Day\", date(2024, 1, 1), True, \"New year, new goals!\"),\n        (2, 14): Holiday(\"Valentine's Day\", date(2024, 2, 14), True, \"Spread the love\"),\n        (7, 4): Holiday(\"Independence Day (US)\", date(2024, 7, 4), True, \"\"),\n        (10, 31): Holiday(\"Halloween\", date(2024, 10, 31), True, \"Spooky content!\"),\n        (11, 28): Holiday(\"Thanksgiving (US)\", date(2024, 11, 28), True, \"Gratitude posts\"),\n        (12, 25): Holiday(\"Christmas\", date(2024, 12, 25), True, \"Holiday content\"),\n        (12, 31): Holiday(\"New Year's Eve\", date(2024, 12, 31), True, \"Year in review\"),\n    }\n\n    # Tech/industry events\n    EVENTS = {\n        (3, 14): \"Pi Day - Math/tech content\",\n        (5, 4): \"Star Wars Day - Pop culture\",\n        (9, 13): \"Programmer's Day - Dev content\",\n    }\n\n    def get_holiday(self, check_date: date = None) -&gt; Holiday:\n        \"\"\"Check if date is a holiday.\"\"\"\n\n        if check_date is None:\n            check_date = date.today()\n\n        key = (check_date.month, check_date.day)\n        return self.HOLIDAYS.get(key)\n\n    def get_upcoming_holidays(\n        self,\n        days_ahead: int = 30\n    ) -&gt; list[tuple[date, Holiday]]:\n        \"\"\"Get holidays in the next N days.\"\"\"\n\n        today = date.today()\n        upcoming = []\n\n        for (month, day), holiday in self.HOLIDAYS.items():\n            try:\n                holiday_date = date(today.year, month, day)\n\n                # Handle year boundary\n                if holiday_date &lt; today:\n                    holiday_date = date(today.year + 1, month, day)\n\n                days_until = (holiday_date - today).days\n\n                if 0 &lt;= days_until &lt;= days_ahead:\n                    upcoming.append((holiday_date, holiday))\n\n            except ValueError:\n                continue\n\n        return sorted(upcoming, key=lambda x: x[0])\n\n    def should_adjust_schedule(self, check_date: date = None) -&gt; tuple[bool, str]:\n        \"\"\"Check if schedule should be adjusted for date.\"\"\"\n\n        holiday = self.get_holiday(check_date)\n\n        if holiday and holiday.adjust_posting:\n            return True, f\"Holiday: {holiday.name}\"\n\n        # Check for day before major holidays\n        tomorrow = (check_date or date.today()) + timedelta(days=1)\n        tomorrow_holiday = self.get_holiday(tomorrow)\n\n        if tomorrow_holiday and tomorrow_holiday.adjust_posting:\n            return True, f\"Day before {tomorrow_holiday.name}\"\n\n        return False, \"\"\n\n    def get_content_suggestions(\n        self,\n        days_ahead: int = 7\n    ) -&gt; list[dict]:\n        \"\"\"Get content suggestions for upcoming holidays.\"\"\"\n\n        suggestions = []\n        upcoming = self.get_upcoming_holidays(days_ahead)\n\n        for holiday_date, holiday in upcoming:\n            if holiday.suggested_content:\n                suggestions.append({\n                    'date': holiday_date,\n                    'holiday': holiday.name,\n                    'suggestion': holiday.suggested_content,\n                    'days_until': (holiday_date - date.today()).days\n                })\n\n        return suggestions\n</code></pre>"},{"location":"cookbook/automation/scheduled-posts/#scheduler-engine","title":"Scheduler Engine","text":"<pre><code># scheduler_engine.py\nimport asyncio\nfrom datetime import datetime, timedelta\nfrom typing import Optional\n\nfrom xeepy import Xeepy\n\nfrom scheduler_models import ScheduledPost, PostStatus\nfrom queue_manager import QueueManager\n\nclass SchedulerEngine:\n    \"\"\"Execute scheduled posts.\"\"\"\n\n    def __init__(self, queue_manager: QueueManager):\n        self.manager = queue_manager\n        self.is_running = False\n\n    async def publish_post(self, post: ScheduledPost) -&gt; Optional[str]:\n        \"\"\"Publish a scheduled post.\"\"\"\n\n        async with Xeepy() as x:\n            try:\n                # Handle different content types\n                if post.is_thread and post.thread_tweets:\n                    # Post thread\n                    tweet_ids = await x.engage.post_thread(\n                        [post.content] + post.thread_tweets,\n                        media=post.media_paths[:1] if post.media_paths else None\n                    )\n                    tweet_id = tweet_ids[0] if tweet_ids else None\n\n                elif post.poll_options:\n                    # Post poll\n                    tweet_id = await x.poll.create(\n                        post.content,\n                        post.poll_options,\n                        duration_minutes=post.poll_duration_hours * 60\n                    )\n\n                else:\n                    # Regular post\n                    tweet_id = await x.engage.tweet(\n                        post.content,\n                        media=post.media_paths if post.media_paths else None\n                    )\n\n                return tweet_id\n\n            except Exception as e:\n                print(f\"Failed to publish post {post.id}: {e}\")\n                return None\n\n    async def run_scheduler(self, check_interval: int = 60):\n        \"\"\"Run continuous scheduler.\"\"\"\n\n        self.is_running = True\n        print(\"\ud83d\udcc5 Scheduler started\")\n\n        while self.is_running:\n            try:\n                # Get posts due in next minute\n                pending = self.manager.get_pending_posts(hours_ahead=0.02)\n\n                for post in pending:\n                    if post.scheduled_time &lt;= datetime.now():\n                        print(f\"Publishing: {post.content[:50]}...\")\n\n                        tweet_id = await self.publish_post(post)\n\n                        if tweet_id:\n                            self.manager.mark_published(post.id, tweet_id)\n                            print(f\"\u2713 Published: {tweet_id}\")\n                        else:\n                            post.status = PostStatus.FAILED\n                            print(f\"\u2717 Failed to publish\")\n\n            except Exception as e:\n                print(f\"Scheduler error: {e}\")\n\n            await asyncio.sleep(check_interval)\n\n    def stop(self):\n        \"\"\"Stop the scheduler.\"\"\"\n        self.is_running = False\n</code></pre>"},{"location":"cookbook/automation/scheduled-posts/#usage-example","title":"Usage Example","text":"<pre><code># main.py\nimport asyncio\nfrom datetime import datetime, timedelta\n\nfrom scheduler_models import ScheduledPost, ContentType\nfrom queue_manager import QueueManager\nfrom evergreen_rotator import EvergreenRotator\nfrom holiday_detector import HolidayDetector\nfrom scheduler_engine import SchedulerEngine\n\nasync def main():\n    # Initialize\n    manager = QueueManager()\n\n    # Create queues\n    manager.create_queue(\n        name=\"main\",\n        content_types=[ContentType.REGULAR, ContentType.THREAD],\n        posts_per_day=3,\n        optimal_hours=[9, 13, 18],\n        timezone=\"America/New_York\"\n    )\n\n    manager.create_queue(\n        name=\"engagement\",\n        content_types=[ContentType.ENGAGEMENT],\n        posts_per_day=2,\n        optimal_hours=[10, 15],\n        timezone=\"America/New_York\"\n    )\n\n    # Add posts\n    post1 = ScheduledPost(\n        content=\"Just shipped a new feature! \ud83d\ude80 Thread below \ud83d\udc47\",\n        content_type=ContentType.THREAD,\n        is_thread=True,\n        thread_tweets=[\n            \"1/ The problem we were solving...\",\n            \"2/ Our approach...\",\n            \"3/ The results...\",\n        ]\n    )\n    manager.add_post(\"main\", post1)\n\n    # Add evergreen content\n    rotator = EvergreenRotator(manager)\n\n    evergreen = ScheduledPost(\n        content=\"10 Python tips every developer should know \ud83d\udc0d\\n\\nA thread:\",\n        engagement_score=150.0\n    )\n    rotator.add_to_pool(evergreen)\n\n    # Schedule evergreen batch\n    rotator.schedule_evergreen_batch(\"main\", count=3)\n\n    # Check holidays\n    holiday_detector = HolidayDetector()\n    upcoming = holiday_detector.get_upcoming_holidays(days_ahead=14)\n\n    print(\"Upcoming holidays:\")\n    for hdate, holiday in upcoming:\n        print(f\"  {hdate}: {holiday.name}\")\n\n    # Get pending posts\n    pending = manager.get_pending_posts(hours_ahead=48)\n\n    print(f\"\\nScheduled posts ({len(pending)}):\")\n    for post in pending:\n        print(f\"  {post.scheduled_time}: {post.content[:40]}...\")\n\n    # Run scheduler\n    engine = SchedulerEngine(manager)\n    # await engine.run_scheduler()  # Uncomment to run\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"cookbook/automation/scheduled-posts/#best-practices","title":"Best Practices","text":"<p>Queue Organization</p> <ul> <li>Separate promotional from value content</li> <li>Balance evergreen with timely content</li> <li>Leave slots for reactive posts</li> </ul> <p>Timing</p> <ul> <li>Respect timezone differences</li> <li>Avoid posting during major events</li> <li>Monitor engagement by time slot</li> </ul>"},{"location":"cookbook/automation/scheduled-posts/#related-recipes","title":"Related Recipes","text":"<ul> <li>Content Calendar - AI-powered planning</li> <li>Optimal Timing - ML-based scheduling</li> <li>Hashtag Strategy - Optimize reach</li> </ul>"},{"location":"cookbook/business/","title":"Business Intelligence Cookbook","text":"<p>Transform X/Twitter data into actionable business intelligence.</p>"},{"location":"cookbook/business/#lead-generation-engine","title":"\ud83c\udfaf Lead Generation Engine","text":"<p>Build a sophisticated lead generation system that finds and qualifies prospects.</p> <pre><code>\"\"\"\nLead Generation Engine\nIdentify, qualify, and track potential customers from Twitter\n\"\"\"\n\nimport asyncio\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Optional\nfrom enum import Enum\n\nclass LeadScore(Enum):\n    \"\"\"Lead qualification scores.\"\"\"\n    HOT = 90      # High intent, good fit\n    WARM = 70     # Shows interest\n    COOL = 50     # Potential fit\n    COLD = 30     # Low priority\n    UNQUALIFIED = 0\n\n@dataclass\nclass Lead:\n    \"\"\"Qualified lead data.\"\"\"\n    username: str\n    user_id: str\n    name: str\n    bio: str\n    followers: int\n    following: int\n    location: Optional[str]\n    website: Optional[str]\n\n    # Qualification\n    score: int = 0\n    score_reasons: list = field(default_factory=list)\n    intent_signals: list = field(default_factory=list)\n\n    # Tracking\n    source: str = \"\"  # How we found them\n    first_seen: datetime = field(default_factory=datetime.now)\n    last_activity: Optional[datetime] = None\n\n    # Engagement history\n    engaged: bool = False\n    engagement_type: Optional[str] = None\n    response_received: bool = False\n\n\nclass LeadGenerator:\n    \"\"\"Generate and qualify leads from Twitter.\"\"\"\n\n    def __init__(self, xeepy, config: dict):\n        self.x = xeepy\n        self.config = config\n\n        # Define your Ideal Customer Profile (ICP)\n        self.icp = config.get(\"icp\", {\n            \"keywords\": [\"startup\", \"founder\", \"ceo\", \"cto\", \"saas\"],\n            \"min_followers\": 500,\n            \"max_followers\": 50000,\n            \"must_have_website\": True,\n            \"industries\": [\"tech\", \"software\", \"ai\"],\n            \"exclude_keywords\": [\"parody\", \"fan account\", \"not affiliated\"]\n        })\n\n        # Intent signals to watch for\n        self.intent_keywords = config.get(\"intent_keywords\", [\n            \"looking for\",\n            \"need help with\",\n            \"anyone recommend\",\n            \"struggling with\",\n            \"wish there was\",\n            \"tired of\",\n            \"alternative to\"\n        ])\n\n        self.leads: list[Lead] = []\n\n    async def find_leads_from_competitors(\n        self,\n        competitor_usernames: list[str],\n        limit_per_competitor: int = 100\n    ) -&gt; list[Lead]:\n        \"\"\"Find leads from competitor followers/engagers.\"\"\"\n        leads = []\n\n        for competitor in competitor_usernames:\n            print(f\"\ud83d\udd0d Analyzing @{competitor}'s audience...\")\n\n            # Get recent engagers (people who reply/like their content)\n            tweets = await self.x.scrape.tweets(competitor, limit=20)\n\n            for tweet in tweets.items:\n                # Get people who replied\n                replies = await self.x.scrape.replies(tweet.url, limit=50)\n\n                for reply in replies.items:\n                    lead = await self._qualify_user(\n                        reply.author,\n                        source=f\"competitor:{competitor}\"\n                    )\n                    if lead and lead.score &gt;= LeadScore.COOL.value:\n                        leads.append(lead)\n\n            # Sample of followers\n            followers = await self.x.scrape.followers(\n                competitor,\n                limit=limit_per_competitor\n            )\n\n            for user in followers.items:\n                lead = await self._qualify_user(\n                    user,\n                    source=f\"competitor_follower:{competitor}\"\n                )\n                if lead and lead.score &gt;= LeadScore.COOL.value:\n                    leads.append(lead)\n\n        # Deduplicate\n        seen = set()\n        unique_leads = []\n        for lead in leads:\n            if lead.username not in seen:\n                seen.add(lead.username)\n                unique_leads.append(lead)\n\n        self.leads.extend(unique_leads)\n        return unique_leads\n\n    async def find_leads_from_intent(\n        self,\n        search_queries: list[str],\n        limit_per_query: int = 50\n    ) -&gt; list[Lead]:\n        \"\"\"Find leads showing purchase intent.\"\"\"\n        leads = []\n\n        for query in search_queries:\n            print(f\"\ud83d\udd0d Searching for: {query}\")\n\n            results = await self.x.scrape.search(query, limit=limit_per_query)\n\n            for tweet in results.items:\n                # Check for intent signals\n                intent_signals = self._detect_intent(tweet.text)\n\n                if intent_signals:\n                    lead = await self._qualify_user(\n                        tweet.author,\n                        source=f\"intent_search:{query}\"\n                    )\n                    if lead:\n                        lead.intent_signals = intent_signals\n                        lead.score += 20  # Bonus for showing intent\n                        leads.append(lead)\n\n        self.leads.extend(leads)\n        return leads\n\n    async def find_leads_from_hashtags(\n        self,\n        hashtags: list[str],\n        limit_per_hashtag: int = 100\n    ) -&gt; list[Lead]:\n        \"\"\"Find leads from relevant hashtags.\"\"\"\n        leads = []\n\n        for hashtag in hashtags:\n            print(f\"\ud83d\udd0d Scanning #{hashtag}...\")\n\n            tweets = await self.x.scrape.hashtag(\n                f\"#{hashtag}\",\n                limit=limit_per_hashtag\n            )\n\n            for tweet in tweets.items:\n                lead = await self._qualify_user(\n                    tweet.author,\n                    source=f\"hashtag:{hashtag}\"\n                )\n                if lead and lead.score &gt;= LeadScore.COOL.value:\n                    leads.append(lead)\n\n        self.leads.extend(leads)\n        return leads\n\n    async def _qualify_user(self, user, source: str) -&gt; Optional[Lead]:\n        \"\"\"Qualify a user against ICP criteria.\"\"\"\n        # Skip if missing key data\n        if not user or not user.username:\n            return None\n\n        # Get full profile if needed\n        if not hasattr(user, 'bio') or not user.bio:\n            try:\n                user = await self.x.scrape.profile(user.username)\n            except:\n                return None\n\n        # Start scoring\n        score = 50  # Base score\n        reasons = []\n\n        bio_lower = (user.bio or \"\").lower()\n\n        # Check ICP keywords in bio\n        for keyword in self.icp.get(\"keywords\", []):\n            if keyword.lower() in bio_lower:\n                score += 10\n                reasons.append(f\"Bio contains '{keyword}'\")\n\n        # Check exclusion keywords\n        for exclude in self.icp.get(\"exclude_keywords\", []):\n            if exclude.lower() in bio_lower:\n                return None  # Disqualify\n\n        # Follower range check\n        min_followers = self.icp.get(\"min_followers\", 0)\n        max_followers = self.icp.get(\"max_followers\", float(\"inf\"))\n\n        if user.followers_count &lt; min_followers:\n            score -= 20\n            reasons.append(\"Below minimum followers\")\n        elif user.followers_count &gt; max_followers:\n            score -= 10\n            reasons.append(\"Above maximum followers\")\n        else:\n            score += 10\n            reasons.append(\"Follower count in range\")\n\n        # Website check\n        if self.icp.get(\"must_have_website\") and user.website:\n            score += 15\n            reasons.append(\"Has website\")\n        elif self.icp.get(\"must_have_website\"):\n            score -= 10\n            reasons.append(\"No website\")\n\n        # Engagement ratio (active account)\n        if user.followers_count &gt; 0:\n            ratio = user.following_count / user.followers_count\n            if 0.5 &lt;= ratio &lt;= 2.0:\n                score += 5\n                reasons.append(\"Healthy follow ratio\")\n\n        # Create lead\n        lead = Lead(\n            username=user.username,\n            user_id=user.id,\n            name=user.name or user.username,\n            bio=user.bio or \"\",\n            followers=user.followers_count,\n            following=user.following_count,\n            location=getattr(user, 'location', None),\n            website=getattr(user, 'website', None),\n            score=max(0, min(100, score)),\n            score_reasons=reasons,\n            source=source\n        )\n\n        return lead\n\n    def _detect_intent(self, text: str) -&gt; list[str]:\n        \"\"\"Detect purchase intent signals in text.\"\"\"\n        text_lower = text.lower()\n        signals = []\n\n        for signal in self.intent_keywords:\n            if signal.lower() in text_lower:\n                signals.append(signal)\n\n        return signals\n\n    def export_leads(self, filepath: str, min_score: int = 50):\n        \"\"\"Export qualified leads to CSV.\"\"\"\n        qualified = [l for l in self.leads if l.score &gt;= min_score]\n\n        self.x.export.to_csv([\n            {\n                \"username\": l.username,\n                \"name\": l.name,\n                \"bio\": l.bio,\n                \"followers\": l.followers,\n                \"website\": l.website,\n                \"score\": l.score,\n                \"score_reasons\": \"; \".join(l.score_reasons),\n                \"intent_signals\": \"; \".join(l.intent_signals),\n                \"source\": l.source,\n                \"twitter_url\": f\"https://x.com/{l.username}\"\n            }\n            for l in qualified\n        ], filepath)\n\n        print(f\"\u2705 Exported {len(qualified)} leads to {filepath}\")\n\n\n# Usage\nasync def main():\n    from xeepy import Xeepy\n\n    async with Xeepy() as x:\n        generator = LeadGenerator(x, {\n            \"icp\": {\n                \"keywords\": [\"founder\", \"ceo\", \"startup\", \"saas\", \"building\"],\n                \"min_followers\": 500,\n                \"max_followers\": 100000,\n                \"must_have_website\": True,\n                \"exclude_keywords\": [\"parody\", \"meme\"]\n            },\n            \"intent_keywords\": [\n                \"looking for a tool\",\n                \"need help with\",\n                \"any recommendations for\",\n                \"alternative to\"\n            ]\n        })\n\n        # Find leads from multiple sources\n        await generator.find_leads_from_competitors(\n            [\"competitor1\", \"competitor2\"],\n            limit_per_competitor=50\n        )\n\n        await generator.find_leads_from_intent([\n            '\"looking for\" automation tool',\n            '\"need help with\" twitter growth'\n        ])\n\n        await generator.find_leads_from_hashtags([\n            \"buildinpublic\",\n            \"indiehackers\"\n        ])\n\n        # Export qualified leads\n        generator.export_leads(\"qualified_leads.csv\", min_score=60)\n\nasyncio.run(main())\n</code></pre>"},{"location":"cookbook/business/#competitor-intelligence-dashboard","title":"\ud83d\udcca Competitor Intelligence Dashboard","text":"<p>Monitor competitors in real-time.</p> <pre><code>\"\"\"\nCompetitor Intelligence Dashboard\nReal-time competitive monitoring and analysis\n\"\"\"\n\nimport asyncio\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nfrom collections import defaultdict\n\n@dataclass\nclass CompetitorProfile:\n    \"\"\"Tracked competitor data.\"\"\"\n    username: str\n    name: str\n    bio: str\n    followers: int\n    following: int\n    tweet_count: int\n\n    # Historical tracking\n    followers_history: list = field(default_factory=list)\n    engagement_history: list = field(default_factory=list)\n\n    # Content analysis\n    top_tweets: list = field(default_factory=list)\n    posting_frequency: float = 0.0\n    avg_engagement: float = 0.0\n    content_themes: dict = field(default_factory=dict)\n\n    # Alerts\n    alerts: list = field(default_factory=list)\n\n\nclass CompetitorIntelligence:\n    \"\"\"Monitor and analyze competitors.\"\"\"\n\n    def __init__(self, xeepy, competitors: list[str]):\n        self.x = xeepy\n        self.competitors = competitors\n        self.profiles: dict[str, CompetitorProfile] = {}\n        self.baseline: dict = {}\n\n    async def initialize(self):\n        \"\"\"Capture baseline data for all competitors.\"\"\"\n        print(\"\ud83d\udcca Capturing competitor baselines...\")\n\n        for username in self.competitors:\n            profile = await self.x.scrape.profile(username)\n\n            self.profiles[username] = CompetitorProfile(\n                username=username,\n                name=profile.name,\n                bio=profile.bio,\n                followers=profile.followers_count,\n                following=profile.following_count,\n                tweet_count=profile.tweets_count\n            )\n\n            # Capture initial metrics\n            await self._analyze_content(username)\n\n        print(f\"\u2705 Tracking {len(self.profiles)} competitors\")\n\n    async def check_for_changes(self) -&gt; list[dict]:\n        \"\"\"Check for significant competitor changes.\"\"\"\n        alerts = []\n\n        for username, profile in self.profiles.items():\n            current = await self.x.scrape.profile(username)\n\n            # Follower spike/drop\n            follower_change = current.followers_count - profile.followers\n            change_pct = (follower_change / profile.followers) * 100 if profile.followers &gt; 0 else 0\n\n            if abs(change_pct) &gt;= 5:\n                alert = {\n                    \"type\": \"follower_change\",\n                    \"competitor\": username,\n                    \"change\": follower_change,\n                    \"change_pct\": change_pct,\n                    \"direction\": \"gained\" if follower_change &gt; 0 else \"lost\",\n                    \"timestamp\": datetime.now()\n                }\n                alerts.append(alert)\n                profile.alerts.append(alert)\n\n            # Bio change\n            if current.bio != profile.bio:\n                alerts.append({\n                    \"type\": \"bio_change\",\n                    \"competitor\": username,\n                    \"old_bio\": profile.bio,\n                    \"new_bio\": current.bio,\n                    \"timestamp\": datetime.now()\n                })\n\n            # Update tracking\n            profile.followers = current.followers_count\n            profile.followers_history.append({\n                \"timestamp\": datetime.now(),\n                \"count\": current.followers_count\n            })\n            profile.bio = current.bio\n\n        return alerts\n\n    async def _analyze_content(self, username: str):\n        \"\"\"Analyze competitor content strategy.\"\"\"\n        profile = self.profiles[username]\n\n        # Get recent tweets\n        tweets = await self.x.scrape.tweets(username, limit=100)\n\n        if not tweets.items:\n            return\n\n        # Calculate metrics\n        total_engagement = sum(\n            (t.likes or 0) + (t.retweets or 0) + (t.replies or 0)\n            for t in tweets.items\n        )\n        profile.avg_engagement = total_engagement / len(tweets.items)\n\n        # Find top performers\n        sorted_tweets = sorted(\n            tweets.items,\n            key=lambda t: (t.likes or 0) + (t.retweets or 0),\n            reverse=True\n        )\n        profile.top_tweets = sorted_tweets[:5]\n\n        # Posting frequency\n        if len(tweets.items) &gt;= 2:\n            time_span = tweets.items[0].created_at - tweets.items[-1].created_at\n            days = max(time_span.days, 1)\n            profile.posting_frequency = len(tweets.items) / days\n\n        # Content themes (simple keyword analysis)\n        themes = defaultdict(int)\n        theme_keywords = {\n            \"product\": [\"launch\", \"ship\", \"built\", \"feature\", \"update\"],\n            \"growth\": [\"grow\", \"scale\", \"revenue\", \"mrr\", \"arb\"],\n            \"education\": [\"learn\", \"tip\", \"how to\", \"thread\", \"guide\"],\n            \"engagement\": [\"what do you\", \"question\", \"thoughts\", \"agree\"],\n            \"personal\": [\"excited\", \"grateful\", \"journey\", \"milestone\"]\n        }\n\n        for tweet in tweets.items:\n            text_lower = tweet.text.lower()\n            for theme, keywords in theme_keywords.items():\n                if any(kw in text_lower for kw in keywords):\n                    themes[theme] += 1\n\n        profile.content_themes = dict(themes)\n\n    async def get_content_gaps(self) -&gt; dict:\n        \"\"\"Find content opportunities competitors are missing.\"\"\"\n        all_themes = defaultdict(int)\n        competitor_themes = {}\n\n        for username, profile in self.profiles.items():\n            await self._analyze_content(username)\n            competitor_themes[username] = profile.content_themes\n            for theme, count in profile.content_themes.items():\n                all_themes[theme] += count\n\n        # Find themes with low coverage\n        gaps = {}\n        for theme, total in all_themes.items():\n            avg = total / len(self.competitors)\n            gaps[theme] = {\n                \"total_posts\": total,\n                \"avg_per_competitor\": avg,\n                \"opportunity\": \"high\" if avg &lt; 5 else \"medium\" if avg &lt; 15 else \"low\"\n            }\n\n        return gaps\n\n    async def benchmark_report(self) -&gt; str:\n        \"\"\"Generate competitive benchmark report.\"\"\"\n        report = [\"# Competitive Intelligence Report\", \"\"]\n        report.append(f\"*Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}*\")\n        report.append(\"\")\n\n        # Summary table\n        report.append(\"## Overview\")\n        report.append(\"\")\n        report.append(\"| Competitor | Followers | Avg Engagement | Posts/Day |\")\n        report.append(\"|------------|-----------|----------------|-----------|\")\n\n        for username, profile in self.profiles.items():\n            report.append(\n                f\"| @{username} | {profile.followers:,} | \"\n                f\"{profile.avg_engagement:.1f} | {profile.posting_frequency:.1f} |\"\n            )\n\n        report.append(\"\")\n\n        # Top performing content\n        report.append(\"## Top Performing Content\")\n        for username, profile in self.profiles.items():\n            report.append(f\"\\n### @{username}\")\n            for i, tweet in enumerate(profile.top_tweets[:3], 1):\n                engagement = (tweet.likes or 0) + (tweet.retweets or 0)\n                report.append(f\"{i}. [{tweet.text[:50]}...]({tweet.url}) - {engagement:,} engagements\")\n\n        # Content themes\n        report.append(\"\\n## Content Strategy Analysis\")\n        for username, profile in self.profiles.items():\n            report.append(f\"\\n### @{username}\")\n            for theme, count in sorted(profile.content_themes.items(), key=lambda x: -x[1]):\n                report.append(f\"- {theme.title()}: {count} posts\")\n\n        # Alerts\n        all_alerts = []\n        for profile in self.profiles.values():\n            all_alerts.extend(profile.alerts)\n\n        if all_alerts:\n            report.append(\"\\n## Recent Alerts\")\n            for alert in sorted(all_alerts, key=lambda x: x[\"timestamp\"], reverse=True)[:10]:\n                report.append(f\"- **{alert['type']}**: @{alert['competitor']} - {alert.get('change', 'N/A')}\")\n\n        return \"\\n\".join(report)\n\n\n# Usage\nasync def run_competitor_monitoring():\n    from xeepy import Xeepy\n\n    async with Xeepy() as x:\n        intel = CompetitorIntelligence(x, [\n            \"competitor1\",\n            \"competitor2\",\n            \"competitor3\"\n        ])\n\n        await intel.initialize()\n\n        # Check for changes\n        alerts = await intel.check_for_changes()\n        for alert in alerts:\n            print(f\"\ud83d\udea8 {alert['type']}: @{alert['competitor']}\")\n\n        # Find content gaps\n        gaps = await intel.get_content_gaps()\n        print(\"\\n\ud83d\udcca Content Opportunities:\")\n        for theme, data in gaps.items():\n            if data['opportunity'] == 'high':\n                print(f\"  - {theme}: HIGH opportunity\")\n\n        # Generate report\n        report = await intel.benchmark_report()\n        with open(\"competitor_report.md\", \"w\") as f:\n            f.write(report)\n        print(\"\\n\u2705 Report saved to competitor_report.md\")\n\nasyncio.run(run_competitor_monitoring())\n</code></pre>"},{"location":"cookbook/business/#roi-tracking-system","title":"\ud83d\udcb0 ROI Tracking System","text":"<p>Measure the business impact of your Twitter presence.</p> <pre><code>\"\"\"\nTwitter ROI Tracking System\nMeasure business outcomes from Twitter activities\n\"\"\"\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom typing import Optional\nimport json\n\n@dataclass\nclass Conversion:\n    \"\"\"A tracked conversion event.\"\"\"\n    event_id: str\n    event_type: str  # \"signup\", \"demo\", \"sale\", \"lead\"\n    source_tweet: Optional[str]\n    source_campaign: Optional[str]\n    username: Optional[str]\n    value: float\n    timestamp: datetime\n    attribution: str  # \"direct\", \"assisted\", \"organic\"\n\n\nclass TwitterROI:\n    \"\"\"Track and measure Twitter marketing ROI.\"\"\"\n\n    def __init__(self, xeepy, config: dict):\n        self.x = xeepy\n        self.config = config\n        self.conversions: list[Conversion] = []\n        self.campaigns: dict = {}\n        self.baseline_metrics: dict = {}\n\n    async def start_campaign(\n        self,\n        campaign_name: str,\n        campaign_type: str,\n        budget: float = 0,\n        goals: dict = None\n    ) -&gt; str:\n        \"\"\"Start tracking a new campaign.\"\"\"\n        campaign_id = f\"camp_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n\n        # Capture baseline\n        my_profile = await self.x.scrape.profile(self.config[\"username\"])\n\n        self.campaigns[campaign_id] = {\n            \"name\": campaign_name,\n            \"type\": campaign_type,  # \"content\", \"engagement\", \"follower\", \"lead_gen\"\n            \"budget\": budget,\n            \"goals\": goals or {},\n            \"start_date\": datetime.now(),\n            \"end_date\": None,\n            \"status\": \"active\",\n            \"baseline\": {\n                \"followers\": my_profile.followers_count,\n                \"following\": my_profile.following_count\n            },\n            \"tweets\": [],\n            \"activities\": [],\n            \"metrics\": {}\n        }\n\n        print(f\"\ud83d\ude80 Campaign '{campaign_name}' started (ID: {campaign_id})\")\n        return campaign_id\n\n    async def track_tweet(self, campaign_id: str, tweet_url: str):\n        \"\"\"Track a tweet as part of a campaign.\"\"\"\n        if campaign_id not in self.campaigns:\n            raise ValueError(f\"Campaign {campaign_id} not found\")\n\n        # Get tweet metrics\n        replies = await self.x.scrape.replies(tweet_url, limit=1)\n        tweet_data = {\n            \"url\": tweet_url,\n            \"tracked_at\": datetime.now().isoformat(),\n            \"initial_metrics\": {}  # Would capture likes, RTs, etc.\n        }\n\n        self.campaigns[campaign_id][\"tweets\"].append(tweet_data)\n\n    async def record_conversion(\n        self,\n        event_type: str,\n        value: float,\n        campaign_id: Optional[str] = None,\n        source_tweet: Optional[str] = None,\n        username: Optional[str] = None,\n        metadata: dict = None\n    ):\n        \"\"\"Record a conversion event.\"\"\"\n        conversion = Conversion(\n            event_id=f\"conv_{datetime.now().strftime('%Y%m%d%H%M%S%f')}\",\n            event_type=event_type,\n            source_tweet=source_tweet,\n            source_campaign=campaign_id,\n            username=username,\n            value=value,\n            timestamp=datetime.now(),\n            attribution=self._determine_attribution(campaign_id, source_tweet)\n        )\n\n        self.conversions.append(conversion)\n\n        # Update campaign metrics\n        if campaign_id and campaign_id in self.campaigns:\n            camp = self.campaigns[campaign_id]\n            camp[\"activities\"].append({\n                \"type\": \"conversion\",\n                \"event_type\": event_type,\n                \"value\": value,\n                \"timestamp\": datetime.now().isoformat()\n            })\n\n        print(f\"\ud83d\udcb0 Conversion recorded: {event_type} - ${value}\")\n\n    def _determine_attribution(\n        self,\n        campaign_id: Optional[str],\n        source_tweet: Optional[str]\n    ) -&gt; str:\n        \"\"\"Determine attribution model for conversion.\"\"\"\n        if source_tweet:\n            return \"direct\"\n        elif campaign_id:\n            return \"assisted\"\n        else:\n            return \"organic\"\n\n    async def end_campaign(self, campaign_id: str) -&gt; dict:\n        \"\"\"End a campaign and calculate final metrics.\"\"\"\n        if campaign_id not in self.campaigns:\n            raise ValueError(f\"Campaign {campaign_id} not found\")\n\n        camp = self.campaigns[campaign_id]\n        camp[\"end_date\"] = datetime.now()\n        camp[\"status\"] = \"completed\"\n\n        # Capture final metrics\n        my_profile = await self.x.scrape.profile(self.config[\"username\"])\n\n        # Calculate results\n        baseline = camp[\"baseline\"]\n        results = {\n            \"duration_days\": (camp[\"end_date\"] - camp[\"start_date\"]).days,\n            \"followers_gained\": my_profile.followers_count - baseline[\"followers\"],\n            \"tweets_sent\": len(camp[\"tweets\"]),\n            \"total_conversions\": sum(\n                1 for c in self.conversions\n                if c.source_campaign == campaign_id\n            ),\n            \"total_revenue\": sum(\n                c.value for c in self.conversions\n                if c.source_campaign == campaign_id\n            ),\n            \"budget\": camp[\"budget\"]\n        }\n\n        # Calculate ROI\n        if camp[\"budget\"] &gt; 0:\n            results[\"roi\"] = ((results[\"total_revenue\"] - camp[\"budget\"]) / camp[\"budget\"]) * 100\n        else:\n            results[\"roi\"] = float(\"inf\") if results[\"total_revenue\"] &gt; 0 else 0\n\n        # Cost per metrics\n        if results[\"followers_gained\"] &gt; 0 and camp[\"budget\"] &gt; 0:\n            results[\"cost_per_follower\"] = camp[\"budget\"] / results[\"followers_gained\"]\n\n        if results[\"total_conversions\"] &gt; 0 and camp[\"budget\"] &gt; 0:\n            results[\"cost_per_conversion\"] = camp[\"budget\"] / results[\"total_conversions\"]\n\n        camp[\"results\"] = results\n        return results\n\n    def calculate_lifetime_value(self) -&gt; dict:\n        \"\"\"Calculate customer lifetime value from Twitter.\"\"\"\n        # Group conversions by user\n        user_values = {}\n        for conv in self.conversions:\n            if conv.username:\n                if conv.username not in user_values:\n                    user_values[conv.username] = {\"total\": 0, \"events\": 0}\n                user_values[conv.username][\"total\"] += conv.value\n                user_values[conv.username][\"events\"] += 1\n\n        if not user_values:\n            return {\"avg_ltv\": 0, \"total_customers\": 0}\n\n        total_value = sum(u[\"total\"] for u in user_values.values())\n\n        return {\n            \"avg_ltv\": total_value / len(user_values),\n            \"total_customers\": len(user_values),\n            \"total_revenue\": total_value,\n            \"top_customers\": sorted(\n                user_values.items(),\n                key=lambda x: x[1][\"total\"],\n                reverse=True\n            )[:10]\n        }\n\n    def generate_roi_report(self) -&gt; str:\n        \"\"\"Generate comprehensive ROI report.\"\"\"\n        report = [\"# Twitter Marketing ROI Report\", \"\"]\n        report.append(f\"*Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}*\")\n        report.append(\"\")\n\n        # Overall metrics\n        total_conversions = len(self.conversions)\n        total_revenue = sum(c.value for c in self.conversions)\n        total_budget = sum(c[\"budget\"] for c in self.campaigns.values())\n\n        report.append(\"## Executive Summary\")\n        report.append(\"\")\n        report.append(f\"- **Total Campaigns**: {len(self.campaigns)}\")\n        report.append(f\"- **Total Conversions**: {total_conversions}\")\n        report.append(f\"- **Total Revenue**: ${total_revenue:,.2f}\")\n        report.append(f\"- **Total Budget**: ${total_budget:,.2f}\")\n        if total_budget &gt; 0:\n            overall_roi = ((total_revenue - total_budget) / total_budget) * 100\n            report.append(f\"- **Overall ROI**: {overall_roi:.1f}%\")\n        report.append(\"\")\n\n        # Campaign breakdown\n        report.append(\"## Campaign Performance\")\n        report.append(\"\")\n        report.append(\"| Campaign | Duration | Followers | Conversions | Revenue | ROI |\")\n        report.append(\"|----------|----------|-----------|-------------|---------|-----|\")\n\n        for camp_id, camp in self.campaigns.items():\n            if \"results\" in camp:\n                r = camp[\"results\"]\n                report.append(\n                    f\"| {camp['name']} | {r.get('duration_days', 'N/A')}d | \"\n                    f\"+{r.get('followers_gained', 0)} | {r.get('total_conversions', 0)} | \"\n                    f\"${r.get('total_revenue', 0):,.0f} | {r.get('roi', 0):.1f}% |\"\n                )\n\n        report.append(\"\")\n\n        # Attribution\n        report.append(\"## Attribution Analysis\")\n        attribution_counts = {\"direct\": 0, \"assisted\": 0, \"organic\": 0}\n        attribution_values = {\"direct\": 0, \"assisted\": 0, \"organic\": 0}\n\n        for conv in self.conversions:\n            attribution_counts[conv.attribution] += 1\n            attribution_values[conv.attribution] += conv.value\n\n        report.append(\"\")\n        for attr in [\"direct\", \"assisted\", \"organic\"]:\n            report.append(\n                f\"- **{attr.title()}**: {attribution_counts[attr]} conversions, \"\n                f\"${attribution_values[attr]:,.2f} revenue\"\n            )\n\n        # LTV\n        ltv = self.calculate_lifetime_value()\n        report.append(\"\")\n        report.append(\"## Customer Value\")\n        report.append(f\"- **Average LTV**: ${ltv['avg_ltv']:,.2f}\")\n        report.append(f\"- **Total Customers**: {ltv['total_customers']}\")\n\n        return \"\\n\".join(report)\n\n\n# Usage\nasync def track_marketing_roi():\n    from xeepy import Xeepy\n\n    async with Xeepy() as x:\n        roi = TwitterROI(x, {\"username\": \"your_account\"})\n\n        # Start a campaign\n        campaign_id = await roi.start_campaign(\n            \"Q1 Product Launch\",\n            \"content\",\n            budget=500,\n            goals={\"followers\": 1000, \"conversions\": 50}\n        )\n\n        # Track activities\n        await roi.track_tweet(campaign_id, \"https://x.com/you/status/123\")\n\n        # Record conversions\n        await roi.record_conversion(\n            \"signup\",\n            value=0,\n            campaign_id=campaign_id,\n            source_tweet=\"https://x.com/you/status/123\",\n            username=\"new_user\"\n        )\n\n        await roi.record_conversion(\n            \"sale\",\n            value=99.00,\n            campaign_id=campaign_id,\n            username=\"new_user\"\n        )\n\n        # End campaign and get results\n        results = await roi.end_campaign(campaign_id)\n        print(f\"\\n\ud83d\udcca Campaign Results:\")\n        print(f\"   ROI: {results.get('roi', 0):.1f}%\")\n        print(f\"   Revenue: ${results.get('total_revenue', 0):,.2f}\")\n\n        # Generate report\n        report = roi.generate_roi_report()\n        with open(\"roi_report.md\", \"w\") as f:\n            f.write(report)\n\nasyncio.run(track_marketing_roi())\n</code></pre>"},{"location":"cookbook/business/#customer-support-monitoring","title":"\ud83c\udfe2 Customer Support Monitoring","text":"<p>Monitor brand mentions for support opportunities.</p> <pre><code>\"\"\"\nCustomer Support Monitor\nTrack and respond to support requests on Twitter\n\"\"\"\n\nimport asyncio\nfrom datetime import datetime\nfrom enum import Enum\nfrom dataclasses import dataclass\n\nclass TicketPriority(Enum):\n    URGENT = \"urgent\"\n    HIGH = \"high\"\n    MEDIUM = \"medium\"\n    LOW = \"low\"\n\nclass TicketStatus(Enum):\n    NEW = \"new\"\n    IN_PROGRESS = \"in_progress\"\n    WAITING = \"waiting\"\n    RESOLVED = \"resolved\"\n\n@dataclass\nclass SupportTicket:\n    \"\"\"Support ticket from Twitter.\"\"\"\n    id: str\n    tweet_url: str\n    username: str\n    text: str\n    priority: TicketPriority\n    status: TicketStatus\n    sentiment: str\n    keywords: list\n    created_at: datetime\n    assigned_to: str = None\n    notes: list = None\n\n\nclass SupportMonitor:\n    \"\"\"Monitor Twitter for support requests.\"\"\"\n\n    def __init__(self, xeepy, config: dict):\n        self.x = xeepy\n        self.config = config\n        self.tickets: dict[str, SupportTicket] = {}\n\n        # Keywords indicating support needs\n        self.support_keywords = config.get(\"support_keywords\", [\n            \"help\", \"support\", \"broken\", \"issue\", \"bug\",\n            \"doesn't work\", \"not working\", \"can't\", \"error\",\n            \"problem\", \"frustrated\", \"disappointed\"\n        ])\n\n        self.urgent_keywords = config.get(\"urgent_keywords\", [\n            \"urgent\", \"asap\", \"immediately\", \"critical\",\n            \"down\", \"outage\", \"broken\"\n        ])\n\n    async def scan_mentions(self, limit: int = 50) -&gt; list[SupportTicket]:\n        \"\"\"Scan mentions for support requests.\"\"\"\n        brand = self.config[\"brand_username\"]\n        mentions = await self.x.scrape.mentions(brand, limit=limit)\n\n        new_tickets = []\n        for mention in mentions.items:\n            if mention.id in self.tickets:\n                continue\n\n            # Check if it's a support request\n            text_lower = mention.text.lower()\n            keywords_found = [\n                kw for kw in self.support_keywords\n                if kw in text_lower\n            ]\n\n            if not keywords_found:\n                continue\n\n            # Determine priority\n            priority = TicketPriority.MEDIUM\n            if any(kw in text_lower for kw in self.urgent_keywords):\n                priority = TicketPriority.URGENT\n            elif \"?\" in mention.text and len(keywords_found) == 1:\n                priority = TicketPriority.LOW\n\n            ticket = SupportTicket(\n                id=mention.id,\n                tweet_url=mention.url,\n                username=mention.author.username,\n                text=mention.text,\n                priority=priority,\n                status=TicketStatus.NEW,\n                sentiment=self._analyze_sentiment(mention.text),\n                keywords=keywords_found,\n                created_at=mention.created_at,\n                notes=[]\n            )\n\n            self.tickets[mention.id] = ticket\n            new_tickets.append(ticket)\n\n        return new_tickets\n\n    def _analyze_sentiment(self, text: str) -&gt; str:\n        \"\"\"Simple sentiment analysis.\"\"\"\n        negative = [\"frustrated\", \"angry\", \"disappointed\", \"hate\", \"terrible\", \"worst\"]\n        positive = [\"thanks\", \"appreciate\", \"love\", \"great\", \"awesome\"]\n\n        text_lower = text.lower()\n\n        neg_count = sum(1 for w in negative if w in text_lower)\n        pos_count = sum(1 for w in positive if w in text_lower)\n\n        if neg_count &gt; pos_count:\n            return \"negative\"\n        elif pos_count &gt; neg_count:\n            return \"positive\"\n        return \"neutral\"\n\n    def prioritize_queue(self) -&gt; list[SupportTicket]:\n        \"\"\"Get prioritized queue of open tickets.\"\"\"\n        open_tickets = [\n            t for t in self.tickets.values()\n            if t.status in [TicketStatus.NEW, TicketStatus.IN_PROGRESS]\n        ]\n\n        # Sort by priority, then sentiment, then age\n        priority_order = {\n            TicketPriority.URGENT: 0,\n            TicketPriority.HIGH: 1,\n            TicketPriority.MEDIUM: 2,\n            TicketPriority.LOW: 3\n        }\n\n        sentiment_order = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n\n        return sorted(open_tickets, key=lambda t: (\n            priority_order[t.priority],\n            sentiment_order[t.sentiment],\n            t.created_at\n        ))\n\n\n# Usage\nasync def monitor_support():\n    from xeepy import Xeepy\n\n    async with Xeepy() as x:\n        monitor = SupportMonitor(x, {\n            \"brand_username\": \"your_brand\",\n            \"support_keywords\": [\"help\", \"broken\", \"issue\", \"bug\", \"not working\"],\n            \"urgent_keywords\": [\"urgent\", \"down\", \"critical\"]\n        })\n\n        # Scan for new tickets\n        new_tickets = await monitor.scan_mentions(limit=100)\n        print(f\"\ud83c\udfab Found {len(new_tickets)} new support requests\")\n\n        # Get prioritized queue\n        queue = monitor.prioritize_queue()\n\n        print(\"\\n\ud83d\udccb Support Queue:\")\n        for ticket in queue[:10]:\n            print(f\"  [{ticket.priority.value}] @{ticket.username}: {ticket.text[:50]}...\")\n\nasyncio.run(monitor_support())\n</code></pre>"},{"location":"cookbook/business/#sales-pipeline-integration","title":"\ud83d\udcc8 Sales Pipeline Integration","text":"<p>Connect Twitter engagement to your CRM.</p> <pre><code>\"\"\"\nTwitter to CRM Pipeline\nSync Twitter interactions with your sales pipeline\n\"\"\"\n\nimport asyncio\nfrom datetime import datetime\nimport httpx  # For API calls\n\nclass TwitterCRMSync:\n    \"\"\"Sync Twitter interactions to CRM.\"\"\"\n\n    def __init__(self, xeepy, crm_config: dict):\n        self.x = xeepy\n        self.crm_config = crm_config\n        self.crm_api = crm_config.get(\"api_url\")\n        self.api_key = crm_config.get(\"api_key\")\n\n    async def sync_follower_as_lead(self, username: str):\n        \"\"\"Create CRM lead from Twitter follower.\"\"\"\n        # Get profile\n        profile = await self.x.scrape.profile(username)\n\n        # Map to CRM fields\n        lead_data = {\n            \"source\": \"twitter\",\n            \"source_id\": profile.id,\n            \"name\": profile.name,\n            \"twitter_handle\": f\"@{username}\",\n            \"twitter_url\": f\"https://x.com/{username}\",\n            \"bio\": profile.bio,\n            \"website\": profile.website,\n            \"followers\": profile.followers_count,\n            \"location\": getattr(profile, 'location', None),\n            \"created_at\": datetime.now().isoformat()\n        }\n\n        # Push to CRM (example with generic REST API)\n        async with httpx.AsyncClient() as client:\n            response = await client.post(\n                f\"{self.crm_api}/leads\",\n                json=lead_data,\n                headers={\"Authorization\": f\"Bearer {self.api_key}\"}\n            )\n            return response.json()\n\n    async def log_interaction(\n        self,\n        username: str,\n        interaction_type: str,  # \"dm\", \"reply\", \"like\", \"follow\"\n        content: str = None,\n        tweet_url: str = None\n    ):\n        \"\"\"Log Twitter interaction to CRM.\"\"\"\n        interaction_data = {\n            \"contact_identifier\": f\"twitter:@{username}\",\n            \"channel\": \"twitter\",\n            \"type\": interaction_type,\n            \"content\": content,\n            \"reference_url\": tweet_url,\n            \"timestamp\": datetime.now().isoformat()\n        }\n\n        async with httpx.AsyncClient() as client:\n            response = await client.post(\n                f\"{self.crm_api}/interactions\",\n                json=interaction_data,\n                headers={\"Authorization\": f\"Bearer {self.api_key}\"}\n            )\n            return response.json()\n\n\n# Usage example\nasync def sync_to_crm():\n    from xeepy import Xeepy\n\n    async with Xeepy() as x:\n        crm = TwitterCRMSync(x, {\n            \"api_url\": \"https://api.yourcrm.com/v1\",\n            \"api_key\": \"your_api_key\"\n        })\n\n        # Sync new followers as leads\n        new_followers = await x.monitor.new_followers()\n\n        for follower in new_followers[:10]:\n            await crm.sync_follower_as_lead(follower.username)\n            print(f\"\u2705 Synced @{follower.username} to CRM\")\n\nasyncio.run(sync_to_crm())\n</code></pre>"},{"location":"cookbook/business/#next-steps","title":"Next Steps","text":"<ul> <li>Growth Recipes - Scale your audience</li> <li>Automation Recipes - Automate workflows</li> <li>Data Science Recipes - Analyze your data</li> </ul>"},{"location":"cookbook/business/brand-monitoring/","title":"Real-Time Brand Monitoring System","text":"<p>Build a comprehensive brand monitoring system that tracks mentions, analyzes sentiment, and alerts you to important conversations about your brand.</p>"},{"location":"cookbook/business/brand-monitoring/#overview","title":"Overview","text":"<p>This recipe creates a production-ready brand monitoring system with:</p> <ul> <li>Multi-keyword tracking - Brand names, products, misspellings</li> <li>Sentiment scoring - Real-time sentiment analysis</li> <li>Competitor comparison - Track competitors alongside your brand</li> <li>Influencer alerts - Prioritize high-impact mentions</li> <li>Health scoring - Weekly brand health metrics</li> <li>Crisis detection - Early warning system</li> </ul>"},{"location":"cookbook/business/brand-monitoring/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Keyword        \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Sentiment  \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Alert         \u2502\n\u2502  Tracker        \u2502     \u2502   Analyzer   \u2502     \u2502   Router        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                       \u2502                     \u2502\n        \u25bc                       \u25bc                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  SQLite         \u2502     \u2502   Dashboard  \u2502     \u2502  Notifications  \u2502\n\u2502  Storage        \u2502     \u2502   Generator  \u2502     \u2502  (Discord/etc)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cookbook/business/brand-monitoring/#complete-implementation","title":"Complete Implementation","text":""},{"location":"cookbook/business/brand-monitoring/#configuration","title":"Configuration","text":"<pre><code># brand_monitor_config.py\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\n@dataclass\nclass BrandConfig:\n    \"\"\"Configuration for brand monitoring.\"\"\"\n\n    # Brand keywords (include misspellings!)\n    brand_keywords: list[str] = field(default_factory=lambda: [\n        \"xeepy\",\n        \"xeepy.io\",\n        \"@xeepy\",\n        \"#xeepy\",\n    ])\n\n    # Product keywords\n    product_keywords: list[str] = field(default_factory=lambda: [\n        \"xeepy scraper\",\n        \"xeepy bot\",\n        \"xeepy automation\",\n    ])\n\n    # Competitor keywords\n    competitor_keywords: dict[str, list[str]] = field(default_factory=lambda: {\n        \"competitor_a\": [\"competitorA\", \"@competitorA\"],\n        \"competitor_b\": [\"competitorB\", \"@competitorB\"],\n    })\n\n    # Influencer thresholds\n    influencer_follower_threshold: int = 10000\n    viral_engagement_threshold: int = 100\n\n    # Alert settings\n    negative_sentiment_threshold: float = -0.3\n    crisis_mention_spike: int = 50  # % increase triggers alert\n\n    # Notification webhooks\n    discord_webhook: Optional[str] = None\n    slack_webhook: Optional[str] = None\n\n    # Scanning interval (seconds)\n    scan_interval: int = 300  # 5 minutes\n</code></pre>"},{"location":"cookbook/business/brand-monitoring/#database-schema","title":"Database Schema","text":"<pre><code># brand_monitor_db.py\nimport sqlite3\nfrom datetime import datetime\nfrom typing import Optional\nfrom contextlib import contextmanager\n\nclass BrandMonitorDB:\n    \"\"\"SQLite database for brand monitoring data.\"\"\"\n\n    def __init__(self, db_path: str = \"brand_monitor.db\"):\n        self.db_path = db_path\n        self._init_db()\n\n    @contextmanager\n    def _get_connection(self):\n        conn = sqlite3.connect(self.db_path)\n        conn.row_factory = sqlite3.Row\n        try:\n            yield conn\n        finally:\n            conn.close()\n\n    def _init_db(self):\n        with self._get_connection() as conn:\n            conn.executescript(\"\"\"\n                CREATE TABLE IF NOT EXISTS mentions (\n                    id INTEGER PRIMARY KEY,\n                    tweet_id TEXT UNIQUE,\n                    author_username TEXT,\n                    author_followers INTEGER,\n                    text TEXT,\n                    sentiment_score REAL,\n                    sentiment_label TEXT,\n                    keyword_matched TEXT,\n                    keyword_category TEXT,\n                    engagement_score INTEGER,\n                    is_influencer BOOLEAN,\n                    created_at TIMESTAMP,\n                    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n                );\n\n                CREATE TABLE IF NOT EXISTS daily_metrics (\n                    id INTEGER PRIMARY KEY,\n                    date DATE UNIQUE,\n                    total_mentions INTEGER,\n                    positive_mentions INTEGER,\n                    neutral_mentions INTEGER,\n                    negative_mentions INTEGER,\n                    avg_sentiment REAL,\n                    influencer_mentions INTEGER,\n                    total_reach INTEGER,\n                    brand_health_score REAL\n                );\n\n                CREATE TABLE IF NOT EXISTS competitor_metrics (\n                    id INTEGER PRIMARY KEY,\n                    date DATE,\n                    competitor_name TEXT,\n                    total_mentions INTEGER,\n                    avg_sentiment REAL,\n                    UNIQUE(date, competitor_name)\n                );\n\n                CREATE TABLE IF NOT EXISTS alerts (\n                    id INTEGER PRIMARY KEY,\n                    alert_type TEXT,\n                    severity TEXT,\n                    message TEXT,\n                    tweet_id TEXT,\n                    handled BOOLEAN DEFAULT FALSE,\n                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n                );\n\n                CREATE INDEX IF NOT EXISTS idx_mentions_date \n                ON mentions(created_at);\n\n                CREATE INDEX IF NOT EXISTS idx_mentions_sentiment \n                ON mentions(sentiment_label);\n            \"\"\")\n            conn.commit()\n\n    def save_mention(self, mention: dict):\n        with self._get_connection() as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO mentions \n                (tweet_id, author_username, author_followers, text,\n                 sentiment_score, sentiment_label, keyword_matched,\n                 keyword_category, engagement_score, is_influencer, created_at)\n                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n            \"\"\", (\n                mention['tweet_id'],\n                mention['author_username'],\n                mention['author_followers'],\n                mention['text'],\n                mention['sentiment_score'],\n                mention['sentiment_label'],\n                mention['keyword_matched'],\n                mention['keyword_category'],\n                mention['engagement_score'],\n                mention['is_influencer'],\n                mention['created_at']\n            ))\n            conn.commit()\n\n    def get_mentions_since(self, since: datetime) -&gt; list[dict]:\n        with self._get_connection() as conn:\n            rows = conn.execute(\"\"\"\n                SELECT * FROM mentions \n                WHERE scraped_at &gt;= ?\n                ORDER BY created_at DESC\n            \"\"\", (since,)).fetchall()\n            return [dict(row) for row in rows]\n\n    def get_daily_stats(self, date: datetime) -&gt; dict:\n        with self._get_connection() as conn:\n            row = conn.execute(\"\"\"\n                SELECT \n                    COUNT(*) as total,\n                    SUM(CASE WHEN sentiment_label = 'positive' THEN 1 ELSE 0 END) as positive,\n                    SUM(CASE WHEN sentiment_label = 'neutral' THEN 1 ELSE 0 END) as neutral,\n                    SUM(CASE WHEN sentiment_label = 'negative' THEN 1 ELSE 0 END) as negative,\n                    AVG(sentiment_score) as avg_sentiment,\n                    SUM(CASE WHEN is_influencer THEN 1 ELSE 0 END) as influencer_mentions,\n                    SUM(author_followers) as total_reach\n                FROM mentions\n                WHERE DATE(created_at) = DATE(?)\n            \"\"\", (date,)).fetchone()\n            return dict(row) if row else {}\n</code></pre>"},{"location":"cookbook/business/brand-monitoring/#sentiment-analyzer","title":"Sentiment Analyzer","text":"<pre><code># sentiment_analyzer.py\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport re\n\n@dataclass\nclass SentimentResult:\n    score: float  # -1.0 to 1.0\n    label: str    # positive, neutral, negative\n    confidence: float\n    keywords_found: list[str]\n\nclass SentimentAnalyzer:\n    \"\"\"Analyze sentiment of brand mentions.\"\"\"\n\n    # Positive indicators\n    POSITIVE_WORDS = {\n        'love', 'amazing', 'awesome', 'great', 'excellent', 'fantastic',\n        'best', 'perfect', 'helpful', 'useful', 'recommend', 'impressed',\n        'thank', 'thanks', 'grateful', 'solved', 'works', 'easy', 'fast',\n        '\ud83d\udd25', '\u2764\ufe0f', '\ud83d\udcaf', '\ud83d\udc4d', '\ud83d\ude4c', '\u2728', '\ud83d\ude80'\n    }\n\n    # Negative indicators\n    NEGATIVE_WORDS = {\n        'hate', 'terrible', 'awful', 'worst', 'bad', 'broken', 'bug',\n        'issue', 'problem', 'disappointed', 'frustrating', 'useless',\n        'scam', 'spam', 'fake', 'slow', 'crash', 'error', 'fail',\n        '\ud83d\udc4e', '\ud83d\ude21', '\ud83e\udd2e', '\ud83d\udca9', '\ud83d\ude24'\n    }\n\n    # Intensifiers\n    INTENSIFIERS = {'very', 'really', 'extremely', 'absolutely', 'totally'}\n\n    # Negators\n    NEGATORS = {\"not\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"don't\", \n                \"doesn't\", \"didn't\", \"won't\", \"wouldn't\", \"can't\", \"couldn't\"}\n\n    def analyze(self, text: str) -&gt; SentimentResult:\n        \"\"\"Analyze sentiment of text.\"\"\"\n        text_lower = text.lower()\n        words = set(re.findall(r'\\w+', text_lower))\n\n        # Find sentiment words\n        pos_found = words &amp; self.POSITIVE_WORDS\n        neg_found = words &amp; self.NEGATIVE_WORDS\n\n        # Check for emoji sentiment\n        for emoji in self.POSITIVE_WORDS:\n            if emoji in text:\n                pos_found.add(emoji)\n        for emoji in self.NEGATIVE_WORDS:\n            if emoji in text:\n                neg_found.add(emoji)\n\n        # Calculate base score\n        pos_score = len(pos_found)\n        neg_score = len(neg_found)\n\n        # Check for intensifiers\n        intensifier_count = len(words &amp; self.INTENSIFIERS)\n\n        # Check for negators (flip sentiment)\n        has_negator = bool(words &amp; self.NEGATORS)\n\n        # Calculate final score\n        if pos_score == 0 and neg_score == 0:\n            score = 0.0\n            label = 'neutral'\n            confidence = 0.5\n        else:\n            raw_score = (pos_score - neg_score) / (pos_score + neg_score)\n\n            # Apply intensifier boost\n            if intensifier_count &gt; 0:\n                raw_score *= (1 + 0.2 * intensifier_count)\n\n            # Apply negator flip\n            if has_negator:\n                raw_score *= -0.5\n\n            # Clamp to [-1, 1]\n            score = max(-1.0, min(1.0, raw_score))\n\n            # Determine label\n            if score &gt; 0.1:\n                label = 'positive'\n            elif score &lt; -0.1:\n                label = 'negative'\n            else:\n                label = 'neutral'\n\n            # Confidence based on evidence\n            confidence = min(1.0, (pos_score + neg_score) / 5)\n\n        return SentimentResult(\n            score=round(score, 3),\n            label=label,\n            confidence=round(confidence, 3),\n            keywords_found=list(pos_found | neg_found)\n        )\n</code></pre>"},{"location":"cookbook/business/brand-monitoring/#main-monitor-system","title":"Main Monitor System","text":"<pre><code># brand_monitor.py\nimport asyncio\nfrom datetime import datetime, timedelta\nfrom typing import Optional\nfrom dataclasses import dataclass\n\nfrom xeepy import Xeepy\nfrom xeepy.notifications import DiscordNotifier\n\nfrom brand_monitor_config import BrandConfig\nfrom brand_monitor_db import BrandMonitorDB\nfrom sentiment_analyzer import SentimentAnalyzer\n\n@dataclass\nclass MentionAlert:\n    alert_type: str  # influencer, negative, viral, crisis\n    severity: str    # low, medium, high, critical\n    mention: dict\n    message: str\n\nclass BrandMonitor:\n    \"\"\"Real-time brand monitoring system.\"\"\"\n\n    def __init__(self, config: BrandConfig):\n        self.config = config\n        self.db = BrandMonitorDB()\n        self.sentiment = SentimentAnalyzer()\n        self.notifier = None\n\n        if config.discord_webhook:\n            self.notifier = DiscordNotifier(config.discord_webhook)\n\n        # Build keyword search query\n        all_keywords = (\n            config.brand_keywords + \n            config.product_keywords\n        )\n        self.search_query = \" OR \".join(f'\"{kw}\"' for kw in all_keywords)\n\n    async def scan_mentions(self) -&gt; list[dict]:\n        \"\"\"Scan for new brand mentions.\"\"\"\n        async with Xeepy() as x:\n            # Search for brand mentions\n            results = await x.scrape.search(\n                query=self.search_query,\n                limit=100,\n                result_type=\"Latest\"\n            )\n\n            processed = []\n            for tweet in results:\n                # Analyze sentiment\n                sentiment = self.sentiment.analyze(tweet.text)\n\n                # Determine which keyword matched\n                keyword_matched = self._find_matched_keyword(tweet.text)\n\n                # Calculate engagement score\n                engagement = (\n                    tweet.like_count + \n                    tweet.retweet_count * 2 + \n                    tweet.reply_count * 3\n                )\n\n                # Check if influencer\n                is_influencer = (\n                    tweet.author.followers_count &gt;= \n                    self.config.influencer_follower_threshold\n                )\n\n                mention = {\n                    'tweet_id': tweet.id,\n                    'author_username': tweet.author.username,\n                    'author_followers': tweet.author.followers_count,\n                    'text': tweet.text,\n                    'sentiment_score': sentiment.score,\n                    'sentiment_label': sentiment.label,\n                    'keyword_matched': keyword_matched,\n                    'keyword_category': self._get_keyword_category(keyword_matched),\n                    'engagement_score': engagement,\n                    'is_influencer': is_influencer,\n                    'created_at': tweet.created_at,\n                }\n\n                # Save to database\n                self.db.save_mention(mention)\n                processed.append(mention)\n\n                # Check for alerts\n                alert = self._check_alerts(mention)\n                if alert:\n                    await self._send_alert(alert)\n\n            return processed\n\n    def _find_matched_keyword(self, text: str) -&gt; str:\n        \"\"\"Find which keyword matched in the text.\"\"\"\n        text_lower = text.lower()\n        for kw in self.config.brand_keywords:\n            if kw.lower() in text_lower:\n                return kw\n        for kw in self.config.product_keywords:\n            if kw.lower() in text_lower:\n                return kw\n        return \"unknown\"\n\n    def _get_keyword_category(self, keyword: str) -&gt; str:\n        \"\"\"Get category for matched keyword.\"\"\"\n        if keyword in self.config.brand_keywords:\n            return \"brand\"\n        elif keyword in self.config.product_keywords:\n            return \"product\"\n        return \"other\"\n\n    def _check_alerts(self, mention: dict) -&gt; Optional[MentionAlert]:\n        \"\"\"Check if mention triggers any alerts.\"\"\"\n        # Influencer mention\n        if mention['is_influencer']:\n            return MentionAlert(\n                alert_type='influencer',\n                severity='high' if mention['sentiment_label'] == 'negative' else 'medium',\n                mention=mention,\n                message=f\"\ud83c\udf1f Influencer mention from @{mention['author_username']} \"\n                        f\"({mention['author_followers']:,} followers)\"\n            )\n\n        # Highly negative mention\n        if mention['sentiment_score'] &lt; self.config.negative_sentiment_threshold:\n            return MentionAlert(\n                alert_type='negative',\n                severity='medium',\n                mention=mention,\n                message=f\"\u26a0\ufe0f Negative mention detected (score: {mention['sentiment_score']})\"\n            )\n\n        # Viral mention\n        if mention['engagement_score'] &gt; self.config.viral_engagement_threshold:\n            return MentionAlert(\n                alert_type='viral',\n                severity='high',\n                mention=mention,\n                message=f\"\ud83d\ude80 Viral mention! Engagement score: {mention['engagement_score']}\"\n            )\n\n        return None\n\n    async def _send_alert(self, alert: MentionAlert):\n        \"\"\"Send alert notification.\"\"\"\n        if not self.notifier:\n            print(f\"[ALERT] {alert.message}\")\n            return\n\n        color = {\n            'low': 0x3498db,      # Blue\n            'medium': 0xf39c12,   # Yellow\n            'high': 0xe74c3c,     # Red\n            'critical': 0x9b59b6  # Purple\n        }.get(alert.severity, 0x95a5a6)\n\n        await self.notifier.send_embed(\n            title=f\"Brand Alert: {alert.alert_type.title()}\",\n            description=alert.message,\n            color=color,\n            fields=[\n                {\"name\": \"Author\", \"value\": f\"@{alert.mention['author_username']}\", \"inline\": True},\n                {\"name\": \"Sentiment\", \"value\": alert.mention['sentiment_label'], \"inline\": True},\n                {\"name\": \"Tweet\", \"value\": alert.mention['text'][:200]},\n            ],\n            url=f\"https://x.com/i/status/{alert.mention['tweet_id']}\"\n        )\n\n    def calculate_brand_health(self, days: int = 7) -&gt; dict:\n        \"\"\"Calculate brand health score.\"\"\"\n        since = datetime.now() - timedelta(days=days)\n        mentions = self.db.get_mentions_since(since)\n\n        if not mentions:\n            return {'score': 0, 'grade': 'N/A', 'mentions': 0}\n\n        # Components of brand health\n        total = len(mentions)\n\n        # 1. Sentiment score (0-40 points)\n        avg_sentiment = sum(m['sentiment_score'] for m in mentions) / total\n        sentiment_score = (avg_sentiment + 1) / 2 * 40  # Normalize to 0-40\n\n        # 2. Volume trend (0-20 points)\n        # Compare to previous period\n        prev_since = since - timedelta(days=days)\n        prev_mentions = self.db.get_mentions_since(prev_since)\n        prev_count = len([m for m in prev_mentions if m['scraped_at'] &lt; since])\n\n        if prev_count &gt; 0:\n            growth = (total - prev_count) / prev_count\n            volume_score = min(20, max(0, 10 + growth * 10))\n        else:\n            volume_score = 10\n\n        # 3. Influencer engagement (0-20 points)\n        influencer_mentions = sum(1 for m in mentions if m['is_influencer'])\n        influencer_score = min(20, influencer_mentions * 2)\n\n        # 4. Positive ratio (0-20 points)\n        positive = sum(1 for m in mentions if m['sentiment_label'] == 'positive')\n        positive_ratio = positive / total\n        positive_score = positive_ratio * 20\n\n        # Total score\n        total_score = sentiment_score + volume_score + influencer_score + positive_score\n\n        # Grade\n        if total_score &gt;= 80:\n            grade = 'A'\n        elif total_score &gt;= 60:\n            grade = 'B'\n        elif total_score &gt;= 40:\n            grade = 'C'\n        elif total_score &gt;= 20:\n            grade = 'D'\n        else:\n            grade = 'F'\n\n        return {\n            'score': round(total_score, 1),\n            'grade': grade,\n            'mentions': total,\n            'components': {\n                'sentiment': round(sentiment_score, 1),\n                'volume': round(volume_score, 1),\n                'influencer': round(influencer_score, 1),\n                'positive_ratio': round(positive_score, 1)\n            },\n            'avg_sentiment': round(avg_sentiment, 3),\n            'influencer_mentions': influencer_mentions,\n            'positive_ratio': round(positive_ratio, 3)\n        }\n\n    async def run_continuous(self):\n        \"\"\"Run continuous monitoring loop.\"\"\"\n        print(f\"\ud83d\ude80 Starting brand monitor...\")\n        print(f\"   Keywords: {self.search_query[:50]}...\")\n        print(f\"   Interval: {self.config.scan_interval}s\")\n\n        while True:\n            try:\n                mentions = await self.scan_mentions()\n                print(f\"[{datetime.now()}] Scanned {len(mentions)} mentions\")\n\n                # Calculate health every hour\n                if datetime.now().minute == 0:\n                    health = self.calculate_brand_health()\n                    print(f\"   Brand Health: {health['grade']} ({health['score']}/100)\")\n\n            except Exception as e:\n                print(f\"Error during scan: {e}\")\n\n            await asyncio.sleep(self.config.scan_interval)\n</code></pre>"},{"location":"cookbook/business/brand-monitoring/#dashboard-generator","title":"Dashboard Generator","text":"<pre><code># brand_dashboard.py\nfrom datetime import datetime, timedelta\nfrom brand_monitor_db import BrandMonitorDB\n\nclass BrandDashboard:\n    \"\"\"Generate brand monitoring dashboard.\"\"\"\n\n    def __init__(self, db: BrandMonitorDB):\n        self.db = db\n\n    def generate_report(self, days: int = 7) -&gt; str:\n        \"\"\"Generate markdown report.\"\"\"\n        since = datetime.now() - timedelta(days=days)\n        mentions = self.db.get_mentions_since(since)\n\n        # Calculate stats\n        total = len(mentions)\n        positive = sum(1 for m in mentions if m['sentiment_label'] == 'positive')\n        negative = sum(1 for m in mentions if m['sentiment_label'] == 'negative')\n        neutral = total - positive - negative\n\n        avg_sentiment = sum(m['sentiment_score'] for m in mentions) / total if total else 0\n\n        influencer_mentions = [m for m in mentions if m['is_influencer']]\n\n        # Top mentions by engagement\n        top_mentions = sorted(mentions, key=lambda m: m['engagement_score'], reverse=True)[:5]\n\n        report = f\"\"\"\n# Brand Monitoring Report\n**Period:** {since.strftime('%Y-%m-%d')} to {datetime.now().strftime('%Y-%m-%d')}\n\n## Overview\n\n| Metric | Value |\n|--------|-------|\n| Total Mentions | {total:,} |\n| Positive | {positive:,} ({positive/total*100:.1f}%) |\n| Neutral | {neutral:,} ({neutral/total*100:.1f}%) |\n| Negative | {negative:,} ({negative/total*100:.1f}%) |\n| Avg Sentiment | {avg_sentiment:.2f} |\n| Influencer Mentions | {len(influencer_mentions):,} |\n\n## Top Mentions\n\n\"\"\"\n        for i, m in enumerate(top_mentions, 1):\n            report += f\"\"\"\n### {i}. @{m['author_username']} ({m['author_followers']:,} followers)\n&gt; {m['text'][:200]}...\n\n- Sentiment: {m['sentiment_label']} ({m['sentiment_score']})\n- Engagement: {m['engagement_score']}\n- [View Tweet](https://x.com/i/status/{m['tweet_id']})\n\n\"\"\"\n\n        return report\n</code></pre>"},{"location":"cookbook/business/brand-monitoring/#usage-example","title":"Usage Example","text":"<pre><code># main.py\nimport asyncio\nfrom brand_monitor import BrandMonitor\nfrom brand_monitor_config import BrandConfig\n\nasync def main():\n    # Configure for your brand\n    config = BrandConfig(\n        brand_keywords=[\n            \"YourBrand\",\n            \"yourbrand\",\n            \"@YourBrand\",\n            \"#YourBrand\",\n        ],\n        product_keywords=[\n            \"YourProduct\",\n            \"your product\",\n        ],\n        discord_webhook=\"https://discord.com/api/webhooks/...\",\n        scan_interval=300,  # 5 minutes\n    )\n\n    monitor = BrandMonitor(config)\n\n    # Run continuous monitoring\n    await monitor.run_continuous()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"cookbook/business/brand-monitoring/#advanced-features","title":"Advanced Features","text":""},{"location":"cookbook/business/brand-monitoring/#competitor-comparison","title":"Competitor Comparison","text":"<pre><code>async def compare_competitors(self):\n    \"\"\"Compare brand metrics against competitors.\"\"\"\n    results = {'brand': await self._get_brand_metrics()}\n\n    for name, keywords in self.config.competitor_keywords.items():\n        query = \" OR \".join(f'\"{kw}\"' for kw in keywords)\n\n        async with Xeepy() as x:\n            tweets = await x.scrape.search(query, limit=100)\n\n            sentiments = [self.sentiment.analyze(t.text) for t in tweets]\n            avg_sentiment = sum(s.score for s in sentiments) / len(sentiments)\n\n            results[name] = {\n                'mentions': len(tweets),\n                'avg_sentiment': avg_sentiment,\n                'positive_ratio': sum(1 for s in sentiments if s.label == 'positive') / len(sentiments)\n            }\n\n    return results\n</code></pre>"},{"location":"cookbook/business/brand-monitoring/#weekly-report-automation","title":"Weekly Report Automation","text":"<pre><code>async def send_weekly_report():\n    \"\"\"Send weekly brand report.\"\"\"\n    monitor = BrandMonitor(BrandConfig())\n    dashboard = BrandDashboard(monitor.db)\n\n    report = dashboard.generate_report(days=7)\n    health = monitor.calculate_brand_health(days=7)\n\n    # Send via Discord\n    await monitor.notifier.send_embed(\n        title=\"\ud83d\udcca Weekly Brand Report\",\n        description=f\"Brand Health: **{health['grade']}** ({health['score']}/100)\",\n        fields=[\n            {\"name\": \"Total Mentions\", \"value\": str(health['mentions']), \"inline\": True},\n            {\"name\": \"Avg Sentiment\", \"value\": f\"{health['avg_sentiment']:.2f}\", \"inline\": True},\n        ]\n    )\n</code></pre>"},{"location":"cookbook/business/brand-monitoring/#best-practices","title":"Best Practices","text":"<p>Keyword Selection</p> <ul> <li>Include common misspellings</li> <li>Add both @ mentions and hashtags</li> <li>Consider product names separately</li> <li>Monitor CEO/founder names for reputation</li> </ul> <p>Rate Limiting</p> <ul> <li>Space out competitor scans</li> <li>Use longer intervals during low-activity hours</li> <li>Cache results to reduce API calls</li> </ul> <p>Response Strategy</p> <ul> <li>Respond to influencers within 1 hour</li> <li>Acknowledge negative feedback quickly</li> <li>Amplify positive mentions</li> </ul>"},{"location":"cookbook/business/brand-monitoring/#related-recipes","title":"Related Recipes","text":"<ul> <li>Crisis Detection System - Automated crisis response</li> <li>Influencer Mapping - Network analysis</li> <li>Sentiment Analysis Guide - Deep sentiment analysis</li> </ul>"},{"location":"cookbook/business/competitor-intel/","title":"Competitor Intelligence","text":"<p>Know what your competitors are doing before they do.</p>"},{"location":"cookbook/business/competitor-intel/#competitor-monitoring-dashboard","title":"Competitor Monitoring Dashboard","text":"<p>Real-time intelligence on competitor activity:</p> <pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom collections import defaultdict\n\n@dataclass\nclass CompetitorIntel:\n    username: str\n    followers: int\n    following: int\n    follower_change_7d: int\n    tweets_per_day: float\n    avg_engagement: float\n    engagement_rate: float\n    top_content_types: dict\n    posting_hours: list\n    recent_campaigns: list\n    audience_overlap: float\n\nasync def competitor_deep_dive(competitor: str, your_username: str = \"me\"):\n    \"\"\"Complete competitor analysis\"\"\"\n\n    async with Xeepy() as x:\n        # Get competitor profile\n        profile = await x.scrape.profile(competitor)\n\n        # Get their tweets\n        tweets = await x.scrape.tweets(competitor, limit=200)\n\n        # Get follower samples for overlap analysis\n        their_followers = await x.scrape.followers(competitor, limit=500)\n        my_followers = await x.scrape.followers(your_username, limit=500)\n\n        # Calculate metrics\n        # 1. Posting frequency\n        if tweets:\n            date_range = (tweets[0].created_at - tweets[-1].created_at).days or 1\n            tweets_per_day = len(tweets) / date_range\n        else:\n            tweets_per_day = 0\n\n        # 2. Engagement metrics\n        total_engagement = sum(t.likes + t.retweets + t.replies for t in tweets)\n        avg_engagement = total_engagement / len(tweets) if tweets else 0\n        engagement_rate = (avg_engagement / profile.followers_count * 100) if profile.followers_count else 0\n\n        # 3. Content type analysis\n        content_types = defaultdict(int)\n        for tweet in tweets:\n            if tweet.media:\n                if \"video\" in str(tweet.media):\n                    content_types[\"video\"] += 1\n                else:\n                    content_types[\"image\"] += 1\n            elif \"\ud83e\uddf5\" in tweet.text or tweet.is_thread:\n                content_types[\"thread\"] += 1\n            elif tweet.poll:\n                content_types[\"poll\"] += 1\n            else:\n                content_types[\"text\"] += 1\n\n        # 4. Posting hours\n        posting_hours = defaultdict(int)\n        for tweet in tweets:\n            posting_hours[tweet.created_at.hour] += 1\n        peak_hours = sorted(posting_hours.items(), key=lambda x: x[1], reverse=True)[:3]\n\n        # 5. Audience overlap\n        their_set = {f.username for f in their_followers}\n        my_set = {f.username for f in my_followers}\n        overlap = len(their_set &amp; my_set) / len(their_set) * 100 if their_set else 0\n\n        # 6. Recent campaigns (detect patterns)\n        recent_hashtags = defaultdict(int)\n        for tweet in tweets[:50]:  # Last 50 tweets\n            for word in tweet.text.split():\n                if word.startswith('#'):\n                    recent_hashtags[word] += 1\n        campaigns = [h for h, c in recent_hashtags.items() if c &gt;= 3]\n\n        intel = CompetitorIntel(\n            username=competitor,\n            followers=profile.followers_count,\n            following=profile.following_count,\n            follower_change_7d=0,  # Would need historical data\n            tweets_per_day=tweets_per_day,\n            avg_engagement=avg_engagement,\n            engagement_rate=engagement_rate,\n            top_content_types=dict(content_types),\n            posting_hours=[h for h, _ in peak_hours],\n            recent_campaigns=campaigns,\n            audience_overlap=overlap,\n        )\n\n        return intel\n\nasync def print_competitor_report(competitor: str):\n    intel = await competitor_deep_dive(competitor)\n\n    print(f\"\"\"\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551           COMPETITOR INTELLIGENCE REPORT                      \u2551\n\u2551           @{intel.username:&lt;52}\u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 AUDIENCE                                                      \u2551\n\u2551   Followers:    {intel.followers:&gt;10,}                                    \u2551\n\u2551   Following:    {intel.following:&gt;10,}                                    \u2551\n\u2551   Overlap:      {intel.audience_overlap:&gt;10.1f}%  (shared audience)              \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 CONTENT STRATEGY                                              \u2551\n\u2551   Posts/day:    {intel.tweets_per_day:&gt;10.1f}                                    \u2551\n\u2551   Avg engage:   {intel.avg_engagement:&gt;10.0f}                                    \u2551\n\u2551   Engage rate:  {intel.engagement_rate:&gt;10.2f}%                                  \u2551\n\u2551                                                               \u2551\n\u2551   Content Mix:                                                \u2551\"\"\")\n\n    for ctype, count in intel.top_content_types.items():\n        pct = count / sum(intel.top_content_types.values()) * 100\n        print(f\"\u2551     {ctype:&lt;10} {pct:&gt;5.1f}%                                      \u2551\")\n\n    print(f\"\"\"\u2551                                                               \u2551\n\u2551   Peak Hours:   {', '.join(f'{h}:00' for h in intel.posting_hours):&lt;42}\u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 ACTIVE CAMPAIGNS                                              \u2551\"\"\")\n\n    for campaign in intel.recent_campaigns[:5]:\n        print(f\"\u2551   \u2022 {campaign:&lt;55}\u2551\")\n\n    print(\"\"\"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\"\"\")\n\nasyncio.run(print_competitor_report(\"competitor_username\"))\n</code></pre>"},{"location":"cookbook/business/competitor-intel/#competitor-content-spy","title":"Competitor Content Spy","text":"<p>See what's working for competitors:</p> <pre><code>import asyncio\nfrom xeepy import Xeepy\n\nasync def spy_on_competitor_content(competitor: str, days: int = 30):\n    \"\"\"Analyze competitor's best performing content\"\"\"\n\n    async with Xeepy() as x:\n        profile = await x.scrape.profile(competitor)\n        tweets = await x.scrape.tweets(competitor, limit=200)\n\n        # Filter to recent tweets\n        cutoff = datetime.now() - timedelta(days=days)\n        recent_tweets = [t for t in tweets if t.created_at &gt; cutoff]\n\n        # Sort by engagement\n        sorted_tweets = sorted(\n            recent_tweets,\n            key=lambda t: t.likes + t.retweets * 2,  # Weight RTs higher\n            reverse=True\n        )\n\n        print(f\"\ud83d\udd0d TOP PERFORMING CONTENT FROM @{competitor}\")\n        print(f\"   (Last {days} days, {len(recent_tweets)} tweets analyzed)\")\n        print(\"=\"*70)\n\n        for i, tweet in enumerate(sorted_tweets[:10], 1):\n            engagement_rate = (tweet.likes + tweet.retweets) / profile.followers_count * 100\n\n            # Detect content patterns\n            patterns = []\n            if \"?\" in tweet.text:\n                patterns.append(\"question\")\n            if any(c in tweet.text for c in \"\ud83d\udcca\ud83d\udcc8\ud83d\udca1\ud83d\udd25\"):\n                patterns.append(\"emoji-heavy\")\n            if len(tweet.text.split('\\n')) &gt; 3:\n                patterns.append(\"formatted\")\n            if tweet.text.startswith('\"') or tweet.text.startswith(\"'\"):\n                patterns.append(\"quote\")\n            if any(w in tweet.text.lower() for w in [\"thread\", \"\ud83e\uddf5\", \"1/\"]):\n                patterns.append(\"thread\")\n\n            print(f\"\\n#{i} | \u2764\ufe0f {tweet.likes:,} | \ud83d\udd04 {tweet.retweets:,} | \ud83d\udcc8 {engagement_rate:.2f}%\")\n            print(f\"   Patterns: {', '.join(patterns) or 'plain text'}\")\n            print(f\"   Posted: {tweet.created_at.strftime('%Y-%m-%d %H:%M')}\")\n            print(f\"   {tweet.text[:200]}{'...' if len(tweet.text) &gt; 200 else ''}\")\n\n        # Pattern summary\n        print(\"\\n\" + \"=\"*70)\n        print(\"\ud83d\udcca CONTENT PATTERN ANALYSIS\")\n\n        pattern_performance = defaultdict(list)\n        for tweet in recent_tweets:\n            if \"?\" in tweet.text:\n                pattern_performance[\"questions\"].append(tweet.likes)\n            if tweet.media:\n                pattern_performance[\"with_media\"].append(tweet.likes)\n            if len(tweet.text) &gt; 200:\n                pattern_performance[\"long_form\"].append(tweet.likes)\n            if len(tweet.text) &lt; 100:\n                pattern_performance[\"short_form\"].append(tweet.likes)\n\n        for pattern, likes_list in pattern_performance.items():\n            avg = sum(likes_list) / len(likes_list) if likes_list else 0\n            print(f\"   {pattern}: {avg:.0f} avg likes ({len(likes_list)} tweets)\")\n\nasyncio.run(spy_on_competitor_content(\"competitor_username\", days=30))\n</code></pre>"},{"location":"cookbook/business/competitor-intel/#audience-steal-strategy","title":"Audience Steal Strategy","text":"<p>Systematically target competitor's audience:</p> <pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom xeepy.actions.base import FollowFilters\n\nasync def steal_competitor_audience(\n    competitor: str,\n    max_follows: int = 100,\n    quality_threshold: float = 0.7\n):\n    \"\"\"\n    Follow competitor's most engaged followers.\n\n    Strategy: Follow people who actively engage with competitor\n    (they're more likely to engage with similar content)\n    \"\"\"\n\n    async with Xeepy() as x:\n        print(f\"\ud83c\udfaf Targeting @{competitor}'s engaged audience\")\n\n        # Get competitor's recent tweets\n        tweets = await x.scrape.tweets(competitor, limit=20)\n\n        # Collect engagers (people who liked/replied)\n        engagers = {}\n\n        for tweet in tweets:\n            # Get likers\n            likers = await x.scrape.likers(tweet.url, limit=50)\n            for user in likers:\n                if user.username not in engagers:\n                    engagers[user.username] = {\n                        \"user\": user,\n                        \"engagement_count\": 0,\n                        \"types\": set()\n                    }\n                engagers[user.username][\"engagement_count\"] += 1\n                engagers[user.username][\"types\"].add(\"like\")\n\n            # Get repliers (higher value)\n            replies = await x.scrape.replies(tweet.url, limit=30)\n            for reply in replies:\n                username = reply.author.username\n                if username not in engagers:\n                    engagers[username] = {\n                        \"user\": reply.author,\n                        \"engagement_count\": 0,\n                        \"types\": set()\n                    }\n                engagers[username][\"engagement_count\"] += 1\n                engagers[username][\"types\"].add(\"reply\")\n\n        # Score and filter engagers\n        quality_filters = FollowFilters(\n            min_followers=100,\n            max_followers=50000,\n            min_tweets=20,\n            must_have_bio=True,\n        )\n\n        scored_engagers = []\n        for username, data in engagers.items():\n            user = data[\"user\"]\n\n            # Quality score\n            matches, _ = quality_filters.matches({\n                \"followers_count\": user.followers_count,\n                \"following_count\": user.following_count,\n                \"tweets_count\": user.tweets_count,\n                \"bio\": user.bio,\n                \"has_profile_pic\": user.has_profile_pic,\n            })\n\n            if not matches:\n                continue\n\n            # Engagement score (reply &gt; like)\n            engagement_score = data[\"engagement_count\"]\n            if \"reply\" in data[\"types\"]:\n                engagement_score *= 2\n\n            # Combined score\n            total_score = engagement_score * (1 if matches else 0)\n\n            scored_engagers.append({\n                \"username\": username,\n                \"user\": user,\n                \"score\": total_score,\n                \"engagement_count\": data[\"engagement_count\"],\n                \"types\": data[\"types\"],\n            })\n\n        # Sort by score\n        scored_engagers.sort(key=lambda x: x[\"score\"], reverse=True)\n\n        # Follow top engagers\n        print(f\"\\n\ud83d\udcca Found {len(scored_engagers)} quality engagers\")\n        print(f\"\ud83c\udfaf Following top {max_follows}...\\n\")\n\n        followed = 0\n        for engager in scored_engagers[:max_follows]:\n            result = await x.follow.user(\n                engager[\"username\"],\n                source=f\"competitor:{competitor}\"\n            )\n\n            if result.success:\n                followed += 1\n                print(f\"\u2713 @{engager['username']} \"\n                      f\"(engaged {engager['engagement_count']}x, \"\n                      f\"types: {', '.join(engager['types'])})\")\n\n            await asyncio.sleep(random.uniform(3, 8))\n\n        print(f\"\\n\u2705 Followed {followed} of @{competitor}'s top engagers\")\n        return followed\n\nasyncio.run(steal_competitor_audience(\"competitor_username\", max_follows=50))\n</code></pre>"},{"location":"cookbook/business/competitor-intel/#competitor-alert-system","title":"Competitor Alert System","text":"<p>Get notified when competitors do something notable:</p> <pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom xeepy.notifications import DiscordWebhook\nfrom datetime import datetime, timedelta\n\nasync def competitor_alert_bot(\n    competitors: list,\n    webhook_url: str,\n    check_interval: int = 300  # 5 minutes\n):\n    \"\"\"\n    Monitor competitors and alert on:\n    - Viral tweets (unusual engagement)\n    - Campaign launches (new hashtags)\n    - Follower milestones\n    - Strategy changes\n    \"\"\"\n\n    webhook = DiscordWebhook(webhook_url)\n\n    # Track state\n    last_tweets = {c: None for c in competitors}\n    last_followers = {c: None for c in competitors}\n    known_hashtags = {c: set() for c in competitors}\n\n    async with Xeepy() as x:\n        # Initialize state\n        for competitor in competitors:\n            profile = await x.scrape.profile(competitor)\n            tweets = await x.scrape.tweets(competitor, limit=10)\n\n            last_followers[competitor] = profile.followers_count\n            last_tweets[competitor] = tweets[0].id if tweets else None\n\n            for tweet in tweets:\n                for word in tweet.text.split():\n                    if word.startswith('#'):\n                        known_hashtags[competitor].add(word.lower())\n\n        print(f\"\ud83d\udd0d Monitoring {len(competitors)} competitors...\")\n\n        while True:\n            for competitor in competitors:\n                try:\n                    profile = await x.scrape.profile(competitor)\n                    tweets = await x.scrape.tweets(competitor, limit=10)\n\n                    # Check for new tweets\n                    new_tweets = []\n                    for tweet in tweets:\n                        if last_tweets[competitor] and tweet.id == last_tweets[competitor]:\n                            break\n                        new_tweets.append(tweet)\n\n                    for tweet in new_tweets:\n                        # Check for viral potential\n                        age_minutes = (datetime.now() - tweet.created_at).total_seconds() / 60\n                        if age_minutes &gt; 0:\n                            velocity = tweet.likes / age_minutes\n\n                            if velocity &gt; 5:  # Going viral\n                                await webhook.send(\n                                    title=f\"\ud83d\udd25 @{competitor} Tweet Going Viral\",\n                                    description=tweet.text[:500],\n                                    fields=[\n                                        {\"name\": \"\u2764\ufe0f Likes\", \"value\": str(tweet.likes), \"inline\": True},\n                                        {\"name\": \"\ud83d\udcc8 Velocity\", \"value\": f\"{velocity:.1f}/min\", \"inline\": True},\n                                        {\"name\": \"\ud83d\udd17 Link\", \"value\": tweet.url, \"inline\": False},\n                                    ],\n                                    color=0xFF6347\n                                )\n\n                        # Check for new hashtags (campaign launch)\n                        for word in tweet.text.split():\n                            if word.startswith('#'):\n                                tag = word.lower()\n                                if tag not in known_hashtags[competitor]:\n                                    known_hashtags[competitor].add(tag)\n                                    await webhook.send(\n                                        title=f\"\ud83c\udff7\ufe0f @{competitor} New Hashtag Campaign\",\n                                        description=f\"Started using **{word}**\",\n                                        fields=[\n                                            {\"name\": \"Tweet\", \"value\": tweet.text[:200], \"inline\": False},\n                                        ],\n                                        color=0x00CED1\n                                    )\n\n                    if new_tweets:\n                        last_tweets[competitor] = tweets[0].id\n\n                    # Check for follower milestones\n                    prev_followers = last_followers[competitor]\n                    curr_followers = profile.followers_count\n\n                    # Milestone check (every 1000)\n                    if curr_followers // 1000 &gt; prev_followers // 1000:\n                        milestone = (curr_followers // 1000) * 1000\n                        await webhook.send(\n                            title=f\"\ud83d\udcc8 @{competitor} Hit {milestone:,} Followers\",\n                            description=f\"Gained {curr_followers - prev_followers:,} since last check\",\n                            color=0x32CD32\n                        )\n\n                    # Significant growth (&gt;5% in one check)\n                    if prev_followers and curr_followers &gt; prev_followers * 1.05:\n                        await webhook.send(\n                            title=f\"\ud83d\ude80 @{competitor} Unusual Growth\",\n                            description=f\"Followers jumped from {prev_followers:,} to {curr_followers:,} (+{((curr_followers/prev_followers)-1)*100:.1f}%)\",\n                            color=0xFFD700\n                        )\n\n                    last_followers[competitor] = curr_followers\n\n                except Exception as e:\n                    print(f\"Error checking @{competitor}: {e}\")\n\n            await asyncio.sleep(check_interval)\n\nasyncio.run(competitor_alert_bot(\n    competitors=[\"competitor1\", \"competitor2\", \"competitor3\"],\n    webhook_url=\"https://discord.com/api/webhooks/...\"\n))\n</code></pre>"},{"location":"cookbook/business/competitor-intel/#competitor-response-analyzer","title":"Competitor Response Analyzer","text":"<p>See how competitors engage with their audience:</p> <pre><code>async def analyze_competitor_engagement_style(competitor: str):\n    \"\"\"Analyze how a competitor engages with their audience\"\"\"\n\n    async with Xeepy() as x:\n        # Get competitor's replies to others\n        # (This requires scraping their profile for replies)\n        tweets = await x.scrape.tweets(\n            competitor,\n            limit=200,\n            include_replies=True\n        )\n\n        replies = [t for t in tweets if t.is_reply]\n        original = [t for t in tweets if not t.is_reply]\n\n        # Analyze reply patterns\n        reply_lengths = [len(t.text) for t in replies]\n        avg_reply_length = sum(reply_lengths) / len(reply_lengths) if replies else 0\n\n        # Sentiment/tone analysis (simple version)\n        positive_words = [\"thanks\", \"great\", \"love\", \"awesome\", \"amazing\", \"helpful\"]\n        question_replies = [r for r in replies if \"?\" in r.text]\n        positive_replies = [r for r in replies if any(w in r.text.lower() for w in positive_words)]\n\n        # Response time (would need more data for accuracy)\n\n        print(f\"\ud83d\udcca @{competitor} ENGAGEMENT ANALYSIS\")\n        print(\"=\"*50)\n        print(f\"\\nContent Split:\")\n        print(f\"  Original tweets: {len(original)} ({len(original)/len(tweets)*100:.1f}%)\")\n        print(f\"  Replies: {len(replies)} ({len(replies)/len(tweets)*100:.1f}%)\")\n\n        print(f\"\\nReply Style:\")\n        print(f\"  Average length: {avg_reply_length:.0f} characters\")\n        print(f\"  Ask questions: {len(question_replies)} ({len(question_replies)/len(replies)*100:.1f}%)\")\n        print(f\"  Positive tone: {len(positive_replies)} ({len(positive_replies)/len(replies)*100:.1f}%)\")\n\n        print(f\"\\n\ud83d\udca1 Insights:\")\n        if len(replies) / len(tweets) &gt; 0.3:\n            print(\"  \u2022 Highly engaged with audience\")\n        if avg_reply_length &gt; 100:\n            print(\"  \u2022 Gives detailed responses\")\n        if len(question_replies) / len(replies) &gt; 0.2:\n            print(\"  \u2022 Asks follow-up questions (builds relationships)\")\n\nasyncio.run(analyze_competitor_engagement_style(\"competitor_username\"))\n</code></pre>"},{"location":"cookbook/business/competitor-intel/#best-practices","title":"Best Practices","text":"<p>Ethical Intelligence</p> <ul> <li>Focus on public information only</li> <li>Use insights to improve your own strategy</li> <li>Don't copy content directly</li> <li>Compete on value, not imitation</li> </ul> <p>What to Track</p> <ol> <li>Content types that perform well</li> <li>Posting frequency and timing</li> <li>Engagement tactics</li> <li>Campaign launches</li> <li>Audience growth patterns</li> </ol>"},{"location":"cookbook/business/competitor-intel/#next-steps","title":"Next Steps","text":"<p> Brand Monitoring - Track mentions of your brand</p> <p> Lead Generation - Find prospects from competitor audiences</p> <p> Crisis Detection - Early warning for competitor missteps</p>"},{"location":"cookbook/business/crisis-detection/","title":"AI-Powered Crisis Detection System","text":"<p>Build an automated crisis detection system that identifies reputation threats before they go viral and enables rapid response.</p>"},{"location":"cookbook/business/crisis-detection/#overview","title":"Overview","text":"<p>This recipe creates an intelligent crisis detection system with:</p> <ul> <li>Anomaly detection - Identify unusual mention spikes</li> <li>Sentiment monitoring - Track negative sentiment trends</li> <li>Viral detection - Catch spreading negative content early</li> <li>Automated escalation - Alert the right people immediately</li> <li>Response tracking - Monitor crisis resolution</li> <li>Post-crisis analysis - Learn from incidents</li> </ul>"},{"location":"cookbook/business/crisis-detection/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Real-time      \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Anomaly    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Severity      \u2502\n\u2502  Monitor        \u2502     \u2502   Detector   \u2502     \u2502   Classifier    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502                     \u2502\n                               \u25bc                     \u25bc\n                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502  Baseline    \u2502     \u2502   Escalation    \u2502\n                        \u2502  Tracker     \u2502     \u2502   Engine        \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                     \u2502\n                               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                               \u25bc                     \u25bc                     \u25bc\n                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502   Discord    \u2502     \u2502   Telegram   \u2502     \u2502   PagerDuty  \u2502\n                        \u2502   Alerts     \u2502     \u2502   Alerts     \u2502     \u2502   Escalation \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cookbook/business/crisis-detection/#complete-implementation","title":"Complete Implementation","text":""},{"location":"cookbook/business/crisis-detection/#crisis-data-models","title":"Crisis Data Models","text":"<pre><code># crisis_models.py\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Optional\n\nclass CrisisSeverity(Enum):\n    LOW = \"low\"           # Minor complaint\n    MEDIUM = \"medium\"     # Growing issue\n    HIGH = \"high\"         # Significant threat\n    CRITICAL = \"critical\" # Viral crisis\n\nclass CrisisType(Enum):\n    SENTIMENT_SPIKE = \"sentiment_spike\"\n    VOLUME_SPIKE = \"volume_spike\"\n    INFLUENCER_NEGATIVE = \"influencer_negative\"\n    VIRAL_NEGATIVE = \"viral_negative\"\n    KEYWORD_TRIGGER = \"keyword_trigger\"\n    COORDINATED_ATTACK = \"coordinated_attack\"\n\n@dataclass\nclass CrisisEvent:\n    id: str\n    crisis_type: CrisisType\n    severity: CrisisSeverity\n    detected_at: datetime\n    trigger_tweets: list[str]\n    affected_keywords: list[str]\n    metrics: dict\n    status: str = \"active\"  # active, monitoring, resolved\n    assigned_to: Optional[str] = None\n    resolution_notes: Optional[str] = None\n    resolved_at: Optional[datetime] = None\n\n@dataclass\nclass BaselineMetrics:\n    \"\"\"Normal operating metrics for comparison.\"\"\"\n    avg_hourly_mentions: float\n    avg_sentiment: float\n    sentiment_std_dev: float\n    avg_negative_ratio: float\n    typical_engagement: float\n    calculated_at: datetime\n</code></pre>"},{"location":"cookbook/business/crisis-detection/#anomaly-detector","title":"Anomaly Detector","text":"<pre><code># anomaly_detector.py\nimport statistics\nfrom datetime import datetime, timedelta\nfrom collections import deque\nfrom typing import Optional\n\nfrom crisis_models import CrisisType, BaselineMetrics\n\nclass AnomalyDetector:\n    \"\"\"Detect anomalies in brand mention patterns.\"\"\"\n\n    def __init__(\n        self,\n        volume_threshold: float = 2.5,    # Std devs for volume spike\n        sentiment_threshold: float = 2.0,  # Std devs for sentiment drop\n        window_hours: int = 24,            # Baseline window\n    ):\n        self.volume_threshold = volume_threshold\n        self.sentiment_threshold = sentiment_threshold\n        self.window_hours = window_hours\n\n        # Rolling windows for baseline calculation\n        self.hourly_volumes = deque(maxlen=window_hours * 7)  # 1 week\n        self.hourly_sentiments = deque(maxlen=window_hours * 7)\n\n        self.baseline: Optional[BaselineMetrics] = None\n\n    def update_baseline(self, hourly_volume: int, avg_sentiment: float):\n        \"\"\"Update rolling baseline with new hourly data.\"\"\"\n        self.hourly_volumes.append(hourly_volume)\n        self.hourly_sentiments.append(avg_sentiment)\n\n        if len(self.hourly_volumes) &gt;= 24:  # Need at least 1 day\n            self._calculate_baseline()\n\n    def _calculate_baseline(self):\n        \"\"\"Calculate baseline metrics from historical data.\"\"\"\n        volumes = list(self.hourly_volumes)\n        sentiments = list(self.hourly_sentiments)\n\n        self.baseline = BaselineMetrics(\n            avg_hourly_mentions=statistics.mean(volumes),\n            avg_sentiment=statistics.mean(sentiments),\n            sentiment_std_dev=statistics.stdev(sentiments) if len(sentiments) &gt; 1 else 0.1,\n            avg_negative_ratio=sum(1 for s in sentiments if s &lt; -0.1) / len(sentiments),\n            typical_engagement=statistics.median(volumes) * 10,  # Rough estimate\n            calculated_at=datetime.now()\n        )\n\n    def detect_volume_spike(self, current_volume: int) -&gt; Optional[dict]:\n        \"\"\"Detect unusual volume increase.\"\"\"\n        if not self.baseline:\n            return None\n\n        if self.baseline.avg_hourly_mentions == 0:\n            return None\n\n        z_score = (\n            (current_volume - self.baseline.avg_hourly_mentions) /\n            max(statistics.stdev(self.hourly_volumes), 1)\n        )\n\n        if z_score &gt; self.volume_threshold:\n            return {\n                'type': CrisisType.VOLUME_SPIKE,\n                'z_score': z_score,\n                'current': current_volume,\n                'baseline': self.baseline.avg_hourly_mentions,\n                'increase_pct': (current_volume / self.baseline.avg_hourly_mentions - 1) * 100\n            }\n\n        return None\n\n    def detect_sentiment_drop(self, current_sentiment: float) -&gt; Optional[dict]:\n        \"\"\"Detect unusual sentiment decrease.\"\"\"\n        if not self.baseline:\n            return None\n\n        z_score = (\n            (self.baseline.avg_sentiment - current_sentiment) /\n            max(self.baseline.sentiment_std_dev, 0.1)\n        )\n\n        if z_score &gt; self.sentiment_threshold:\n            return {\n                'type': CrisisType.SENTIMENT_SPIKE,\n                'z_score': z_score,\n                'current': current_sentiment,\n                'baseline': self.baseline.avg_sentiment,\n                'drop': self.baseline.avg_sentiment - current_sentiment\n            }\n\n        return None\n\n    def detect_negative_ratio_spike(\n        self, \n        negative_count: int, \n        total_count: int\n    ) -&gt; Optional[dict]:\n        \"\"\"Detect spike in negative mention ratio.\"\"\"\n        if not self.baseline or total_count == 0:\n            return None\n\n        current_ratio = negative_count / total_count\n        baseline_ratio = self.baseline.avg_negative_ratio\n\n        # Alert if negative ratio doubles or exceeds 30%\n        if current_ratio &gt; baseline_ratio * 2 or current_ratio &gt; 0.3:\n            return {\n                'type': CrisisType.SENTIMENT_SPIKE,\n                'current_ratio': current_ratio,\n                'baseline_ratio': baseline_ratio,\n                'negative_count': negative_count,\n                'total_count': total_count\n            }\n\n        return None\n</code></pre>"},{"location":"cookbook/business/crisis-detection/#severity-classifier","title":"Severity Classifier","text":"<pre><code># severity_classifier.py\nfrom crisis_models import CrisisSeverity, CrisisType\n\nclass SeverityClassifier:\n    \"\"\"Classify crisis severity based on multiple factors.\"\"\"\n\n    def __init__(\n        self,\n        influencer_threshold: int = 50000,\n        viral_engagement_threshold: int = 500,\n        critical_keywords: list[str] = None\n    ):\n        self.influencer_threshold = influencer_threshold\n        self.viral_engagement_threshold = viral_engagement_threshold\n        self.critical_keywords = critical_keywords or [\n            'lawsuit', 'fraud', 'scam', 'hack', 'breach', 'leaked',\n            'racist', 'sexist', 'discrimination', 'harassment', 'assault',\n            'death', 'injury', 'recall', 'investigation', 'fbi', 'sec'\n        ]\n\n    def classify(\n        self,\n        crisis_type: CrisisType,\n        mentions: list[dict],\n        anomaly_data: dict\n    ) -&gt; CrisisSeverity:\n        \"\"\"Classify severity of a crisis event.\"\"\"\n        score = 0\n\n        # Factor 1: Volume/Sentiment anomaly severity\n        z_score = anomaly_data.get('z_score', 0)\n        if z_score &gt; 4:\n            score += 40\n        elif z_score &gt; 3:\n            score += 25\n        elif z_score &gt; 2:\n            score += 10\n\n        # Factor 2: Influencer involvement\n        influencer_mentions = [\n            m for m in mentions \n            if m.get('author_followers', 0) &gt;= self.influencer_threshold\n        ]\n        if len(influencer_mentions) &gt;= 3:\n            score += 30\n        elif len(influencer_mentions) &gt;= 1:\n            score += 15\n\n        # Factor 3: Viral engagement\n        max_engagement = max(\n            (m.get('engagement_score', 0) for m in mentions), \n            default=0\n        )\n        if max_engagement &gt; self.viral_engagement_threshold * 2:\n            score += 25\n        elif max_engagement &gt; self.viral_engagement_threshold:\n            score += 15\n\n        # Factor 4: Critical keyword presence\n        all_text = \" \".join(m.get('text', '').lower() for m in mentions)\n        critical_found = [kw for kw in self.critical_keywords if kw in all_text]\n        if len(critical_found) &gt;= 2:\n            score += 30\n        elif len(critical_found) &gt;= 1:\n            score += 15\n\n        # Factor 5: Velocity (mentions per minute)\n        if len(mentions) &gt; 0:\n            time_span = (\n                max(m.get('created_at') for m in mentions) -\n                min(m.get('created_at') for m in mentions)\n            )\n            if time_span.total_seconds() &gt; 0:\n                velocity = len(mentions) / (time_span.total_seconds() / 60)\n                if velocity &gt; 10:  # 10+ mentions per minute\n                    score += 20\n                elif velocity &gt; 5:\n                    score += 10\n\n        # Classify based on score\n        if score &gt;= 80:\n            return CrisisSeverity.CRITICAL\n        elif score &gt;= 50:\n            return CrisisSeverity.HIGH\n        elif score &gt;= 25:\n            return CrisisSeverity.MEDIUM\n        else:\n            return CrisisSeverity.LOW\n</code></pre>"},{"location":"cookbook/business/crisis-detection/#escalation-engine","title":"Escalation Engine","text":"<pre><code># escalation_engine.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Optional\nimport json\n\nfrom crisis_models import CrisisEvent, CrisisSeverity\n\nclass EscalationEngine:\n    \"\"\"Handle crisis escalation and notifications.\"\"\"\n\n    def __init__(\n        self,\n        discord_webhook: Optional[str] = None,\n        telegram_bot_token: Optional[str] = None,\n        telegram_chat_id: Optional[str] = None,\n        pagerduty_key: Optional[str] = None,\n        email_config: Optional[dict] = None\n    ):\n        self.discord_webhook = discord_webhook\n        self.telegram_bot_token = telegram_bot_token\n        self.telegram_chat_id = telegram_chat_id\n        self.pagerduty_key = pagerduty_key\n        self.email_config = email_config\n\n        # Escalation matrix\n        self.escalation_matrix = {\n            CrisisSeverity.LOW: ['discord'],\n            CrisisSeverity.MEDIUM: ['discord', 'telegram'],\n            CrisisSeverity.HIGH: ['discord', 'telegram', 'email'],\n            CrisisSeverity.CRITICAL: ['discord', 'telegram', 'email', 'pagerduty'],\n        }\n\n    async def escalate(self, crisis: CrisisEvent):\n        \"\"\"Escalate crisis through appropriate channels.\"\"\"\n        channels = self.escalation_matrix.get(crisis.severity, ['discord'])\n\n        tasks = []\n        for channel in channels:\n            if channel == 'discord' and self.discord_webhook:\n                tasks.append(self._send_discord(crisis))\n            elif channel == 'telegram' and self.telegram_bot_token:\n                tasks.append(self._send_telegram(crisis))\n            elif channel == 'email' and self.email_config:\n                tasks.append(self._send_email(crisis))\n            elif channel == 'pagerduty' and self.pagerduty_key:\n                tasks.append(self._send_pagerduty(crisis))\n\n        await asyncio.gather(*tasks, return_exceptions=True)\n\n    async def _send_discord(self, crisis: CrisisEvent):\n        \"\"\"Send Discord alert.\"\"\"\n        import aiohttp\n\n        color = {\n            CrisisSeverity.LOW: 0x3498db,\n            CrisisSeverity.MEDIUM: 0xf39c12,\n            CrisisSeverity.HIGH: 0xe74c3c,\n            CrisisSeverity.CRITICAL: 0x9b59b6\n        }.get(crisis.severity, 0x95a5a6)\n\n        severity_emoji = {\n            CrisisSeverity.LOW: \"\ud83d\udd35\",\n            CrisisSeverity.MEDIUM: \"\ud83d\udfe1\",\n            CrisisSeverity.HIGH: \"\ud83d\udd34\",\n            CrisisSeverity.CRITICAL: \"\ud83d\udea8\"\n        }.get(crisis.severity, \"\u26aa\")\n\n        embed = {\n            \"title\": f\"{severity_emoji} Crisis Alert: {crisis.crisis_type.value}\",\n            \"description\": f\"**Severity:** {crisis.severity.value.upper()}\\n\"\n                          f\"**Status:** {crisis.status}\",\n            \"color\": color,\n            \"fields\": [\n                {\n                    \"name\": \"Trigger Tweets\",\n                    \"value\": str(len(crisis.trigger_tweets)),\n                    \"inline\": True\n                },\n                {\n                    \"name\": \"Keywords Affected\",\n                    \"value\": \", \".join(crisis.affected_keywords[:5]),\n                    \"inline\": True\n                },\n                {\n                    \"name\": \"Metrics\",\n                    \"value\": f\"```json\\n{json.dumps(crisis.metrics, indent=2)[:500]}\\n```\",\n                    \"inline\": False\n                }\n            ],\n            \"timestamp\": crisis.detected_at.isoformat()\n        }\n\n        # Add sample tweet links\n        if crisis.trigger_tweets:\n            tweet_links = \"\\n\".join(\n                f\"\u2022 https://x.com/i/status/{tid}\" \n                for tid in crisis.trigger_tweets[:3]\n            )\n            embed[\"fields\"].append({\n                \"name\": \"Sample Tweets\",\n                \"value\": tweet_links,\n                \"inline\": False\n            })\n\n        async with aiohttp.ClientSession() as session:\n            await session.post(\n                self.discord_webhook,\n                json={\n                    \"embeds\": [embed],\n                    \"content\": \"@here\" if crisis.severity in [\n                        CrisisSeverity.HIGH, \n                        CrisisSeverity.CRITICAL\n                    ] else None\n                }\n            )\n\n    async def _send_telegram(self, crisis: CrisisEvent):\n        \"\"\"Send Telegram alert.\"\"\"\n        import aiohttp\n\n        severity_emoji = \"\ud83d\udea8\" if crisis.severity == CrisisSeverity.CRITICAL else \"\u26a0\ufe0f\"\n\n        message = f\"\"\"\n{severity_emoji} &lt;b&gt;CRISIS ALERT&lt;/b&gt;\n\n&lt;b&gt;Type:&lt;/b&gt; {crisis.crisis_type.value}\n&lt;b&gt;Severity:&lt;/b&gt; {crisis.severity.value.upper()}\n&lt;b&gt;Time:&lt;/b&gt; {crisis.detected_at.strftime('%Y-%m-%d %H:%M UTC')}\n\n&lt;b&gt;Trigger Count:&lt;/b&gt; {len(crisis.trigger_tweets)} tweets\n&lt;b&gt;Keywords:&lt;/b&gt; {', '.join(crisis.affected_keywords[:3])}\n\n&lt;b&gt;Action Required:&lt;/b&gt; Review and respond immediately.\n\"\"\"\n\n        async with aiohttp.ClientSession() as session:\n            await session.post(\n                f\"https://api.telegram.org/bot{self.telegram_bot_token}/sendMessage\",\n                json={\n                    \"chat_id\": self.telegram_chat_id,\n                    \"text\": message,\n                    \"parse_mode\": \"HTML\"\n                }\n            )\n\n    async def _send_pagerduty(self, crisis: CrisisEvent):\n        \"\"\"Trigger PagerDuty incident for critical crises.\"\"\"\n        import aiohttp\n\n        payload = {\n            \"routing_key\": self.pagerduty_key,\n            \"event_action\": \"trigger\",\n            \"dedup_key\": crisis.id,\n            \"payload\": {\n                \"summary\": f\"Brand Crisis: {crisis.crisis_type.value} - {crisis.severity.value}\",\n                \"severity\": \"critical\" if crisis.severity == CrisisSeverity.CRITICAL else \"error\",\n                \"source\": \"brand-monitor\",\n                \"custom_details\": {\n                    \"crisis_type\": crisis.crisis_type.value,\n                    \"trigger_count\": len(crisis.trigger_tweets),\n                    \"keywords\": crisis.affected_keywords,\n                    \"metrics\": crisis.metrics\n                }\n            }\n        }\n\n        async with aiohttp.ClientSession() as session:\n            await session.post(\n                \"https://events.pagerduty.com/v2/enqueue\",\n                json=payload\n            )\n\n    async def _send_email(self, crisis: CrisisEvent):\n        \"\"\"Send email alert (implement with your email service).\"\"\"\n        # Placeholder - implement with your email service\n        print(f\"Would send email for crisis: {crisis.id}\")\n</code></pre>"},{"location":"cookbook/business/crisis-detection/#main-crisis-detector","title":"Main Crisis Detector","text":"<pre><code># crisis_detector.py\nimport asyncio\nimport uuid\nfrom datetime import datetime, timedelta\nfrom typing import Optional\n\nfrom xeepy import Xeepy\n\nfrom crisis_models import CrisisEvent, CrisisType, CrisisSeverity\nfrom anomaly_detector import AnomalyDetector\nfrom severity_classifier import SeverityClassifier\nfrom escalation_engine import EscalationEngine\n\nclass CrisisDetector:\n    \"\"\"Main crisis detection system.\"\"\"\n\n    def __init__(\n        self,\n        brand_keywords: list[str],\n        discord_webhook: Optional[str] = None,\n        check_interval: int = 60,  # seconds\n    ):\n        self.brand_keywords = brand_keywords\n        self.search_query = \" OR \".join(f'\"{kw}\"' for kw in brand_keywords)\n        self.check_interval = check_interval\n\n        self.anomaly_detector = AnomalyDetector()\n        self.severity_classifier = SeverityClassifier()\n        self.escalation = EscalationEngine(discord_webhook=discord_webhook)\n\n        self.active_crises: dict[str, CrisisEvent] = {}\n        self.response_times: list[float] = []\n\n    async def monitor(self):\n        \"\"\"Run continuous crisis monitoring.\"\"\"\n        print(\"\ud83d\udea8 Crisis Detection System Active\")\n        print(f\"   Monitoring: {self.search_query[:50]}...\")\n\n        last_check = datetime.now() - timedelta(hours=1)\n\n        while True:\n            try:\n                # Fetch recent mentions\n                mentions = await self._fetch_recent_mentions(since=last_check)\n                last_check = datetime.now()\n\n                if not mentions:\n                    await asyncio.sleep(self.check_interval)\n                    continue\n\n                # Analyze for anomalies\n                crisis = await self._analyze_mentions(mentions)\n\n                if crisis:\n                    await self._handle_crisis(crisis)\n\n                # Update baseline with normal data\n                if not crisis:\n                    hourly_volume = len(mentions)\n                    avg_sentiment = sum(m['sentiment_score'] for m in mentions) / len(mentions)\n                    self.anomaly_detector.update_baseline(hourly_volume, avg_sentiment)\n\n            except Exception as e:\n                print(f\"Error in crisis monitor: {e}\")\n\n            await asyncio.sleep(self.check_interval)\n\n    async def _fetch_recent_mentions(self, since: datetime) -&gt; list[dict]:\n        \"\"\"Fetch and analyze recent mentions.\"\"\"\n        async with Xeepy() as x:\n            tweets = await x.scrape.search(\n                query=self.search_query,\n                limit=200,\n                result_type=\"Latest\"\n            )\n\n            # Filter to only new tweets\n            mentions = []\n            for tweet in tweets:\n                if tweet.created_at &gt; since:\n                    # Quick sentiment analysis\n                    sentiment = self._quick_sentiment(tweet.text)\n\n                    mentions.append({\n                        'tweet_id': tweet.id,\n                        'text': tweet.text,\n                        'author_username': tweet.author.username,\n                        'author_followers': tweet.author.followers_count,\n                        'sentiment_score': sentiment,\n                        'engagement_score': (\n                            tweet.like_count + \n                            tweet.retweet_count * 2 + \n                            tweet.reply_count * 3\n                        ),\n                        'created_at': tweet.created_at\n                    })\n\n            return mentions\n\n    def _quick_sentiment(self, text: str) -&gt; float:\n        \"\"\"Quick sentiment scoring.\"\"\"\n        text_lower = text.lower()\n\n        positive = ['love', 'great', 'amazing', 'awesome', 'thanks', '\u2764\ufe0f', '\ud83d\udd25']\n        negative = ['hate', 'terrible', 'awful', 'worst', 'scam', 'broken', '\ud83d\ude21']\n\n        pos_count = sum(1 for w in positive if w in text_lower)\n        neg_count = sum(1 for w in negative if w in text_lower)\n\n        if pos_count == 0 and neg_count == 0:\n            return 0.0\n\n        return (pos_count - neg_count) / (pos_count + neg_count)\n\n    async def _analyze_mentions(self, mentions: list[dict]) -&gt; Optional[CrisisEvent]:\n        \"\"\"Analyze mentions for crisis indicators.\"\"\"\n        # Check volume spike\n        volume_anomaly = self.anomaly_detector.detect_volume_spike(len(mentions))\n\n        # Check sentiment drop\n        avg_sentiment = sum(m['sentiment_score'] for m in mentions) / len(mentions)\n        sentiment_anomaly = self.anomaly_detector.detect_sentiment_drop(avg_sentiment)\n\n        # Check negative ratio\n        negative_count = sum(1 for m in mentions if m['sentiment_score'] &lt; -0.2)\n        ratio_anomaly = self.anomaly_detector.detect_negative_ratio_spike(\n            negative_count, len(mentions)\n        )\n\n        # If any anomaly detected, create crisis event\n        anomaly = volume_anomaly or sentiment_anomaly or ratio_anomaly\n        if not anomaly:\n            return None\n\n        # Classify severity\n        severity = self.severity_classifier.classify(\n            anomaly['type'],\n            mentions,\n            anomaly\n        )\n\n        # Only escalate MEDIUM+ severity\n        if severity == CrisisSeverity.LOW:\n            return None\n\n        # Get trigger tweets (most negative or most engaging)\n        trigger_tweets = sorted(\n            mentions,\n            key=lambda m: m['engagement_score'] - m['sentiment_score'] * 100,\n            reverse=True\n        )[:10]\n\n        crisis = CrisisEvent(\n            id=f\"crisis_{uuid.uuid4().hex[:8]}\",\n            crisis_type=anomaly['type'],\n            severity=severity,\n            detected_at=datetime.now(),\n            trigger_tweets=[t['tweet_id'] for t in trigger_tweets],\n            affected_keywords=self.brand_keywords,\n            metrics={\n                'total_mentions': len(mentions),\n                'avg_sentiment': avg_sentiment,\n                'negative_count': negative_count,\n                'anomaly_data': anomaly\n            }\n        )\n\n        return crisis\n\n    async def _handle_crisis(self, crisis: CrisisEvent):\n        \"\"\"Handle detected crisis.\"\"\"\n        print(f\"\\n\ud83d\udea8 CRISIS DETECTED: {crisis.crisis_type.value}\")\n        print(f\"   Severity: {crisis.severity.value}\")\n        print(f\"   Trigger tweets: {len(crisis.trigger_tweets)}\")\n\n        # Track active crisis\n        self.active_crises[crisis.id] = crisis\n\n        # Escalate\n        await self.escalation.escalate(crisis)\n\n        # Track response time\n        self.response_times.append(0)  # Response starts now\n\n    def get_crisis_summary(self) -&gt; dict:\n        \"\"\"Get summary of crisis activity.\"\"\"\n        return {\n            'active_crises': len(self.active_crises),\n            'crises_today': len([\n                c for c in self.active_crises.values()\n                if c.detected_at.date() == datetime.now().date()\n            ]),\n            'avg_response_time': (\n                sum(self.response_times) / len(self.response_times)\n                if self.response_times else 0\n            ),\n            'baseline_available': self.anomaly_detector.baseline is not None\n        }\n</code></pre>"},{"location":"cookbook/business/crisis-detection/#usage-example","title":"Usage Example","text":"<pre><code># main.py\nimport asyncio\nfrom crisis_detector import CrisisDetector\n\nasync def main():\n    detector = CrisisDetector(\n        brand_keywords=[\n            \"YourBrand\",\n            \"@YourBrand\",\n            \"#YourBrand\",\n            \"yourbrand.com\"\n        ],\n        discord_webhook=\"https://discord.com/api/webhooks/...\",\n        check_interval=60  # Check every minute\n    )\n\n    await detector.monitor()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"cookbook/business/crisis-detection/#post-crisis-analysis","title":"Post-Crisis Analysis","text":"<pre><code># crisis_analysis.py\nfrom datetime import datetime, timedelta\nfrom typing import Optional\n\nclass CrisisAnalyzer:\n    \"\"\"Analyze crisis events for learnings.\"\"\"\n\n    def analyze_crisis(\n        self, \n        crisis: CrisisEvent, \n        all_mentions: list[dict]\n    ) -&gt; dict:\n        \"\"\"Generate post-crisis analysis report.\"\"\"\n\n        # Timeline analysis\n        timeline = self._build_timeline(crisis, all_mentions)\n\n        # Key actors\n        top_spreaders = self._identify_spreaders(all_mentions)\n\n        # Content themes\n        themes = self._extract_themes(all_mentions)\n\n        # Response effectiveness\n        response_metrics = self._calculate_response_metrics(crisis, all_mentions)\n\n        return {\n            'crisis_id': crisis.id,\n            'duration_hours': (\n                (crisis.resolved_at or datetime.now()) - crisis.detected_at\n            ).total_seconds() / 3600,\n            'peak_negative_mentions': max(\n                self._hourly_negative_count(all_mentions)\n            ),\n            'total_reach': sum(m['author_followers'] for m in all_mentions),\n            'timeline': timeline,\n            'top_spreaders': top_spreaders,\n            'themes': themes,\n            'response_metrics': response_metrics,\n            'recommendations': self._generate_recommendations(\n                themes, top_spreaders, response_metrics\n            )\n        }\n\n    def _generate_recommendations(self, themes, spreaders, metrics) -&gt; list[str]:\n        \"\"\"Generate actionable recommendations.\"\"\"\n        recommendations = []\n\n        if metrics.get('response_time_minutes', 0) &gt; 30:\n            recommendations.append(\n                \"Improve response time - aim for &lt;15 minutes for high severity\"\n            )\n\n        if len(spreaders) &gt; 0 and spreaders[0]['followers'] &gt; 50000:\n            recommendations.append(\n                f\"Prioritize direct outreach to @{spreaders[0]['username']}\"\n            )\n\n        return recommendations\n</code></pre>"},{"location":"cookbook/business/crisis-detection/#best-practices","title":"Best Practices","text":"<p>False Positive Management</p> <ul> <li>Start with conservative thresholds</li> <li>Require multiple signals before CRITICAL</li> <li>Review and tune weekly</li> <li>Track false positive rate</li> </ul> <p>Response Readiness</p> <ul> <li>Pre-write response templates</li> <li>Define escalation contacts</li> <li>Practice crisis drills</li> <li>Document decision authority</li> </ul>"},{"location":"cookbook/business/crisis-detection/#related-recipes","title":"Related Recipes","text":"<ul> <li>Brand Monitoring - Continuous monitoring</li> <li>Influencer Mapping - Identify key voices</li> <li>Sentiment Analysis - Deep analysis</li> </ul>"},{"location":"cookbook/business/influencer-mapping/","title":"Influencer Network Mapping &amp; Scoring","text":"<p>Build a comprehensive influencer discovery and scoring system using network analysis and engagement metrics.</p>"},{"location":"cookbook/business/influencer-mapping/#overview","title":"Overview","text":"<p>This recipe creates an influencer intelligence system with:</p> <ul> <li>Network graph construction - Map relationships between accounts</li> <li>PageRank-style scoring - Measure true influence</li> <li>Engagement authenticity - Detect fake engagement</li> <li>Niche categorization - Classify influencer topics</li> <li>Outreach prioritization - Rank by ROI potential</li> <li>Visualization - Interactive network graphs</li> </ul>"},{"location":"cookbook/business/influencer-mapping/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Profile        \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Network    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Influence     \u2502\n\u2502  Scraper        \u2502     \u2502   Builder    \u2502     \u2502   Calculator    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                       \u2502                     \u2502\n        \u25bc                       \u25bc                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Engagement     \u2502     \u2502   Graph      \u2502     \u2502   Niche         \u2502\n\u2502  Analyzer       \u2502     \u2502   Storage    \u2502     \u2502   Classifier    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502   Visualizer \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cookbook/business/influencer-mapping/#data-models","title":"Data Models","text":"<pre><code># influencer_models.py\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Optional\n\n@dataclass\nclass InfluencerProfile:\n    user_id: str\n    username: str\n    display_name: str\n    followers: int\n    following: int\n    tweet_count: int\n    created_at: datetime\n    bio: str\n    location: Optional[str]\n    website: Optional[str]\n    verified: bool\n    profile_image: str\n\n    # Calculated metrics\n    follower_ratio: float = 0.0\n    avg_engagement_rate: float = 0.0\n    influence_score: float = 0.0\n    authenticity_score: float = 0.0\n    niche_categories: list[str] = field(default_factory=list)\n\n    def __post_init__(self):\n        if self.following &gt; 0:\n            self.follower_ratio = self.followers / self.following\n\n@dataclass\nclass InfluencerConnection:\n    source_id: str\n    target_id: str\n    connection_type: str  # follows, mentioned, replied, retweeted\n    weight: float = 1.0\n    timestamp: Optional[datetime] = None\n\n@dataclass\nclass EngagementMetrics:\n    user_id: str\n    avg_likes: float\n    avg_retweets: float\n    avg_replies: float\n    engagement_rate: float\n    reply_ratio: float  # replies received vs given\n    unique_engagers: int\n    top_engager_concentration: float  # % from top 10 engagers\n</code></pre>"},{"location":"cookbook/business/influencer-mapping/#network-builder","title":"Network Builder","text":"<pre><code># network_builder.py\nimport asyncio\nfrom collections import defaultdict\nfrom typing import Optional\nfrom datetime import datetime, timedelta\n\nfrom xeepy import Xeepy\nfrom influencer_models import InfluencerProfile, InfluencerConnection\n\nclass NetworkBuilder:\n    \"\"\"Build influencer relationship network.\"\"\"\n\n    def __init__(self, niche_keywords: list[str]):\n        self.niche_keywords = niche_keywords\n        self.profiles: dict[str, InfluencerProfile] = {}\n        self.connections: list[InfluencerConnection] = []\n\n    async def discover_influencers(\n        self,\n        seed_accounts: list[str],\n        depth: int = 2,\n        min_followers: int = 5000\n    ) -&gt; list[InfluencerProfile]:\n        \"\"\"Discover influencers starting from seed accounts.\"\"\"\n        discovered = set(seed_accounts)\n        to_process = list(seed_accounts)\n\n        for current_depth in range(depth):\n            print(f\"Processing depth {current_depth + 1}/{depth}\")\n            next_level = []\n\n            async with Xeepy() as x:\n                for username in to_process:\n                    try:\n                        # Get profile\n                        profile = await self._fetch_profile(x, username)\n                        if profile and profile.followers &gt;= min_followers:\n                            self.profiles[profile.user_id] = profile\n\n                            # Get connections\n                            connections = await self._fetch_connections(\n                                x, username, min_followers\n                            )\n\n                            for conn in connections:\n                                self.connections.append(conn)\n                                if conn.target_id not in discovered:\n                                    discovered.add(conn.target_id)\n                                    next_level.append(conn.target_id)\n\n                        await asyncio.sleep(1)  # Rate limiting\n\n                    except Exception as e:\n                        print(f\"Error processing {username}: {e}\")\n\n            to_process = next_level[:100]  # Limit breadth\n\n        return list(self.profiles.values())\n\n    async def _fetch_profile(\n        self, \n        x: Xeepy, \n        username: str\n    ) -&gt; Optional[InfluencerProfile]:\n        \"\"\"Fetch user profile.\"\"\"\n        try:\n            profile = await x.scrape.profile(username)\n\n            return InfluencerProfile(\n                user_id=profile.id,\n                username=profile.username,\n                display_name=profile.name,\n                followers=profile.followers_count,\n                following=profile.following_count,\n                tweet_count=profile.tweet_count,\n                created_at=profile.created_at,\n                bio=profile.bio or \"\",\n                location=profile.location,\n                website=profile.website,\n                verified=profile.verified,\n                profile_image=profile.profile_image_url\n            )\n        except Exception:\n            return None\n\n    async def _fetch_connections(\n        self,\n        x: Xeepy,\n        username: str,\n        min_followers: int\n    ) -&gt; list[InfluencerConnection]:\n        \"\"\"Fetch connections for a user.\"\"\"\n        connections = []\n\n        # Get recent tweets to find mentions and interactions\n        tweets = await x.scrape.tweets(username, limit=50)\n\n        user_profile = self.profiles.get(username)\n        if not user_profile:\n            return connections\n\n        mentioned_users = set()\n        for tweet in tweets:\n            # Extract mentions\n            for mention in tweet.mentions:\n                if mention not in mentioned_users:\n                    mentioned_users.add(mention)\n                    connections.append(InfluencerConnection(\n                        source_id=user_profile.user_id,\n                        target_id=mention,\n                        connection_type='mentioned',\n                        weight=1.0,\n                        timestamp=tweet.created_at\n                    ))\n\n            # Track retweets\n            if tweet.is_retweet and tweet.retweeted_user:\n                connections.append(InfluencerConnection(\n                    source_id=user_profile.user_id,\n                    target_id=tweet.retweeted_user,\n                    connection_type='retweeted',\n                    weight=1.5,  # Higher weight for retweets\n                    timestamp=tweet.created_at\n                ))\n\n        return connections\n</code></pre>"},{"location":"cookbook/business/influencer-mapping/#influence-calculator-pagerank-style","title":"Influence Calculator (PageRank-Style)","text":"<pre><code># influence_calculator.py\nimport math\nfrom collections import defaultdict\n\nclass InfluenceCalculator:\n    \"\"\"Calculate influence scores using PageRank-style algorithm.\"\"\"\n\n    def __init__(self, damping_factor: float = 0.85, iterations: int = 50):\n        self.damping_factor = damping_factor\n        self.iterations = iterations\n\n    def calculate_pagerank(\n        self,\n        profiles: dict[str, 'InfluencerProfile'],\n        connections: list['InfluencerConnection']\n    ) -&gt; dict[str, float]:\n        \"\"\"Calculate PageRank-style influence scores.\"\"\"\n\n        # Build adjacency lists\n        outgoing = defaultdict(list)\n        incoming = defaultdict(list)\n\n        for conn in connections:\n            if conn.source_id in profiles and conn.target_id in profiles:\n                outgoing[conn.source_id].append((conn.target_id, conn.weight))\n                incoming[conn.target_id].append((conn.source_id, conn.weight))\n\n        # Initialize scores\n        n = len(profiles)\n        scores = {uid: 1.0 / n for uid in profiles}\n\n        # Iterate\n        for _ in range(self.iterations):\n            new_scores = {}\n\n            for user_id in profiles:\n                # Base score (random jump)\n                score = (1 - self.damping_factor) / n\n\n                # Score from incoming connections\n                for source_id, weight in incoming[user_id]:\n                    out_count = len(outgoing[source_id])\n                    if out_count &gt; 0:\n                        score += (\n                            self.damping_factor * \n                            scores[source_id] * \n                            weight / \n                            out_count\n                        )\n\n                new_scores[user_id] = score\n\n            # Normalize\n            total = sum(new_scores.values())\n            scores = {uid: s / total for uid, s in new_scores.items()}\n\n        return scores\n\n    def calculate_composite_score(\n        self,\n        profile: 'InfluencerProfile',\n        pagerank: float,\n        engagement_metrics: 'EngagementMetrics'\n    ) -&gt; float:\n        \"\"\"Calculate composite influence score.\"\"\"\n\n        # Components (normalized to 0-100)\n        reach_score = min(100, math.log10(profile.followers + 1) * 15)\n\n        pagerank_score = min(100, pagerank * 10000)\n\n        engagement_score = min(100, engagement_metrics.engagement_rate * 1000)\n\n        authenticity_score = min(100, (\n            (1 - engagement_metrics.top_engager_concentration) * 50 +\n            min(50, engagement_metrics.unique_engagers / 10)\n        ))\n\n        # Weighted combination\n        composite = (\n            reach_score * 0.25 +\n            pagerank_score * 0.30 +\n            engagement_score * 0.25 +\n            authenticity_score * 0.20\n        )\n\n        return round(composite, 2)\n</code></pre>"},{"location":"cookbook/business/influencer-mapping/#engagement-authenticity-analyzer","title":"Engagement Authenticity Analyzer","text":"<pre><code># authenticity_analyzer.py\nfrom collections import Counter\nfrom datetime import datetime, timedelta\nfrom typing import Optional\n\nfrom xeepy import Xeepy\nfrom influencer_models import EngagementMetrics\n\nclass AuthenticityAnalyzer:\n    \"\"\"Analyze engagement authenticity.\"\"\"\n\n    async def analyze_engagement(\n        self,\n        username: str,\n        tweet_count: int = 20\n    ) -&gt; EngagementMetrics:\n        \"\"\"Analyze engagement patterns for authenticity.\"\"\"\n\n        async with Xeepy() as x:\n            # Get recent tweets with engagement\n            tweets = await x.scrape.tweets(username, limit=tweet_count)\n\n            if not tweets:\n                return self._empty_metrics(username)\n\n            # Calculate averages\n            likes = [t.like_count for t in tweets]\n            retweets = [t.retweet_count for t in tweets]\n            replies = [t.reply_count for t in tweets]\n\n            avg_likes = sum(likes) / len(likes)\n            avg_retweets = sum(retweets) / len(retweets)\n            avg_replies = sum(replies) / len(replies)\n\n            # Get follower count for engagement rate\n            profile = await x.scrape.profile(username)\n            followers = profile.followers_count\n\n            avg_engagement = avg_likes + avg_retweets + avg_replies\n            engagement_rate = avg_engagement / max(followers, 1)\n\n            # Analyze engager diversity\n            engager_analysis = await self._analyze_engagers(x, tweets[:5])\n\n            return EngagementMetrics(\n                user_id=profile.id,\n                avg_likes=avg_likes,\n                avg_retweets=avg_retweets,\n                avg_replies=avg_replies,\n                engagement_rate=engagement_rate,\n                reply_ratio=avg_replies / max(avg_likes, 1),\n                unique_engagers=engager_analysis['unique_count'],\n                top_engager_concentration=engager_analysis['concentration']\n            )\n\n    async def _analyze_engagers(\n        self, \n        x: Xeepy, \n        tweets: list\n    ) -&gt; dict:\n        \"\"\"Analyze who engages with tweets.\"\"\"\n        all_engagers = []\n\n        for tweet in tweets:\n            try:\n                # Get users who liked\n                likers = await x.scrape.likes(\n                    f\"https://x.com/i/status/{tweet.id}\",\n                    limit=50\n                )\n                all_engagers.extend([u.username for u in likers])\n            except Exception:\n                pass\n\n        if not all_engagers:\n            return {'unique_count': 0, 'concentration': 1.0}\n\n        # Count unique engagers\n        engager_counts = Counter(all_engagers)\n        unique_count = len(engager_counts)\n\n        # Calculate concentration (% from top 10)\n        total_engagements = len(all_engagers)\n        top_10_engagements = sum(count for _, count in engager_counts.most_common(10))\n        concentration = top_10_engagements / total_engagements\n\n        return {\n            'unique_count': unique_count,\n            'concentration': concentration\n        }\n\n    def _empty_metrics(self, user_id: str) -&gt; EngagementMetrics:\n        return EngagementMetrics(\n            user_id=user_id,\n            avg_likes=0,\n            avg_retweets=0,\n            avg_replies=0,\n            engagement_rate=0,\n            reply_ratio=0,\n            unique_engagers=0,\n            top_engager_concentration=1.0\n        )\n\n    def calculate_authenticity_score(self, metrics: EngagementMetrics) -&gt; float:\n        \"\"\"Calculate authenticity score (0-100).\"\"\"\n        score = 100.0\n\n        # Red flags\n\n        # Very high engagement rate (&gt; 10%) is suspicious\n        if metrics.engagement_rate &gt; 0.10:\n            score -= 20\n\n        # High concentration in top engagers\n        if metrics.top_engager_concentration &gt; 0.5:\n            score -= 30 * metrics.top_engager_concentration\n\n        # Very low unique engagers\n        if metrics.unique_engagers &lt; 10:\n            score -= 20\n\n        # Abnormal like-to-reply ratio\n        if metrics.avg_likes &gt; 0 and metrics.reply_ratio &lt; 0.01:\n            score -= 15  # Likes but no conversation\n\n        return max(0, min(100, score))\n</code></pre>"},{"location":"cookbook/business/influencer-mapping/#niche-classifier","title":"Niche Classifier","text":"<pre><code># niche_classifier.py\nfrom collections import Counter\nimport re\n\nclass NicheClassifier:\n    \"\"\"Classify influencers into niche categories.\"\"\"\n\n    NICHE_KEYWORDS = {\n        'tech': ['tech', 'software', 'coding', 'programming', 'developer', \n                 'startup', 'ai', 'ml', 'data', 'cloud', 'saas'],\n        'crypto': ['crypto', 'bitcoin', 'btc', 'eth', 'web3', 'defi', \n                   'nft', 'blockchain', 'token', 'dao'],\n        'finance': ['finance', 'investing', 'stocks', 'trading', 'market',\n                    'portfolio', 'wealth', 'money', 'fintech'],\n        'marketing': ['marketing', 'growth', 'seo', 'content', 'brand',\n                      'social media', 'ads', 'copywriting', 'funnel'],\n        'design': ['design', 'ui', 'ux', 'figma', 'creative', 'visual',\n                   'branding', 'graphic', 'illustration'],\n        'gaming': ['gaming', 'games', 'esports', 'streamer', 'twitch',\n                   'playstation', 'xbox', 'nintendo'],\n        'fitness': ['fitness', 'workout', 'gym', 'health', 'nutrition',\n                    'wellness', 'training', 'muscle'],\n        'lifestyle': ['lifestyle', 'travel', 'food', 'fashion', 'luxury',\n                      'photography', 'adventure'],\n    }\n\n    def classify(\n        self, \n        bio: str, \n        tweets: list[str]\n    ) -&gt; list[tuple[str, float]]:\n        \"\"\"Classify into niches with confidence scores.\"\"\"\n\n        # Combine text\n        all_text = (bio + \" \" + \" \".join(tweets)).lower()\n\n        # Count keyword matches\n        niche_scores = {}\n        for niche, keywords in self.NICHE_KEYWORDS.items():\n            matches = sum(1 for kw in keywords if kw in all_text)\n            if matches &gt; 0:\n                niche_scores[niche] = matches / len(keywords)\n\n        # Sort by score\n        sorted_niches = sorted(\n            niche_scores.items(), \n            key=lambda x: x[1], \n            reverse=True\n        )\n\n        return sorted_niches[:3]  # Top 3 niches\n\n    def extract_topics(self, tweets: list[str]) -&gt; list[str]:\n        \"\"\"Extract common topics from tweets.\"\"\"\n        # Extract hashtags\n        hashtags = []\n        for tweet in tweets:\n            hashtags.extend(re.findall(r'#(\\w+)', tweet.lower()))\n\n        # Most common\n        counter = Counter(hashtags)\n        return [tag for tag, _ in counter.most_common(10)]\n</code></pre>"},{"location":"cookbook/business/influencer-mapping/#outreach-prioritization","title":"Outreach Prioritization","text":"<pre><code># outreach_prioritizer.py\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass OutreachCandidate:\n    profile: 'InfluencerProfile'\n    priority_score: float\n    estimated_roi: float\n    outreach_angle: str\n    contact_method: str\n    notes: list[str]\n\nclass OutreachPrioritizer:\n    \"\"\"Prioritize influencers for outreach.\"\"\"\n\n    def __init__(\n        self,\n        target_niches: list[str],\n        budget_per_influencer: float = 500,\n        min_followers: int = 5000,\n        max_followers: int = 500000\n    ):\n        self.target_niches = target_niches\n        self.budget = budget_per_influencer\n        self.min_followers = min_followers\n        self.max_followers = max_followers\n\n    def prioritize(\n        self,\n        profiles: list['InfluencerProfile'],\n        engagement_metrics: dict[str, 'EngagementMetrics']\n    ) -&gt; list[OutreachCandidate]:\n        \"\"\"Generate prioritized outreach list.\"\"\"\n\n        candidates = []\n\n        for profile in profiles:\n            # Filter by follower range\n            if not (self.min_followers &lt;= profile.followers &lt;= self.max_followers):\n                continue\n\n            # Check niche match\n            niche_match = any(\n                niche in profile.niche_categories \n                for niche in self.target_niches\n            )\n            if not niche_match:\n                continue\n\n            # Get engagement metrics\n            metrics = engagement_metrics.get(profile.user_id)\n\n            # Calculate priority score\n            priority = self._calculate_priority(profile, metrics)\n\n            # Estimate ROI\n            roi = self._estimate_roi(profile, metrics)\n\n            # Determine outreach angle\n            angle = self._determine_angle(profile)\n\n            candidates.append(OutreachCandidate(\n                profile=profile,\n                priority_score=priority,\n                estimated_roi=roi,\n                outreach_angle=angle,\n                contact_method='dm' if profile.followers &lt; 50000 else 'email',\n                notes=self._generate_notes(profile, metrics)\n            ))\n\n        # Sort by priority\n        candidates.sort(key=lambda c: c.priority_score, reverse=True)\n\n        return candidates\n\n    def _calculate_priority(\n        self, \n        profile: 'InfluencerProfile',\n        metrics: Optional['EngagementMetrics']\n    ) -&gt; float:\n        \"\"\"Calculate outreach priority score.\"\"\"\n        score = 0.0\n\n        # Influence score\n        score += profile.influence_score * 0.3\n\n        # Engagement rate\n        if metrics:\n            score += min(30, metrics.engagement_rate * 1000)\n\n        # Authenticity\n        score += profile.authenticity_score * 0.2\n\n        # Follower sweet spot (10k-100k)\n        if 10000 &lt;= profile.followers &lt;= 100000:\n            score += 20\n\n        return score\n\n    def _estimate_roi(\n        self,\n        profile: 'InfluencerProfile',\n        metrics: Optional['EngagementMetrics']\n    ) -&gt; float:\n        \"\"\"Estimate ROI for partnership.\"\"\"\n        if not metrics:\n            return 0.0\n\n        # Estimated reach per post\n        reach = profile.followers * 0.1  # 10% reach assumption\n\n        # Estimated engagements\n        engagements = reach * metrics.engagement_rate\n\n        # Value per engagement (adjust based on your metrics)\n        value_per_engagement = 0.50\n\n        estimated_value = engagements * value_per_engagement\n        roi = (estimated_value - self.budget) / self.budget * 100\n\n        return round(roi, 2)\n\n    def _determine_angle(self, profile: 'InfluencerProfile') -&gt; str:\n        \"\"\"Determine best outreach angle.\"\"\"\n        if 'tech' in profile.niche_categories:\n            return \"product_review\"\n        elif 'marketing' in profile.niche_categories:\n            return \"case_study\"\n        elif 'crypto' in profile.niche_categories:\n            return \"partnership\"\n        else:\n            return \"sponsored_content\"\n\n    def _generate_notes(\n        self,\n        profile: 'InfluencerProfile',\n        metrics: Optional['EngagementMetrics']\n    ) -&gt; list[str]:\n        \"\"\"Generate outreach notes.\"\"\"\n        notes = []\n\n        if profile.authenticity_score &gt; 80:\n            notes.append(\"\u2713 High authenticity - genuine engagement\")\n\n        if metrics and metrics.engagement_rate &gt; 0.05:\n            notes.append(\"\u2713 Excellent engagement rate\")\n\n        if profile.follower_ratio &gt; 10:\n            notes.append(\"\u2713 Strong follower ratio\")\n\n        return notes\n</code></pre>"},{"location":"cookbook/business/influencer-mapping/#network-visualization","title":"Network Visualization","text":"<pre><code># network_visualizer.py\nimport json\nfrom typing import Optional\n\nclass NetworkVisualizer:\n    \"\"\"Generate network visualizations.\"\"\"\n\n    def generate_d3_json(\n        self,\n        profiles: dict[str, 'InfluencerProfile'],\n        connections: list['InfluencerConnection']\n    ) -&gt; str:\n        \"\"\"Generate D3.js compatible JSON.\"\"\"\n\n        nodes = []\n        for uid, profile in profiles.items():\n            nodes.append({\n                'id': uid,\n                'name': profile.username,\n                'followers': profile.followers,\n                'influence': profile.influence_score,\n                'niches': profile.niche_categories,\n                'group': profile.niche_categories[0] if profile.niche_categories else 'other'\n            })\n\n        links = []\n        for conn in connections:\n            if conn.source_id in profiles and conn.target_id in profiles:\n                links.append({\n                    'source': conn.source_id,\n                    'target': conn.target_id,\n                    'type': conn.connection_type,\n                    'weight': conn.weight\n                })\n\n        return json.dumps({'nodes': nodes, 'links': links}, indent=2)\n\n    def generate_html_visualization(\n        self,\n        profiles: dict[str, 'InfluencerProfile'],\n        connections: list['InfluencerConnection'],\n        output_file: str = \"network.html\"\n    ):\n        \"\"\"Generate interactive HTML visualization.\"\"\"\n\n        data = self.generate_d3_json(profiles, connections)\n\n        html = f\"\"\"\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Influencer Network&lt;/title&gt;\n    &lt;script src=\"https://d3js.org/d3.v7.min.js\"&gt;&lt;/script&gt;\n    &lt;style&gt;\n        body {{ margin: 0; font-family: Arial, sans-serif; }}\n        svg {{ width: 100vw; height: 100vh; }}\n        .node {{ cursor: pointer; }}\n        .link {{ stroke: #999; stroke-opacity: 0.6; }}\n        .tooltip {{ \n            position: absolute; padding: 10px; \n            background: white; border: 1px solid #ddd;\n            border-radius: 4px; pointer-events: none;\n        }}\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;svg&gt;&lt;/svg&gt;\n    &lt;script&gt;\n        const data = {data};\n\n        const svg = d3.select(\"svg\");\n        const width = window.innerWidth;\n        const height = window.innerHeight;\n\n        const color = d3.scaleOrdinal(d3.schemeCategory10);\n\n        const simulation = d3.forceSimulation(data.nodes)\n            .force(\"link\", d3.forceLink(data.links).id(d =&gt; d.id))\n            .force(\"charge\", d3.forceManyBody().strength(-100))\n            .force(\"center\", d3.forceCenter(width / 2, height / 2));\n\n        const link = svg.append(\"g\")\n            .selectAll(\"line\")\n            .data(data.links)\n            .join(\"line\")\n            .attr(\"class\", \"link\")\n            .attr(\"stroke-width\", d =&gt; d.weight);\n\n        const node = svg.append(\"g\")\n            .selectAll(\"circle\")\n            .data(data.nodes)\n            .join(\"circle\")\n            .attr(\"class\", \"node\")\n            .attr(\"r\", d =&gt; Math.sqrt(d.followers) / 100 + 5)\n            .attr(\"fill\", d =&gt; color(d.group))\n            .call(drag(simulation));\n\n        node.append(\"title\")\n            .text(d =&gt; `@${{d.name}} (${{d.followers.toLocaleString()}} followers)`);\n\n        simulation.on(\"tick\", () =&gt; {{\n            link\n                .attr(\"x1\", d =&gt; d.source.x)\n                .attr(\"y1\", d =&gt; d.source.y)\n                .attr(\"x2\", d =&gt; d.target.x)\n                .attr(\"y2\", d =&gt; d.target.y);\n\n            node\n                .attr(\"cx\", d =&gt; d.x)\n                .attr(\"cy\", d =&gt; d.y);\n        }});\n\n        function drag(simulation) {{\n            return d3.drag()\n                .on(\"start\", (event, d) =&gt; {{\n                    if (!event.active) simulation.alphaTarget(0.3).restart();\n                    d.fx = d.x; d.fy = d.y;\n                }})\n                .on(\"drag\", (event, d) =&gt; {{\n                    d.fx = event.x; d.fy = event.y;\n                }})\n                .on(\"end\", (event, d) =&gt; {{\n                    if (!event.active) simulation.alphaTarget(0);\n                    d.fx = null; d.fy = null;\n                }});\n        }}\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\"\"\"\n\n        with open(output_file, 'w') as f:\n            f.write(html)\n\n        print(f\"Visualization saved to {output_file}\")\n</code></pre>"},{"location":"cookbook/business/influencer-mapping/#complete-usage-example","title":"Complete Usage Example","text":"<pre><code># main.py\nimport asyncio\nfrom network_builder import NetworkBuilder\nfrom influence_calculator import InfluenceCalculator\nfrom authenticity_analyzer import AuthenticityAnalyzer\nfrom niche_classifier import NicheClassifier\nfrom outreach_prioritizer import OutreachPrioritizer\nfrom network_visualizer import NetworkVisualizer\n\nasync def main():\n    # 1. Build network from seed accounts\n    builder = NetworkBuilder(niche_keywords=['tech', 'startup', 'ai'])\n\n    profiles = await builder.discover_influencers(\n        seed_accounts=['elonmusk', 'paulg', 'naval'],\n        depth=2,\n        min_followers=10000\n    )\n\n    print(f\"Discovered {len(profiles)} influencers\")\n\n    # 2. Calculate influence scores\n    calculator = InfluenceCalculator()\n    pagerank_scores = calculator.calculate_pagerank(\n        builder.profiles,\n        builder.connections\n    )\n\n    # 3. Analyze engagement authenticity\n    analyzer = AuthenticityAnalyzer()\n    engagement_metrics = {}\n\n    for profile in profiles[:20]:  # Top 20\n        metrics = await analyzer.analyze_engagement(profile.username)\n        engagement_metrics[profile.user_id] = metrics\n        profile.authenticity_score = analyzer.calculate_authenticity_score(metrics)\n\n    # 4. Classify niches\n    classifier = NicheClassifier()\n    # (Would need tweets for full classification)\n\n    # 5. Calculate composite scores\n    for profile in profiles:\n        if profile.user_id in engagement_metrics:\n            profile.influence_score = calculator.calculate_composite_score(\n                profile,\n                pagerank_scores.get(profile.user_id, 0),\n                engagement_metrics[profile.user_id]\n            )\n\n    # 6. Generate outreach list\n    prioritizer = OutreachPrioritizer(\n        target_niches=['tech', 'startup'],\n        budget_per_influencer=500\n    )\n\n    candidates = prioritizer.prioritize(profiles, engagement_metrics)\n\n    print(\"\\nTop Outreach Candidates:\")\n    for i, c in enumerate(candidates[:10], 1):\n        print(f\"{i}. @{c.profile.username}\")\n        print(f\"   Score: {c.priority_score:.1f} | ROI: {c.estimated_roi:.1f}%\")\n        print(f\"   Angle: {c.outreach_angle}\")\n\n    # 7. Generate visualization\n    visualizer = NetworkVisualizer()\n    visualizer.generate_html_visualization(\n        builder.profiles,\n        builder.connections,\n        \"influencer_network.html\"\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"cookbook/business/influencer-mapping/#best-practices","title":"Best Practices","text":"<p>Network Discovery</p> <ul> <li>Start with 3-5 well-connected seed accounts</li> <li>Limit depth to 2-3 to avoid noise</li> <li>Filter by minimum engagement rate</li> </ul> <p>Authenticity Checks</p> <ul> <li>Always verify engagement authenticity</li> <li>Look for engagement pods</li> <li>Check follower growth patterns</li> </ul>"},{"location":"cookbook/business/influencer-mapping/#related-recipes","title":"Related Recipes","text":"<ul> <li>Brand Monitoring - Track brand mentions</li> <li>Hashtag Strategy - Find niche hashtags</li> <li>Engagement Analytics - Deep metrics</li> </ul>"},{"location":"cookbook/business/lead-generation/","title":"Lead Generation Pipeline","text":"<p>Turn X/Twitter into a lead generation machine.</p>"},{"location":"cookbook/business/lead-generation/#the-lead-generation-framework","title":"The Lead Generation Framework","text":"<pre><code>flowchart LR\n    A[\ud83d\udd0d Discovery] --&gt; B[\ud83d\udcca Qualification]\n    B --&gt; C[\ud83d\udcac Engagement]\n    C --&gt; D[\ud83c\udfaf Conversion]\n    D --&gt; E[\ud83d\udcc8 Nurture]\n\n    style A fill:#4ECDC4\n    style B fill:#45B7D1\n    style C fill:#96CEB4\n    style D fill:#FFEAA7\n    style E fill:#DDA0DD</code></pre>"},{"location":"cookbook/business/lead-generation/#intent-based-lead-discovery","title":"Intent-Based Lead Discovery","text":"<p>Find people actively looking for solutions:</p> <pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom enum import Enum\n\nclass LeadIntent(Enum):\n    HIGH = \"high\"      # Actively seeking solution\n    MEDIUM = \"medium\"  # Has problem, exploring\n    LOW = \"low\"        # General interest\n\n@dataclass\nclass Lead:\n    username: str\n    name: str\n    bio: str\n    followers: int\n    intent: LeadIntent\n    intent_signals: list\n    trigger_tweet: str\n    score: float\n    discovered_at: datetime\n\nasync def discover_high_intent_leads(\n    problem_keywords: list,\n    solution_keywords: list,\n    competitor_mentions: list = None,\n):\n    \"\"\"\n    Find leads showing buying intent signals:\n    - Asking \"how to\" questions\n    - Expressing frustration with current solutions\n    - Asking for recommendations\n    - Mentioning competitors negatively\n    \"\"\"\n\n    async with Xeepy() as x:\n        leads = []\n\n        # Intent signal patterns\n        high_intent_patterns = [\n            \"looking for\",\n            \"need help with\",\n            \"anyone recommend\",\n            \"what do you use for\",\n            \"best tool for\",\n            \"alternative to\",\n            \"switching from\",\n            \"frustrated with\",\n            \"hate using\",\n        ]\n\n        medium_intent_patterns = [\n            \"how do you\",\n            \"how to\",\n            \"tips for\",\n            \"advice on\",\n            \"struggling with\",\n        ]\n\n        # Search for problem keywords\n        for keyword in problem_keywords:\n            tweets = await x.scrape.search(\n                keyword,\n                search_type=\"latest\",\n                limit=100,\n                max_age_hours=48\n            )\n\n            for tweet in tweets:\n                text_lower = tweet.text.lower()\n                signals = []\n                intent = LeadIntent.LOW\n\n                # Check for high intent signals\n                for pattern in high_intent_patterns:\n                    if pattern in text_lower:\n                        signals.append(f\"high:{pattern}\")\n                        intent = LeadIntent.HIGH\n\n                # Check for medium intent signals\n                if intent != LeadIntent.HIGH:\n                    for pattern in medium_intent_patterns:\n                        if pattern in text_lower:\n                            signals.append(f\"medium:{pattern}\")\n                            intent = LeadIntent.MEDIUM\n\n                # Check for competitor mentions\n                if competitor_mentions:\n                    for comp in competitor_mentions:\n                        if comp.lower() in text_lower:\n                            signals.append(f\"competitor:{comp}\")\n                            if \"problem\" in text_lower or \"issue\" in text_lower:\n                                intent = LeadIntent.HIGH\n\n                if not signals:\n                    continue\n\n                # Score the lead\n                score = calculate_lead_score(tweet.author, intent, signals)\n\n                lead = Lead(\n                    username=tweet.author.username,\n                    name=tweet.author.name,\n                    bio=tweet.author.bio or \"\",\n                    followers=tweet.author.followers_count,\n                    intent=intent,\n                    intent_signals=signals,\n                    trigger_tweet=tweet.text,\n                    score=score,\n                    discovered_at=datetime.now()\n                )\n\n                leads.append(lead)\n\n        # Deduplicate and sort by score\n        seen = set()\n        unique_leads = []\n        for lead in leads:\n            if lead.username not in seen:\n                seen.add(lead.username)\n                unique_leads.append(lead)\n\n        unique_leads.sort(key=lambda x: x.score, reverse=True)\n\n        return unique_leads\n\ndef calculate_lead_score(author, intent: LeadIntent, signals: list) -&gt; float:\n    \"\"\"Score leads 0-100 based on quality indicators\"\"\"\n    score = 0\n\n    # Intent score (40 points max)\n    if intent == LeadIntent.HIGH:\n        score += 40\n    elif intent == LeadIntent.MEDIUM:\n        score += 20\n    else:\n        score += 5\n\n    # Signal count (20 points max)\n    score += min(len(signals) * 5, 20)\n\n    # Follower score (20 points max) - sweet spot is 500-10000\n    followers = author.followers_count\n    if 500 &lt;= followers &lt;= 10000:\n        score += 20\n    elif 100 &lt;= followers &lt;= 50000:\n        score += 10\n    elif followers &gt; 50000:\n        score += 5  # Too big, probably won't respond\n\n    # Profile quality (20 points max)\n    if author.bio:\n        score += 10\n    if author.has_profile_pic:\n        score += 5\n    if not author.is_default_profile:\n        score += 5\n\n    return score\n\n# Usage\nasync def main():\n    leads = await discover_high_intent_leads(\n        problem_keywords=[\n            \"twitter automation\",\n            \"social media scheduling\",\n            \"grow twitter following\",\n        ],\n        solution_keywords=[\n            \"automation tool\",\n            \"scheduling app\",\n            \"growth tool\",\n        ],\n        competitor_mentions=[\n            \"tweepy\",\n            \"buffer\",\n            \"hootsuite\",\n        ]\n    )\n\n    print(\"\ud83c\udfaf HIGH-INTENT LEADS DISCOVERED\")\n    print(\"=\"*70)\n\n    for lead in leads[:20]:\n        print(f\"\\n@{lead.username} | Score: {lead.score:.0f} | Intent: {lead.intent.value}\")\n        print(f\"   {lead.bio[:60]}...\" if lead.bio else \"   (no bio)\")\n        print(f\"   Signals: {', '.join(lead.intent_signals)}\")\n        print(f\"   Trigger: \\\"{lead.trigger_tweet[:100]}...\\\"\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"cookbook/business/lead-generation/#lead-qualification-pipeline","title":"Lead Qualification Pipeline","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\n\n@dataclass\nclass QualifiedLead:\n    lead: Lead\n    qualification_score: float\n    company_size: str  # startup, smb, enterprise\n    decision_maker: bool\n    budget_signals: list\n    urgency: str  # low, medium, high\n    fit_score: float\n\nasync def qualify_leads(leads: list, ideal_customer_profile: dict):\n    \"\"\"\n    Qualify leads based on ICP matching:\n    - Company size (from bio/job title)\n    - Role/decision making authority\n    - Budget indicators\n    - Urgency signals\n    \"\"\"\n\n    async with Xeepy() as x:\n        qualified = []\n\n        # Decision maker keywords\n        dm_keywords = [\n            \"founder\", \"ceo\", \"cto\", \"vp\", \"head of\",\n            \"director\", \"manager\", \"owner\", \"co-founder\"\n        ]\n\n        # Budget indicators\n        budget_signals_keywords = [\n            \"hiring\", \"growing\", \"funded\", \"series\",\n            \"enterprise\", \"team of\", \"employees\"\n        ]\n\n        for lead in leads:\n            # Get full profile\n            try:\n                profile = await x.scrape.profile(lead.username)\n                recent_tweets = await x.scrape.tweets(lead.username, limit=20)\n            except:\n                continue\n\n            bio_lower = (profile.bio or \"\").lower()\n\n            # Check decision maker status\n            is_dm = any(kw in bio_lower for kw in dm_keywords)\n\n            # Detect company size\n            if any(kw in bio_lower for kw in [\"enterprise\", \"fortune\"]):\n                company_size = \"enterprise\"\n            elif any(kw in bio_lower for kw in [\"startup\", \"founder\", \"indie\"]):\n                company_size = \"startup\"\n            else:\n                company_size = \"smb\"\n\n            # Find budget signals in tweets\n            budget_signals = []\n            for tweet in recent_tweets:\n                for kw in budget_signals_keywords:\n                    if kw in tweet.text.lower():\n                        budget_signals.append(kw)\n\n            # Calculate urgency\n            urgency_keywords = [\"asap\", \"urgent\", \"need now\", \"immediately\"]\n            urgency = \"high\" if any(kw in lead.trigger_tweet.lower() for kw in urgency_keywords) else \"medium\"\n\n            # Calculate fit score against ICP\n            fit_score = calculate_icp_fit(\n                lead, profile, company_size, is_dm, \n                ideal_customer_profile\n            )\n\n            # Overall qualification score\n            qual_score = (\n                lead.score * 0.4 +  # Intent\n                fit_score * 0.4 +   # ICP fit\n                (30 if is_dm else 0) +  # Decision maker bonus\n                (10 if urgency == \"high\" else 0)  # Urgency bonus\n            )\n\n            qualified.append(QualifiedLead(\n                lead=lead,\n                qualification_score=qual_score,\n                company_size=company_size,\n                decision_maker=is_dm,\n                budget_signals=list(set(budget_signals)),\n                urgency=urgency,\n                fit_score=fit_score\n            ))\n\n        # Sort by qualification score\n        qualified.sort(key=lambda x: x.qualification_score, reverse=True)\n\n        return qualified\n\ndef calculate_icp_fit(lead, profile, company_size, is_dm, icp: dict) -&gt; float:\n    \"\"\"Score 0-100 based on ICP match\"\"\"\n    score = 0\n\n    # Company size match\n    if company_size == icp.get(\"company_size\"):\n        score += 30\n    elif company_size in icp.get(\"acceptable_sizes\", []):\n        score += 15\n\n    # Industry match (from bio keywords)\n    bio = (profile.bio or \"\").lower()\n    for industry in icp.get(\"industries\", []):\n        if industry.lower() in bio:\n            score += 20\n            break\n\n    # Decision maker preference\n    if icp.get(\"decision_maker_required\") and is_dm:\n        score += 25\n    elif is_dm:\n        score += 15\n\n    # Follower range\n    min_f, max_f = icp.get(\"follower_range\", (0, float(\"inf\")))\n    if min_f &lt;= profile.followers_count &lt;= max_f:\n        score += 15\n\n    # Negative signals\n    for neg in icp.get(\"exclude_keywords\", []):\n        if neg.lower() in bio:\n            score -= 30\n\n    return max(0, min(100, score))\n\n# Usage\nasync def main():\n    leads = await discover_high_intent_leads(...)\n\n    icp = {\n        \"company_size\": \"startup\",\n        \"acceptable_sizes\": [\"smb\"],\n        \"industries\": [\"saas\", \"tech\", \"software\", \"marketing\"],\n        \"decision_maker_required\": True,\n        \"follower_range\": (500, 50000),\n        \"exclude_keywords\": [\"agency\", \"consultant\", \"freelance\"]\n    }\n\n    qualified = await qualify_leads(leads, icp)\n\n    print(\"\ud83c\udfaf QUALIFIED LEADS\")\n    print(\"=\"*70)\n\n    for ql in qualified[:10]:\n        print(f\"\\n@{ql.lead.username} | Qual Score: {ql.qualification_score:.0f}\")\n        print(f\"   Size: {ql.company_size} | DM: {'\u2713' if ql.decision_maker else '\u2717'} | Urgency: {ql.urgency}\")\n        print(f\"   Fit: {ql.fit_score:.0f} | Budget signals: {', '.join(ql.budget_signals) or 'none'}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"cookbook/business/lead-generation/#automated-outreach-sequences","title":"Automated Outreach Sequences","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom xeepy.ai import ContentGenerator\nfrom datetime import datetime, timedelta\n\nclass OutreachCampaign:\n    \"\"\"Automated multi-touch outreach campaign\"\"\"\n\n    def __init__(self, x: Xeepy, ai: ContentGenerator):\n        self.x = x\n        self.ai = ai\n        self.sequences = {}\n\n    async def start_sequence(self, lead: QualifiedLead, campaign_id: str):\n        \"\"\"\n        Start a multi-step outreach sequence:\n        1. Day 0: Helpful reply to their tweet\n        2. Day 1: Like 3 of their tweets\n        3. Day 3: Follow them\n        4. Day 5: Reply to another tweet\n        5. Day 7: DM if they followed back\n        \"\"\"\n\n        sequence = {\n            \"lead\": lead,\n            \"campaign_id\": campaign_id,\n            \"started_at\": datetime.now(),\n            \"step\": 0,\n            \"completed\": False,\n            \"engaged\": False,\n        }\n\n        self.sequences[lead.lead.username] = sequence\n\n        return await self.execute_step(lead, 0)\n\n    async def execute_step(self, lead: QualifiedLead, step: int):\n        \"\"\"Execute a single step in the sequence\"\"\"\n\n        username = lead.lead.username\n\n        if step == 0:\n            # Helpful reply to trigger tweet\n            reply = await self.ai.generate_reply(\n                tweet_text=lead.lead.trigger_tweet,\n                style=\"helpful\",\n                context=f\"This person is looking for {lead.lead.intent_signals}\",\n                include_value=True,\n                subtle_pitch=False,  # Not yet\n            )\n\n            # Find the original tweet\n            tweets = await self.x.scrape.tweets(username, limit=20)\n            trigger_tweet = None\n            for t in tweets:\n                if lead.lead.trigger_tweet[:50] in t.text:\n                    trigger_tweet = t\n                    break\n\n            if trigger_tweet:\n                await self.x.engage.reply(trigger_tweet.url, reply)\n                print(f\"Step 0: Replied to @{username}\")\n                return True\n\n        elif step == 1:\n            # Like 3 of their recent tweets\n            tweets = await self.x.scrape.tweets(username, limit=10)\n            liked = 0\n            for tweet in tweets[:5]:\n                if liked &gt;= 3:\n                    break\n                await self.x.engage.like(tweet.url)\n                liked += 1\n                await asyncio.sleep(2)\n            print(f\"Step 1: Liked {liked} tweets from @{username}\")\n            return True\n\n        elif step == 2:\n            # Follow them\n            await self.x.follow.user(username, source=\"lead_gen\")\n            print(f\"Step 2: Followed @{username}\")\n            return True\n\n        elif step == 3:\n            # Another helpful reply\n            tweets = await self.x.scrape.tweets(username, limit=10)\n\n            # Find a tweet we haven't replied to\n            for tweet in tweets:\n                reply = await self.ai.generate_reply(\n                    tweet_text=tweet.text,\n                    style=\"conversational\",\n                    build_relationship=True,\n                )\n                await self.x.engage.reply(tweet.url, reply)\n                print(f\"Step 3: Second reply to @{username}\")\n                return True\n\n        elif step == 4:\n            # DM if they followed back\n            profile = await self.x.scrape.profile(username)\n\n            if profile.following_you:\n                dm = await self.ai.generate_dm(\n                    context=f\"This person showed interest in: {lead.lead.intent_signals}\",\n                    bio=lead.lead.bio,\n                    style=\"friendly\",\n                    offer_value=True,\n                )\n                await self.x.dm.send(dm, username)\n                print(f\"Step 4: DM sent to @{username}\")\n                self.sequences[username][\"completed\"] = True\n                return True\n            else:\n                print(f\"Step 4: @{username} didn't follow back, sequence ended\")\n                self.sequences[username][\"completed\"] = True\n                return False\n\n        return False\n\n# Usage\nasync def run_outreach_campaign():\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\")\n        campaign = OutreachCampaign(x, ai)\n\n        # Get qualified leads\n        leads = await discover_high_intent_leads(...)\n        qualified = await qualify_leads(leads, icp)\n\n        # Start sequences for top leads\n        for lead in qualified[:10]:\n            await campaign.start_sequence(lead, \"q1_campaign\")\n            await asyncio.sleep(60)  # Space out initial contacts\n\n        # Run daily to progress sequences\n        while True:\n            for username, seq in campaign.sequences.items():\n                if seq[\"completed\"]:\n                    continue\n\n                days_since_start = (datetime.now() - seq[\"started_at\"]).days\n\n                # Progress to next step based on timing\n                step_timing = [0, 1, 3, 5, 7]  # Days for each step\n\n                for step, day in enumerate(step_timing):\n                    if days_since_start &gt;= day and seq[\"step\"] == step:\n                        await campaign.execute_step(seq[\"lead\"], step)\n                        seq[\"step\"] = step + 1\n                        break\n\n            await asyncio.sleep(3600)  # Check hourly\n\nasyncio.run(run_outreach_campaign())\n</code></pre>"},{"location":"cookbook/business/lead-generation/#lead-scoring-dashboard","title":"Lead Scoring Dashboard","text":"<pre><code>from xeepy.storage import Database\nfrom datetime import datetime\n\nclass LeadDatabase:\n    \"\"\"Track and score leads over time\"\"\"\n\n    def __init__(self, db_path: str = \"leads.db\"):\n        self.db = Database(db_path)\n        self._init_tables()\n\n    def _init_tables(self):\n        self.db.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS leads (\n                username TEXT PRIMARY KEY,\n                name TEXT,\n                bio TEXT,\n                followers INTEGER,\n                intent TEXT,\n                intent_signals TEXT,\n                trigger_tweet TEXT,\n                initial_score REAL,\n                current_score REAL,\n                status TEXT DEFAULT 'new',\n                discovered_at TIMESTAMP,\n                last_contact TIMESTAMP,\n                notes TEXT\n            )\n        \"\"\")\n\n        self.db.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS interactions (\n                id INTEGER PRIMARY KEY,\n                username TEXT,\n                type TEXT,\n                content TEXT,\n                response TEXT,\n                timestamp TIMESTAMP,\n                FOREIGN KEY (username) REFERENCES leads(username)\n            )\n        \"\"\")\n\n    def add_lead(self, lead: Lead):\n        self.db.execute(\"\"\"\n            INSERT OR REPLACE INTO leads \n            (username, name, bio, followers, intent, intent_signals, \n             trigger_tweet, initial_score, current_score, discovered_at)\n            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n        \"\"\", (\n            lead.username, lead.name, lead.bio, lead.followers,\n            lead.intent.value, \",\".join(lead.intent_signals),\n            lead.trigger_tweet, lead.score, lead.score, lead.discovered_at\n        ))\n\n    def log_interaction(self, username: str, type: str, content: str, response: str = None):\n        self.db.execute(\"\"\"\n            INSERT INTO interactions (username, type, content, response, timestamp)\n            VALUES (?, ?, ?, ?, ?)\n        \"\"\", (username, type, content, response, datetime.now()))\n\n        # Update score based on interaction\n        self._update_score(username, type, response)\n\n    def _update_score(self, username: str, interaction_type: str, response: str):\n        \"\"\"Adjust lead score based on interactions\"\"\"\n        score_adjustments = {\n            \"reply_received\": 20,\n            \"follow_back\": 15,\n            \"like_received\": 5,\n            \"dm_opened\": 25,\n            \"dm_replied\": 30,\n            \"meeting_scheduled\": 50,\n            \"no_response\": -5,\n        }\n\n        adjustment = score_adjustments.get(interaction_type, 0)\n\n        self.db.execute(\"\"\"\n            UPDATE leads \n            SET current_score = current_score + ?,\n                last_contact = ?\n            WHERE username = ?\n        \"\"\", (adjustment, datetime.now(), username))\n\n    def get_hot_leads(self, limit: int = 20):\n        \"\"\"Get highest scoring leads\"\"\"\n        return self.db.query(\"\"\"\n            SELECT * FROM leads \n            WHERE status != 'closed'\n            ORDER BY current_score DESC\n            LIMIT ?\n        \"\"\", (limit,))\n\n    def get_stale_leads(self, days: int = 7):\n        \"\"\"Get leads needing attention\"\"\"\n        cutoff = datetime.now() - timedelta(days=days)\n        return self.db.query(\"\"\"\n            SELECT * FROM leads\n            WHERE status = 'active'\n            AND last_contact &lt; ?\n            ORDER BY current_score DESC\n        \"\"\", (cutoff,))\n\n# Usage\ndb = LeadDatabase()\n\n# Add leads\nfor lead in discovered_leads:\n    db.add_lead(lead)\n\n# Log interactions\ndb.log_interaction(\"username\", \"reply_sent\", \"Great question about...\")\ndb.log_interaction(\"username\", \"reply_received\", \"Thanks for the help!\")\n\n# Get hot leads\nhot = db.get_hot_leads(10)\nprint(\"\ud83d\udd25 Hot Leads:\")\nfor lead in hot:\n    print(f\"  @{lead['username']}: Score {lead['current_score']}\")\n</code></pre>"},{"location":"cookbook/business/lead-generation/#export-to-crm","title":"Export to CRM","text":"<pre><code>import csv\nimport json\nfrom datetime import datetime\n\nasync def export_leads_to_crm(leads: list, format: str = \"csv\"):\n    \"\"\"Export qualified leads to CRM-compatible format\"\"\"\n\n    if format == \"csv\":\n        with open(\"leads_export.csv\", \"w\", newline=\"\") as f:\n            writer = csv.DictWriter(f, fieldnames=[\n                \"Username\", \"Name\", \"Bio\", \"Followers\", \"Intent\",\n                \"Signals\", \"Score\", \"Company Size\", \"Decision Maker\",\n                \"Trigger Tweet\", \"Twitter URL\", \"Discovered\"\n            ])\n            writer.writeheader()\n\n            for lead in leads:\n                writer.writerow({\n                    \"Username\": lead.lead.username,\n                    \"Name\": lead.lead.name,\n                    \"Bio\": lead.lead.bio,\n                    \"Followers\": lead.lead.followers,\n                    \"Intent\": lead.lead.intent.value,\n                    \"Signals\": \", \".join(lead.lead.intent_signals),\n                    \"Score\": lead.qualification_score,\n                    \"Company Size\": lead.company_size,\n                    \"Decision Maker\": \"Yes\" if lead.decision_maker else \"No\",\n                    \"Trigger Tweet\": lead.lead.trigger_tweet[:200],\n                    \"Twitter URL\": f\"https://x.com/{lead.lead.username}\",\n                    \"Discovered\": lead.lead.discovered_at.isoformat()\n                })\n\n    elif format == \"hubspot\":\n        # HubSpot-compatible JSON\n        contacts = []\n        for lead in leads:\n            contacts.append({\n                \"email\": f\"{lead.lead.username}@twitter.placeholder\",  # Placeholder\n                \"properties\": {\n                    \"twitter_handle\": lead.lead.username,\n                    \"firstname\": lead.lead.name.split()[0] if lead.lead.name else \"\",\n                    \"lastname\": \" \".join(lead.lead.name.split()[1:]) if lead.lead.name else \"\",\n                    \"company\": \"\",  # Would need enrichment\n                    \"lead_score\": lead.qualification_score,\n                    \"lead_source\": \"Twitter Intent Discovery\",\n                    \"notes\": f\"Intent: {lead.lead.intent.value}\\nSignals: {', '.join(lead.lead.intent_signals)}\"\n                }\n            })\n\n        with open(\"leads_hubspot.json\", \"w\") as f:\n            json.dump(contacts, f, indent=2)\n\n    print(f\"\u2705 Exported {len(leads)} leads to {format} format\")\n</code></pre>"},{"location":"cookbook/business/lead-generation/#best-practices","title":"Best Practices","text":"<p>Lead Gen Do's</p> <ul> <li>\u2705 Focus on intent signals, not just keywords</li> <li>\u2705 Provide value before pitching</li> <li>\u2705 Personalize every interaction</li> <li>\u2705 Track and score all interactions</li> <li>\u2705 Use multi-touch sequences</li> </ul> <p>Lead Gen Don'ts</p> <ul> <li>\u274c Spam DMs to cold leads</li> <li>\u274c Pitch in your first interaction</li> <li>\u274c Use generic templates</li> <li>\u274c Ignore low-score leads entirely</li> <li>\u274c Over-automate personalization</li> </ul>"},{"location":"cookbook/business/lead-generation/#next-steps","title":"Next Steps","text":"<p> Competitor Intel - Find leads from competitor audiences</p> <p> Brand Monitoring - Catch leads mentioning you</p> <p> Crisis Detection - Turn complaints into opportunities</p>"},{"location":"cookbook/data-science/","title":"\ud83d\udcca Data Science Cookbook","text":"<p>Advanced data science recipes for X/Twitter analytics. Transform raw social data into actionable insights using pandas, visualization, and machine learning.</p>"},{"location":"cookbook/data-science/#setup","title":"Setup","text":"<pre><code># Install data science dependencies\npip install \"xeepy[data]\"  # Includes pandas, numpy, matplotlib, seaborn, scikit-learn\n</code></pre>"},{"location":"cookbook/data-science/#comprehensive-twitter-dataset-builder","title":"Comprehensive Twitter Dataset Builder","text":"<p>Build a complete dataset of any account for analysis.</p> <pre><code>\"\"\"\nTwitter Dataset Builder\n=======================\nCreate comprehensive datasets for data science analysis.\n\"\"\"\nimport asyncio\nimport pandas as pd\nfrom datetime import datetime\nfrom xeepy import Xeepy\n\n\nclass TwitterDatasetBuilder:\n    \"\"\"Build comprehensive Twitter datasets for analysis\"\"\"\n\n    def __init__(self, username: str):\n        self.username = username\n        self.data = {}\n\n    async def build(self, include_network: bool = True) -&gt; dict:\n        \"\"\"Build complete dataset\"\"\"\n        async with Xeepy() as x:\n            print(f\"\ud83d\udcca Building dataset for @{self.username}...\")\n\n            # 1. Profile data\n            print(\"  Fetching profile...\")\n            profile = await x.scrape.profile(self.username)\n            self.data[\"profile\"] = self._profile_to_dict(profile)\n\n            # 2. Tweet history\n            print(\"  Fetching tweets...\")\n            tweets = await x.scrape.tweets(self.username, limit=1000)\n            self.data[\"tweets\"] = pd.DataFrame([\n                self._tweet_to_dict(t) for t in tweets\n            ])\n\n            # 3. Network data (optional, can be slow)\n            if include_network:\n                print(\"  Fetching followers (sample)...\")\n                followers = await x.scrape.followers(self.username, limit=500)\n                self.data[\"followers\"] = pd.DataFrame([\n                    self._user_to_dict(u) for u in followers\n                ])\n\n                print(\"  Fetching following...\")\n                following = await x.scrape.following(self.username, limit=500)\n                self.data[\"following\"] = pd.DataFrame([\n                    self._user_to_dict(u) for u in following\n                ])\n\n            # 4. Engagement patterns\n            print(\"  Analyzing engagement...\")\n            self.data[\"engagement_by_hour\"] = self._analyze_hourly_engagement()\n            self.data[\"engagement_by_day\"] = self._analyze_daily_engagement()\n\n            print(f\"\u2705 Dataset complete!\")\n            return self.data\n\n    def _tweet_to_dict(self, tweet) -&gt; dict:\n        return {\n            \"id\": tweet.id,\n            \"text\": tweet.text,\n            \"created_at\": tweet.created_at,\n            \"likes\": tweet.likes,\n            \"retweets\": tweet.retweets,\n            \"replies\": tweet.replies,\n            \"is_retweet\": tweet.is_retweet,\n            \"is_reply\": tweet.is_reply,\n            \"is_thread\": tweet.is_thread_start,\n            \"has_media\": bool(tweet.media),\n            \"has_link\": bool(tweet.urls),\n            \"hashtags\": tweet.hashtags,\n            \"mentions\": tweet.mentions,\n            \"char_count\": len(tweet.text),\n            \"word_count\": len(tweet.text.split()),\n            \"hour\": tweet.created_at.hour,\n            \"day_of_week\": tweet.created_at.strftime(\"%A\"),\n            \"engagement\": tweet.likes + tweet.retweets + tweet.replies,\n        }\n\n    def _user_to_dict(self, user) -&gt; dict:\n        return {\n            \"id\": user.id,\n            \"username\": user.username,\n            \"name\": user.name,\n            \"bio\": user.bio,\n            \"followers_count\": user.followers_count,\n            \"following_count\": user.following_count,\n            \"tweet_count\": user.tweet_count,\n            \"verified\": user.verified,\n            \"created_at\": user.created_at,\n            \"has_bio\": bool(user.bio),\n            \"has_website\": bool(user.website),\n            \"follower_ratio\": user.followers_count / max(user.following_count, 1),\n        }\n\n    def _profile_to_dict(self, profile) -&gt; dict:\n        return {\n            \"username\": profile.username,\n            \"name\": profile.name,\n            \"bio\": profile.bio,\n            \"followers\": profile.followers_count,\n            \"following\": profile.following_count,\n            \"tweets\": profile.tweet_count,\n            \"verified\": profile.verified,\n            \"created_at\": profile.created_at,\n        }\n\n    def _analyze_hourly_engagement(self) -&gt; pd.DataFrame:\n        \"\"\"Engagement by hour of day\"\"\"\n        df = self.data[\"tweets\"]\n        return df.groupby(\"hour\").agg({\n            \"likes\": \"mean\",\n            \"retweets\": \"mean\",\n            \"engagement\": \"mean\",\n            \"id\": \"count\"\n        }).rename(columns={\"id\": \"tweet_count\"})\n\n    def _analyze_daily_engagement(self) -&gt; pd.DataFrame:\n        \"\"\"Engagement by day of week\"\"\"\n        df = self.data[\"tweets\"]\n        day_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n        result = df.groupby(\"day_of_week\").agg({\n            \"likes\": \"mean\",\n            \"retweets\": \"mean\",\n            \"engagement\": \"mean\",\n            \"id\": \"count\"\n        }).rename(columns={\"id\": \"tweet_count\"})\n        return result.reindex(day_order)\n\n    def save(self, output_dir: str = \"dataset\"):\n        \"\"\"Save dataset to files\"\"\"\n        import os\n        os.makedirs(output_dir, exist_ok=True)\n\n        # Save DataFrames\n        for name, data in self.data.items():\n            if isinstance(data, pd.DataFrame):\n                data.to_csv(f\"{output_dir}/{name}.csv\", index=True)\n                data.to_parquet(f\"{output_dir}/{name}.parquet\")\n            elif isinstance(data, dict):\n                pd.DataFrame([data]).to_csv(f\"{output_dir}/{name}.csv\", index=False)\n\n        print(f\"\ud83d\udcbe Dataset saved to {output_dir}/\")\n\n\n# Usage\nasync def main():\n    builder = TwitterDatasetBuilder(\"elonmusk\")\n    data = await builder.build(include_network=True)\n    builder.save(\"elon_dataset\")\n\n    # Quick analysis\n    print(f\"\\nQuick Stats:\")\n    print(f\"  Total tweets: {len(data['tweets'])}\")\n    print(f\"  Avg engagement: {data['tweets']['engagement'].mean():.1f}\")\n    print(f\"  Best hour: {data['engagement_by_hour']['engagement'].idxmax()}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"cookbook/data-science/#engagement-prediction-model","title":"Engagement Prediction Model","text":"<p>Predict how well a tweet will perform before posting.</p> <pre><code>\"\"\"\nTweet Engagement Predictor\n==========================\nML model to predict engagement before posting.\n\"\"\"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, r2_score\nimport pickle\n\n\nclass EngagementPredictor:\n    \"\"\"Predict tweet engagement using machine learning\"\"\"\n\n    def __init__(self):\n        self.model = None\n        self.scaler = StandardScaler()\n        self.feature_columns = None\n\n    def train(self, tweets_df: pd.DataFrame):\n        \"\"\"Train the engagement prediction model\"\"\"\n        print(\"\ud83e\udde0 Training engagement predictor...\")\n\n        # Feature engineering\n        df = self._engineer_features(tweets_df)\n\n        # Prepare features and target\n        self.feature_columns = [\n            \"char_count\", \"word_count\", \"has_media\", \"has_link\",\n            \"hashtag_count\", \"mention_count\", \"hour\", \"is_weekend\",\n            \"is_thread\", \"question_mark\", \"exclamation\"\n        ]\n\n        X = df[self.feature_columns]\n        y = df[\"engagement\"]\n\n        # Split data\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.2, random_state=42\n        )\n\n        # Scale features\n        X_train_scaled = self.scaler.fit_transform(X_train)\n        X_test_scaled = self.scaler.transform(X_test)\n\n        # Train model\n        self.model = RandomForestRegressor(\n            n_estimators=100,\n            max_depth=10,\n            random_state=42\n        )\n        self.model.fit(X_train_scaled, y_train)\n\n        # Evaluate\n        y_pred = self.model.predict(X_test_scaled)\n        mae = mean_absolute_error(y_test, y_pred)\n        r2 = r2_score(y_test, y_pred)\n\n        print(f\"  MAE: {mae:.2f}\")\n        print(f\"  R\u00b2: {r2:.3f}\")\n\n        # Feature importance\n        importance = pd.DataFrame({\n            \"feature\": self.feature_columns,\n            \"importance\": self.model.feature_importances_\n        }).sort_values(\"importance\", ascending=False)\n\n        print(\"\\n\ud83d\udcca Feature Importance:\")\n        for _, row in importance.head(5).iterrows():\n            print(f\"  {row['feature']}: {row['importance']:.3f}\")\n\n        return {\"mae\": mae, \"r2\": r2, \"feature_importance\": importance}\n\n    def predict(self, tweet_text: str, hour: int = None, has_media: bool = False, \n                has_link: bool = False, is_thread: bool = False) -&gt; dict:\n        \"\"\"Predict engagement for a new tweet\"\"\"\n        if self.model is None:\n            raise ValueError(\"Model not trained. Call train() first.\")\n\n        # Extract features\n        features = {\n            \"char_count\": len(tweet_text),\n            \"word_count\": len(tweet_text.split()),\n            \"has_media\": int(has_media),\n            \"has_link\": int(has_link),\n            \"hashtag_count\": tweet_text.count(\"#\"),\n            \"mention_count\": tweet_text.count(\"@\"),\n            \"hour\": hour or 12,\n            \"is_weekend\": 0,  # Assume weekday by default\n            \"is_thread\": int(is_thread),\n            \"question_mark\": int(\"?\" in tweet_text),\n            \"exclamation\": int(\"!\" in tweet_text),\n        }\n\n        # Create feature vector\n        X = pd.DataFrame([features])[self.feature_columns]\n        X_scaled = self.scaler.transform(X)\n\n        # Predict\n        prediction = self.model.predict(X_scaled)[0]\n\n        # Get confidence interval (using tree predictions)\n        tree_predictions = np.array([\n            tree.predict(X_scaled)[0] \n            for tree in self.model.estimators_\n        ])\n        confidence_low = np.percentile(tree_predictions, 10)\n        confidence_high = np.percentile(tree_predictions, 90)\n\n        return {\n            \"predicted_engagement\": round(prediction),\n            \"confidence_range\": (round(confidence_low), round(confidence_high)),\n            \"features_used\": features\n        }\n\n    def _engineer_features(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Engineer features for the model\"\"\"\n        df = df.copy()\n\n        df[\"hashtag_count\"] = df[\"hashtags\"].apply(len) if \"hashtags\" in df else 0\n        df[\"mention_count\"] = df[\"mentions\"].apply(len) if \"mentions\" in df else 0\n        df[\"is_weekend\"] = df[\"day_of_week\"].isin([\"Saturday\", \"Sunday\"]).astype(int)\n        df[\"question_mark\"] = df[\"text\"].str.contains(r\"\\?\").astype(int)\n        df[\"exclamation\"] = df[\"text\"].str.contains(\"!\").astype(int)\n\n        return df\n\n    def save(self, filepath: str):\n        \"\"\"Save trained model\"\"\"\n        with open(filepath, \"wb\") as f:\n            pickle.dump({\n                \"model\": self.model,\n                \"scaler\": self.scaler,\n                \"feature_columns\": self.feature_columns\n            }, f)\n        print(f\"\ud83d\udcbe Model saved to {filepath}\")\n\n    def load(self, filepath: str):\n        \"\"\"Load trained model\"\"\"\n        with open(filepath, \"rb\") as f:\n            data = pickle.load(f)\n            self.model = data[\"model\"]\n            self.scaler = data[\"scaler\"]\n            self.feature_columns = data[\"feature_columns\"]\n        print(f\"\ud83d\udcc2 Model loaded from {filepath}\")\n\n\n# Usage\nasync def main():\n    from xeepy import Xeepy\n\n    async with Xeepy() as x:\n        # Get training data\n        tweets = await x.scrape.tweets(\"your_username\", limit=500)\n        df = pd.DataFrame([tweet_to_dict(t) for t in tweets])\n\n        # Train model\n        predictor = EngagementPredictor()\n        predictor.train(df)\n\n        # Predict new tweet\n        result = predictor.predict(\n            tweet_text=\"Just launched my new product! \ud83d\ude80 Check it out: example.com #launch\",\n            hour=14,\n            has_media=True,\n            has_link=True\n        )\n\n        print(f\"\\n\ud83d\udd2e Prediction:\")\n        print(f\"  Expected engagement: {result['predicted_engagement']}\")\n        print(f\"  Range: {result['confidence_range'][0]} - {result['confidence_range'][1]}\")\n\n        # Save model\n        predictor.save(\"engagement_model.pkl\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"cookbook/data-science/#audience-segmentation","title":"Audience Segmentation","text":"<p>Cluster your followers into meaningful segments.</p> <pre><code>\"\"\"\nAudience Segmentation\n=====================\nCluster followers into actionable segments.\n\"\"\"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nclass AudienceSegmenter:\n    \"\"\"Segment followers using clustering\"\"\"\n\n    def __init__(self, followers_df: pd.DataFrame):\n        self.df = followers_df\n        self.segments = None\n        self.segment_profiles = None\n\n    def segment(self, n_segments: int = 5) -&gt; pd.DataFrame:\n        \"\"\"Perform audience segmentation\"\"\"\n        print(\"\ud83c\udfaf Segmenting audience...\")\n\n        # Prepare features\n        features = self._prepare_features()\n\n        # Scale features\n        scaler = StandardScaler()\n        features_scaled = scaler.fit_transform(features)\n\n        # Cluster\n        kmeans = KMeans(n_clusters=n_segments, random_state=42, n_init=10)\n        self.df[\"segment\"] = kmeans.fit_predict(features_scaled)\n\n        # Name segments based on characteristics\n        self.segment_profiles = self._profile_segments()\n        self.df[\"segment_name\"] = self.df[\"segment\"].map(\n            {i: p[\"name\"] for i, p in self.segment_profiles.items()}\n        )\n\n        return self.df\n\n    def _prepare_features(self) -&gt; pd.DataFrame:\n        \"\"\"Prepare features for clustering\"\"\"\n        return self.df[[\n            \"followers_count\",\n            \"following_count\",\n            \"tweet_count\",\n            \"follower_ratio\"\n        ]].fillna(0)\n\n    def _profile_segments(self) -&gt; dict:\n        \"\"\"Create profiles for each segment\"\"\"\n        profiles = {}\n\n        for seg in self.df[\"segment\"].unique():\n            seg_data = self.df[self.df[\"segment\"] == seg]\n\n            avg_followers = seg_data[\"followers_count\"].mean()\n            avg_tweets = seg_data[\"tweet_count\"].mean()\n            avg_ratio = seg_data[\"follower_ratio\"].mean()\n\n            # Name based on characteristics\n            if avg_followers &gt; 10000:\n                name = \"\ud83c\udf1f Influencers\"\n            elif avg_ratio &gt; 2:\n                name = \"\ud83d\udc51 Thought Leaders\"\n            elif avg_tweets &gt; 5000:\n                name = \"\ud83d\udde3\ufe0f Power Users\"\n            elif avg_followers &lt; 100:\n                name = \"\ud83c\udf31 Newcomers\"\n            else:\n                name = \"\ud83d\udcbc Regular Users\"\n\n            profiles[seg] = {\n                \"name\": name,\n                \"count\": len(seg_data),\n                \"avg_followers\": avg_followers,\n                \"avg_tweets\": avg_tweets,\n                \"avg_ratio\": avg_ratio,\n                \"sample_users\": seg_data[\"username\"].head(5).tolist()\n            }\n\n        return profiles\n\n    def visualize(self, save_path: str = None):\n        \"\"\"Visualize segments\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n        # Segment sizes\n        ax1 = axes[0, 0]\n        segment_counts = self.df[\"segment_name\"].value_counts()\n        ax1.pie(segment_counts, labels=segment_counts.index, autopct='%1.1f%%')\n        ax1.set_title(\"Segment Distribution\")\n\n        # Followers distribution by segment\n        ax2 = axes[0, 1]\n        sns.boxplot(data=self.df, x=\"segment_name\", y=\"followers_count\", ax=ax2)\n        ax2.set_title(\"Followers by Segment\")\n        ax2.set_yscale(\"log\")\n        plt.xticks(rotation=45)\n\n        # Engagement potential (followers vs tweets)\n        ax3 = axes[1, 0]\n        sns.scatterplot(\n            data=self.df.sample(min(500, len(self.df))),\n            x=\"followers_count\", y=\"tweet_count\",\n            hue=\"segment_name\", alpha=0.6, ax=ax3\n        )\n        ax3.set_xscale(\"log\")\n        ax3.set_yscale(\"log\")\n        ax3.set_title(\"Followers vs Tweets by Segment\")\n\n        # Segment profiles\n        ax4 = axes[1, 1]\n        profile_df = pd.DataFrame([\n            {\"segment\": p[\"name\"], \"metric\": \"Avg Followers\", \"value\": p[\"avg_followers\"]}\n            for p in self.segment_profiles.values()\n        ] + [\n            {\"segment\": p[\"name\"], \"metric\": \"Avg Tweets\", \"value\": p[\"avg_tweets\"]}\n            for p in self.segment_profiles.values()\n        ])\n        sns.barplot(data=profile_df, x=\"segment\", y=\"value\", hue=\"metric\", ax=ax4)\n        ax4.set_title(\"Segment Profiles\")\n        plt.xticks(rotation=45)\n\n        plt.tight_layout()\n\n        if save_path:\n            plt.savefig(save_path, dpi=150)\n            print(f\"\ud83d\udcca Visualization saved to {save_path}\")\n        else:\n            plt.show()\n\n    def get_segment_recommendations(self) -&gt; dict:\n        \"\"\"Get actionable recommendations for each segment\"\"\"\n        recommendations = {}\n\n        for seg_id, profile in self.segment_profiles.items():\n            name = profile[\"name\"]\n\n            if \"Influencer\" in name:\n                recommendations[name] = {\n                    \"strategy\": \"Build relationships, seek collaborations\",\n                    \"content\": \"High-quality, shareable threads\",\n                    \"engagement\": \"Meaningful replies, DMs for partnerships\"\n                }\n            elif \"Thought Leader\" in name:\n                recommendations[name] = {\n                    \"strategy\": \"Provide value, establish expertise\",\n                    \"content\": \"Educational content, insights\",\n                    \"engagement\": \"Thoughtful discussions\"\n                }\n            elif \"Power User\" in name:\n                recommendations[name] = {\n                    \"strategy\": \"Engage actively, build community\",\n                    \"content\": \"Interactive content, polls\",\n                    \"engagement\": \"Regular interaction, replies\"\n                }\n            elif \"Newcomer\" in name:\n                recommendations[name] = {\n                    \"strategy\": \"Welcome, provide value\",\n                    \"content\": \"Beginner-friendly content\",\n                    \"engagement\": \"Supportive comments\"\n                }\n            else:\n                recommendations[name] = {\n                    \"strategy\": \"Consistent value delivery\",\n                    \"content\": \"Mix of educational and entertaining\",\n                    \"engagement\": \"Regular, genuine engagement\"\n                }\n\n        return recommendations\n\n\n# Usage\nasync def main():\n    from xeepy import Xeepy\n\n    async with Xeepy() as x:\n        # Get followers\n        followers = await x.scrape.followers(\"your_username\", limit=1000)\n        df = pd.DataFrame([user_to_dict(u) for u in followers])\n\n        # Segment\n        segmenter = AudienceSegmenter(df)\n        segmented_df = segmenter.segment(n_segments=5)\n\n        # Print profiles\n        print(\"\\n\ud83d\udcca Segment Profiles:\")\n        for seg_id, profile in segmenter.segment_profiles.items():\n            print(f\"\\n{profile['name']} ({profile['count']} followers)\")\n            print(f\"  Avg followers: {profile['avg_followers']:,.0f}\")\n            print(f\"  Avg tweets: {profile['avg_tweets']:,.0f}\")\n            print(f\"  Sample: {', '.join('@' + u for u in profile['sample_users'][:3])}\")\n\n        # Visualize\n        segmenter.visualize(\"segments.png\")\n\n        # Get recommendations\n        recs = segmenter.get_segment_recommendations()\n        print(\"\\n\ud83d\udca1 Recommendations:\")\n        for segment, rec in recs.items():\n            print(f\"\\n{segment}:\")\n            print(f\"  Strategy: {rec['strategy']}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"cookbook/data-science/#time-series-forecasting","title":"Time Series Forecasting","text":"<p>Forecast follower growth and engagement trends.</p> <pre><code>\"\"\"\nTime Series Forecasting\n=======================\nPredict future follower counts and engagement.\n\"\"\"\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\n\n\nclass GrowthForecaster:\n    \"\"\"Forecast follower growth using time series analysis\"\"\"\n\n    def __init__(self, historical_data: pd.DataFrame):\n        \"\"\"\n        historical_data should have columns: date, followers\n        \"\"\"\n        self.data = historical_data.copy()\n        self.data[\"date\"] = pd.to_datetime(self.data[\"date\"])\n        self.data = self.data.sort_values(\"date\").set_index(\"date\")\n\n    def forecast(self, days_ahead: int = 30) -&gt; pd.DataFrame:\n        \"\"\"Forecast follower count\"\"\"\n        print(f\"\ud83d\udd2e Forecasting {days_ahead} days ahead...\")\n\n        # Simple moving average + trend\n        self.data[\"ma7\"] = self.data[\"followers\"].rolling(7).mean()\n        self.data[\"ma30\"] = self.data[\"followers\"].rolling(30).mean()\n\n        # Calculate daily growth rate\n        self.data[\"growth_rate\"] = self.data[\"followers\"].pct_change()\n        avg_growth_rate = self.data[\"growth_rate\"].mean()\n\n        # Calculate trend\n        recent_data = self.data.tail(30)\n        X = np.arange(len(recent_data)).reshape(-1, 1)\n        y = recent_data[\"followers\"].values\n\n        # Linear regression for trend\n        from sklearn.linear_model import LinearRegression\n        model = LinearRegression()\n        model.fit(X, y)\n\n        daily_trend = model.coef_[0]\n\n        # Generate forecast\n        last_date = self.data.index[-1]\n        last_value = self.data[\"followers\"].iloc[-1]\n\n        forecast_dates = pd.date_range(\n            start=last_date + timedelta(days=1),\n            periods=days_ahead\n        )\n\n        forecasts = []\n        current_value = last_value\n\n        for i, date in enumerate(forecast_dates):\n            # Combine trend and seasonality\n            predicted = current_value + daily_trend\n\n            # Add some confidence bounds\n            std = self.data[\"followers\"].diff().std()\n            lower = predicted - 1.96 * std * np.sqrt(i + 1)\n            upper = predicted + 1.96 * std * np.sqrt(i + 1)\n\n            forecasts.append({\n                \"date\": date,\n                \"forecast\": predicted,\n                \"lower_bound\": lower,\n                \"upper_bound\": upper\n            })\n\n            current_value = predicted\n\n        forecast_df = pd.DataFrame(forecasts).set_index(\"date\")\n\n        # Calculate milestones\n        milestones = self._calculate_milestones(forecast_df, last_value)\n\n        return {\n            \"forecast\": forecast_df,\n            \"daily_trend\": daily_trend,\n            \"avg_growth_rate\": avg_growth_rate,\n            \"milestones\": milestones\n        }\n\n    def _calculate_milestones(self, forecast_df: pd.DataFrame, current: float) -&gt; list:\n        \"\"\"Calculate when milestones will be reached\"\"\"\n        milestones = []\n        targets = [1000, 5000, 10000, 25000, 50000, 100000]\n\n        for target in targets:\n            if target &gt; current:\n                # Find when forecast exceeds target\n                above_target = forecast_df[forecast_df[\"forecast\"] &gt;= target]\n                if len(above_target) &gt; 0:\n                    milestone_date = above_target.index[0]\n                    milestones.append({\n                        \"target\": target,\n                        \"estimated_date\": milestone_date,\n                        \"days_away\": (milestone_date - datetime.now()).days\n                    })\n\n        return milestones\n\n    def visualize(self, forecast_result: dict, save_path: str = None):\n        \"\"\"Visualize forecast\"\"\"\n        fig, ax = plt.subplots(figsize=(14, 6))\n\n        # Historical data\n        ax.plot(self.data.index, self.data[\"followers\"], \n                label=\"Historical\", color=\"blue\", linewidth=2)\n\n        # Forecast\n        forecast = forecast_result[\"forecast\"]\n        ax.plot(forecast.index, forecast[\"forecast\"],\n                label=\"Forecast\", color=\"red\", linewidth=2, linestyle=\"--\")\n\n        # Confidence interval\n        ax.fill_between(forecast.index, \n                       forecast[\"lower_bound\"], \n                       forecast[\"upper_bound\"],\n                       color=\"red\", alpha=0.2, label=\"95% CI\")\n\n        # Milestones\n        for m in forecast_result[\"milestones\"][:3]:\n            ax.axhline(y=m[\"target\"], color=\"green\", linestyle=\":\", alpha=0.5)\n            ax.annotate(f'{m[\"target\"]:,}', \n                       xy=(forecast.index[-1], m[\"target\"]),\n                       fontsize=10)\n\n        ax.set_xlabel(\"Date\")\n        ax.set_ylabel(\"Followers\")\n        ax.set_title(\"Follower Growth Forecast\")\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n        # Format y-axis\n        ax.yaxis.set_major_formatter(\n            plt.FuncFormatter(lambda x, p: f'{x:,.0f}')\n        )\n\n        plt.tight_layout()\n\n        if save_path:\n            plt.savefig(save_path, dpi=150)\n        else:\n            plt.show()\n\n\n# Usage\nasync def main():\n    from xeepy import Xeepy\n\n    async with Xeepy() as x:\n        # Get historical data\n        growth_data = await x.analytics.growth_history(period=\"90d\")\n\n        df = pd.DataFrame([\n            {\"date\": d.date, \"followers\": d.followers}\n            for d in growth_data.daily_data\n        ])\n\n        # Forecast\n        forecaster = GrowthForecaster(df)\n        result = forecaster.forecast(days_ahead=60)\n\n        print(f\"\\n\ud83d\udcc8 Growth Analysis:\")\n        print(f\"  Daily trend: {result['daily_trend']:+.1f} followers/day\")\n        print(f\"  Avg growth rate: {result['avg_growth_rate']:.2%}\")\n\n        print(f\"\\n\ud83c\udfaf Upcoming Milestones:\")\n        for m in result[\"milestones\"]:\n            print(f\"  {m['target']:,}: ~{m['days_away']} days ({m['estimated_date'].strftime('%Y-%m-%d')})\")\n\n        # Visualize\n        forecaster.visualize(result, \"forecast.png\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"cookbook/data-science/#sentiment-dashboard","title":"Sentiment Dashboard","text":"<p>Real-time sentiment analysis dashboard.</p> <pre><code>\"\"\"\nSentiment Analysis Dashboard\n============================\nTrack sentiment trends over time.\n\"\"\"\nimport pandas as pd\nimport asyncio\nfrom datetime import datetime, timedelta\nfrom xeepy import Xeepy\nfrom xeepy.ai import SentimentAnalyzer\n\n\nclass SentimentDashboard:\n    \"\"\"Real-time sentiment tracking\"\"\"\n\n    def __init__(self):\n        self.data = []\n        self.analyzer = SentimentAnalyzer(provider=\"openai\")\n\n    async def analyze_mentions(self, username: str, period: str = \"24h\"):\n        \"\"\"Analyze sentiment of mentions\"\"\"\n        async with Xeepy() as x:\n            mentions = await x.scrape.mentions(username, limit=200)\n\n            for mention in mentions:\n                result = await self.analyzer.analyze(mention.text)\n\n                self.data.append({\n                    \"timestamp\": mention.created_at,\n                    \"text\": mention.text,\n                    \"author\": mention.author.username,\n                    \"sentiment\": result.label,\n                    \"score\": result.score,\n                    \"likes\": mention.likes\n                })\n\n            return self._generate_report()\n\n    def _generate_report(self) -&gt; dict:\n        \"\"\"Generate sentiment report\"\"\"\n        df = pd.DataFrame(self.data)\n\n        if len(df) == 0:\n            return {\"error\": \"No data collected\"}\n\n        # Overall sentiment distribution\n        sentiment_dist = df[\"sentiment\"].value_counts(normalize=True)\n\n        # Average score\n        avg_score = df[\"score\"].mean()\n\n        # Sentiment over time\n        df[\"hour\"] = pd.to_datetime(df[\"timestamp\"]).dt.floor(\"H\")\n        hourly_sentiment = df.groupby(\"hour\")[\"score\"].mean()\n\n        # Most positive/negative mentions\n        most_positive = df.nlargest(3, \"score\")[[\"text\", \"author\", \"score\"]]\n        most_negative = df.nsmallest(3, \"score\")[[\"text\", \"author\", \"score\"]]\n\n        # Weighted sentiment (by engagement)\n        df[\"weighted_score\"] = df[\"score\"] * (1 + df[\"likes\"] / 100)\n        weighted_avg = df[\"weighted_score\"].mean()\n\n        return {\n            \"total_mentions\": len(df),\n            \"sentiment_distribution\": sentiment_dist.to_dict(),\n            \"average_score\": avg_score,\n            \"weighted_average\": weighted_avg,\n            \"hourly_trend\": hourly_sentiment.to_dict(),\n            \"most_positive\": most_positive.to_dict(\"records\"),\n            \"most_negative\": most_negative.to_dict(\"records\"),\n        }\n\n    def visualize(self, report: dict, save_path: str = None):\n        \"\"\"Visualize sentiment analysis\"\"\"\n        import matplotlib.pyplot as plt\n\n        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n        # Sentiment distribution\n        ax1 = axes[0, 0]\n        dist = report[\"sentiment_distribution\"]\n        colors = {\"positive\": \"green\", \"neutral\": \"gray\", \"negative\": \"red\"}\n        ax1.pie(dist.values(), labels=dist.keys(), \n               colors=[colors.get(k, \"blue\") for k in dist.keys()],\n               autopct='%1.1f%%')\n        ax1.set_title(\"Sentiment Distribution\")\n\n        # Sentiment over time\n        ax2 = axes[0, 1]\n        hourly = report[\"hourly_trend\"]\n        ax2.plot(list(hourly.keys()), list(hourly.values()), marker=\"o\")\n        ax2.axhline(y=0, color=\"gray\", linestyle=\"--\")\n        ax2.set_title(\"Sentiment Over Time\")\n        ax2.set_ylabel(\"Average Sentiment Score\")\n        plt.xticks(rotation=45)\n\n        # Score histogram\n        ax3 = axes[1, 0]\n        df = pd.DataFrame(self.data)\n        ax3.hist(df[\"score\"], bins=20, edgecolor=\"black\")\n        ax3.axvline(x=report[\"average_score\"], color=\"red\", linestyle=\"--\", \n                   label=f\"Avg: {report['average_score']:.2f}\")\n        ax3.set_title(\"Sentiment Score Distribution\")\n        ax3.legend()\n\n        # Summary stats\n        ax4 = axes[1, 1]\n        ax4.axis(\"off\")\n        summary_text = f\"\"\"\n        Sentiment Analysis Summary\n        ==========================\n\n        Total Mentions: {report['total_mentions']}\n\n        Average Score: {report['average_score']:.2f}\n        Weighted Average: {report['weighted_average']:.2f}\n\n        Distribution:\n        - Positive: {report['sentiment_distribution'].get('positive', 0):.1%}\n        - Neutral: {report['sentiment_distribution'].get('neutral', 0):.1%}\n        - Negative: {report['sentiment_distribution'].get('negative', 0):.1%}\n        \"\"\"\n        ax4.text(0.1, 0.5, summary_text, fontsize=12, family=\"monospace\",\n                verticalalignment=\"center\")\n\n        plt.tight_layout()\n\n        if save_path:\n            plt.savefig(save_path, dpi=150)\n        else:\n            plt.show()\n\n\n# Usage\nasync def main():\n    dashboard = SentimentDashboard()\n    report = await dashboard.analyze_mentions(\"your_username\", period=\"24h\")\n\n    print(\"\ud83d\udcca Sentiment Report:\")\n    print(f\"  Total mentions: {report['total_mentions']}\")\n    print(f\"  Average sentiment: {report['average_score']:.2f}\")\n    print(f\"  Distribution: {report['sentiment_distribution']}\")\n\n    dashboard.visualize(report, \"sentiment_dashboard.png\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"cookbook/data-science/#next-steps","title":"Next Steps","text":"<ul> <li> <p>Business Intelligence</p> <p>Turn insights into business value</p> </li> <li> <p>Research Applications</p> <p>Academic and market research</p> </li> </ul>"},{"location":"cookbook/data-science/network-analysis/","title":"Network Analysis","text":"<p>Map influence networks, find key connectors, and understand community structure.</p>"},{"location":"cookbook/data-science/network-analysis/#influence-network-mapping","title":"Influence Network Mapping","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom dataclasses import dataclass\nfrom collections import defaultdict\nimport json\n\n@dataclass\nclass NetworkNode:\n    username: str\n    followers: int\n    following: int\n    influence_score: float\n    connections: list\n    cluster: str = None\n\nclass InfluenceNetwork:\n    \"\"\"Map and analyze influence networks\"\"\"\n\n    def __init__(self):\n        self.nodes = {}\n        self.edges = []\n\n    async def build_network(\n        self,\n        seed_users: list,\n        depth: int = 2,\n        max_connections: int = 50\n    ):\n        \"\"\"\n        Build influence network starting from seed users.\n\n        depth=1: Direct connections only\n        depth=2: Friends of friends\n        \"\"\"\n\n        async with Xeepy() as x:\n            to_process = [(u, 0) for u in seed_users]\n            processed = set()\n\n            while to_process:\n                username, current_depth = to_process.pop(0)\n\n                if username in processed or current_depth &gt; depth:\n                    continue\n\n                processed.add(username)\n                print(f\"Processing @{username} (depth {current_depth})\")\n\n                try:\n                    # Get profile\n                    profile = await x.scrape.profile(username)\n\n                    # Get who they follow (their influencers)\n                    following = await x.scrape.following(username, limit=max_connections)\n\n                    # Calculate influence score\n                    influence_score = self._calculate_influence(profile)\n\n                    # Create node\n                    self.nodes[username] = NetworkNode(\n                        username=username,\n                        followers=profile.followers_count,\n                        following=profile.following_count,\n                        influence_score=influence_score,\n                        connections=[f.username for f in following]\n                    )\n\n                    # Create edges\n                    for followed in following:\n                        self.edges.append({\n                            \"source\": username,\n                            \"target\": followed.username,\n                            \"type\": \"follows\"\n                        })\n\n                        # Add to processing queue\n                        if current_depth &lt; depth and followed.username not in processed:\n                            to_process.append((followed.username, current_depth + 1))\n\n                    await asyncio.sleep(1)  # Rate limiting\n\n                except Exception as e:\n                    print(f\"  Error: {e}\")\n\n        return self\n\n    def _calculate_influence(self, profile) -&gt; float:\n        \"\"\"Calculate influence score 0-100\"\"\"\n\n        # Follower count (log scale)\n        import math\n        follower_score = min(math.log10(profile.followers_count + 1) * 20, 40)\n\n        # Follower/following ratio\n        ratio = profile.followers_count / max(profile.following_count, 1)\n        ratio_score = min(ratio * 10, 30)\n\n        # Engagement (if available)\n        engagement_score = 20  # Default\n\n        # Verification bonus\n        verified_score = 10 if profile.verified else 0\n\n        return min(follower_score + ratio_score + engagement_score + verified_score, 100)\n\n    def find_key_connectors(self, top_n: int = 10):\n        \"\"\"\n        Find bridge nodes - people who connect different clusters.\n        High betweenness centrality = key connector.\n        \"\"\"\n\n        # Simple approach: count unique connections\n        connection_diversity = {}\n\n        for username, node in self.nodes.items():\n            # Count how many different people this user connects\n            connections = set(node.connections)\n\n            # Add reverse connections\n            for edge in self.edges:\n                if edge[\"target\"] == username:\n                    connections.add(edge[\"source\"])\n\n            connection_diversity[username] = {\n                \"total_connections\": len(connections),\n                \"unique_ratio\": len(connections) / max(node.following, 1),\n                \"influence\": node.influence_score\n            }\n\n        # Score = connections * influence\n        scored = {\n            u: data[\"total_connections\"] * data[\"influence\"] / 100\n            for u, data in connection_diversity.items()\n        }\n\n        return sorted(scored.items(), key=lambda x: -x[1])[:top_n]\n\n    def find_clusters(self):\n        \"\"\"Identify community clusters\"\"\"\n\n        # Simple clustering: group by shared connections\n        from collections import Counter\n\n        clusters = {}\n\n        for username, node in self.nodes.items():\n            # Find most common connections among this user's network\n            connection_counts = Counter()\n\n            for connected in node.connections:\n                if connected in self.nodes:\n                    for their_connection in self.nodes[connected].connections:\n                        connection_counts[their_connection] += 1\n\n            # Cluster = most connected hub\n            if connection_counts:\n                cluster_center = connection_counts.most_common(1)[0][0]\n                clusters[username] = cluster_center\n\n        # Group by cluster\n        cluster_groups = defaultdict(list)\n        for user, cluster in clusters.items():\n            cluster_groups[cluster].append(user)\n\n        return dict(cluster_groups)\n\n    def find_hidden_influencers(self, min_connections: int = 5):\n        \"\"\"\n        Find users who are followed by many influential people\n        but aren't famous themselves.\n        \"\"\"\n\n        # Count how many times each user is followed\n        followed_by = defaultdict(list)\n\n        for edge in self.edges:\n            target = edge[\"target\"]\n            source = edge[\"source\"]\n\n            if source in self.nodes:\n                followed_by[target].append({\n                    \"username\": source,\n                    \"influence\": self.nodes[source].influence_score\n                })\n\n        # Score: sum of followers' influence / own follower count\n        hidden_scores = {}\n\n        for username, followers_data in followed_by.items():\n            if len(followers_data) &lt; min_connections:\n                continue\n\n            total_influence = sum(f[\"influence\"] for f in followers_data)\n            avg_influence = total_influence / len(followers_data)\n\n            # If user is in our network, compare to their own influence\n            if username in self.nodes:\n                own_influence = self.nodes[username].influence_score\n                # Hidden = followed by influential people but low own influence\n                if own_influence &lt; avg_influence:\n                    hidden_scores[username] = {\n                        \"followers_influence\": avg_influence,\n                        \"own_influence\": own_influence,\n                        \"hidden_score\": avg_influence - own_influence,\n                        \"influential_followers\": [f[\"username\"] for f in followers_data if f[\"influence\"] &gt; 50]\n                    }\n\n        return sorted(hidden_scores.items(), key=lambda x: -x[1][\"hidden_score\"])\n\n    def export_for_visualization(self, filename: str = \"network.json\"):\n        \"\"\"Export for D3.js or Gephi visualization\"\"\"\n\n        export_data = {\n            \"nodes\": [\n                {\n                    \"id\": username,\n                    \"label\": username,\n                    \"followers\": node.followers,\n                    \"influence\": node.influence_score,\n                    \"size\": node.influence_score / 10\n                }\n                for username, node in self.nodes.items()\n            ],\n            \"edges\": self.edges\n        }\n\n        with open(filename, \"w\") as f:\n            json.dump(export_data, f, indent=2)\n\n        return filename\n\n# Usage\nasync def map_influence_network():\n    network = InfluenceNetwork()\n\n    # Start from key accounts in your niche\n    await network.build_network(\n        seed_users=[\"influencer1\", \"influencer2\", \"competitor\"],\n        depth=2,\n        max_connections=30\n    )\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"\ud83d\udd78\ufe0f NETWORK ANALYSIS RESULTS\")\n    print(\"=\"*60)\n\n    print(f\"\\nNodes: {len(network.nodes)}\")\n    print(f\"Edges: {len(network.edges)}\")\n\n    # Key connectors\n    print(\"\\n\ud83d\udd17 KEY CONNECTORS (Bridge Nodes):\")\n    for user, score in network.find_key_connectors(5):\n        print(f\"   @{user}: {score:.1f}\")\n\n    # Hidden influencers\n    print(\"\\n\ud83d\udd0d HIDDEN INFLUENCERS:\")\n    for user, data in network.find_hidden_influencers()[:5]:\n        print(f\"   @{user}:\")\n        print(f\"      Own influence: {data['own_influence']:.1f}\")\n        print(f\"      Followers' avg influence: {data['followers_influence']:.1f}\")\n        print(f\"      Followed by: {', '.join(data['influential_followers'][:3])}\")\n\n    # Clusters\n    print(\"\\n\ud83d\udcca COMMUNITY CLUSTERS:\")\n    clusters = network.find_clusters()\n    for center, members in sorted(clusters.items(), key=lambda x: -len(x[1]))[:5]:\n        print(f\"   @{center} cluster: {len(members)} members\")\n\n    # Export\n    network.export_for_visualization(\"network_data.json\")\n    print(\"\\n\u2705 Exported to network_data.json for visualization\")\n\nasyncio.run(map_influence_network())\n</code></pre>"},{"location":"cookbook/data-science/network-analysis/#find-your-ideal-audience-network","title":"Find Your Ideal Audience Network","text":"<pre><code>async def find_ideal_audience_network(\n    your_username: str,\n    ideal_profiles: list,  # Usernames of your best followers/customers\n    expansion_depth: int = 2\n):\n    \"\"\"\n    Find more people like your best followers by mapping their network.\n\n    Strategy:\n    1. Take your best followers\n    2. See who they follow\n    3. Find common patterns\n    4. Discover similar users\n    \"\"\"\n\n    async with Xeepy() as x:\n        # Step 1: Map ideal profiles\n        common_follows = defaultdict(list)  # Who do ideal customers follow?\n\n        for ideal_user in ideal_profiles:\n            print(f\"Analyzing @{ideal_user}...\")\n\n            following = await x.scrape.following(ideal_user, limit=100)\n\n            for followed in following:\n                common_follows[followed.username].append(ideal_user)\n\n        # Step 2: Find accounts followed by multiple ideal customers\n        # These are likely influencers in your ideal audience\n        shared_interests = sorted(\n            [(user, followers) for user, followers in common_follows.items()\n             if len(followers) &gt;= 2],\n            key=lambda x: -len(x[1])\n        )\n\n        print(\"\\n\ud83c\udfaf ACCOUNTS FOLLOWED BY MULTIPLE IDEAL CUSTOMERS:\")\n        for account, followers in shared_interests[:10]:\n            profile = await x.scrape.profile(account)\n            print(f\"   @{account} ({profile.followers_count:,} followers)\")\n            print(f\"      Followed by: {', '.join(followers)}\")\n\n        # Step 3: Find followers of shared interests who aren't your followers yet\n        your_followers = {f.username for f in await x.scrape.followers(your_username, limit=1000)}\n\n        potential_audience = defaultdict(int)\n\n        for account, _ in shared_interests[:5]:\n            followers = await x.scrape.followers(account, limit=200)\n\n            for follower in followers:\n                if follower.username not in your_followers:\n                    potential_audience[follower.username] += 1\n\n        # Step 4: Rank by overlap (followed multiple shared interests)\n        lookalikes = sorted(\n            [(user, count) for user, count in potential_audience.items()],\n            key=lambda x: -x[1]\n        )\n\n        print(\"\\n\ud83d\udc65 LOOKALIKE AUDIENCE (follow these users!):\")\n        for user, overlap_score in lookalikes[:20]:\n            profile = await x.scrape.profile(user)\n            print(f\"   @{user} (score: {overlap_score}, followers: {profile.followers_count:,})\")\n\n        return [user for user, _ in lookalikes[:100]]\n\nasyncio.run(find_ideal_audience_network(\n    your_username=\"your_username\",\n    ideal_profiles=[\"best_customer1\", \"best_customer2\", \"best_customer3\"]\n))\n</code></pre>"},{"location":"cookbook/data-science/network-analysis/#competitive-network-analysis","title":"Competitive Network Analysis","text":"<pre><code>async def competitive_network_analysis(your_username: str, competitor: str):\n    \"\"\"\n    Analyze network overlap and unique advantages vs competitor.\n    \"\"\"\n\n    async with Xeepy() as x:\n        print(f\"Comparing @{your_username} vs @{competitor}\")\n\n        # Get followers\n        your_followers = {f.username for f in await x.scrape.followers(your_username, limit=1000)}\n        their_followers = {f.username for f in await x.scrape.followers(competitor, limit=1000)}\n\n        # Calculate overlaps\n        shared = your_followers &amp; their_followers\n        only_yours = your_followers - their_followers\n        only_theirs = their_followers - your_followers\n\n        print(f\"\\n\ud83d\udcca AUDIENCE OVERLAP ANALYSIS\")\n        print(\"=\"*50)\n        print(f\"Your followers:       {len(your_followers):,}\")\n        print(f\"Their followers:      {len(their_followers):,}\")\n        print(f\"Shared audience:      {len(shared):,} ({len(shared)/len(your_followers)*100:.1f}%)\")\n        print(f\"Unique to you:        {len(only_yours):,}\")\n        print(f\"Unique to competitor: {len(only_theirs):,}\")\n\n        # Analyze unique competitor followers (opportunity)\n        print(f\"\\n\ud83c\udfaf OPPORTUNITY: {len(only_theirs):,} users follow competitor but not you\")\n        print(\"   Sample accounts:\")\n\n        for username in list(only_theirs)[:10]:\n            try:\n                profile = await x.scrape.profile(username)\n                print(f\"   @{username}: {profile.followers_count:,} followers - {profile.bio[:50] if profile.bio else 'No bio'}...\")\n            except:\n                pass\n\n        return {\n            \"shared\": list(shared),\n            \"only_yours\": list(only_yours),\n            \"only_theirs\": list(only_theirs),\n            \"overlap_percentage\": len(shared) / len(your_followers) * 100\n        }\n\nasyncio.run(competitive_network_analysis(\"your_username\", \"competitor\"))\n</code></pre>"},{"location":"cookbook/data-science/network-analysis/#engagement-network","title":"Engagement Network","text":"<pre><code>async def engagement_network(tweet_url: str):\n    \"\"\"\n    Map the network of people who engaged with a specific tweet.\n    Useful for understanding viral spread patterns.\n    \"\"\"\n\n    async with Xeepy() as x:\n        print(f\"Analyzing engagement network for:\\n{tweet_url}\\n\")\n\n        # Get engagers\n        likers = await x.scrape.likers(tweet_url, limit=200)\n        retweeters = await x.scrape.retweeters(tweet_url, limit=200)\n\n        # Combine and deduplicate\n        all_engagers = {}\n\n        for user in likers:\n            all_engagers[user.username] = {\"type\": [\"like\"], \"user\": user}\n\n        for user in retweeters:\n            if user.username in all_engagers:\n                all_engagers[user.username][\"type\"].append(\"retweet\")\n            else:\n                all_engagers[user.username] = {\"type\": [\"retweet\"], \"user\": user}\n\n        print(f\"Total engagers: {len(all_engagers)}\")\n        print(f\"Likers: {len(likers)}\")\n        print(f\"Retweeters: {len(retweeters)}\")\n\n        # Analyze engager network\n        # Who follows who among engagers?\n        engager_connections = defaultdict(list)\n\n        for username, data in list(all_engagers.items())[:50]:  # Sample for performance\n            following = await x.scrape.following(username, limit=100)\n\n            for followed in following:\n                if followed.username in all_engagers:\n                    engager_connections[username].append(followed.username)\n\n        # Find super-connectors (people connected to many other engagers)\n        connector_scores = {\n            user: len(connections)\n            for user, connections in engager_connections.items()\n        }\n\n        print(\"\\n\ud83d\udd17 SUPER-CONNECTORS (amplified spread):\")\n        for user, score in sorted(connector_scores.items(), key=lambda x: -x[1])[:10]:\n            data = all_engagers[user]\n            print(f\"   @{user}: connected to {score} other engagers\")\n            print(f\"      Engagement: {', '.join(data['type'])}\")\n            print(f\"      Followers: {data['user'].followers_count:,}\")\n\n        # Find engagement clusters\n        print(\"\\n\ud83d\udcca ENGAGEMENT CLUSTERS:\")\n\n        # Group by follower count\n        tiers = {\n            \"mega (100k+)\": [],\n            \"macro (10k-100k)\": [],\n            \"micro (1k-10k)\": [],\n            \"nano (&lt;1k)\": []\n        }\n\n        for username, data in all_engagers.items():\n            followers = data[\"user\"].followers_count\n            if followers &gt;= 100000:\n                tiers[\"mega (100k+)\"].append(username)\n            elif followers &gt;= 10000:\n                tiers[\"macro (10k-100k)\"].append(username)\n            elif followers &gt;= 1000:\n                tiers[\"micro (1k-10k)\"].append(username)\n            else:\n                tiers[\"nano (&lt;1k)\"].append(username)\n\n        for tier, users in tiers.items():\n            if users:\n                total_reach = sum(all_engagers[u][\"user\"].followers_count for u in users)\n                print(f\"   {tier}: {len(users)} users, {total_reach:,} total reach\")\n\nasyncio.run(engagement_network(\"https://x.com/user/status/123\"))\n</code></pre>"},{"location":"cookbook/data-science/network-analysis/#network-visualization","title":"Network Visualization","text":"<p>Generate visualizations using your data:</p> <pre><code>async def generate_network_viz_data(seed_users: list):\n    \"\"\"Generate D3.js compatible network visualization data\"\"\"\n\n    network = InfluenceNetwork()\n    await network.build_network(seed_users, depth=2, max_connections=30)\n\n    # Generate HTML visualization\n    html_template = \"\"\"\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;script src=\"https://d3js.org/d3.v7.min.js\"&gt;&lt;/script&gt;\n    &lt;style&gt;\n        body { margin: 0; overflow: hidden; }\n        .node { cursor: pointer; }\n        .link { stroke: #999; stroke-opacity: 0.6; }\n        .label { font: 10px sans-serif; pointer-events: none; }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;script&gt;\n        const data = NETWORK_DATA;\n\n        const width = window.innerWidth;\n        const height = window.innerHeight;\n\n        const svg = d3.select(\"body\").append(\"svg\")\n            .attr(\"width\", width)\n            .attr(\"height\", height);\n\n        const simulation = d3.forceSimulation(data.nodes)\n            .force(\"link\", d3.forceLink(data.edges).id(d =&gt; d.id).distance(100))\n            .force(\"charge\", d3.forceManyBody().strength(-300))\n            .force(\"center\", d3.forceCenter(width / 2, height / 2));\n\n        const link = svg.append(\"g\")\n            .selectAll(\"line\")\n            .data(data.edges)\n            .join(\"line\")\n            .attr(\"class\", \"link\");\n\n        const node = svg.append(\"g\")\n            .selectAll(\"circle\")\n            .data(data.nodes)\n            .join(\"circle\")\n            .attr(\"class\", \"node\")\n            .attr(\"r\", d =&gt; Math.sqrt(d.influence) * 2)\n            .attr(\"fill\", d =&gt; d3.interpolateViridis(d.influence / 100))\n            .call(drag(simulation));\n\n        const label = svg.append(\"g\")\n            .selectAll(\"text\")\n            .data(data.nodes)\n            .join(\"text\")\n            .attr(\"class\", \"label\")\n            .text(d =&gt; \"@\" + d.label);\n\n        simulation.on(\"tick\", () =&gt; {\n            link.attr(\"x1\", d =&gt; d.source.x)\n                .attr(\"y1\", d =&gt; d.source.y)\n                .attr(\"x2\", d =&gt; d.target.x)\n                .attr(\"y2\", d =&gt; d.target.y);\n\n            node.attr(\"cx\", d =&gt; d.x).attr(\"cy\", d =&gt; d.y);\n            label.attr(\"x\", d =&gt; d.x + 10).attr(\"y\", d =&gt; d.y + 3);\n        });\n\n        function drag(simulation) {\n            return d3.drag()\n                .on(\"start\", (event, d) =&gt; {\n                    if (!event.active) simulation.alphaTarget(0.3).restart();\n                    d.fx = d.x; d.fy = d.y;\n                })\n                .on(\"drag\", (event, d) =&gt; { d.fx = event.x; d.fy = event.y; })\n                .on(\"end\", (event, d) =&gt; {\n                    if (!event.active) simulation.alphaTarget(0);\n                    d.fx = null; d.fy = null;\n                });\n        }\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\"\"\"\n\n    # Export network data\n    network_json = network.export_for_visualization(\"network_data.json\")\n\n    # Read and embed in HTML\n    with open(network_json) as f:\n        data = f.read()\n\n    html = html_template.replace(\"NETWORK_DATA\", data)\n\n    with open(\"network_visualization.html\", \"w\") as f:\n        f.write(html)\n\n    print(\"\u2705 Visualization saved to network_visualization.html\")\n    print(\"   Open in browser to view interactive network map\")\n\nasyncio.run(generate_network_viz_data([\"user1\", \"user2\", \"user3\"]))\n</code></pre>"},{"location":"cookbook/data-science/network-analysis/#best-practices","title":"Best Practices","text":"<p>Network Analysis Tips</p> <ul> <li>Start with 3-5 seed users for manageable networks</li> <li>Depth 2 gives good insights without overwhelming data</li> <li>Focus on connection patterns, not just follower counts</li> <li>Look for hidden influencers (high influence followers, low profile)</li> <li>Export to Gephi for advanced analysis</li> </ul> <p>Rate Limiting</p> <p>Network analysis requires many API calls. Use: - Aggressive rate limiting - Caching of profile data - Sampling for large networks - Background processing for deep analysis</p>"},{"location":"cookbook/data-science/network-analysis/#next-steps","title":"Next Steps","text":"<p> Trend Prediction - Predict emerging trends</p> <p> Influencer Mapping - Find influencers to partner with</p> <p> Sentiment Dashboard - Combine with sentiment analysis</p>"},{"location":"cookbook/data-science/sentiment-dashboard/","title":"Sentiment Analysis Dashboard","text":"<p>Build a real-time sentiment monitoring system for any topic, brand, or keyword.</p>"},{"location":"cookbook/data-science/sentiment-dashboard/#the-sentiment-dashboard","title":"The Sentiment Dashboard","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom xeepy.ai import SentimentAnalyzer\nfrom dataclasses import dataclass\nfrom collections import defaultdict\nfrom datetime import datetime, timedelta\nimport json\n\n@dataclass\nclass SentimentResult:\n    tweet_id: str\n    text: str\n    author: str\n    sentiment: str  # positive, negative, neutral\n    confidence: float\n    emotions: list  # joy, anger, fear, sadness, surprise\n    topics: list\n    timestamp: datetime\n\nclass SentimentDashboard:\n    \"\"\"Real-time sentiment monitoring dashboard\"\"\"\n\n    def __init__(self, keywords: list, ai_provider: str = \"openai\"):\n        self.keywords = keywords\n        self.analyzer = SentimentAnalyzer(provider=ai_provider)\n        self.results = []\n        self.hourly_stats = defaultdict(lambda: {\"positive\": 0, \"negative\": 0, \"neutral\": 0})\n\n    async def collect_and_analyze(self, limit: int = 100):\n        \"\"\"Collect tweets and analyze sentiment\"\"\"\n\n        async with Xeepy() as x:\n            for keyword in self.keywords:\n                tweets = await x.scrape.search(\n                    keyword,\n                    search_type=\"latest\",\n                    limit=limit\n                )\n\n                for tweet in tweets:\n                    # Analyze sentiment\n                    analysis = await self.analyzer.analyze(\n                        tweet.text,\n                        detect_emotions=True,\n                        extract_topics=True\n                    )\n\n                    result = SentimentResult(\n                        tweet_id=tweet.id,\n                        text=tweet.text,\n                        author=tweet.author.username,\n                        sentiment=analysis.sentiment,\n                        confidence=analysis.confidence,\n                        emotions=analysis.emotions,\n                        topics=analysis.topics,\n                        timestamp=tweet.created_at\n                    )\n\n                    self.results.append(result)\n\n                    # Update hourly stats\n                    hour_key = tweet.created_at.strftime(\"%Y-%m-%d %H:00\")\n                    self.hourly_stats[hour_key][analysis.sentiment] += 1\n\n        return self.results\n\n    def get_summary(self):\n        \"\"\"Get sentiment summary\"\"\"\n\n        if not self.results:\n            return {}\n\n        total = len(self.results)\n        positive = sum(1 for r in self.results if r.sentiment == \"positive\")\n        negative = sum(1 for r in self.results if r.sentiment == \"negative\")\n        neutral = sum(1 for r in self.results if r.sentiment == \"neutral\")\n\n        # Emotion breakdown\n        emotion_counts = defaultdict(int)\n        for r in self.results:\n            for emotion in r.emotions:\n                emotion_counts[emotion] += 1\n\n        # Topic breakdown\n        topic_sentiments = defaultdict(lambda: {\"positive\": 0, \"negative\": 0, \"neutral\": 0})\n        for r in self.results:\n            for topic in r.topics:\n                topic_sentiments[topic][r.sentiment] += 1\n\n        # Average confidence\n        avg_confidence = sum(r.confidence for r in self.results) / total\n\n        return {\n            \"total_analyzed\": total,\n            \"sentiment_distribution\": {\n                \"positive\": positive / total * 100,\n                \"negative\": negative / total * 100,\n                \"neutral\": neutral / total * 100,\n            },\n            \"emotion_breakdown\": dict(emotion_counts),\n            \"topic_sentiments\": dict(topic_sentiments),\n            \"average_confidence\": avg_confidence,\n            \"hourly_trend\": dict(self.hourly_stats),\n        }\n\n    def get_alerts(self, threshold: float = 0.3):\n        \"\"\"Get alerts for sentiment spikes\"\"\"\n\n        alerts = []\n\n        # Check for negative sentiment spike\n        recent = [r for r in self.results \n                  if r.timestamp &gt; datetime.now() - timedelta(hours=1)]\n\n        if recent:\n            negative_pct = sum(1 for r in recent if r.sentiment == \"negative\") / len(recent)\n\n            if negative_pct &gt; threshold:\n                # Find most negative tweets\n                negative_tweets = sorted(\n                    [r for r in recent if r.sentiment == \"negative\"],\n                    key=lambda x: x.confidence,\n                    reverse=True\n                )[:5]\n\n                alerts.append({\n                    \"type\": \"negative_spike\",\n                    \"severity\": \"high\" if negative_pct &gt; 0.5 else \"medium\",\n                    \"message\": f\"Negative sentiment at {negative_pct*100:.1f}% in last hour\",\n                    \"sample_tweets\": negative_tweets\n                })\n\n        return alerts\n\n    def export_report(self, filename: str = \"sentiment_report.json\"):\n        \"\"\"Export detailed report\"\"\"\n\n        report = {\n            \"generated_at\": datetime.now().isoformat(),\n            \"keywords_monitored\": self.keywords,\n            \"summary\": self.get_summary(),\n            \"alerts\": self.get_alerts(),\n            \"detailed_results\": [\n                {\n                    \"tweet_id\": r.tweet_id,\n                    \"text\": r.text,\n                    \"author\": r.author,\n                    \"sentiment\": r.sentiment,\n                    \"confidence\": r.confidence,\n                    \"emotions\": r.emotions,\n                    \"topics\": r.topics,\n                    \"timestamp\": r.timestamp.isoformat()\n                }\n                for r in self.results\n            ]\n        }\n\n        with open(filename, \"w\") as f:\n            json.dump(report, f, indent=2)\n\n        return filename\n\n# Usage\nasync def run_sentiment_dashboard():\n    dashboard = SentimentDashboard(\n        keywords=[\"your brand\", \"your product\", \"competitor\"],\n        ai_provider=\"openai\"\n    )\n\n    # Collect and analyze\n    results = await dashboard.collect_and_analyze(limit=200)\n\n    # Get summary\n    summary = dashboard.get_summary()\n\n    print(\"\ud83d\udcca SENTIMENT DASHBOARD\")\n    print(\"=\"*60)\n    print(f\"\\nKeywords: {', '.join(dashboard.keywords)}\")\n    print(f\"Tweets analyzed: {summary['total_analyzed']}\")\n\n    print(f\"\\n\ud83d\udcc8 Sentiment Distribution:\")\n    dist = summary['sentiment_distribution']\n    print(f\"   \ud83d\ude0a Positive: {dist['positive']:.1f}%\")\n    print(f\"   \ud83d\ude10 Neutral:  {dist['neutral']:.1f}%\")\n    print(f\"   \ud83d\ude1e Negative: {dist['negative']:.1f}%\")\n\n    print(f\"\\n\ud83d\udcad Emotions Detected:\")\n    for emotion, count in sorted(summary['emotion_breakdown'].items(), key=lambda x: -x[1])[:5]:\n        print(f\"   {emotion}: {count}\")\n\n    print(f\"\\n\ud83d\udccc Topics &amp; Sentiment:\")\n    for topic, sentiments in list(summary['topic_sentiments'].items())[:5]:\n        total = sum(sentiments.values())\n        neg_pct = sentiments['negative'] / total * 100 if total &gt; 0 else 0\n        print(f\"   {topic}: {neg_pct:.1f}% negative ({total} mentions)\")\n\n    # Check alerts\n    alerts = dashboard.get_alerts()\n    if alerts:\n        print(f\"\\n\ud83d\udea8 ALERTS:\")\n        for alert in alerts:\n            print(f\"   [{alert['severity'].upper()}] {alert['message']}\")\n\n    # Export\n    dashboard.export_report(\"sentiment_report.json\")\n    print(f\"\\n\u2705 Report exported to sentiment_report.json\")\n\nasyncio.run(run_sentiment_dashboard())\n</code></pre>"},{"location":"cookbook/data-science/sentiment-dashboard/#real-time-monitoring","title":"Real-Time Monitoring","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom xeepy.ai import SentimentAnalyzer\nfrom xeepy.notifications import DiscordWebhook\n\nasync def realtime_sentiment_monitor(\n    keywords: list,\n    webhook_url: str,\n    check_interval: int = 60,\n    alert_threshold: float = 0.4\n):\n    \"\"\"\n    Continuously monitor sentiment and alert on issues.\n    Perfect for brand monitoring and crisis detection.\n    \"\"\"\n\n    webhook = DiscordWebhook(webhook_url)\n    analyzer = SentimentAnalyzer(provider=\"openai\")\n    seen_tweets = set()\n\n    # Rolling window for trend detection\n    rolling_sentiments = []\n\n    async with Xeepy() as x:\n        print(f\"\ud83d\udd0d Monitoring: {', '.join(keywords)}\")\n        print(f\"   Alert threshold: {alert_threshold*100}% negative\")\n\n        while True:\n            batch_results = []\n\n            for keyword in keywords:\n                tweets = await x.scrape.search(\n                    keyword,\n                    search_type=\"latest\",\n                    limit=20,\n                    max_age_hours=1\n                )\n\n                for tweet in tweets:\n                    if tweet.id in seen_tweets:\n                        continue\n\n                    seen_tweets.add(tweet.id)\n\n                    # Analyze\n                    analysis = await analyzer.analyze(tweet.text)\n                    batch_results.append({\n                        \"tweet\": tweet,\n                        \"sentiment\": analysis.sentiment,\n                        \"confidence\": analysis.confidence\n                    })\n\n                    # Immediate alert for highly negative\n                    if analysis.sentiment == \"negative\" and analysis.confidence &gt; 0.9:\n                        await webhook.send(\n                            title=\"\u26a0\ufe0f Highly Negative Mention\",\n                            description=tweet.text[:500],\n                            fields=[\n                                {\"name\": \"Author\", \"value\": f\"@{tweet.author.username}\", \"inline\": True},\n                                {\"name\": \"Confidence\", \"value\": f\"{analysis.confidence*100:.0f}%\", \"inline\": True},\n                                {\"name\": \"Link\", \"value\": tweet.url, \"inline\": False},\n                            ],\n                            color=0xFF0000\n                        )\n\n            # Update rolling window\n            rolling_sentiments.extend(batch_results)\n\n            # Keep last 100 results\n            rolling_sentiments = rolling_sentiments[-100:]\n\n            # Calculate current sentiment\n            if rolling_sentiments:\n                negative_pct = sum(1 for r in rolling_sentiments if r[\"sentiment\"] == \"negative\") / len(rolling_sentiments)\n\n                if negative_pct &gt; alert_threshold:\n                    await webhook.send(\n                        title=\"\ud83d\udea8 Sentiment Alert\",\n                        description=f\"Negative sentiment is at {negative_pct*100:.1f}%\",\n                        fields=[\n                            {\"name\": \"Sample Size\", \"value\": str(len(rolling_sentiments)), \"inline\": True},\n                            {\"name\": \"Keywords\", \"value\": \", \".join(keywords), \"inline\": True},\n                        ],\n                        color=0xFF6347\n                    )\n\n            print(f\"[{datetime.now().strftime('%H:%M')}] Analyzed {len(batch_results)} new tweets\")\n            await asyncio.sleep(check_interval)\n\nasyncio.run(realtime_sentiment_monitor(\n    keywords=[\"your brand\", \"@yourusername\"],\n    webhook_url=\"https://discord.com/api/webhooks/...\",\n    check_interval=120,\n    alert_threshold=0.35\n))\n</code></pre>"},{"location":"cookbook/data-science/sentiment-dashboard/#comparative-sentiment-analysis","title":"Comparative Sentiment Analysis","text":"<p>Compare sentiment between you and competitors:</p> <pre><code>async def comparative_sentiment(your_brand: str, competitors: list):\n    \"\"\"Compare sentiment across brands\"\"\"\n\n    async with Xeepy() as x:\n        analyzer = SentimentAnalyzer(provider=\"openai\")\n\n        brands = [your_brand] + competitors\n        brand_sentiments = {}\n\n        for brand in brands:\n            tweets = await x.scrape.search(brand, limit=100)\n\n            sentiments = {\"positive\": 0, \"negative\": 0, \"neutral\": 0}\n            for tweet in tweets:\n                analysis = await analyzer.analyze(tweet.text)\n                sentiments[analysis.sentiment] += 1\n\n            total = sum(sentiments.values())\n            brand_sentiments[brand] = {\n                \"positive_pct\": sentiments[\"positive\"] / total * 100 if total &gt; 0 else 0,\n                \"negative_pct\": sentiments[\"negative\"] / total * 100 if total &gt; 0 else 0,\n                \"net_sentiment\": (sentiments[\"positive\"] - sentiments[\"negative\"]) / total * 100 if total &gt; 0 else 0,\n                \"total_mentions\": total\n            }\n\n        # Print comparison\n        print(\"\ud83d\udcca BRAND SENTIMENT COMPARISON\")\n        print(\"=\"*60)\n\n        for brand, data in sorted(brand_sentiments.items(), key=lambda x: -x[1][\"net_sentiment\"]):\n            sentiment_bar = \"+\" * int(data[\"positive_pct\"] / 5) + \"-\" * int(data[\"negative_pct\"] / 5)\n            print(f\"\\n{brand}:\")\n            print(f\"   Net Sentiment: {data['net_sentiment']:+.1f}%\")\n            print(f\"   Positive: {data['positive_pct']:.1f}% | Negative: {data['negative_pct']:.1f}%\")\n            print(f\"   [{sentiment_bar}]\")\n\nasyncio.run(comparative_sentiment(\"your_brand\", [\"competitor1\", \"competitor2\"]))\n</code></pre>"},{"location":"cookbook/data-science/sentiment-dashboard/#sentiment-based-actions","title":"Sentiment-Based Actions","text":"<p>Automatically respond based on sentiment:</p> <pre><code>async def sentiment_based_engagement():\n    \"\"\"Engage differently based on sentiment\"\"\"\n\n    async with Xeepy() as x:\n        analyzer = SentimentAnalyzer(provider=\"openai\")\n        ai = ContentGenerator(provider=\"openai\")\n\n        # Monitor mentions\n        mentions = await x.scrape.mentions(\"me\", limit=50)\n\n        for mention in mentions:\n            analysis = await analyzer.analyze(mention.text)\n\n            if analysis.sentiment == \"positive\":\n                # Like and thank\n                await x.engage.like(mention.url)\n\n                reply = await ai.generate_reply(\n                    mention.text,\n                    style=\"grateful\",\n                    tone=\"friendly\"\n                )\n                await x.engage.reply(mention.url, reply)\n                print(f\"\ud83d\udc9a Thanked @{mention.author.username}\")\n\n            elif analysis.sentiment == \"negative\":\n                # Prioritize for manual review, but acknowledge\n                reply = await ai.generate_reply(\n                    mention.text,\n                    style=\"apologetic\",\n                    tone=\"professional\",\n                    acknowledge_issue=True,\n                    offer_help=True\n                )\n\n                # Flag for human review\n                print(f\"\ud83d\udd34 NEEDS ATTENTION: @{mention.author.username}\")\n                print(f\"   Tweet: {mention.text[:100]}...\")\n                print(f\"   Suggested reply: {reply}\")\n\n                if input(\"   Send reply? (y/n): \").lower() == \"y\":\n                    await x.engage.reply(mention.url, reply)\n\nasyncio.run(sentiment_based_engagement())\n</code></pre>"},{"location":"cookbook/data-science/sentiment-dashboard/#historical-sentiment-trends","title":"Historical Sentiment Trends","text":"<p>Track sentiment over time:</p> <pre><code>async def sentiment_history(keyword: str, days: int = 30):\n    \"\"\"Build historical sentiment data\"\"\"\n\n    from collections import defaultdict\n\n    async with Xeepy() as x:\n        analyzer = SentimentAnalyzer(provider=\"openai\")\n\n        daily_sentiments = defaultdict(lambda: {\"positive\": 0, \"negative\": 0, \"neutral\": 0, \"total\": 0})\n\n        # Get historical tweets\n        tweets = await x.scrape.search(\n            keyword,\n            limit=1000,  # Get as many as possible\n            sort=\"latest\"\n        )\n\n        for tweet in tweets:\n            day_key = tweet.created_at.strftime(\"%Y-%m-%d\")\n\n            analysis = await analyzer.analyze(tweet.text)\n            daily_sentiments[day_key][analysis.sentiment] += 1\n            daily_sentiments[day_key][\"total\"] += 1\n\n        # Print trend\n        print(f\"\ud83d\udcc8 SENTIMENT TREND: {keyword}\")\n        print(\"=\"*60)\n        print(f\"{'Date':&lt;12} {'Positive':&gt;10} {'Negative':&gt;10} {'Net':&gt;10}\")\n        print(\"-\"*60)\n\n        for day in sorted(daily_sentiments.keys())[-days:]:\n            data = daily_sentiments[day]\n            if data[\"total\"] &gt; 0:\n                pos_pct = data[\"positive\"] / data[\"total\"] * 100\n                neg_pct = data[\"negative\"] / data[\"total\"] * 100\n                net = pos_pct - neg_pct\n\n                indicator = \"\ud83d\udcc8\" if net &gt; 0 else \"\ud83d\udcc9\" if net &lt; 0 else \"\u2796\"\n                print(f\"{day:&lt;12} {pos_pct:&gt;9.1f}% {neg_pct:&gt;9.1f}% {net:&gt;+9.1f}% {indicator}\")\n\nasyncio.run(sentiment_history(\"your brand\", days=14))\n</code></pre>"},{"location":"cookbook/data-science/sentiment-dashboard/#best-practices","title":"Best Practices","text":"<p>Sentiment Analysis Tips</p> <ul> <li>Use AI for nuanced understanding (sarcasm, context)</li> <li>Set realistic thresholds (some negativity is normal)</li> <li>Focus on trends, not individual tweets</li> <li>Cross-reference with engagement metrics</li> <li>Consider context and industry norms</li> </ul> <p>Limitations</p> <ul> <li>AI can miss sarcasm and cultural context</li> <li>High volume may require batch processing</li> <li>Real-time monitoring has API costs</li> <li>Not all negativity is actionable</li> </ul>"},{"location":"cookbook/data-science/sentiment-dashboard/#next-steps","title":"Next Steps","text":"<p> Network Analysis - Map influence networks</p> <p> Trend Prediction - Predict what's next</p> <p> Crisis Detection - Early warning systems</p>"},{"location":"cookbook/data-science/trend-prediction/","title":"Trend Prediction","text":"<p>Use data analysis to predict emerging trends before they go mainstream.</p>"},{"location":"cookbook/data-science/trend-prediction/#early-trend-detection","title":"Early Trend Detection","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom datetime import datetime, timedelta\nfrom collections import Counter, defaultdict\nimport re\n\nclass TrendPredictor:\n    \"\"\"Detect emerging trends before they peak\"\"\"\n\n    def __init__(self):\n        self.keyword_history = defaultdict(list)\n        self.velocity_threshold = 2.0  # 2x growth = trending\n\n    async def scan_niche(\n        self,\n        seed_keywords: list,\n        niche_accounts: list,\n        hours: int = 24\n    ):\n        \"\"\"\n        Scan a niche for emerging keywords and topics.\n\n        Strategy:\n        1. Monitor seed keywords for new associated terms\n        2. Track what niche leaders are talking about\n        3. Identify keywords with accelerating velocity\n        \"\"\"\n\n        async with Xeepy() as x:\n            all_text = []\n\n            # Gather recent content from niche\n            for keyword in seed_keywords:\n                results = await x.scrape.search(\n                    keyword,\n                    limit=200,\n                    since_hours=hours\n                )\n                all_text.extend([r.text for r in results])\n\n            # Get tweets from niche leaders\n            for account in niche_accounts:\n                tweets = await x.scrape.tweets(account, limit=50)\n                all_text.extend([t.text for t in tweets])\n\n            # Extract and count keywords\n            keywords = self._extract_keywords(all_text)\n\n            # Calculate velocity for each keyword\n            trending = []\n\n            for keyword, count in keywords.most_common(100):\n                velocity = self._calculate_velocity(keyword, count)\n\n                if velocity &gt;= self.velocity_threshold:\n                    trending.append({\n                        \"keyword\": keyword,\n                        \"mentions\": count,\n                        \"velocity\": velocity,\n                        \"predicted_peak\": self._predict_peak(velocity)\n                    })\n\n            return sorted(trending, key=lambda x: -x[\"velocity\"])\n\n    def _extract_keywords(self, texts: list) -&gt; Counter:\n        \"\"\"Extract meaningful keywords from text\"\"\"\n\n        keywords = Counter()\n\n        # Stop words to ignore\n        stop_words = {\n            \"the\", \"a\", \"an\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\",\n            \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"will\", \"would\",\n            \"could\", \"should\", \"may\", \"might\", \"must\", \"this\", \"that\",\n            \"these\", \"those\", \"i\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\",\n            \"what\", \"which\", \"who\", \"when\", \"where\", \"why\", \"how\", \"all\",\n            \"each\", \"every\", \"both\", \"few\", \"more\", \"most\", \"other\", \"some\",\n            \"such\", \"no\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\",\n            \"very\", \"just\", \"can\", \"now\", \"new\", \"one\", \"get\", \"got\", \"like\",\n            \"make\", \"know\", \"think\", \"see\", \"come\", \"want\", \"use\", \"find\",\n            \"give\", \"tell\", \"try\", \"leave\", \"call\", \"keep\", \"let\", \"begin\",\n            \"seem\", \"help\", \"show\", \"hear\", \"play\", \"run\", \"move\", \"live\",\n            \"believe\", \"bring\", \"happen\", \"write\", \"sit\", \"stand\", \"lose\",\n            \"pay\", \"meet\", \"include\", \"continue\", \"set\", \"learn\", \"change\",\n            \"lead\", \"understand\", \"watch\", \"follow\", \"stop\", \"create\", \"speak\",\n            \"read\", \"spend\", \"grow\", \"open\", \"walk\", \"win\", \"teach\", \"offer\",\n            \"remember\", \"consider\", \"appear\", \"buy\", \"wait\", \"serve\", \"die\",\n            \"send\", \"build\", \"stay\", \"fall\", \"cut\", \"reach\", \"kill\", \"remain\",\n            \"https\", \"http\", \"com\", \"www\", \"amp\", \"rt\"\n        }\n\n        for text in texts:\n            # Clean and tokenize\n            text = text.lower()\n            text = re.sub(r'https?://\\S+', '', text)  # Remove URLs\n            text = re.sub(r'@\\w+', '', text)  # Remove mentions\n            words = re.findall(r'\\b[a-z]{3,15}\\b', text)\n\n            for word in words:\n                if word not in stop_words:\n                    keywords[word] += 1\n\n            # Extract hashtags separately (high signal)\n            hashtags = re.findall(r'#(\\w+)', text)\n            for tag in hashtags:\n                keywords[f\"#{tag.lower()}\"] += 2  # Weight hashtags higher\n\n        return keywords\n\n    def _calculate_velocity(self, keyword: str, current_count: int) -&gt; float:\n        \"\"\"Calculate growth velocity of a keyword\"\"\"\n\n        history = self.keyword_history[keyword]\n\n        if not history:\n            # First observation\n            self.keyword_history[keyword].append({\n                \"timestamp\": datetime.now(),\n                \"count\": current_count\n            })\n            return 1.0\n\n        # Compare to previous observation\n        last = history[-1]\n        hours_elapsed = (datetime.now() - last[\"timestamp\"]).total_seconds() / 3600\n\n        if hours_elapsed &lt; 1:\n            return 1.0\n\n        # Calculate hourly growth rate\n        if last[\"count\"] &gt; 0:\n            growth_rate = current_count / last[\"count\"]\n        else:\n            growth_rate = current_count\n\n        # Normalize by time\n        velocity = growth_rate / max(hours_elapsed, 1)\n\n        # Update history\n        history.append({\n            \"timestamp\": datetime.now(),\n            \"count\": current_count\n        })\n\n        # Keep last 10 observations\n        self.keyword_history[keyword] = history[-10:]\n\n        return velocity\n\n    def _predict_peak(self, velocity: float) -&gt; str:\n        \"\"\"Estimate when trend will peak\"\"\"\n\n        if velocity &gt;= 5.0:\n            return \"6-12 hours\"\n        elif velocity &gt;= 3.0:\n            return \"12-24 hours\"\n        elif velocity &gt;= 2.0:\n            return \"24-48 hours\"\n        else:\n            return \"48+ hours\"\n\n# Usage\nasync def find_emerging_trends():\n    predictor = TrendPredictor()\n\n    trends = await predictor.scan_niche(\n        seed_keywords=[\"AI\", \"GPT\", \"LLM\", \"machine learning\"],\n        niche_accounts=[\"OpenAI\", \"AnthropicAI\", \"GoogleAI\"],\n        hours=24\n    )\n\n    print(\"\ud83d\udd2e EMERGING TRENDS\")\n    print(\"=\" * 60)\n\n    for trend in trends[:15]:\n        peak = trend[\"predicted_peak\"]\n        velocity = trend[\"velocity\"]\n\n        # Visual indicator\n        if velocity &gt;= 5:\n            indicator = \"\ud83d\udd25\ud83d\udd25\ud83d\udd25\"\n        elif velocity &gt;= 3:\n            indicator = \"\ud83d\udd25\ud83d\udd25\"\n        else:\n            indicator = \"\ud83d\udd25\"\n\n        print(f\"\\n{indicator} {trend['keyword']}\")\n        print(f\"   Mentions: {trend['mentions']}\")\n        print(f\"   Velocity: {velocity:.1f}x\")\n        print(f\"   Peak in: {peak}\")\n\nasyncio.run(find_emerging_trends())\n</code></pre>"},{"location":"cookbook/data-science/trend-prediction/#hashtag-trend-analysis","title":"Hashtag Trend Analysis","text":"<pre><code>async def analyze_hashtag_trajectory(hashtag: str, days: int = 7):\n    \"\"\"\n    Analyze a hashtag's growth trajectory to predict if it will trend.\n    \"\"\"\n\n    async with Xeepy() as x:\n        # Get historical data by searching different time periods\n        time_series = []\n\n        for day_offset in range(days, 0, -1):\n            since = datetime.now() - timedelta(days=day_offset)\n            until = datetime.now() - timedelta(days=day_offset-1)\n\n            results = await x.scrape.search(\n                f\"#{hashtag}\",\n                since=since.strftime(\"%Y-%m-%d\"),\n                until=until.strftime(\"%Y-%m-%d\"),\n                limit=500\n            )\n\n            time_series.append({\n                \"date\": since.date(),\n                \"count\": len(results),\n                \"avg_engagement\": sum(r.like_count + r.retweet_count for r in results) / max(len(results), 1)\n            })\n\n        # Analyze trajectory\n        counts = [t[\"count\"] for t in time_series]\n\n        # Calculate trend direction\n        if len(counts) &gt;= 3:\n            recent_avg = sum(counts[-3:]) / 3\n            earlier_avg = sum(counts[:3]) / 3\n\n            if earlier_avg &gt; 0:\n                growth_rate = (recent_avg - earlier_avg) / earlier_avg * 100\n            else:\n                growth_rate = 100 if recent_avg &gt; 0 else 0\n        else:\n            growth_rate = 0\n\n        # Predict future\n        if growth_rate &gt; 50:\n            prediction = \"\ud83d\udcc8 ACCELERATING - Likely to trend soon\"\n        elif growth_rate &gt; 20:\n            prediction = \"\u2197\ufe0f GROWING - Building momentum\"\n        elif growth_rate &gt; -10:\n            prediction = \"\u27a1\ufe0f STABLE - Consistent usage\"\n        elif growth_rate &gt; -30:\n            prediction = \"\u2198\ufe0f DECLINING - Losing interest\"\n        else:\n            prediction = \"\ud83d\udcc9 FALLING - Trend is over\"\n\n        print(f\"\\n#{hashtag} Trajectory Analysis\")\n        print(\"=\" * 50)\n        print(f\"7-day Growth: {growth_rate:+.1f}%\")\n        print(f\"Prediction: {prediction}\")\n        print(f\"\\nDaily breakdown:\")\n\n        for t in time_series:\n            bar = \"\u2588\" * min(int(t[\"count\"] / 10), 30)\n            print(f\"  {t['date']}: {bar} {t['count']}\")\n\n        return {\n            \"hashtag\": hashtag,\n            \"time_series\": time_series,\n            \"growth_rate\": growth_rate,\n            \"prediction\": prediction\n        }\n\nasyncio.run(analyze_hashtag_trajectory(\"AIagents\"))\n</code></pre>"},{"location":"cookbook/data-science/trend-prediction/#viral-content-predictor","title":"Viral Content Predictor","text":"<pre><code>async def predict_viral_potential(tweet_url: str):\n    \"\"\"\n    Analyze a tweet's early metrics to predict viral potential.\n\n    Based on research: Tweets that go viral typically show\n    specific patterns in their first 1-2 hours.\n    \"\"\"\n\n    async with Xeepy() as x:\n        tweet = await x.scrape.tweet(tweet_url)\n        author = await x.scrape.profile(tweet.author_username)\n\n        # Calculate metrics\n        hours_since_post = (datetime.now() - tweet.created_at).total_seconds() / 3600\n\n        if hours_since_post &lt; 0.5:\n            print(\"\u26a0\ufe0f Too early to predict (wait 30+ minutes)\")\n            return None\n\n        # Engagement velocity\n        likes_per_hour = tweet.like_count / max(hours_since_post, 0.5)\n        retweets_per_hour = tweet.retweet_count / max(hours_since_post, 0.5)\n        replies_per_hour = tweet.reply_count / max(hours_since_post, 0.5)\n\n        # Relative to author's typical performance\n        expected_likes = author.followers_count * 0.001  # ~0.1% engagement baseline\n        performance_ratio = tweet.like_count / max(expected_likes, 1)\n\n        # Viral signals\n        signals = []\n        score = 0\n\n        # High velocity\n        if likes_per_hour &gt; 100:\n            signals.append(\"\ud83d\udd25 High like velocity (100+/hr)\")\n            score += 30\n        elif likes_per_hour &gt; 50:\n            signals.append(\"\u2728 Good like velocity (50+/hr)\")\n            score += 20\n\n        # Retweet ratio (retweets spread content)\n        rt_ratio = tweet.retweet_count / max(tweet.like_count, 1)\n        if rt_ratio &gt; 0.3:\n            signals.append(\"\ud83d\udd04 High retweet ratio (spreading)\")\n            score += 25\n        elif rt_ratio &gt; 0.15:\n            signals.append(\"\ud83d\udce4 Good retweet ratio\")\n            score += 15\n\n        # Reply engagement (conversations = algorithm boost)\n        if replies_per_hour &gt; 20:\n            signals.append(\"\ud83d\udcac High reply engagement\")\n            score += 20\n\n        # Outperforming baseline\n        if performance_ratio &gt; 10:\n            signals.append(\"\ud83d\udcca 10x above baseline!\")\n            score += 25\n        elif performance_ratio &gt; 5:\n            signals.append(\"\ud83d\udcc8 5x above baseline\")\n            score += 15\n\n        # Quote tweets (high signal)\n        if tweet.quote_count &gt; tweet.retweet_count * 0.2:\n            signals.append(\"\ud83d\udcad High quote engagement\")\n            score += 10\n\n        # Predict outcome\n        if score &gt;= 70:\n            prediction = \"\ud83d\ude80 HIGH VIRAL POTENTIAL - Act now!\"\n            recommendation = \"Engage immediately, ride the wave\"\n        elif score &gt;= 50:\n            prediction = \"\ud83d\udcc8 GOOD POTENTIAL - Worth watching\"\n            recommendation = \"Monitor closely, engage if relevant\"\n        elif score &gt;= 30:\n            prediction = \"\ud83c\udf31 MODERATE POTENTIAL\"\n            recommendation = \"May grow slowly, selective engagement\"\n        else:\n            prediction = \"\ud83d\udcca LOW VIRAL POTENTIAL\"\n            recommendation = \"Standard content, normal engagement\"\n\n        print(f\"\\n\ud83d\udd2e VIRAL POTENTIAL ANALYSIS\")\n        print(\"=\" * 50)\n        print(f\"Tweet: {tweet.text[:100]}...\")\n        print(f\"Author: @{tweet.author_username} ({author.followers_count:,} followers)\")\n        print(f\"Age: {hours_since_post:.1f} hours\")\n        print(f\"\\nCurrent Metrics:\")\n        print(f\"  \u2764\ufe0f {tweet.like_count:,} likes ({likes_per_hour:.0f}/hr)\")\n        print(f\"  \ud83d\udd04 {tweet.retweet_count:,} retweets ({retweets_per_hour:.0f}/hr)\")\n        print(f\"  \ud83d\udcac {tweet.reply_count:,} replies ({replies_per_hour:.0f}/hr)\")\n        print(f\"\\nViral Signals:\")\n        for signal in signals:\n            print(f\"  {signal}\")\n        print(f\"\\nViral Score: {score}/100\")\n        print(f\"Prediction: {prediction}\")\n        print(f\"Recommendation: {recommendation}\")\n\n        return {\n            \"score\": score,\n            \"prediction\": prediction,\n            \"signals\": signals,\n            \"metrics\": {\n                \"likes_per_hour\": likes_per_hour,\n                \"retweets_per_hour\": retweets_per_hour,\n                \"performance_ratio\": performance_ratio\n            }\n        }\n\nasyncio.run(predict_viral_potential(\"https://x.com/user/status/123\"))\n</code></pre>"},{"location":"cookbook/data-science/trend-prediction/#topic-lifecycle-prediction","title":"Topic Lifecycle Prediction","text":"<pre><code>async def analyze_topic_lifecycle(topic: str):\n    \"\"\"\n    Determine where a topic is in its lifecycle:\n    - Emerging (early adopters)\n    - Growing (rapid adoption)\n    - Mainstream (peak awareness)\n    - Declining (interest waning)\n    - Niche (stable but small)\n    \"\"\"\n\n    async with Xeepy() as x:\n        # Get tweets from different time periods\n        now = datetime.now()\n        periods = []\n\n        for weeks_ago in [4, 3, 2, 1, 0]:\n            since = now - timedelta(weeks=weeks_ago+1)\n            until = now - timedelta(weeks=weeks_ago) if weeks_ago &gt; 0 else now\n\n            results = await x.scrape.search(\n                topic,\n                since=since.strftime(\"%Y-%m-%d\"),\n                until=until.strftime(\"%Y-%m-%d\"),\n                limit=500\n            )\n\n            # Analyze this period\n            unique_authors = len(set(r.author_username for r in results))\n            avg_likes = sum(r.like_count for r in results) / max(len(results), 1)\n\n            periods.append({\n                \"week\": weeks_ago,\n                \"volume\": len(results),\n                \"unique_authors\": unique_authors,\n                \"avg_engagement\": avg_likes\n            })\n\n        # Analyze trajectory\n        volumes = [p[\"volume\"] for p in periods]\n        authors = [p[\"unique_authors\"] for p in periods]\n\n        # Week-over-week growth\n        if volumes[-2] &gt; 0:\n            volume_growth = (volumes[-1] - volumes[-2]) / volumes[-2] * 100\n        else:\n            volume_growth = 100 if volumes[-1] &gt; 0 else 0\n\n        # Author diversity (more authors = broader adoption)\n        author_ratio = authors[-1] / max(volumes[-1], 1) * 100  # % unique\n\n        # Determine lifecycle stage\n        if volume_growth &gt; 50 and author_ratio &gt; 50:\n            stage = \"\ud83d\ude80 EMERGING\"\n            advice = \"Get in early! High potential for first-mover advantage.\"\n        elif volume_growth &gt; 20 and volumes[-1] &gt; volumes[0]:\n            stage = \"\ud83d\udcc8 GROWING\"\n            advice = \"Active growth phase. Good time to establish presence.\"\n        elif -10 &lt;= volume_growth &lt;= 20 and volumes[-1] &gt; 100:\n            stage = \"\ud83c\udfaf MAINSTREAM\"\n            advice = \"Peak awareness. Content here needs to be exceptional.\"\n        elif volume_growth &lt; -20:\n            stage = \"\ud83d\udcc9 DECLINING\"\n            advice = \"Interest waning. Consider pivoting to related topics.\"\n        else:\n            stage = \"\ud83d\udd2c NICHE\"\n            advice = \"Small but stable audience. Good for targeted engagement.\"\n\n        print(f\"\\n\ud83d\udcca TOPIC LIFECYCLE ANALYSIS: {topic}\")\n        print(\"=\" * 50)\n        print(f\"\\nLifecycle Stage: {stage}\")\n        print(f\"Volume Growth: {volume_growth:+.1f}%\")\n        print(f\"Author Diversity: {author_ratio:.0f}%\")\n\n        print(f\"\\nWeekly Volume:\")\n        for p in periods:\n            bar = \"\u2588\" * min(int(p[\"volume\"] / 10), 30)\n            week_label = \"This week\" if p[\"week\"] == 0 else f\"{p['week']} weeks ago\"\n            print(f\"  {week_label:15}: {bar} {p['volume']}\")\n\n        print(f\"\\n\ud83d\udca1 Advice: {advice}\")\n\n        return {\n            \"topic\": topic,\n            \"stage\": stage,\n            \"volume_growth\": volume_growth,\n            \"advice\": advice,\n            \"periods\": periods\n        }\n\nasyncio.run(analyze_topic_lifecycle(\"AI agents\"))\n</code></pre>"},{"location":"cookbook/data-science/trend-prediction/#continuous-trend-monitoring","title":"Continuous Trend Monitoring","text":"<pre><code>async def continuous_trend_monitor(\n    niches: dict,  # {\"niche_name\": {\"keywords\": [...], \"accounts\": [...]}}\n    check_interval_hours: int = 6\n):\n    \"\"\"\n    Run continuous trend monitoring across multiple niches.\n    \"\"\"\n\n    predictor = TrendPredictor()\n\n    async with Xeepy() as x:\n        while True:\n            all_trends = {}\n\n            for niche_name, config in niches.items():\n                print(f\"\\n\ud83d\udd0d Scanning {niche_name}...\")\n\n                trends = await predictor.scan_niche(\n                    seed_keywords=config[\"keywords\"],\n                    niche_accounts=config[\"accounts\"],\n                    hours=24\n                )\n\n                all_trends[niche_name] = trends[:10]  # Top 10 per niche\n\n            # Generate report\n            report = f\"\"\"\n\ud83d\udd2e TREND PREDICTION REPORT\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n{'='*50}\n\"\"\"\n\n            for niche, trends in all_trends.items():\n                report += f\"\\n\\n\ud83d\udcca {niche.upper()}\\n\"\n                report += \"-\" * 30 + \"\\n\"\n\n                for t in trends[:5]:\n                    velocity = t[\"velocity\"]\n                    emoji = \"\ud83d\udd25\" if velocity &gt;= 3 else \"\u2728\" if velocity &gt;= 2 else \"\ud83d\udcc8\"\n                    report += f\"{emoji} {t['keyword']}: {velocity:.1f}x velocity\\n\"\n\n            # Send to notifications\n            await x.notify.discord(\n                webhook_url=\"...\",\n                content=report\n            )\n\n            print(report)\n            print(f\"\\n\u23f0 Next scan in {check_interval_hours} hours...\")\n            await asyncio.sleep(check_interval_hours * 3600)\n\n# Configure and run\nniches = {\n    \"AI/ML\": {\n        \"keywords\": [\"AI\", \"GPT\", \"LLM\", \"machine learning\"],\n        \"accounts\": [\"OpenAI\", \"AnthropicAI\"]\n    },\n    \"Crypto\": {\n        \"keywords\": [\"Bitcoin\", \"Ethereum\", \"DeFi\", \"Web3\"],\n        \"accounts\": [\"VitalikButerin\", \"caborik\"]\n    },\n    \"Startups\": {\n        \"keywords\": [\"startup\", \"founder\", \"YC\", \"Series A\"],\n        \"accounts\": [\"ycombinator\", \"paulg\"]\n    }\n}\n\nasyncio.run(continuous_trend_monitor(niches, check_interval_hours=6))\n</code></pre>"},{"location":"cookbook/data-science/trend-prediction/#best-practices","title":"Best Practices","text":"<p>Trend Prediction Tips</p> <ul> <li>Velocity matters more than absolute numbers</li> <li>Track multiple signals (volume, authors, engagement)</li> <li>Compare within niches, not across them</li> <li>Act fast on emerging trends</li> <li>Validate with multiple data points</li> </ul> <p>Limitations</p> <ul> <li>Past performance doesn't guarantee future trends</li> <li>External events can cause sudden shifts</li> <li>Sample sizes may be limited</li> <li>Use as one input, not the only decision factor</li> </ul>"},{"location":"cookbook/data-science/trend-prediction/#next-steps","title":"Next Steps","text":"<p> Viral Content Hunting - Find viral content early</p> <p> Network Analysis - Map influence networks</p> <p> Sentiment Analysis - Understand market mood</p>"},{"location":"cookbook/growth/","title":"\ud83d\ude80 Growth Hacking Cookbook","text":"<p>Advanced strategies and automation recipes for rapid X/Twitter growth. These are battle-tested techniques used by growth marketers and indie hackers.</p> <p>Use Responsibly</p> <p>These techniques are powerful. Always prioritize authentic engagement over pure metrics.</p>"},{"location":"cookbook/growth/#the-10k-follower-playbook","title":"The 10K Follower Playbook","text":"<p>A systematic approach to reach 10,000 followers in 90 days.</p>"},{"location":"cookbook/growth/#phase-1-foundation-days-1-30","title":"Phase 1: Foundation (Days 1-30)","text":"<pre><code>\"\"\"\nThe Foundation Phase: Build your profile, find your niche, start engaging.\nGoal: 0 \u2192 1,000 followers\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\nfrom xeepy.ai import ContentGenerator\n\nasync def foundation_phase():\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\")\n\n        # Step 1: Identify your niche's top accounts\n        niche_leaders = [\n            \"indiehackers\", \"levelsio\", \"marc_louvion\",\n            \"arlogilbert\", \"csallen\"\n        ]\n\n        # Step 2: Analyze what works in your niche\n        print(\"\ud83d\udd0d Analyzing successful content in your niche...\")\n        content_insights = []\n\n        for leader in niche_leaders:\n            tweets = await x.scrape.tweets(leader, limit=100)\n\n            # Find their best performing tweets\n            top_tweets = sorted(tweets, key=lambda t: t.likes, reverse=True)[:10]\n\n            for tweet in top_tweets:\n                content_insights.append({\n                    \"author\": leader,\n                    \"text\": tweet.text,\n                    \"likes\": tweet.likes,\n                    \"type\": categorize_content(tweet.text)\n                })\n\n        # Step 3: Daily engagement routine\n        print(\"\ud83d\udcac Starting daily engagement...\")\n\n        # Engage with your target audience\n        for leader in niche_leaders[:3]:  # 3 accounts per day\n            followers = await x.scrape.followers(leader, limit=100)\n\n            # Filter for ideal connections\n            ideal = [\n                f for f in followers\n                if 500 &lt; f.followers_count &lt; 50000  # Similar size accounts\n                and f.tweet_count &gt; 100  # Active\n                and not f.is_following  # Not already following\n            ]\n\n            # Follow and engage\n            for user in ideal[:10]:  # 10 per leader = 30/day\n                await x.follow.user(user.username)\n\n                # Like their recent tweet\n                their_tweets = await x.scrape.tweets(user.username, limit=3)\n                if their_tweets:\n                    await x.engage.like(their_tweets[0].url)\n\n        print(\"\u2705 Foundation phase daily routine complete!\")\n\ndef categorize_content(text: str) -&gt; str:\n    \"\"\"Categorize content type for analysis\"\"\"\n    if \"thread\" in text.lower() or \"\ud83e\uddf5\" in text:\n        return \"thread\"\n    elif \"?\" in text:\n        return \"question\"\n    elif any(word in text.lower() for word in [\"tip\", \"trick\", \"how to\", \"guide\"]):\n        return \"educational\"\n    elif any(word in text.lower() for word in [\"launch\", \"shipped\", \"built\"]):\n        return \"build_in_public\"\n    else:\n        return \"general\"\n</code></pre>"},{"location":"cookbook/growth/#phase-2-content-engine-days-31-60","title":"Phase 2: Content Engine (Days 31-60)","text":"<pre><code>\"\"\"\nThe Content Engine: Create viral-worthy content consistently.\nGoal: 1,000 \u2192 5,000 followers\n\"\"\"\nimport asyncio\nfrom datetime import datetime, timedelta\nfrom xeepy import Xeepy\nfrom xeepy.ai import ContentGenerator\n\nasync def content_engine_phase():\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\")\n\n        # Step 1: Find your best posting times\n        best_times = await x.analytics.best_time_to_post()\n        print(f\"\ud83d\udcc5 Your optimal posting times:\")\n        for slot in best_times.top_slots[:5]:\n            print(f\"  {slot.day} at {slot.hour}:00\")\n\n        # Step 2: Generate content calendar\n        content_calendar = []\n        content_types = [\n            {\"type\": \"thread\", \"frequency\": 2, \"day\": [\"Tuesday\", \"Thursday\"]},\n            {\"type\": \"tip\", \"frequency\": 5, \"day\": \"daily\"},\n            {\"type\": \"personal_story\", \"frequency\": 1, \"day\": [\"Sunday\"]},\n            {\"type\": \"engagement_hook\", \"frequency\": 7, \"day\": \"daily\"},\n        ]\n\n        for content in content_types:\n            ideas = await ai.generate_content_ideas(\n                niche=\"indie hacking/building\",\n                content_type=content[\"type\"],\n                count=content[\"frequency\"] * 4  # 4 weeks worth\n            )\n            content_calendar.extend(ideas)\n\n        # Step 3: The Viral Thread Formula\n        print(\"\ud83e\uddf5 Generating viral thread framework...\")\n\n        thread_topics = [\n            \"How I went from $0 to $10k MRR\",\n            \"10 tools that 10x'd my productivity\",\n            \"What I learned from 100 failed projects\",\n            \"The exact strategy I used to get my first 1000 users\"\n        ]\n\n        for topic in thread_topics:\n            thread = await ai.generate_thread(\n                topic=topic,\n                style=\"storytelling\",\n                thread_length=10,\n                hooks={\n                    \"opener\": \"contrarian_statement\",\n                    \"closer\": \"call_to_action\"\n                }\n            )\n\n            print(f\"\\n\ud83d\udcdd Thread: {topic}\")\n            for i, tweet in enumerate(thread.tweets[:3]):\n                print(f\"  {i+1}. {tweet[:80]}...\")\n\n        # Step 4: Engagement amplification\n        print(\"\\n\ud83d\udd25 Setting up engagement amplification...\")\n\n        # Find tweets in your niche that are getting traction\n        trending = await x.scrape.search(\n            \"indie hackers OR building in public\",\n            limit=50,\n            min_likes=50,\n            max_age_hours=6  # Recent and gaining traction\n        )\n\n        for tweet in trending[:10]:\n            # Generate a valuable comment\n            comment = await ai.generate_reply(\n                tweet_text=tweet.text,\n                style=\"add_value\",  # Add insight, not just \"great post!\"\n                max_length=280\n            )\n\n            print(f\"  Comment on @{tweet.author.username}'s tweet:\")\n            print(f\"    Original: {tweet.text[:60]}...\")\n            print(f\"    Reply: {comment}\")\n\n        print(\"\u2705 Content engine phase complete!\")\n</code></pre>"},{"location":"cookbook/growth/#phase-3-scale-days-61-90","title":"Phase 3: Scale (Days 61-90)","text":"<pre><code>\"\"\"\nThe Scale Phase: Leverage systems and automation.\nGoal: 5,000 \u2192 10,000 followers\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\nfrom xeepy.ai import ContentGenerator\n\nasync def scale_phase():\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\")\n\n        # Step 1: Optimize your content based on data\n        print(\"\ud83d\udcca Analyzing your content performance...\")\n\n        my_tweets = await x.scrape.tweets(\"your_username\", limit=200)\n\n        # Find patterns in your best content\n        top_performers = sorted(my_tweets, key=lambda t: t.engagement_rate, reverse=True)[:20]\n\n        patterns = {\n            \"best_hours\": [],\n            \"best_formats\": [],\n            \"best_topics\": [],\n            \"avg_length\": 0\n        }\n\n        for tweet in top_performers:\n            patterns[\"best_hours\"].append(tweet.created_at.hour)\n            patterns[\"avg_length\"] += len(tweet.text)\n\n        patterns[\"avg_length\"] //= len(top_performers)\n\n        print(f\"  Best posting hours: {set(patterns['best_hours'])}\")\n        print(f\"  Optimal tweet length: ~{patterns['avg_length']} chars\")\n\n        # Step 2: Set up collaboration pipeline\n        print(\"\\n\ud83e\udd1d Identifying collaboration opportunities...\")\n\n        # Find accounts in your range that engage with similar content\n        collab_candidates = await find_collab_candidates(x, ai)\n\n        for candidate in collab_candidates[:10]:\n            print(f\"  @{candidate.username} - {candidate.reason}\")\n\n        # Step 3: Automate maintenance\n        print(\"\\n\ud83e\udd16 Setting up automation...\")\n\n        # Daily cleanup: unfollow non-followers\n        await x.unfollow.non_followers(\n            max_unfollows=25,\n            whitelist_file=\"whitelist.txt\",\n            min_days_following=7  # Give them a week\n        )\n\n        # Maintain healthy ratio\n        profile = await x.scrape.profile(\"your_username\")\n        ratio = profile.followers_count / max(profile.following_count, 1)\n\n        if ratio &lt; 1.0:\n            print(f\"  \u26a0\ufe0f Follower ratio is {ratio:.2f}. Cleaning up following...\")\n            await x.unfollow.smart(\n                max_unfollows=50,\n                criteria={\"inactive_days\": 30, \"not_following_back\": True}\n            )\n\n        print(\"\u2705 Scale phase complete!\")\n\nasync def find_collab_candidates(x, ai):\n    \"\"\"Find accounts perfect for collaboration\"\"\"\n    # Get your engaged followers\n    my_engagers = await x.analytics.top_engagers(limit=100)\n\n    candidates = []\n    for user in my_engagers:\n        # Check if they're in similar follower range\n        profile = await x.scrape.profile(user.username)\n\n        if 0.5 &lt; (profile.followers_count / 10000) &lt; 2:  # 50% to 200% of your size\n            candidates.append({\n                \"username\": profile.username,\n                \"followers\": profile.followers_count,\n                \"engagement_rate\": profile.engagement_rate,\n                \"reason\": f\"Engaged with you {user.engagement_count}x, {profile.followers_count} followers\"\n            })\n\n    return sorted(candidates, key=lambda c: c[\"engagement_rate\"], reverse=True)\n</code></pre>"},{"location":"cookbook/growth/#the-engagement-multiplier","title":"The Engagement Multiplier","text":"<p>A system to maximize engagement on every tweet you post.</p> <pre><code>\"\"\"\nThe Engagement Multiplier: Get more eyes on every tweet.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def engagement_multiplier(tweet_url: str):\n    \"\"\"\n    Run this after posting an important tweet to maximize its reach.\n    \"\"\"\n    async with Xeepy() as x:\n        print(f\"\ud83d\ude80 Amplifying tweet: {tweet_url}\")\n\n        # Step 1: Engage with accounts that typically share your content\n        amplifiers = await x.analytics.top_amplifiers(limit=20)\n\n        for amp in amplifiers:\n            # Like their recent tweet to put yourself on their radar\n            their_tweets = await x.scrape.tweets(amp.username, limit=3)\n            if their_tweets:\n                await x.engage.like(their_tweets[0].url)\n                print(f\"  \ud83d\udccd Pinged @{amp.username}\")\n\n        # Step 2: Share in relevant conversations\n        related = await x.scrape.search(\n            extract_keywords(tweet_url),\n            limit=20,\n            min_engagement=10,\n            max_age_hours=2\n        )\n\n        # Step 3: Monitor and respond to every reply quickly\n        async for reply in x.monitor.tweet_replies(tweet_url, duration=\"2h\"):\n            # Quick response increases algorithm favor\n            quick_reply = await ai.generate_reply(reply.text, style=\"grateful\")\n            await x.engage.reply(reply.url, quick_reply)\n            await x.engage.like(reply.url)\n            print(f\"  \ud83d\udcac Replied to @{reply.author.username}\")\n\n        # Step 4: Self-reply to bump the tweet\n        stats = await x.scrape.tweet(tweet_url)\n        if stats.replies &lt; 5:\n            # Add valuable self-reply to keep the conversation going\n            follow_up = await ai.generate_follow_up(stats.text)\n            await x.engage.reply(tweet_url, follow_up)\n</code></pre>"},{"location":"cookbook/growth/#network-effect-hacking","title":"Network Effect Hacking","text":"<p>Leverage network effects for exponential growth.</p> <pre><code>\"\"\"\nNetwork Effect Hacking: Turn followers into recruiters.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def network_effect_hack():\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\")\n\n        # Strategy 1: The Shoutout Chain\n        print(\"\ud83d\udd17 Building shoutout chain...\")\n\n        # Find followers who would benefit from exposure\n        deserving = await find_deserving_followers(x)\n\n        for user in deserving[:5]:  # Weekly featured followers\n            # Create genuine shoutout\n            profile = await x.scrape.profile(user.username)\n            shoutout = f\"\"\"\n            Weekly follow recommendation \ud83d\udc47\n\n            @{user.username} is building some cool stuff.\n\n            What I love:\n            \u2022 {await ai.summarize(profile.bio)}\n            \u2022 Great engagement in the community\n\n            Give them a follow! \ud83d\ude4c\n            \"\"\"\n\n            print(f\"  Recommending @{user.username}\")\n\n        # Strategy 2: The Reply Guy Promotion\n        print(\"\\n\ud83d\udcac Reply guy promotion strategy...\")\n\n        # Find big accounts in your niche\n        big_accounts = [\"naval\", \"paulg\", \"levelsio\"]\n\n        for account in big_accounts:\n            # Wait for their tweet\n            latest = await x.scrape.tweets(account, limit=1)\n\n            if latest and (datetime.now() - latest[0].created_at).seconds &lt; 300:\n                # They just tweeted! Be early with a valuable reply\n                valuable_reply = await ai.generate_reply(\n                    tweet_text=latest[0].text,\n                    style=\"add_unique_insight\",\n                    include_question=True  # Encourage response\n                )\n\n                await x.engage.reply(latest[0].url, valuable_reply)\n                print(f\"  Early reply to @{account}\")\n\n        # Strategy 3: The Collaboration Multiplier\n        print(\"\\n\ud83e\udd1d Collaboration multiplier...\")\n\n        # Find your top engaged followers for collaboration\n        top_engaged = await x.analytics.most_engaged_followers(limit=20)\n\n        # Propose mutual amplification\n        for follower in top_engaged[:5]:\n            dm_template = f\"\"\"\n            Hey @{follower.username}! \n\n            I noticed you engage with my content a lot (thank you! \ud83d\ude4f)\n\n            Would you be interested in cross-promoting? \n            I'll share your best tweet this week if you share mine.\n\n            Let me know!\n            \"\"\"\n\n            print(f\"  Collab proposal to @{follower.username}\")\n\nasync def find_deserving_followers(x):\n    \"\"\"Find followers creating quality content who deserve exposure\"\"\"\n    followers = await x.scrape.followers(\"your_username\", limit=500)\n\n    deserving = []\n    for f in followers:\n        if 100 &lt; f.followers_count &lt; 5000:  # Small but active\n            tweets = await x.scrape.tweets(f.username, limit=10)\n            if tweets:\n                avg_engagement = sum(t.likes for t in tweets) / len(tweets)\n                if avg_engagement &gt; 5:  # Getting some traction\n                    deserving.append(f)\n\n    return deserving\n</code></pre>"},{"location":"cookbook/growth/#the-viral-content-framework","title":"The Viral Content Framework","text":"<p>Templates for content that spreads.</p> <pre><code>\"\"\"\nViral Content Framework: Patterns that get shared.\n\"\"\"\nfrom xeepy.ai import ContentGenerator\n\nasync def generate_viral_content():\n    ai = ContentGenerator(provider=\"openai\")\n\n    # Framework 1: The Contrarian Take\n    contrarian = await ai.generate_tweet(\n        framework=\"contrarian\",\n        topic=\"startup advice\",\n        template=\"\"\"\n        Unpopular opinion: {contrarian_statement}\n\n        Here's why: {supporting_point}\n\n        {provocative_question}\n        \"\"\"\n    )\n\n    # Framework 2: The Value Thread\n    value_thread = await ai.generate_thread(\n        framework=\"value_list\",\n        topic=\"productivity tools\",\n        template=\"\"\"\n        Tweet 1: {hook_question_or_promise}\n\n        Tweets 2-9: {item_number}. {tool_name}\n        {one_sentence_description}\n        {why_its_great}\n\n        Tweet 10: {call_to_action_and_summary}\n        \"\"\"\n    )\n\n    # Framework 3: The Before/After Story\n    transformation = await ai.generate_tweet(\n        framework=\"transformation\",\n        topic=\"learning to code\",\n        template=\"\"\"\n        1 year ago: {before_state}\n\n        Today: {after_state}\n\n        The difference? {key_insight}\n\n        {lesson_learned}\n        \"\"\"\n    )\n\n    # Framework 4: The Hot Take Response\n    hot_take = await ai.generate_tweet(\n        framework=\"quote_tweet_response\",\n        reference_topic=\"recent tech news\",\n        template=\"\"\"\n        Everyone is saying {common_opinion}.\n\n        But they're missing: {overlooked_point}\n\n        Here's what's actually happening: {insight}\n        \"\"\"\n    )\n\n    return {\n        \"contrarian\": contrarian,\n        \"value_thread\": value_thread,\n        \"transformation\": transformation,\n        \"hot_take\": hot_take\n    }\n</code></pre>"},{"location":"cookbook/growth/#metrics-that-matter","title":"Metrics That Matter","text":"<p>Track the metrics that actually predict growth.</p> <pre><code>\"\"\"\nGrowth Metrics Dashboard: Focus on what matters.\n\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def growth_metrics_dashboard():\n    async with Xeepy() as x:\n        print(\"\"\"\n        \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n        \u2551           \ud83d\udcca GROWTH METRICS DASHBOARD                  \u2551\n        \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n        \"\"\")\n\n        # Core Metrics\n        profile = await x.scrape.profile(\"your_username\")\n        growth_7d = await x.analytics.track_growth(\"7d\")\n        engagement = await x.analytics.engagement_analysis(\"7d\")\n\n        # 1. Follower Velocity (followers gained per day)\n        velocity = growth_7d.net_change / 7\n        print(f\"\ud83d\udcc8 Follower Velocity: {velocity:+.1f}/day\")\n\n        if velocity &lt; 10:\n            print(\"   \u26a0\ufe0f Below target. Focus on content quality.\")\n        elif velocity &lt; 50:\n            print(\"   \u2713 Good progress. Consider scaling engagement.\")\n        else:\n            print(\"   \ud83d\udd25 Excellent! Maintain current strategy.\")\n\n        # 2. Engagement Rate (most important metric!)\n        eng_rate = engagement.rate\n        print(f\"\\n\ud83d\udcac Engagement Rate: {eng_rate:.2%}\")\n\n        if eng_rate &lt; 0.01:\n            print(\"   \u26a0\ufe0f Low. Try more questions and hooks.\")\n        elif eng_rate &lt; 0.03:\n            print(\"   \u2713 Average. Room for improvement.\")\n        else:\n            print(\"   \ud83d\udd25 Excellent! Your content resonates.\")\n\n        # 3. Reply Rate (indicates community building)\n        reply_rate = engagement.total_replies / engagement.total_tweets\n        print(f\"\\n\ud83d\udcad Reply Rate: {reply_rate:.1f} replies/tweet\")\n\n        # 4. Follower Quality Score\n        quality_sample = await x.scrape.followers(\"your_username\", limit=100)\n        quality_score = calculate_follower_quality(quality_sample)\n        print(f\"\\n\u2b50 Follower Quality: {quality_score:.0f}/100\")\n\n        # 5. Conversion Metrics\n        link_clicks = await x.analytics.link_performance(\"7d\")\n        print(f\"\\n\ud83d\udd17 Link Click Rate: {link_clicks.rate:.2%}\")\n\n        # 6. Best Content Types\n        print(\"\\n\ud83d\udcdd Top Performing Content Types:\")\n        by_type = await x.analytics.engagement_by_type()\n        for ct in by_type.types[:3]:\n            print(f\"   {ct.name}: {ct.avg_engagement:.1f} avg engagement\")\n\ndef calculate_follower_quality(followers):\n    \"\"\"Score follower quality 0-100\"\"\"\n    scores = []\n    for f in followers:\n        score = 0\n        if f.bio:\n            score += 20\n        if f.followers_count &gt; 100:\n            score += 20\n        if f.tweet_count &gt; 50:\n            score += 20\n        if not f.is_default_profile:\n            score += 20\n        if f.following_count &lt; f.followers_count * 3:  # Not follow-spam\n            score += 20\n        scores.append(score)\n\n    return sum(scores) / len(scores)\n</code></pre>"},{"location":"cookbook/growth/#growth-experiments","title":"Growth Experiments","text":"<p>A/B testing for Twitter growth.</p> <pre><code>\"\"\"\nGrowth Experiments: Test what works for YOUR audience.\n\"\"\"\nimport asyncio\nfrom datetime import datetime, timedelta\nfrom xeepy import Xeepy\n\nclass GrowthExperiment:\n    \"\"\"Run controlled growth experiments\"\"\"\n\n    def __init__(self, name: str, duration_days: int = 7):\n        self.name = name\n        self.duration = duration_days\n        self.start_metrics = None\n        self.end_metrics = None\n\n    async def start(self, x: Xeepy):\n        \"\"\"Record starting metrics\"\"\"\n        self.start_metrics = await x.analytics.snapshot()\n        print(f\"\ud83e\uddea Experiment '{self.name}' started\")\n        print(f\"   Starting followers: {self.start_metrics.followers}\")\n\n    async def end(self, x: Xeepy):\n        \"\"\"Record ending metrics and analyze\"\"\"\n        self.end_metrics = await x.analytics.snapshot()\n\n        results = {\n            \"follower_change\": self.end_metrics.followers - self.start_metrics.followers,\n            \"engagement_change\": self.end_metrics.engagement_rate - self.start_metrics.engagement_rate,\n            \"impressions_change\": self.end_metrics.impressions - self.start_metrics.impressions\n        }\n\n        print(f\"\\n\ud83d\udcca Experiment '{self.name}' Results:\")\n        print(f\"   Follower change: {results['follower_change']:+d}\")\n        print(f\"   Engagement change: {results['engagement_change']:+.2%}\")\n\n        return results\n\nasync def run_posting_time_experiment():\n    \"\"\"Test different posting times\"\"\"\n    async with Xeepy() as x:\n        experiments = [\n            GrowthExperiment(\"Morning Posts (6-9 AM)\", 7),\n            GrowthExperiment(\"Afternoon Posts (12-3 PM)\", 7),\n            GrowthExperiment(\"Evening Posts (6-9 PM)\", 7),\n        ]\n\n        # Run each experiment for a week\n        for exp in experiments:\n            await exp.start(x)\n            # ... post during specific time window for a week ...\n            await asyncio.sleep(7 * 86400)  # Wait 7 days\n            await exp.end(x)\n\nasync def run_content_type_experiment():\n    \"\"\"Test different content types\"\"\"\n    async with Xeepy() as x:\n        content_types = [\n            (\"threads\", \"Post 1 thread per day\"),\n            (\"single_tweets\", \"Post 5 single tweets per day\"),\n            (\"quote_tweets\", \"Quote tweet 3 interesting posts per day\"),\n            (\"questions\", \"Ask 2 engaging questions per day\"),\n        ]\n\n        results = {}\n        for content_type, description in content_types:\n            exp = GrowthExperiment(content_type, 7)\n            await exp.start(x)\n            # ... post according to description ...\n            results[content_type] = await exp.end(x)\n\n        # Compare results\n        best = max(results.items(), key=lambda x: x[1][\"follower_change\"])\n        print(f\"\\n\ud83c\udfc6 Winner: {best[0]} with {best[1]['follower_change']:+d} followers\")\n</code></pre>"},{"location":"cookbook/growth/#next-steps","title":"Next Steps","text":"<ul> <li> <p>Automation Workflows</p> <p>Automate your growth strategies</p> </li> <li> <p>Data Science Recipes</p> <p>Analyze your growth data</p> </li> <li> <p>Business Intelligence</p> <p>Turn followers into customers</p> </li> </ul>"},{"location":"cookbook/growth/engagement-pods/","title":"Smart Engagement Pod System","text":"<p>Build a fair and intelligent engagement pod system with reciprocity tracking and anti-detection measures.</p>"},{"location":"cookbook/growth/engagement-pods/#overview","title":"Overview","text":"<p>Educational Purpose</p> <p>This recipe is for educational purposes only. Engagement pods may violate platform terms of service. Use responsibly and understand the risks.</p> <p>This recipe creates a smart engagement pod system with:</p> <ul> <li>Member discovery - Find compatible pod members</li> <li>Reciprocity tracking - Ensure fair participation</li> <li>Fairness algorithms - Balance engagement distribution</li> <li>Anti-detection timing - Natural engagement patterns</li> <li>Performance analytics - Track pod effectiveness</li> <li>Health monitoring - Identify inactive members</li> </ul>"},{"location":"cookbook/growth/engagement-pods/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Pod            \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Reciprocity \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Engagement     \u2502\n\u2502  Manager        \u2502     \u2502  Tracker     \u2502     \u2502  Queue          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                       \u2502                     \u2502\n        \u25bc                       \u25bc                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Member         \u2502     \u2502  Fairness    \u2502     \u2502  Anti-Detection \u2502\n\u2502  Discovery      \u2502     \u2502  Calculator  \u2502     \u2502  Scheduler      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cookbook/growth/engagement-pods/#data-models","title":"Data Models","text":"<pre><code># pod_models.py\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Optional\nfrom enum import Enum\n\nclass EngagementType(Enum):\n    LIKE = \"like\"\n    RETWEET = \"retweet\"\n    REPLY = \"reply\"\n    QUOTE = \"quote\"\n\n@dataclass\nclass PodMember:\n    user_id: str\n    username: str\n    joined_at: datetime\n    followers: int\n    avg_engagement_rate: float\n    timezone: str = \"UTC\"\n\n    # Reciprocity metrics\n    engagements_given: int = 0\n    engagements_received: int = 0\n    reciprocity_score: float = 1.0\n\n    # Activity tracking\n    last_active: Optional[datetime] = None\n    consecutive_misses: int = 0\n    is_active: bool = True\n\n    # Performance\n    avg_response_time_minutes: float = 0.0\n    quality_score: float = 1.0\n\n@dataclass\nclass EngagementRequest:\n    id: str\n    tweet_id: str\n    tweet_url: str\n    author_id: str\n    author_username: str\n    requested_at: datetime\n    engagement_types: list[EngagementType]\n    priority: int = 1  # Higher = more urgent\n\n    # Fulfillment tracking\n    fulfilled_by: list[str] = field(default_factory=list)\n    deadline: Optional[datetime] = None\n\n@dataclass\nclass EngagementRecord:\n    request_id: str\n    member_id: str\n    engagement_type: EngagementType\n    completed_at: datetime\n    response_time_minutes: float\n</code></pre>"},{"location":"cookbook/growth/engagement-pods/#pod-manager","title":"Pod Manager","text":"<pre><code># pod_manager.py\nimport asyncio\nimport uuid\nfrom datetime import datetime, timedelta\nfrom typing import Optional\n\nfrom xeepy import Xeepy\n\nfrom pod_models import PodMember, EngagementRequest, EngagementType\n\nclass PodManager:\n    \"\"\"Manage engagement pod operations.\"\"\"\n\n    def __init__(self, pod_name: str, max_members: int = 20):\n        self.pod_name = pod_name\n        self.max_members = max_members\n        self.members: dict[str, PodMember] = {}\n        self.pending_requests: list[EngagementRequest] = []\n        self.completed_requests: list[EngagementRequest] = []\n\n    def add_member(self, member: PodMember) -&gt; bool:\n        \"\"\"Add member to pod.\"\"\"\n        if len(self.members) &gt;= self.max_members:\n            return False\n\n        if member.user_id in self.members:\n            return False\n\n        self.members[member.user_id] = member\n        return True\n\n    def remove_member(self, user_id: str):\n        \"\"\"Remove member from pod.\"\"\"\n        if user_id in self.members:\n            del self.members[user_id]\n\n    def submit_request(\n        self,\n        tweet_id: str,\n        tweet_url: str,\n        author_id: str,\n        engagement_types: list[EngagementType] = None,\n        priority: int = 1\n    ) -&gt; EngagementRequest:\n        \"\"\"Submit engagement request to pod.\"\"\"\n\n        if author_id not in self.members:\n            raise ValueError(\"Author must be a pod member\")\n\n        request = EngagementRequest(\n            id=f\"req_{uuid.uuid4().hex[:8]}\",\n            tweet_id=tweet_id,\n            tweet_url=tweet_url,\n            author_id=author_id,\n            author_username=self.members[author_id].username,\n            requested_at=datetime.now(),\n            engagement_types=engagement_types or [EngagementType.LIKE],\n            priority=priority,\n            deadline=datetime.now() + timedelta(hours=24)\n        )\n\n        self.pending_requests.append(request)\n        return request\n\n    def get_queue_for_member(\n        self, \n        member_id: str,\n        limit: int = 10\n    ) -&gt; list[EngagementRequest]:\n        \"\"\"Get engagement queue for a specific member.\"\"\"\n\n        member = self.members.get(member_id)\n        if not member:\n            return []\n\n        # Filter requests not from this member and not yet fulfilled by them\n        eligible = [\n            r for r in self.pending_requests\n            if r.author_id != member_id and member_id not in r.fulfilled_by\n        ]\n\n        # Sort by priority and reciprocity\n        def sort_key(req):\n            author = self.members.get(req.author_id)\n            author_reciprocity = author.reciprocity_score if author else 0\n            return (req.priority, author_reciprocity, req.requested_at)\n\n        eligible.sort(key=sort_key, reverse=True)\n\n        return eligible[:limit]\n\n    def record_engagement(\n        self,\n        request_id: str,\n        member_id: str,\n        engagement_type: EngagementType\n    ):\n        \"\"\"Record completed engagement.\"\"\"\n\n        # Find request\n        request = next(\n            (r for r in self.pending_requests if r.id == request_id),\n            None\n        )\n\n        if not request:\n            return\n\n        # Update request\n        request.fulfilled_by.append(member_id)\n\n        # Update member stats\n        member = self.members.get(member_id)\n        if member:\n            member.engagements_given += 1\n            member.last_active = datetime.now()\n\n        author = self.members.get(request.author_id)\n        if author:\n            author.engagements_received += 1\n\n        # Check if fully fulfilled\n        if len(request.fulfilled_by) &gt;= len(self.members) - 1:\n            self.pending_requests.remove(request)\n            self.completed_requests.append(request)\n</code></pre>"},{"location":"cookbook/growth/engagement-pods/#reciprocity-tracker","title":"Reciprocity Tracker","text":"<pre><code># reciprocity_tracker.py\nfrom datetime import datetime, timedelta\nfrom collections import defaultdict\n\nclass ReciprocityTracker:\n    \"\"\"Track and enforce reciprocity in the pod.\"\"\"\n\n    def __init__(self, lookback_days: int = 7):\n        self.lookback_days = lookback_days\n        self.engagement_log: list[dict] = []\n\n    def log_engagement(\n        self,\n        giver_id: str,\n        receiver_id: str,\n        engagement_type: str,\n        timestamp: datetime = None\n    ):\n        \"\"\"Log an engagement action.\"\"\"\n        self.engagement_log.append({\n            'giver_id': giver_id,\n            'receiver_id': receiver_id,\n            'engagement_type': engagement_type,\n            'timestamp': timestamp or datetime.now()\n        })\n\n    def calculate_reciprocity_scores(\n        self,\n        members: dict[str, 'PodMember']\n    ) -&gt; dict[str, float]:\n        \"\"\"Calculate reciprocity scores for all members.\"\"\"\n\n        cutoff = datetime.now() - timedelta(days=self.lookback_days)\n        recent_logs = [l for l in self.engagement_log if l['timestamp'] &gt; cutoff]\n\n        # Count given and received\n        given = defaultdict(int)\n        received = defaultdict(int)\n\n        for log in recent_logs:\n            given[log['giver_id']] += 1\n            received[log['receiver_id']] += 1\n\n        # Calculate scores\n        scores = {}\n        for user_id in members:\n            g = given.get(user_id, 0)\n            r = received.get(user_id, 0)\n\n            if r == 0:\n                # Never received = perfect score (new member)\n                scores[user_id] = 1.0\n            elif g == 0:\n                # Received but never gave = poor score\n                scores[user_id] = 0.1\n            else:\n                # Ratio of given to received\n                ratio = g / r\n                # Score between 0.1 and 2.0\n                scores[user_id] = min(2.0, max(0.1, ratio))\n\n        return scores\n\n    def get_debt_matrix(\n        self,\n        members: dict[str, 'PodMember']\n    ) -&gt; dict[str, dict[str, int]]:\n        \"\"\"Calculate engagement debt between pairs.\"\"\"\n\n        cutoff = datetime.now() - timedelta(days=self.lookback_days)\n        recent_logs = [l for l in self.engagement_log if l['timestamp'] &gt; cutoff]\n\n        # Count pairwise engagements\n        given_to = defaultdict(lambda: defaultdict(int))\n\n        for log in recent_logs:\n            given_to[log['giver_id']][log['receiver_id']] += 1\n\n        # Calculate debt (positive = owes, negative = owed)\n        debt = {}\n        for a_id in members:\n            debt[a_id] = {}\n            for b_id in members:\n                if a_id != b_id:\n                    a_to_b = given_to[a_id][b_id]\n                    b_to_a = given_to[b_id][a_id]\n                    debt[a_id][b_id] = b_to_a - a_to_b  # How much A owes B\n\n        return debt\n\n    def identify_freeloaders(\n        self,\n        members: dict[str, 'PodMember'],\n        threshold: float = 0.3\n    ) -&gt; list[str]:\n        \"\"\"Identify members who take more than they give.\"\"\"\n\n        scores = self.calculate_reciprocity_scores(members)\n        return [uid for uid, score in scores.items() if score &lt; threshold]\n</code></pre>"},{"location":"cookbook/growth/engagement-pods/#fairness-calculator","title":"Fairness Calculator","text":"<pre><code># fairness_calculator.py\nfrom typing import Optional\n\nclass FairnessCalculator:\n    \"\"\"Ensure fair engagement distribution.\"\"\"\n\n    def __init__(self, reciprocity_tracker: 'ReciprocityTracker'):\n        self.tracker = reciprocity_tracker\n\n    def prioritize_requests(\n        self,\n        requests: list['EngagementRequest'],\n        members: dict[str, 'PodMember'],\n        current_user_id: str\n    ) -&gt; list['EngagementRequest']:\n        \"\"\"Prioritize requests based on fairness.\"\"\"\n\n        # Get debt matrix\n        debt = self.tracker.get_debt_matrix(members)\n        user_debts = debt.get(current_user_id, {})\n\n        # Get reciprocity scores\n        scores = self.tracker.calculate_reciprocity_scores(members)\n\n        def priority_score(request):\n            author_id = request.author_id\n\n            # Base priority\n            score = request.priority * 10\n\n            # Debt factor (prioritize those we owe)\n            debt_to_author = user_debts.get(author_id, 0)\n            score += debt_to_author * 5\n\n            # Reciprocity factor (prioritize good contributors)\n            author_reciprocity = scores.get(author_id, 1.0)\n            score += author_reciprocity * 10\n\n            # Urgency factor (older requests)\n            hours_old = (datetime.now() - request.requested_at).total_seconds() / 3600\n            score += min(hours_old, 24)  # Cap at 24 hours\n\n            return score\n\n        return sorted(requests, key=priority_score, reverse=True)\n\n    def calculate_fair_quota(\n        self,\n        member: 'PodMember',\n        total_pending: int,\n        member_count: int\n    ) -&gt; int:\n        \"\"\"Calculate fair engagement quota for member.\"\"\"\n\n        base_quota = total_pending // (member_count - 1)\n\n        # Adjust by reciprocity\n        adjusted = int(base_quota * member.reciprocity_score)\n\n        # Minimum and maximum bounds\n        return max(1, min(adjusted, base_quota * 2))\n</code></pre>"},{"location":"cookbook/growth/engagement-pods/#anti-detection-scheduler","title":"Anti-Detection Scheduler","text":"<pre><code># anti_detection_scheduler.py\nimport random\nfrom datetime import datetime, timedelta\nfrom typing import Generator\n\nclass AntiDetectionScheduler:\n    \"\"\"Schedule engagements with natural timing patterns.\"\"\"\n\n    def __init__(\n        self,\n        min_delay_seconds: int = 30,\n        max_delay_seconds: int = 300,\n        daily_limit: int = 100,\n        active_hours: tuple[int, int] = (8, 23)\n    ):\n        self.min_delay = min_delay_seconds\n        self.max_delay = max_delay_seconds\n        self.daily_limit = daily_limit\n        self.active_hours = active_hours\n\n        self.daily_count = 0\n        self.last_reset = datetime.now().date()\n\n    def get_next_delay(self) -&gt; int:\n        \"\"\"Get natural-looking delay before next action.\"\"\"\n\n        # Check daily limit\n        if datetime.now().date() != self.last_reset:\n            self.daily_count = 0\n            self.last_reset = datetime.now().date()\n\n        if self.daily_count &gt;= self.daily_limit:\n            # Wait until tomorrow\n            tomorrow = datetime.now().replace(\n                hour=self.active_hours[0], \n                minute=0, \n                second=0\n            ) + timedelta(days=1)\n            return int((tomorrow - datetime.now()).total_seconds())\n\n        # Check active hours\n        current_hour = datetime.now().hour\n        if not (self.active_hours[0] &lt;= current_hour &lt; self.active_hours[1]):\n            # Wait until active hours\n            if current_hour &lt; self.active_hours[0]:\n                wait_hours = self.active_hours[0] - current_hour\n            else:\n                wait_hours = 24 - current_hour + self.active_hours[0]\n            return wait_hours * 3600 + random.randint(0, 1800)\n\n        # Normal delay with natural variation\n        base_delay = random.randint(self.min_delay, self.max_delay)\n\n        # Add occasional longer pauses (simulates breaks)\n        if random.random() &lt; 0.1:  # 10% chance\n            base_delay += random.randint(300, 900)  # 5-15 minute break\n\n        # Add micro-variations\n        variation = random.gauss(0, base_delay * 0.2)\n\n        return max(self.min_delay, int(base_delay + variation))\n\n    def schedule_batch(\n        self,\n        count: int\n    ) -&gt; Generator[datetime, None, None]:\n        \"\"\"Generate scheduled times for batch of engagements.\"\"\"\n\n        current_time = datetime.now()\n\n        for _ in range(count):\n            delay = self.get_next_delay()\n            current_time += timedelta(seconds=delay)\n            self.daily_count += 1\n            yield current_time\n\n    def is_safe_to_engage(self) -&gt; bool:\n        \"\"\"Check if it's safe to perform engagement.\"\"\"\n\n        # Check daily limit\n        if datetime.now().date() != self.last_reset:\n            self.daily_count = 0\n            self.last_reset = datetime.now().date()\n\n        if self.daily_count &gt;= self.daily_limit:\n            return False\n\n        # Check active hours\n        current_hour = datetime.now().hour\n        if not (self.active_hours[0] &lt;= current_hour &lt; self.active_hours[1]):\n            return False\n\n        return True\n</code></pre>"},{"location":"cookbook/growth/engagement-pods/#pod-executor","title":"Pod Executor","text":"<pre><code># pod_executor.py\nimport asyncio\nfrom datetime import datetime\n\nfrom xeepy import Xeepy\n\nfrom pod_models import EngagementType, EngagementRequest\nfrom pod_manager import PodManager\nfrom anti_detection_scheduler import AntiDetectionScheduler\n\nclass PodExecutor:\n    \"\"\"Execute pod engagement tasks.\"\"\"\n\n    def __init__(\n        self,\n        pod_manager: PodManager,\n        scheduler: AntiDetectionScheduler\n    ):\n        self.pod = pod_manager\n        self.scheduler = scheduler\n\n    async def execute_queue(self, member_id: str):\n        \"\"\"Execute engagement queue for a member.\"\"\"\n\n        queue = self.pod.get_queue_for_member(member_id)\n\n        if not queue:\n            print(\"No pending engagements\")\n            return\n\n        print(f\"Processing {len(queue)} engagement requests...\")\n\n        async with Xeepy() as x:\n            for request in queue:\n                # Check if safe\n                if not self.scheduler.is_safe_to_engage():\n                    print(\"Rate limit reached, stopping\")\n                    break\n\n                # Wait natural delay\n                delay = self.scheduler.get_next_delay()\n                print(f\"Waiting {delay}s before next engagement...\")\n                await asyncio.sleep(delay)\n\n                # Execute engagements\n                for eng_type in request.engagement_types:\n                    try:\n                        await self._execute_engagement(\n                            x, request.tweet_url, eng_type\n                        )\n\n                        self.pod.record_engagement(\n                            request.id,\n                            member_id,\n                            eng_type\n                        )\n\n                        print(f\"\u2713 {eng_type.value} on {request.tweet_url}\")\n\n                    except Exception as e:\n                        print(f\"\u2717 Failed: {e}\")\n\n    async def _execute_engagement(\n        self,\n        x: Xeepy,\n        tweet_url: str,\n        engagement_type: EngagementType\n    ):\n        \"\"\"Execute single engagement action.\"\"\"\n\n        if engagement_type == EngagementType.LIKE:\n            await x.engage.like(tweet_url)\n\n        elif engagement_type == EngagementType.RETWEET:\n            await x.engage.retweet(tweet_url)\n\n        elif engagement_type == EngagementType.REPLY:\n            # Would need reply text\n            pass\n\n        elif engagement_type == EngagementType.QUOTE:\n            # Would need quote text\n            pass\n</code></pre>"},{"location":"cookbook/growth/engagement-pods/#usage-example","title":"Usage Example","text":"<pre><code># main.py\nimport asyncio\nfrom datetime import datetime\n\nfrom pod_manager import PodManager\nfrom pod_models import PodMember, EngagementType\nfrom reciprocity_tracker import ReciprocityTracker\nfrom anti_detection_scheduler import AntiDetectionScheduler\nfrom pod_executor import PodExecutor\n\nasync def main():\n    # Create pod\n    pod = PodManager(\"tech_pod\", max_members=10)\n\n    # Add members\n    members = [\n        PodMember(\"1\", \"user1\", datetime.now(), 5000, 0.05, \"America/New_York\"),\n        PodMember(\"2\", \"user2\", datetime.now(), 8000, 0.04, \"Europe/London\"),\n        PodMember(\"3\", \"user3\", datetime.now(), 3000, 0.06, \"Asia/Tokyo\"),\n    ]\n\n    for member in members:\n        pod.add_member(member)\n\n    # Submit engagement request\n    request = pod.submit_request(\n        tweet_id=\"123456789\",\n        tweet_url=\"https://x.com/user1/status/123456789\",\n        author_id=\"1\",\n        engagement_types=[EngagementType.LIKE, EngagementType.RETWEET],\n        priority=2\n    )\n\n    print(f\"Request submitted: {request.id}\")\n\n    # Execute for member 2\n    scheduler = AntiDetectionScheduler(\n        min_delay_seconds=60,\n        max_delay_seconds=300,\n        daily_limit=50\n    )\n\n    executor = PodExecutor(pod, scheduler)\n    await executor.execute_queue(\"2\")\n\n    # Check reciprocity\n    tracker = ReciprocityTracker()\n    scores = tracker.calculate_reciprocity_scores(pod.members)\n\n    print(\"\\nReciprocity Scores:\")\n    for uid, score in scores.items():\n        print(f\"  {pod.members[uid].username}: {score:.2f}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"cookbook/growth/engagement-pods/#ethical-considerations","title":"Ethical Considerations","text":"<p>Platform Terms</p> <p>Engagement pods may violate X/Twitter's Terms of Service. Accounts may be suspended. Use at your own risk.</p> <p>Authenticity</p> <ul> <li>Pod engagement creates artificial metrics</li> <li>It can damage genuine community trust</li> <li>Consider organic growth strategies instead</li> </ul> <p>If You Must</p> <ul> <li>Keep pods small (&lt;10 members)</li> <li>Focus on genuine content</li> <li>Use natural timing</li> <li>Don't over-engage</li> <li>Be prepared for consequences</li> </ul>"},{"location":"cookbook/growth/engagement-pods/#related-recipes","title":"Related Recipes","text":"<ul> <li>Optimal Timing - Post at best times</li> <li>Hashtag Strategy - Organic reach</li> <li>Content Calendar - Better content</li> </ul>"},{"location":"cookbook/growth/hashtag-strategy/","title":"Data-Driven Hashtag Optimization","text":"<p>Build a comprehensive hashtag research and optimization system using performance data and machine learning.</p>"},{"location":"cookbook/growth/hashtag-strategy/#overview","title":"Overview","text":"<p>This recipe creates a hashtag optimization system with:</p> <ul> <li>Performance scraping - Analyze hashtag metrics</li> <li>Reach vs competition - Find optimal hashtags</li> <li>Niche discovery - Uncover hidden gems</li> <li>Trending analysis - Time hashtag usage</li> <li>A/B testing - Validate strategies</li> <li>Weekly reports - Track progress</li> </ul>"},{"location":"cookbook/growth/hashtag-strategy/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Hashtag        \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Performance \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Opportunity    \u2502\n\u2502  Scraper        \u2502     \u2502  Analyzer    \u2502     \u2502  Scorer         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                       \u2502                     \u2502\n        \u25bc                       \u25bc                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Trending       \u2502     \u2502  A/B Test    \u2502     \u2502  Strategy       \u2502\n\u2502  Monitor        \u2502     \u2502  Framework   \u2502     \u2502  Generator      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cookbook/growth/hashtag-strategy/#data-models","title":"Data Models","text":"<pre><code># hashtag_models.py\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Optional\n\n@dataclass\nclass HashtagMetrics:\n    tag: str\n    total_tweets: int\n    tweets_per_hour: float\n    avg_likes: float\n    avg_retweets: float\n    avg_replies: float\n    engagement_rate: float\n    top_accounts_ratio: float  # % from top 10 accounts\n    unique_authors: int\n    peak_hours: list[int]\n    scraped_at: datetime = field(default_factory=datetime.now)\n\n@dataclass\nclass HashtagScore:\n    tag: str\n    reach_score: float      # 0-100\n    competition_score: float  # 0-100 (lower = less competition)\n    relevance_score: float   # 0-100\n    opportunity_score: float  # Combined score\n    recommendation: str      # use_always, use_sometimes, avoid\n    notes: list[str] = field(default_factory=list)\n\n@dataclass\nclass HashtagStrategy:\n    primary_tags: list[str]    # Always use (3-5)\n    secondary_tags: list[str]  # Rotate (5-10)\n    trending_slots: int        # Reserved for trending\n    niche_tags: list[str]      # Low competition gems\n    avoid_tags: list[str]      # Overused/spammy\n</code></pre>"},{"location":"cookbook/growth/hashtag-strategy/#hashtag-scraper","title":"Hashtag Scraper","text":"<pre><code># hashtag_scraper.py\nimport asyncio\nfrom datetime import datetime, timedelta\nfrom collections import Counter\n\nfrom xeepy import Xeepy\nfrom hashtag_models import HashtagMetrics\n\nclass HashtagScraper:\n    \"\"\"Scrape and analyze hashtag performance.\"\"\"\n\n    def __init__(self):\n        self.cache: dict[str, HashtagMetrics] = {}\n\n    async def analyze_hashtag(\n        self,\n        tag: str,\n        sample_size: int = 200\n    ) -&gt; HashtagMetrics:\n        \"\"\"Analyze performance metrics for a hashtag.\"\"\"\n\n        # Remove # if present\n        tag = tag.lstrip('#')\n\n        async with Xeepy() as x:\n            # Scrape recent tweets with hashtag\n            tweets = await x.scrape.hashtag(\n                f\"#{tag}\",\n                limit=sample_size\n            )\n\n            if not tweets:\n                return self._empty_metrics(tag)\n\n            # Calculate metrics\n            total = len(tweets)\n\n            # Time span\n            oldest = min(t.created_at for t in tweets)\n            newest = max(t.created_at for t in tweets)\n            hours_span = max(1, (newest - oldest).total_seconds() / 3600)\n            tweets_per_hour = total / hours_span\n\n            # Engagement metrics\n            likes = [t.like_count for t in tweets]\n            retweets = [t.retweet_count for t in tweets]\n            replies = [t.reply_count for t in tweets]\n\n            avg_likes = sum(likes) / total\n            avg_retweets = sum(retweets) / total\n            avg_replies = sum(replies) / total\n\n            # Author analysis\n            authors = [t.author.username for t in tweets]\n            unique_authors = len(set(authors))\n\n            # Top accounts concentration\n            author_counts = Counter(authors)\n            top_10_tweets = sum(c for _, c in author_counts.most_common(10))\n            top_accounts_ratio = top_10_tweets / total\n\n            # Calculate engagement rate\n            total_followers = sum(t.author.followers_count for t in tweets)\n            total_engagement = sum(likes) + sum(retweets) + sum(replies)\n            engagement_rate = total_engagement / max(total_followers, 1)\n\n            # Peak hours\n            hour_counts = Counter(t.created_at.hour for t in tweets)\n            peak_hours = [h for h, _ in hour_counts.most_common(3)]\n\n            metrics = HashtagMetrics(\n                tag=tag,\n                total_tweets=total,\n                tweets_per_hour=tweets_per_hour,\n                avg_likes=avg_likes,\n                avg_retweets=avg_retweets,\n                avg_replies=avg_replies,\n                engagement_rate=engagement_rate,\n                top_accounts_ratio=top_accounts_ratio,\n                unique_authors=unique_authors,\n                peak_hours=peak_hours\n            )\n\n            self.cache[tag] = metrics\n            return metrics\n\n    async def analyze_multiple(\n        self,\n        tags: list[str],\n        sample_size: int = 100\n    ) -&gt; list[HashtagMetrics]:\n        \"\"\"Analyze multiple hashtags.\"\"\"\n\n        results = []\n        for tag in tags:\n            metrics = await self.analyze_hashtag(tag, sample_size)\n            results.append(metrics)\n            await asyncio.sleep(2)  # Rate limiting\n\n        return results\n\n    def _empty_metrics(self, tag: str) -&gt; HashtagMetrics:\n        return HashtagMetrics(\n            tag=tag,\n            total_tweets=0,\n            tweets_per_hour=0,\n            avg_likes=0,\n            avg_retweets=0,\n            avg_replies=0,\n            engagement_rate=0,\n            top_accounts_ratio=0,\n            unique_authors=0,\n            peak_hours=[]\n        )\n</code></pre>"},{"location":"cookbook/growth/hashtag-strategy/#opportunity-scorer","title":"Opportunity Scorer","text":"<pre><code># opportunity_scorer.py\nimport math\nfrom hashtag_models import HashtagMetrics, HashtagScore\n\nclass OpportunityScorer:\n    \"\"\"Score hashtags for opportunity potential.\"\"\"\n\n    def __init__(\n        self,\n        target_tweets_per_hour: float = 50,  # Ideal activity level\n        target_engagement_rate: float = 0.02,\n        min_unique_authors: int = 20\n    ):\n        self.target_tph = target_tweets_per_hour\n        self.target_engagement = target_engagement_rate\n        self.min_authors = min_unique_authors\n\n    def score(\n        self,\n        metrics: HashtagMetrics,\n        relevance: float = 1.0  # 0-1, how relevant to your niche\n    ) -&gt; HashtagScore:\n        \"\"\"Calculate opportunity score for hashtag.\"\"\"\n\n        # Reach score (0-100)\n        # Based on activity level - too low = no reach, too high = noise\n        if metrics.tweets_per_hour &lt; 1:\n            reach = 10  # Very low activity\n        elif metrics.tweets_per_hour &gt; self.target_tph * 10:\n            reach = 30  # Very high activity (saturated)\n        else:\n            # Optimal is around target\n            ratio = metrics.tweets_per_hour / self.target_tph\n            reach = 100 - abs(math.log10(max(0.1, ratio))) * 30\n        reach = max(0, min(100, reach))\n\n        # Competition score (0-100, lower competition = higher score)\n        # Based on unique authors and top account concentration\n        if metrics.unique_authors &lt; self.min_authors:\n            competition = 20  # Too few authors = dominated by few\n        else:\n            author_diversity = min(1, metrics.unique_authors / 100)\n            concentration_penalty = metrics.top_accounts_ratio * 50\n            competition = author_diversity * 100 - concentration_penalty\n        competition = max(0, min(100, competition))\n\n        # Relevance score\n        relevance_score = relevance * 100\n\n        # Engagement bonus\n        engagement_bonus = min(20, metrics.engagement_rate / self.target_engagement * 10)\n\n        # Combined opportunity score\n        opportunity = (\n            reach * 0.30 +\n            competition * 0.30 +\n            relevance_score * 0.30 +\n            engagement_bonus\n        )\n\n        # Generate recommendation\n        notes = []\n        if opportunity &gt;= 70:\n            recommendation = \"use_always\"\n            notes.append(\"Excellent opportunity hashtag\")\n        elif opportunity &gt;= 50:\n            recommendation = \"use_sometimes\"\n            notes.append(\"Good for rotation\")\n        else:\n            recommendation = \"avoid\"\n            if competition &lt; 40:\n                notes.append(\"Too competitive/saturated\")\n            if reach &lt; 40:\n                notes.append(\"Low visibility potential\")\n\n        # Add specific notes\n        if metrics.tweets_per_hour &gt; 500:\n            notes.append(\"Very high volume - tweets get buried quickly\")\n        if metrics.top_accounts_ratio &gt; 0.5:\n            notes.append(\"Dominated by few accounts\")\n        if metrics.engagement_rate &gt; 0.05:\n            notes.append(\"High engagement hashtag!\")\n\n        return HashtagScore(\n            tag=metrics.tag,\n            reach_score=round(reach, 1),\n            competition_score=round(competition, 1),\n            relevance_score=round(relevance_score, 1),\n            opportunity_score=round(opportunity, 1),\n            recommendation=recommendation,\n            notes=notes\n        )\n</code></pre>"},{"location":"cookbook/growth/hashtag-strategy/#niche-hashtag-discoverer","title":"Niche Hashtag Discoverer","text":"<pre><code># niche_discoverer.py\nimport re\nfrom collections import Counter\n\nfrom xeepy import Xeepy\n\nclass NicheDiscoverer:\n    \"\"\"Discover niche hashtags from successful accounts.\"\"\"\n\n    async def discover_from_accounts(\n        self,\n        successful_accounts: list[str],\n        tweets_per_account: int = 50,\n        min_occurrences: int = 3\n    ) -&gt; list[tuple[str, int]]:\n        \"\"\"Discover hashtags used by successful accounts.\"\"\"\n\n        all_hashtags = []\n\n        async with Xeepy() as x:\n            for username in successful_accounts:\n                tweets = await x.scrape.tweets(username, limit=tweets_per_account)\n\n                for tweet in tweets:\n                    # Extract hashtags\n                    tags = re.findall(r'#(\\w+)', tweet.text.lower())\n                    all_hashtags.extend(tags)\n\n        # Count occurrences\n        counter = Counter(all_hashtags)\n\n        # Filter by minimum occurrences\n        common_tags = [\n            (tag, count) for tag, count in counter.most_common(100)\n            if count &gt;= min_occurrences\n        ]\n\n        return common_tags\n\n    async def discover_related(\n        self,\n        seed_hashtag: str,\n        depth: int = 2\n    ) -&gt; list[str]:\n        \"\"\"Discover related hashtags from a seed.\"\"\"\n\n        discovered = set()\n        to_process = [seed_hashtag.lstrip('#')]\n\n        async with Xeepy() as x:\n            for _ in range(depth):\n                next_level = []\n\n                for tag in to_process:\n                    if tag in discovered:\n                        continue\n\n                    discovered.add(tag)\n\n                    # Scrape tweets with this hashtag\n                    tweets = await x.scrape.hashtag(f\"#{tag}\", limit=50)\n\n                    # Extract co-occurring hashtags\n                    for tweet in tweets:\n                        co_tags = re.findall(r'#(\\w+)', tweet.text.lower())\n                        for co_tag in co_tags:\n                            if co_tag not in discovered:\n                                next_level.append(co_tag)\n\n                to_process = list(set(next_level))[:20]  # Limit breadth\n\n        return list(discovered)\n</code></pre>"},{"location":"cookbook/growth/hashtag-strategy/#ab-testing-framework","title":"A/B Testing Framework","text":"<pre><code># hashtag_ab_test.py\nimport random\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport statistics\n\n@dataclass\nclass ABTestResult:\n    test_name: str\n    variant_a_tags: list[str]\n    variant_b_tags: list[str]\n    variant_a_tweets: int\n    variant_b_tweets: int\n    variant_a_avg_engagement: float\n    variant_b_avg_engagement: float\n    winner: str\n    confidence: float\n    recommendation: str\n\nclass HashtagABTest:\n    \"\"\"A/B test hashtag strategies.\"\"\"\n\n    def __init__(self, test_name: str):\n        self.test_name = test_name\n        self.variant_a_tags: list[str] = []\n        self.variant_b_tags: list[str] = []\n        self.results_a: list[dict] = []\n        self.results_b: list[dict] = []\n        self.started_at: Optional[datetime] = None\n\n    def setup(\n        self,\n        variant_a: list[str],\n        variant_b: list[str]\n    ):\n        \"\"\"Setup test variants.\"\"\"\n        self.variant_a_tags = variant_a\n        self.variant_b_tags = variant_b\n        self.started_at = datetime.now()\n\n    def get_tags_for_tweet(self) -&gt; tuple[list[str], str]:\n        \"\"\"Get hashtags for next tweet (random assignment).\"\"\"\n        variant = random.choice(['A', 'B'])\n\n        if variant == 'A':\n            return self.variant_a_tags, 'A'\n        else:\n            return self.variant_b_tags, 'B'\n\n    def record_result(\n        self,\n        variant: str,\n        tweet_id: str,\n        likes: int,\n        retweets: int,\n        replies: int\n    ):\n        \"\"\"Record tweet performance.\"\"\"\n        result = {\n            'tweet_id': tweet_id,\n            'likes': likes,\n            'retweets': retweets,\n            'replies': replies,\n            'engagement': likes + retweets * 2 + replies * 3,\n            'recorded_at': datetime.now()\n        }\n\n        if variant == 'A':\n            self.results_a.append(result)\n        else:\n            self.results_b.append(result)\n\n    def analyze(self) -&gt; ABTestResult:\n        \"\"\"Analyze test results.\"\"\"\n\n        if len(self.results_a) &lt; 5 or len(self.results_b) &lt; 5:\n            return ABTestResult(\n                test_name=self.test_name,\n                variant_a_tags=self.variant_a_tags,\n                variant_b_tags=self.variant_b_tags,\n                variant_a_tweets=len(self.results_a),\n                variant_b_tweets=len(self.results_b),\n                variant_a_avg_engagement=0,\n                variant_b_avg_engagement=0,\n                winner=\"insufficient_data\",\n                confidence=0,\n                recommendation=\"Need at least 5 tweets per variant\"\n            )\n\n        # Calculate averages\n        eng_a = [r['engagement'] for r in self.results_a]\n        eng_b = [r['engagement'] for r in self.results_b]\n\n        avg_a = statistics.mean(eng_a)\n        avg_b = statistics.mean(eng_b)\n\n        # Simple statistical test (t-test approximation)\n        std_a = statistics.stdev(eng_a) if len(eng_a) &gt; 1 else 0\n        std_b = statistics.stdev(eng_b) if len(eng_b) &gt; 1 else 0\n\n        # Calculate effect size\n        pooled_std = ((std_a ** 2 + std_b ** 2) / 2) ** 0.5\n        if pooled_std &gt; 0:\n            effect_size = abs(avg_a - avg_b) / pooled_std\n        else:\n            effect_size = 0\n\n        # Determine winner\n        if effect_size &lt; 0.2:\n            winner = \"no_difference\"\n            confidence = 0.5\n        elif avg_a &gt; avg_b:\n            winner = \"A\"\n            confidence = min(0.95, 0.5 + effect_size * 0.2)\n        else:\n            winner = \"B\"\n            confidence = min(0.95, 0.5 + effect_size * 0.2)\n\n        # Generate recommendation\n        if winner == \"no_difference\":\n            recommendation = \"No significant difference. Consider testing other variations.\"\n        elif confidence &gt; 0.8:\n            recommendation = f\"Strong evidence for Variant {winner}. Implement these hashtags.\"\n        else:\n            recommendation = f\"Slight edge for Variant {winner}. Continue testing for confirmation.\"\n\n        return ABTestResult(\n            test_name=self.test_name,\n            variant_a_tags=self.variant_a_tags,\n            variant_b_tags=self.variant_b_tags,\n            variant_a_tweets=len(self.results_a),\n            variant_b_tweets=len(self.results_b),\n            variant_a_avg_engagement=round(avg_a, 2),\n            variant_b_avg_engagement=round(avg_b, 2),\n            winner=winner,\n            confidence=round(confidence, 2),\n            recommendation=recommendation\n        )\n</code></pre>"},{"location":"cookbook/growth/hashtag-strategy/#strategy-generator","title":"Strategy Generator","text":"<pre><code># strategy_generator.py\nfrom hashtag_models import HashtagScore, HashtagStrategy\n\nclass StrategyGenerator:\n    \"\"\"Generate optimized hashtag strategies.\"\"\"\n\n    def __init__(self, max_hashtags: int = 5):\n        self.max_hashtags = max_hashtags\n\n    def generate_strategy(\n        self,\n        scored_hashtags: list[HashtagScore],\n        trending: list[str] = None\n    ) -&gt; HashtagStrategy:\n        \"\"\"Generate optimized hashtag strategy.\"\"\"\n\n        # Sort by opportunity score\n        sorted_tags = sorted(\n            scored_hashtags,\n            key=lambda s: s.opportunity_score,\n            reverse=True\n        )\n\n        # Categorize\n        primary = []\n        secondary = []\n        niche = []\n        avoid = []\n\n        for score in sorted_tags:\n            if score.recommendation == \"use_always\":\n                if len(primary) &lt; 3:\n                    primary.append(score.tag)\n                else:\n                    secondary.append(score.tag)\n\n            elif score.recommendation == \"use_sometimes\":\n                if score.competition_score &gt; 70:  # Low competition\n                    niche.append(score.tag)\n                else:\n                    secondary.append(score.tag)\n\n            else:  # avoid\n                avoid.append(score.tag)\n\n        # Determine trending slots\n        trending_slots = max(0, self.max_hashtags - len(primary) - 1)\n\n        return HashtagStrategy(\n            primary_tags=primary[:3],\n            secondary_tags=secondary[:10],\n            trending_slots=trending_slots,\n            niche_tags=niche[:5],\n            avoid_tags=avoid[:10]\n        )\n\n    def get_tags_for_post(\n        self,\n        strategy: HashtagStrategy,\n        trending: list[str] = None,\n        max_tags: int = 5\n    ) -&gt; list[str]:\n        \"\"\"Get optimized hashtags for a specific post.\"\"\"\n\n        tags = []\n\n        # Always include primary tags\n        tags.extend(strategy.primary_tags)\n\n        # Add trending if available and relevant\n        if trending and strategy.trending_slots &gt; 0:\n            relevant_trending = [\n                t for t in trending\n                if t not in strategy.avoid_tags\n            ][:strategy.trending_slots]\n            tags.extend(relevant_trending)\n\n        # Fill remaining with secondary/niche\n        remaining = max_tags - len(tags)\n        if remaining &gt; 0:\n            import random\n\n            # Mix secondary and niche\n            pool = strategy.secondary_tags + strategy.niche_tags\n            random.shuffle(pool)\n            tags.extend(pool[:remaining])\n\n        return tags[:max_tags]\n</code></pre>"},{"location":"cookbook/growth/hashtag-strategy/#weekly-report-generator","title":"Weekly Report Generator","text":"<pre><code># hashtag_report.py\nfrom datetime import datetime\nfrom hashtag_models import HashtagMetrics, HashtagScore\n\nclass HashtagReportGenerator:\n    \"\"\"Generate weekly hashtag performance reports.\"\"\"\n\n    def generate_report(\n        self,\n        metrics: list[HashtagMetrics],\n        scores: list[HashtagScore],\n        your_usage: dict[str, int] = None\n    ) -&gt; str:\n        \"\"\"Generate markdown report.\"\"\"\n\n        report = f\"\"\"\n# Hashtag Performance Report\n**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n## Top Performing Hashtags\n\n| Hashtag | Opportunity | Reach | Competition | Recommendation |\n|---------|-------------|-------|-------------|----------------|\n\"\"\"\n\n        # Sort by opportunity\n        sorted_scores = sorted(scores, key=lambda s: s.opportunity_score, reverse=True)\n\n        for score in sorted_scores[:15]:\n            emoji = {\n                \"use_always\": \"\u2705\",\n                \"use_sometimes\": \"\ud83d\udd04\",\n                \"avoid\": \"\u274c\"\n            }.get(score.recommendation, \"\")\n\n            report += f\"| #{score.tag} | {score.opportunity_score} | {score.reach_score} | {score.competition_score} | {emoji} {score.recommendation} |\\n\"\n\n        report += \"\"\"\n## Hashtag Insights\n\n### Best Opportunities\n\"\"\"\n\n        best = [s for s in sorted_scores if s.recommendation == \"use_always\"][:5]\n        for score in best:\n            report += f\"\\n**#{score.tag}** (Score: {score.opportunity_score})\\n\"\n            for note in score.notes:\n                report += f\"- {note}\\n\"\n\n        report += \"\"\"\n### Niche Gems (Low Competition)\n\"\"\"\n\n        niche = sorted(\n            [s for s in scores if s.competition_score &gt; 70],\n            key=lambda s: s.opportunity_score,\n            reverse=True\n        )[:5]\n\n        for score in niche:\n            metrics_obj = next((m for m in metrics if m.tag == score.tag), None)\n            if metrics_obj:\n                report += f\"- **#{score.tag}**: {metrics_obj.tweets_per_hour:.1f} tweets/hr, {metrics_obj.engagement_rate*100:.2f}% engagement\\n\"\n\n        report += \"\"\"\n### Avoid These\n\"\"\"\n\n        avoid = [s for s in sorted_scores if s.recommendation == \"avoid\"][:5]\n        for score in avoid:\n            report += f\"- #{score.tag}: {', '.join(score.notes)}\\n\"\n\n        return report\n</code></pre>"},{"location":"cookbook/growth/hashtag-strategy/#complete-usage-example","title":"Complete Usage Example","text":"<pre><code># main.py\nimport asyncio\nfrom hashtag_scraper import HashtagScraper\nfrom opportunity_scorer import OpportunityScorer\nfrom niche_discoverer import NicheDiscoverer\nfrom strategy_generator import StrategyGenerator\nfrom hashtag_report import HashtagReportGenerator\n\nasync def main():\n    # 1. Define hashtags to analyze\n    hashtags_to_test = [\n        \"python\", \"programming\", \"coding\", \"developer\",\n        \"tech\", \"startup\", \"ai\", \"machinelearning\",\n        \"100DaysOfCode\", \"CodeNewbie\", \"DevCommunity\"\n    ]\n\n    # 2. Scrape metrics\n    scraper = HashtagScraper()\n    metrics = await scraper.analyze_multiple(hashtags_to_test)\n\n    print(f\"Analyzed {len(metrics)} hashtags\")\n\n    # 3. Score opportunities\n    scorer = OpportunityScorer()\n    scores = [scorer.score(m, relevance=0.8) for m in metrics]\n\n    # 4. Discover niche hashtags\n    discoverer = NicheDiscoverer()\n    niche_tags = await discoverer.discover_from_accounts(\n        ['successful_account_1', 'successful_account_2'],\n        tweets_per_account=50\n    )\n\n    print(f\"Discovered {len(niche_tags)} niche hashtags\")\n\n    # 5. Generate strategy\n    generator = StrategyGenerator(max_hashtags=5)\n    strategy = generator.generate_strategy(scores)\n\n    print(\"\\nHashtag Strategy:\")\n    print(f\"  Primary: {strategy.primary_tags}\")\n    print(f\"  Secondary: {strategy.secondary_tags[:5]}\")\n    print(f\"  Niche: {strategy.niche_tags}\")\n\n    # 6. Get tags for a post\n    post_tags = generator.get_tags_for_post(strategy, max_tags=5)\n    print(f\"\\nTags for next post: {' '.join('#' + t for t in post_tags)}\")\n\n    # 7. Generate report\n    report_gen = HashtagReportGenerator()\n    report = report_gen.generate_report(metrics, scores)\n\n    with open(\"hashtag_report.md\", \"w\") as f:\n        f.write(report)\n\n    print(\"\\nReport saved to hashtag_report.md\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"cookbook/growth/hashtag-strategy/#best-practices","title":"Best Practices","text":"<p>Hashtag Count</p> <ul> <li>Use 3-5 hashtags maximum</li> <li>Quality over quantity</li> <li>Mix popular and niche</li> </ul> <p>Avoid</p> <ul> <li>Banned or shadowbanned hashtags</li> <li>Hashtags with &gt;1M tweets/day</li> <li>Irrelevant trending tags</li> </ul>"},{"location":"cookbook/growth/hashtag-strategy/#related-recipes","title":"Related Recipes","text":"<ul> <li>Optimal Timing - When to post</li> <li>Content Calendar - Plan content</li> <li>Brand Monitoring - Track mentions</li> </ul>"},{"location":"cookbook/growth/optimal-timing/","title":"ML-Powered Posting Time Optimizer","text":"<p>Build a machine learning system to discover your optimal posting times based on historical engagement data.</p>"},{"location":"cookbook/growth/optimal-timing/#overview","title":"Overview","text":"<p>This recipe creates a posting time optimization system with:</p> <ul> <li>Historical data collection - Gather your engagement data</li> <li>Pattern analysis - Identify engagement trends</li> <li>Timezone detection - Understand your audience</li> <li>ML prediction - Random forest model for timing</li> <li>Schedule generation - Personalized posting calendar</li> <li>Continuous learning - Improve over time</li> </ul>"},{"location":"cookbook/growth/optimal-timing/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Data           \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Feature     \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  ML Model       \u2502\n\u2502  Collector      \u2502     \u2502  Engineer    \u2502     \u2502  (RandomForest) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                       \u2502                     \u2502\n        \u25bc                       \u25bc                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Timezone       \u2502     \u2502  Cross       \u2502     \u2502  Schedule       \u2502\n\u2502  Analyzer       \u2502     \u2502  Validator   \u2502     \u2502  Generator      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cookbook/growth/optimal-timing/#data-collection","title":"Data Collection","text":"<pre><code># data_collector.py\nimport asyncio\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass\nimport json\n\nfrom xeepy import Xeepy\n\n@dataclass\nclass TweetPerformance:\n    tweet_id: str\n    created_at: datetime\n    hour: int\n    day_of_week: int  # 0=Monday\n    text_length: int\n    has_media: bool\n    has_link: bool\n    hashtag_count: int\n\n    # Engagement metrics\n    likes: int\n    retweets: int\n    replies: int\n    quotes: int\n    impressions: int  # If available\n\n    # Calculated\n    engagement_score: float = 0.0\n\n    def __post_init__(self):\n        self.engagement_score = (\n            self.likes + \n            self.retweets * 2 + \n            self.replies * 3 +\n            self.quotes * 2\n        )\n\nclass DataCollector:\n    \"\"\"Collect historical tweet performance data.\"\"\"\n\n    def __init__(self, username: str):\n        self.username = username\n        self.data: list[TweetPerformance] = []\n\n    async def collect(\n        self,\n        tweet_count: int = 200,\n        days_back: int = 90\n    ) -&gt; list[TweetPerformance]:\n        \"\"\"Collect tweet performance data.\"\"\"\n\n        async with Xeepy() as x:\n            # Get tweets\n            tweets = await x.scrape.tweets(\n                self.username,\n                limit=tweet_count\n            )\n\n            cutoff = datetime.now() - timedelta(days=days_back)\n\n            for tweet in tweets:\n                if tweet.created_at &lt; cutoff:\n                    continue\n\n                # Skip retweets\n                if tweet.is_retweet:\n                    continue\n\n                # Detect features\n                has_media = bool(tweet.media)\n                has_link = 'http' in tweet.text\n                hashtag_count = tweet.text.count('#')\n\n                perf = TweetPerformance(\n                    tweet_id=tweet.id,\n                    created_at=tweet.created_at,\n                    hour=tweet.created_at.hour,\n                    day_of_week=tweet.created_at.weekday(),\n                    text_length=len(tweet.text),\n                    has_media=has_media,\n                    has_link=has_link,\n                    hashtag_count=hashtag_count,\n                    likes=tweet.like_count,\n                    retweets=tweet.retweet_count,\n                    replies=tweet.reply_count,\n                    quotes=tweet.quote_count,\n                    impressions=getattr(tweet, 'impressions', 0)\n                )\n\n                self.data.append(perf)\n\n        return self.data\n\n    def save(self, filepath: str):\n        \"\"\"Save collected data to JSON.\"\"\"\n        data = []\n        for perf in self.data:\n            data.append({\n                'tweet_id': perf.tweet_id,\n                'created_at': perf.created_at.isoformat(),\n                'hour': perf.hour,\n                'day_of_week': perf.day_of_week,\n                'text_length': perf.text_length,\n                'has_media': perf.has_media,\n                'has_link': perf.has_link,\n                'hashtag_count': perf.hashtag_count,\n                'likes': perf.likes,\n                'retweets': perf.retweets,\n                'replies': perf.replies,\n                'quotes': perf.quotes,\n                'impressions': perf.impressions,\n                'engagement_score': perf.engagement_score\n            })\n\n        with open(filepath, 'w') as f:\n            json.dump(data, f, indent=2)\n\n    def load(self, filepath: str):\n        \"\"\"Load data from JSON.\"\"\"\n        with open(filepath) as f:\n            data = json.load(f)\n\n        self.data = []\n        for d in data:\n            self.data.append(TweetPerformance(\n                tweet_id=d['tweet_id'],\n                created_at=datetime.fromisoformat(d['created_at']),\n                hour=d['hour'],\n                day_of_week=d['day_of_week'],\n                text_length=d['text_length'],\n                has_media=d['has_media'],\n                has_link=d['has_link'],\n                hashtag_count=d['hashtag_count'],\n                likes=d['likes'],\n                retweets=d['retweets'],\n                replies=d['replies'],\n                quotes=d['quotes'],\n                impressions=d['impressions']\n            ))\n</code></pre>"},{"location":"cookbook/growth/optimal-timing/#audience-timezone-analyzer","title":"Audience Timezone Analyzer","text":"<pre><code># timezone_analyzer.py\nfrom collections import Counter\nfrom datetime import datetime\nimport pytz\n\nfrom xeepy import Xeepy\n\nclass TimezoneAnalyzer:\n    \"\"\"Analyze audience timezone distribution.\"\"\"\n\n    # Major timezone mappings\n    TIMEZONE_HINTS = {\n        'PST': 'America/Los_Angeles',\n        'EST': 'America/New_York',\n        'CST': 'America/Chicago',\n        'GMT': 'Europe/London',\n        'CET': 'Europe/Paris',\n        'IST': 'Asia/Kolkata',\n        'JST': 'Asia/Tokyo',\n        'AEST': 'Australia/Sydney',\n    }\n\n    LOCATION_TIMEZONES = {\n        'new york': 'America/New_York',\n        'los angeles': 'America/Los_Angeles',\n        'san francisco': 'America/Los_Angeles',\n        'london': 'Europe/London',\n        'paris': 'Europe/Paris',\n        'berlin': 'Europe/Berlin',\n        'tokyo': 'Asia/Tokyo',\n        'india': 'Asia/Kolkata',\n        'singapore': 'Asia/Singapore',\n        'sydney': 'Australia/Sydney',\n        'toronto': 'America/Toronto',\n        'chicago': 'America/Chicago',\n    }\n\n    async def analyze_followers(\n        self,\n        username: str,\n        sample_size: int = 200\n    ) -&gt; dict[str, float]:\n        \"\"\"Estimate timezone distribution from follower locations.\"\"\"\n\n        async with Xeepy() as x:\n            followers = await x.scrape.followers(username, limit=sample_size)\n\n            timezone_counts = Counter()\n\n            for follower in followers:\n                if follower.location:\n                    tz = self._guess_timezone(follower.location.lower())\n                    if tz:\n                        timezone_counts[tz] += 1\n\n            # Convert to percentages\n            total = sum(timezone_counts.values())\n            if total == 0:\n                return {}\n\n            return {\n                tz: count / total\n                for tz, count in timezone_counts.most_common(10)\n            }\n\n    def _guess_timezone(self, location: str) -&gt; str:\n        \"\"\"Guess timezone from location string.\"\"\"\n        location = location.lower()\n\n        for city, tz in self.LOCATION_TIMEZONES.items():\n            if city in location:\n                return tz\n\n        return None\n\n    def get_optimal_hours_by_timezone(\n        self,\n        timezone_distribution: dict[str, float],\n        optimal_local_hours: list[int] = None\n    ) -&gt; list[tuple[int, float]]:\n        \"\"\"Get optimal posting hours considering audience timezones.\"\"\"\n\n        if optimal_local_hours is None:\n            optimal_local_hours = [9, 12, 17, 20]  # Default optimal hours\n\n        # Weight each UTC hour by timezone distribution\n        hour_weights = Counter()\n\n        for tz_name, weight in timezone_distribution.items():\n            try:\n                tz = pytz.timezone(tz_name)\n\n                for local_hour in optimal_local_hours:\n                    # Convert local hour to UTC\n                    now = datetime.now(tz)\n                    local_time = now.replace(hour=local_hour, minute=0)\n                    utc_time = local_time.astimezone(pytz.UTC)\n                    utc_hour = utc_time.hour\n\n                    hour_weights[utc_hour] += weight\n            except Exception:\n                continue\n\n        # Sort by weight\n        return sorted(hour_weights.items(), key=lambda x: x[1], reverse=True)\n</code></pre>"},{"location":"cookbook/growth/optimal-timing/#feature-engineering","title":"Feature Engineering","text":"<pre><code># feature_engineer.py\nimport numpy as np\nfrom typing import Tuple\n\nclass FeatureEngineer:\n    \"\"\"Engineer features for ML model.\"\"\"\n\n    def prepare_features(\n        self,\n        data: list['TweetPerformance']\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Prepare feature matrix and target vector.\"\"\"\n\n        features = []\n        targets = []\n\n        for perf in data:\n            # Time features\n            hour = perf.hour\n            day = perf.day_of_week\n\n            # Cyclical encoding for hour\n            hour_sin = np.sin(2 * np.pi * hour / 24)\n            hour_cos = np.cos(2 * np.pi * hour / 24)\n\n            # Cyclical encoding for day\n            day_sin = np.sin(2 * np.pi * day / 7)\n            day_cos = np.cos(2 * np.pi * day / 7)\n\n            # Is weekend\n            is_weekend = 1 if day &gt;= 5 else 0\n\n            # Is business hours (9-17)\n            is_business = 1 if 9 &lt;= hour &lt;= 17 else 0\n\n            # Is evening (18-22)\n            is_evening = 1 if 18 &lt;= hour &lt;= 22 else 0\n\n            # Content features\n            text_length_norm = min(perf.text_length / 280, 1.0)\n            has_media = 1 if perf.has_media else 0\n            has_link = 1 if perf.has_link else 0\n            hashtag_count_norm = min(perf.hashtag_count / 5, 1.0)\n\n            feature_vector = [\n                hour_sin, hour_cos,\n                day_sin, day_cos,\n                is_weekend,\n                is_business,\n                is_evening,\n                text_length_norm,\n                has_media,\n                has_link,\n                hashtag_count_norm\n            ]\n\n            features.append(feature_vector)\n            targets.append(perf.engagement_score)\n\n        return np.array(features), np.array(targets)\n\n    def prepare_prediction_features(\n        self,\n        hour: int,\n        day_of_week: int,\n        has_media: bool = False,\n        has_link: bool = False,\n        hashtag_count: int = 2,\n        text_length: int = 200\n    ) -&gt; np.ndarray:\n        \"\"\"Prepare features for prediction.\"\"\"\n\n        hour_sin = np.sin(2 * np.pi * hour / 24)\n        hour_cos = np.cos(2 * np.pi * hour / 24)\n        day_sin = np.sin(2 * np.pi * day_of_week / 7)\n        day_cos = np.cos(2 * np.pi * day_of_week / 7)\n        is_weekend = 1 if day_of_week &gt;= 5 else 0\n        is_business = 1 if 9 &lt;= hour &lt;= 17 else 0\n        is_evening = 1 if 18 &lt;= hour &lt;= 22 else 0\n        text_length_norm = min(text_length / 280, 1.0)\n        has_media_val = 1 if has_media else 0\n        has_link_val = 1 if has_link else 0\n        hashtag_count_norm = min(hashtag_count / 5, 1.0)\n\n        return np.array([[\n            hour_sin, hour_cos,\n            day_sin, day_cos,\n            is_weekend,\n            is_business,\n            is_evening,\n            text_length_norm,\n            has_media_val,\n            has_link_val,\n            hashtag_count_norm\n        ]])\n</code></pre>"},{"location":"cookbook/growth/optimal-timing/#ml-model","title":"ML Model","text":"<pre><code># timing_model.py\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score, TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler\nimport pickle\n\nclass TimingModel:\n    \"\"\"Random Forest model for timing prediction.\"\"\"\n\n    def __init__(self):\n        self.model = RandomForestRegressor(\n            n_estimators=100,\n            max_depth=10,\n            min_samples_split=5,\n            min_samples_leaf=2,\n            random_state=42\n        )\n        self.scaler = StandardScaler()\n        self.is_trained = False\n\n    def train(\n        self,\n        X: np.ndarray,\n        y: np.ndarray\n    ) -&gt; dict:\n        \"\"\"Train the model.\"\"\"\n\n        # Scale features\n        X_scaled = self.scaler.fit_transform(X)\n\n        # Cross-validation\n        tscv = TimeSeriesSplit(n_splits=5)\n        cv_scores = cross_val_score(\n            self.model, X_scaled, y,\n            cv=tscv,\n            scoring='neg_mean_squared_error'\n        )\n\n        # Train final model\n        self.model.fit(X_scaled, y)\n        self.is_trained = True\n\n        # Feature importance\n        feature_names = [\n            'hour_sin', 'hour_cos',\n            'day_sin', 'day_cos',\n            'is_weekend', 'is_business', 'is_evening',\n            'text_length', 'has_media', 'has_link', 'hashtag_count'\n        ]\n\n        importance = dict(zip(\n            feature_names,\n            self.model.feature_importances_\n        ))\n\n        return {\n            'cv_rmse': np.sqrt(-cv_scores.mean()),\n            'cv_std': np.sqrt(-cv_scores).std(),\n            'feature_importance': importance\n        }\n\n    def predict(self, X: np.ndarray) -&gt; float:\n        \"\"\"Predict engagement score.\"\"\"\n        if not self.is_trained:\n            raise ValueError(\"Model not trained\")\n\n        X_scaled = self.scaler.transform(X)\n        return self.model.predict(X_scaled)[0]\n\n    def save(self, filepath: str):\n        \"\"\"Save model to file.\"\"\"\n        with open(filepath, 'wb') as f:\n            pickle.dump({\n                'model': self.model,\n                'scaler': self.scaler,\n                'is_trained': self.is_trained\n            }, f)\n\n    def load(self, filepath: str):\n        \"\"\"Load model from file.\"\"\"\n        with open(filepath, 'rb') as f:\n            data = pickle.load(f)\n\n        self.model = data['model']\n        self.scaler = data['scaler']\n        self.is_trained = data['is_trained']\n</code></pre>"},{"location":"cookbook/growth/optimal-timing/#schedule-generator","title":"Schedule Generator","text":"<pre><code># schedule_generator.py\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass\n\n@dataclass\nclass ScheduleSlot:\n    day_of_week: int  # 0=Monday\n    hour: int\n    predicted_engagement: float\n    rank: int\n\nclass ScheduleGenerator:\n    \"\"\"Generate optimized posting schedule.\"\"\"\n\n    def __init__(\n        self,\n        model: 'TimingModel',\n        feature_engineer: 'FeatureEngineer'\n    ):\n        self.model = model\n        self.engineer = feature_engineer\n\n    def generate_heatmap(\n        self,\n        has_media: bool = False\n    ) -&gt; list[list[float]]:\n        \"\"\"Generate engagement heatmap (7 days x 24 hours).\"\"\"\n\n        heatmap = []\n\n        for day in range(7):\n            day_scores = []\n            for hour in range(24):\n                features = self.engineer.prepare_prediction_features(\n                    hour=hour,\n                    day_of_week=day,\n                    has_media=has_media\n                )\n                score = self.model.predict(features)\n                day_scores.append(score)\n            heatmap.append(day_scores)\n\n        return heatmap\n\n    def get_top_slots(\n        self,\n        posts_per_day: int = 3,\n        has_media: bool = False\n    ) -&gt; list[ScheduleSlot]:\n        \"\"\"Get top posting slots for each day.\"\"\"\n\n        all_slots = []\n\n        for day in range(7):\n            day_slots = []\n\n            for hour in range(24):\n                features = self.engineer.prepare_prediction_features(\n                    hour=hour,\n                    day_of_week=day,\n                    has_media=has_media\n                )\n                score = self.model.predict(features)\n                day_slots.append((hour, score))\n\n            # Sort by score and get top slots\n            day_slots.sort(key=lambda x: x[1], reverse=True)\n\n            for rank, (hour, score) in enumerate(day_slots[:posts_per_day], 1):\n                all_slots.append(ScheduleSlot(\n                    day_of_week=day,\n                    hour=hour,\n                    predicted_engagement=score,\n                    rank=rank\n                ))\n\n        return all_slots\n\n    def generate_week_schedule(\n        self,\n        start_date: datetime,\n        posts_per_day: int = 2\n    ) -&gt; list[datetime]:\n        \"\"\"Generate specific posting times for the week.\"\"\"\n\n        slots = self.get_top_slots(posts_per_day=posts_per_day)\n\n        schedule = []\n        current_date = start_date\n\n        # Find next Monday\n        days_until_monday = (7 - current_date.weekday()) % 7\n        week_start = current_date + timedelta(days=days_until_monday)\n\n        for slot in slots:\n            post_date = week_start + timedelta(days=slot.day_of_week)\n            post_time = post_date.replace(\n                hour=slot.hour,\n                minute=0,\n                second=0\n            )\n            schedule.append(post_time)\n\n        return sorted(schedule)\n\n    def print_schedule(self, slots: list[ScheduleSlot]):\n        \"\"\"Print formatted schedule.\"\"\"\n\n        days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', \n                'Friday', 'Saturday', 'Sunday']\n\n        print(\"\\n\ud83d\udcc5 Optimal Posting Schedule\\n\")\n        print(\"=\" * 50)\n\n        for day in range(7):\n            day_slots = [s for s in slots if s.day_of_week == day]\n            day_slots.sort(key=lambda s: s.rank)\n\n            print(f\"\\n{days[day]}:\")\n            for slot in day_slots:\n                time_str = f\"{slot.hour:02d}:00\"\n                bar = \"\u2588\" * int(slot.predicted_engagement / 10)\n                print(f\"  {time_str} | {bar} ({slot.predicted_engagement:.1f})\")\n</code></pre>"},{"location":"cookbook/growth/optimal-timing/#continuous-learning-system","title":"Continuous Learning System","text":"<pre><code># continuous_learner.py\nfrom datetime import datetime, timedelta\nimport asyncio\n\nclass ContinuousLearner:\n    \"\"\"Continuously improve model with new data.\"\"\"\n\n    def __init__(\n        self,\n        collector: 'DataCollector',\n        model: 'TimingModel',\n        engineer: 'FeatureEngineer',\n        retrain_interval_days: int = 7\n    ):\n        self.collector = collector\n        self.model = model\n        self.engineer = engineer\n        self.retrain_interval = timedelta(days=retrain_interval_days)\n        self.last_trained = None\n\n    async def update_model(self) -&gt; dict:\n        \"\"\"Collect new data and retrain model.\"\"\"\n\n        # Collect recent data\n        new_data = await self.collector.collect(\n            tweet_count=50,\n            days_back=14\n        )\n\n        if len(new_data) &lt; 20:\n            return {'status': 'insufficient_data', 'samples': len(new_data)}\n\n        # Combine with existing data\n        all_data = self.collector.data\n\n        # Prepare features\n        X, y = self.engineer.prepare_features(all_data)\n\n        # Retrain\n        metrics = self.model.train(X, y)\n\n        self.last_trained = datetime.now()\n\n        return {\n            'status': 'success',\n            'samples': len(all_data),\n            'metrics': metrics\n        }\n\n    async def run_continuous(self):\n        \"\"\"Run continuous learning loop.\"\"\"\n\n        while True:\n            if self.last_trained is None or \\\n               datetime.now() - self.last_trained &gt; self.retrain_interval:\n\n                print(\"Updating model with new data...\")\n                result = await self.update_model()\n                print(f\"Update result: {result['status']}\")\n\n                if 'metrics' in result:\n                    print(f\"  RMSE: {result['metrics']['cv_rmse']:.2f}\")\n\n            # Wait before next check\n            await asyncio.sleep(3600)  # Check every hour\n</code></pre>"},{"location":"cookbook/growth/optimal-timing/#complete-usage-example","title":"Complete Usage Example","text":"<pre><code># main.py\nimport asyncio\nfrom data_collector import DataCollector\nfrom feature_engineer import FeatureEngineer\nfrom timing_model import TimingModel\nfrom schedule_generator import ScheduleGenerator\nfrom timezone_analyzer import TimezoneAnalyzer\nfrom datetime import datetime\n\nasync def main():\n    username = \"your_username\"\n\n    # 1. Collect historical data\n    print(\"Collecting data...\")\n    collector = DataCollector(username)\n    data = await collector.collect(tweet_count=200)\n    print(f\"Collected {len(data)} tweets\")\n\n    # Save for future use\n    collector.save(\"tweet_data.json\")\n\n    # 2. Analyze audience timezones\n    print(\"\\nAnalyzing audience timezones...\")\n    tz_analyzer = TimezoneAnalyzer()\n    tz_dist = await tz_analyzer.analyze_followers(username, sample_size=200)\n\n    print(\"Timezone distribution:\")\n    for tz, pct in list(tz_dist.items())[:5]:\n        print(f\"  {tz}: {pct*100:.1f}%\")\n\n    # 3. Prepare features and train model\n    print(\"\\nTraining model...\")\n    engineer = FeatureEngineer()\n    X, y = engineer.prepare_features(data)\n\n    model = TimingModel()\n    metrics = model.train(X, y)\n\n    print(f\"Model RMSE: {metrics['cv_rmse']:.2f}\")\n    print(\"\\nFeature importance:\")\n    for feat, imp in sorted(\n        metrics['feature_importance'].items(),\n        key=lambda x: x[1],\n        reverse=True\n    )[:5]:\n        print(f\"  {feat}: {imp:.3f}\")\n\n    # Save model\n    model.save(\"timing_model.pkl\")\n\n    # 4. Generate schedule\n    print(\"\\nGenerating optimal schedule...\")\n    generator = ScheduleGenerator(model, engineer)\n\n    slots = generator.get_top_slots(posts_per_day=3)\n    generator.print_schedule(slots)\n\n    # 5. Get specific times for next week\n    schedule = generator.generate_week_schedule(\n        start_date=datetime.now(),\n        posts_per_day=2\n    )\n\n    print(\"\\n\ud83d\udcc6 Next Week's Posting Times:\")\n    for dt in schedule[:10]:\n        print(f\"  {dt.strftime('%A %H:%M')}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"cookbook/growth/optimal-timing/#best-practices","title":"Best Practices","text":"<p>Data Quality</p> <ul> <li>Need at least 50 tweets for meaningful results</li> <li>More data = better predictions</li> <li>Exclude outliers (viral tweets)</li> </ul> <p>Limitations</p> <ul> <li>Model reflects YOUR past audience</li> <li>Audience behavior changes over time</li> <li>Retrain regularly (weekly)</li> </ul>"},{"location":"cookbook/growth/optimal-timing/#related-recipes","title":"Related Recipes","text":"<ul> <li>Content Calendar - Plan content</li> <li>Hashtag Strategy - Optimize hashtags</li> <li>Scheduled Posts - Automate posting</li> </ul>"},{"location":"cookbook/growth/viral-content/","title":"Viral Content Detection","text":"<p>Detect trending content before it peaks. Be first, not last.</p>"},{"location":"cookbook/growth/viral-content/#the-science-of-virality","title":"The Science of Virality","text":"<p>Viral tweets follow predictable patterns:</p> <pre><code>graph LR\n    A[\ud83c\udf31 Early Stage&lt;br/&gt;0-100 likes] --&gt; B[\ud83d\udcc8 Growth Stage&lt;br/&gt;100-1000 likes]\n    B --&gt; C[\ud83d\udd25 Viral Stage&lt;br/&gt;1000+ likes]\n    C --&gt; D[\ud83d\udcc9 Decline&lt;br/&gt;Engagement drops]\n\n    style A fill:#90EE90\n    style B fill:#FFD700\n    style C fill:#FF6347\n    style D fill:#D3D3D3</code></pre> <p>The key insight: Catch content in the early-to-growth transition for maximum opportunity.</p>"},{"location":"cookbook/growth/viral-content/#viral-velocity-tracker","title":"Viral Velocity Tracker","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass\n\n@dataclass\nclass ViralCandidate:\n    tweet_id: str\n    url: str\n    text: str\n    author: str\n    likes: int\n    retweets: int\n    age_minutes: int\n    velocity: float  # likes per minute\n    score: float\n\nasync def find_viral_candidates(keywords: list, min_velocity: float = 1.0):\n    \"\"\"\n    Find tweets going viral RIGHT NOW.\n\n    Velocity = likes / age_in_minutes\n\n    A 2-hour-old tweet with 200 likes = 1.67 velocity (going viral!)\n    A 24-hour-old tweet with 200 likes = 0.14 velocity (normal)\n    \"\"\"\n\n    async with Xeepy() as x:\n        candidates = []\n\n        for keyword in keywords:\n            # Search recent tweets\n            tweets = await x.scrape.search(\n                keyword,\n                search_type=\"latest\",\n                limit=100,\n                max_age_hours=6  # Only last 6 hours\n            )\n\n            for tweet in tweets:\n                # Calculate age\n                age = datetime.now() - tweet.created_at\n                age_minutes = age.total_seconds() / 60\n\n                if age_minutes &lt; 5:  # Too new to judge\n                    continue\n\n                # Calculate velocity\n                velocity = tweet.likes / age_minutes\n\n                # Score = velocity * engagement multiplier\n                engagement_multiplier = 1 + (tweet.retweets / max(tweet.likes, 1))\n                score = velocity * engagement_multiplier\n\n                if velocity &gt;= min_velocity:\n                    candidates.append(ViralCandidate(\n                        tweet_id=tweet.id,\n                        url=tweet.url,\n                        text=tweet.text[:100],\n                        author=tweet.author.username,\n                        likes=tweet.likes,\n                        retweets=tweet.retweets,\n                        age_minutes=int(age_minutes),\n                        velocity=velocity,\n                        score=score\n                    ))\n\n        # Sort by score\n        candidates.sort(key=lambda x: x.score, reverse=True)\n\n        return candidates[:20]\n\n# Usage\nasync def main():\n    candidates = await find_viral_candidates(\n        keywords=[\"python\", \"AI\", \"startup\", \"tech\"],\n        min_velocity=0.5\n    )\n\n    print(\"\ud83d\udd25 VIRAL CANDIDATES\")\n    print(\"=\"*70)\n\n    for i, c in enumerate(candidates, 1):\n        print(f\"\\n#{i} @{c.author} (velocity: {c.velocity:.2f}/min)\")\n        print(f\"   {c.text}...\")\n        print(f\"   \u2764\ufe0f {c.likes} | \ud83d\udd04 {c.retweets} | \u23f1\ufe0f {c.age_minutes}min old\")\n        print(f\"   {c.url}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"cookbook/growth/viral-content/#early-engagement-opportunity-finder","title":"Early Engagement Opportunity Finder","text":"<p>Find tweets with high potential but low current engagement:</p> <pre><code>import asyncio\nfrom xeepy import Xeepy\n\nasync def find_early_opportunities():\n    \"\"\"\n    Find tweets from high-follower accounts that haven't\n    gotten engagement yet - perfect for early comments.\n    \"\"\"\n\n    async with Xeepy() as x:\n        opportunities = []\n\n        # Search for recent tweets\n        tweets = await x.scrape.search(\n            \"python OR programming OR tech\",\n            search_type=\"latest\",\n            limit=200\n        )\n\n        for tweet in tweets:\n            # Calculate opportunity score\n            # High follower + low engagement = opportunity!\n\n            follower_count = tweet.author.followers_count\n            engagement = tweet.likes + tweet.retweets\n            age_minutes = (datetime.now() - tweet.created_at).total_seconds() / 60\n\n            # Skip if too old\n            if age_minutes &gt; 60:\n                continue\n\n            # Opportunity score\n            # High followers + low engagement = high score\n            if engagement &lt; 10 and follower_count &gt; 5000:\n                opportunity_score = follower_count / max(engagement + 1, 1)\n\n                opportunities.append({\n                    \"tweet\": tweet,\n                    \"score\": opportunity_score,\n                    \"followers\": follower_count,\n                    \"engagement\": engagement,\n                    \"age_minutes\": age_minutes\n                })\n\n        # Sort by opportunity score\n        opportunities.sort(key=lambda x: x[\"score\"], reverse=True)\n\n        print(\"\ud83c\udfaf EARLY ENGAGEMENT OPPORTUNITIES\")\n        print(\"=\"*70)\n        print(\"These tweets are from big accounts but haven't gone viral YET\")\n        print()\n\n        for opp in opportunities[:10]:\n            t = opp[\"tweet\"]\n            print(f\"@{t.author.username} ({opp['followers']:,} followers)\")\n            print(f\"  {t.text[:80]}...\")\n            print(f\"  \u2764\ufe0f {opp['engagement']} | \u23f1\ufe0f {opp['age_minutes']:.0f}min old\")\n            print(f\"  Score: {opp['score']:.0f}\")\n            print()\n\nasyncio.run(find_early_opportunities())\n</code></pre>"},{"location":"cookbook/growth/viral-content/#trend-prediction-model","title":"Trend Prediction Model","text":"<p>Use engagement patterns to predict virality:</p> <pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom datetime import datetime\nimport statistics\n\nasync def predict_virality(tweet_url: str):\n    \"\"\"\n    Monitor a tweet's engagement over time to predict if it will go viral.\n\n    Viral indicators:\n    1. Accelerating engagement (likes/min increasing)\n    2. High retweet-to-like ratio (&gt; 0.2)\n    3. Quote tweets appearing\n    4. Blue check engagement\n    \"\"\"\n\n    async with Xeepy() as x:\n        measurements = []\n\n        print(f\"\ud83d\udcca Monitoring: {tweet_url}\")\n        print(\"=\"*50)\n\n        # Take measurements every 5 minutes for 30 minutes\n        for i in range(7):\n            tweet = await x.scrape.tweet(tweet_url)\n\n            measurements.append({\n                \"time\": datetime.now(),\n                \"likes\": tweet.likes,\n                \"retweets\": tweet.retweets,\n                \"quotes\": tweet.quote_count,\n                \"replies\": tweet.reply_count,\n            })\n\n            if i &gt; 0:\n                # Calculate velocity change\n                prev = measurements[-2]\n                curr = measurements[-1]\n\n                time_diff = (curr[\"time\"] - prev[\"time\"]).total_seconds() / 60\n                like_velocity = (curr[\"likes\"] - prev[\"likes\"]) / time_diff\n                rt_velocity = (curr[\"retweets\"] - prev[\"retweets\"]) / time_diff\n\n                print(f\"[{i*5}min] \u2764\ufe0f {curr['likes']} (+{like_velocity:.1f}/min) \"\n                      f\"\ud83d\udd04 {curr['retweets']} (+{rt_velocity:.1f}/min)\")\n\n            if i &lt; 6:\n                await asyncio.sleep(300)  # Wait 5 minutes\n\n        # Analyze pattern\n        velocities = []\n        for i in range(1, len(measurements)):\n            prev, curr = measurements[i-1], measurements[i]\n            time_diff = (curr[\"time\"] - prev[\"time\"]).total_seconds() / 60\n            velocities.append((curr[\"likes\"] - prev[\"likes\"]) / time_diff)\n\n        # Is it accelerating?\n        if len(velocities) &gt;= 3:\n            early_avg = statistics.mean(velocities[:len(velocities)//2])\n            late_avg = statistics.mean(velocities[len(velocities)//2:])\n            acceleration = late_avg - early_avg\n\n            # Calculate viral probability\n            final = measurements[-1]\n            rt_ratio = final[\"retweets\"] / max(final[\"likes\"], 1)\n\n            viral_score = 0\n\n            if acceleration &gt; 0.5:\n                viral_score += 30\n                print(\"\u2705 Engagement accelerating\")\n\n            if rt_ratio &gt; 0.2:\n                viral_score += 25\n                print(\"\u2705 High retweet ratio\")\n\n            if late_avg &gt; 2:\n                viral_score += 25\n                print(\"\u2705 High velocity\")\n\n            if final[\"quotes\"] &gt; 5:\n                viral_score += 20\n                print(\"\u2705 Quote tweets appearing\")\n\n            print(f\"\\n\ud83c\udfaf Viral Probability: {viral_score}%\")\n\n            if viral_score &gt;= 70:\n                print(\"\ud83d\udd25 HIGH CHANCE OF GOING VIRAL!\")\n            elif viral_score &gt;= 40:\n                print(\"\ud83d\udcc8 Moderate viral potential\")\n            else:\n                print(\"\ud83d\udcca Normal engagement pattern\")\n\nasyncio.run(predict_virality(\"https://x.com/user/status/123\"))\n</code></pre>"},{"location":"cookbook/growth/viral-content/#trending-hashtag-surfer","title":"Trending Hashtag Surfer","text":"<p>Ride hashtag waves at the perfect moment:</p> <pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom collections import defaultdict\nfrom datetime import datetime, timedelta\n\nasync def find_rising_hashtags():\n    \"\"\"Find hashtags that are gaining momentum but haven't peaked\"\"\"\n\n    async with Xeepy() as x:\n        # Get current trends\n        trends = await x.trends()\n\n        hashtag_data = {}\n\n        for trend in trends:\n            if not trend.name.startswith('#'):\n                continue\n\n            # Get recent tweets with this hashtag\n            tweets = await x.scrape.hashtag(\n                trend.name,\n                limit=50,\n                sort=\"latest\"\n            )\n\n            if not tweets:\n                continue\n\n            # Analyze time distribution\n            now = datetime.now()\n            last_hour = [t for t in tweets if (now - t.created_at) &lt; timedelta(hours=1)]\n            prev_hour = [t for t in tweets if timedelta(hours=1) &lt;= (now - t.created_at) &lt; timedelta(hours=2)]\n\n            if prev_hour:\n                growth_rate = len(last_hour) / len(prev_hour)\n            else:\n                growth_rate = len(last_hour)\n\n            # Calculate average engagement\n            avg_engagement = sum(t.likes + t.retweets for t in tweets) / len(tweets)\n\n            hashtag_data[trend.name] = {\n                \"volume\": trend.tweet_count,\n                \"last_hour\": len(last_hour),\n                \"growth_rate\": growth_rate,\n                \"avg_engagement\": avg_engagement,\n            }\n\n        # Find rising hashtags (growing but not saturated)\n        rising = []\n        for tag, data in hashtag_data.items():\n            if data[\"growth_rate\"] &gt; 1.5 and data[\"volume\"] &lt; 50000:\n                rising.append((tag, data))\n\n        rising.sort(key=lambda x: x[1][\"growth_rate\"], reverse=True)\n\n        print(\"\ud83c\udf0a RISING HASHTAGS (jump on these!)\")\n        print(\"=\"*50)\n\n        for tag, data in rising[:10]:\n            print(f\"\\n{tag}\")\n            print(f\"  \ud83d\udcc8 Growth: {data['growth_rate']:.1f}x last hour\")\n            print(f\"  \ud83d\udcca Volume: {data['volume']:,} tweets\")\n            print(f\"  \u2764\ufe0f Avg engagement: {data['avg_engagement']:.0f}\")\n\nasyncio.run(find_rising_hashtags())\n</code></pre>"},{"location":"cookbook/growth/viral-content/#content-format-analyzer","title":"Content Format Analyzer","text":"<p>Discover what content formats perform best:</p> <pre><code>import asyncio\nfrom xeepy import Xeepy\nimport re\n\nasync def analyze_viral_formats(niche_keywords: list):\n    \"\"\"Analyze what content formats go viral in your niche\"\"\"\n\n    async with Xeepy() as x:\n        format_stats = {\n            \"thread\": {\"count\": 0, \"avg_likes\": 0, \"total_likes\": 0},\n            \"image\": {\"count\": 0, \"avg_likes\": 0, \"total_likes\": 0},\n            \"video\": {\"count\": 0, \"avg_likes\": 0, \"total_likes\": 0},\n            \"link\": {\"count\": 0, \"avg_likes\": 0, \"total_likes\": 0},\n            \"text_only\": {\"count\": 0, \"avg_likes\": 0, \"total_likes\": 0},\n            \"poll\": {\"count\": 0, \"avg_likes\": 0, \"total_likes\": 0},\n            \"list\": {\"count\": 0, \"avg_likes\": 0, \"total_likes\": 0},\n        }\n\n        for keyword in niche_keywords:\n            tweets = await x.scrape.search(\n                keyword,\n                min_likes=100,  # Only successful tweets\n                limit=200\n            )\n\n            for tweet in tweets:\n                # Detect format\n                if \"\ud83e\uddf5\" in tweet.text or \"/1\" in tweet.text:\n                    fmt = \"thread\"\n                elif tweet.media and \"video\" in str(tweet.media):\n                    fmt = \"video\"\n                elif tweet.media:\n                    fmt = \"image\"\n                elif \"http\" in tweet.text:\n                    fmt = \"link\"\n                elif tweet.poll:\n                    fmt = \"poll\"\n                elif re.search(r'^\\d\\.', tweet.text, re.MULTILINE):\n                    fmt = \"list\"\n                else:\n                    fmt = \"text_only\"\n\n                format_stats[fmt][\"count\"] += 1\n                format_stats[fmt][\"total_likes\"] += tweet.likes\n\n        # Calculate averages\n        for fmt, stats in format_stats.items():\n            if stats[\"count\"] &gt; 0:\n                stats[\"avg_likes\"] = stats[\"total_likes\"] / stats[\"count\"]\n\n        # Sort by average likes\n        sorted_formats = sorted(\n            format_stats.items(),\n            key=lambda x: x[1][\"avg_likes\"],\n            reverse=True\n        )\n\n        print(\"\ud83d\udcca CONTENT FORMAT PERFORMANCE\")\n        print(\"=\"*50)\n        print(f\"Analyzed tweets about: {', '.join(niche_keywords)}\")\n        print()\n\n        for fmt, stats in sorted_formats:\n            if stats[\"count\"] &gt; 0:\n                bar = \"\u2588\" * int(stats[\"avg_likes\"] / 50)\n                print(f\"{fmt:12} | {bar} {stats['avg_likes']:.0f} avg likes ({stats['count']} samples)\")\n\nasyncio.run(analyze_viral_formats([\"python\", \"coding\", \"tech\"]))\n</code></pre>"},{"location":"cookbook/growth/viral-content/#automated-viral-alert-system","title":"Automated Viral Alert System","text":"<p>Get notified when content in your niche goes viral:</p> <pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom xeepy.notifications import DiscordWebhook\n\nasync def viral_alert_bot(keywords: list, webhook_url: str):\n    \"\"\"\n    Monitor for viral content and send alerts.\n    Run this as a background service.\n    \"\"\"\n\n    webhook = DiscordWebhook(webhook_url)\n    seen_tweets = set()\n\n    async with Xeepy() as x:\n        while True:\n            print(f\"[{datetime.now()}] Scanning for viral content...\")\n\n            for keyword in keywords:\n                candidates = await find_viral_candidates([keyword], min_velocity=2.0)\n\n                for c in candidates[:3]:\n                    if c.tweet_id not in seen_tweets:\n                        seen_tweets.add(c.tweet_id)\n\n                        # Send alert\n                        await webhook.send(\n                            title=\"\ud83d\udd25 Viral Content Alert\",\n                            description=f\"**@{c.author}** is going viral!\\n\\n{c.text}\",\n                            fields=[\n                                {\"name\": \"\u2764\ufe0f Likes\", \"value\": str(c.likes), \"inline\": True},\n                                {\"name\": \"\ud83d\udcc8 Velocity\", \"value\": f\"{c.velocity:.1f}/min\", \"inline\": True},\n                                {\"name\": \"\ud83d\udd17 Link\", \"value\": c.url, \"inline\": False},\n                            ],\n                            color=0xFF6347\n                        )\n\n                        print(f\"Alert sent for @{c.author}\")\n\n            # Check every 5 minutes\n            await asyncio.sleep(300)\n\n# Run as daemon\nasyncio.run(viral_alert_bot(\n    keywords=[\"python\", \"AI\", \"startup\"],\n    webhook_url=\"https://discord.com/api/webhooks/...\"\n))\n</code></pre>"},{"location":"cookbook/growth/viral-content/#best-practices","title":"Best Practices","text":"<p>Timing is Everything</p> <ul> <li>Early bird wins: First quality reply on a viral tweet = exposure</li> <li>Don't chase peaks: By the time it's \"trending,\" you're too late</li> <li>Watch velocity, not volume: 100 likes in 10 min &gt; 1000 likes in 24 hours</li> </ul> <p>Viral Engagement Tactics</p> <ol> <li>Be in the first 10 replies on rising content</li> <li>Add genuine value (not \"Great post!\")</li> <li>Quote tweet with your own insight</li> <li>Follow the author before engaging</li> </ol>"},{"location":"cookbook/growth/viral-content/#next-steps","title":"Next Steps","text":"<p> Engagement Pods - Coordinate for maximum impact</p> <p> Hashtag Strategy - Master hashtag surfing</p> <p> Optimal Timing - When to post for maximum reach</p>"},{"location":"cookbook/research/","title":"Research &amp; Analysis Cookbook","text":"<p>Academic and professional research techniques using X/Twitter data.</p>"},{"location":"cookbook/research/#academic-research-framework","title":"\ud83d\udd2c Academic Research Framework","text":"<p>Build a research-grade data collection system.</p> <pre><code>\"\"\"\nAcademic Research Framework\nEthical, reproducible Twitter research methodology\n\"\"\"\n\nimport asyncio\nimport hashlib\nimport json\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Optional\n\n@dataclass\nclass ResearchMetadata:\n    \"\"\"Metadata for research dataset.\"\"\"\n    study_id: str\n    researcher: str\n    institution: str\n    irb_approval: Optional[str]\n    research_question: str\n    data_collection_start: datetime\n    data_collection_end: Optional[datetime]\n    sampling_method: str\n    ethical_considerations: list\n    data_processing_notes: list\n\n\n@dataclass\nclass DataCollectionLog:\n    \"\"\"Audit log for data collection.\"\"\"\n    timestamp: datetime\n    operation: str\n    parameters: dict\n    records_collected: int\n    errors: list\n    checksum: str\n\n\nclass AcademicResearcher:\n    \"\"\"Framework for ethical Twitter research.\"\"\"\n\n    def __init__(self, xeepy, metadata: ResearchMetadata):\n        self.x = xeepy\n        self.metadata = metadata\n        self.collection_logs: list[DataCollectionLog] = []\n\n        # Setup directories\n        self.data_dir = Path(f\"research_{metadata.study_id}\")\n        self.data_dir.mkdir(exist_ok=True)\n        (self.data_dir / \"raw\").mkdir(exist_ok=True)\n        (self.data_dir / \"processed\").mkdir(exist_ok=True)\n        (self.data_dir / \"logs\").mkdir(exist_ok=True)\n\n    def _anonymize_user(self, user: dict) -&gt; dict:\n        \"\"\"Anonymize user data for privacy.\"\"\"\n        # Create consistent pseudonym\n        user_hash = hashlib.sha256(\n            user.get(\"username\", \"\").encode()\n        ).hexdigest()[:12]\n\n        return {\n            \"id_hash\": user_hash,\n            \"follower_range\": self._bucket_followers(user.get(\"followers\", 0)),\n            \"account_age_category\": self._categorize_age(user.get(\"created_at\")),\n            \"verified\": user.get(\"verified\", False),\n            \"bio_length\": len(user.get(\"bio\", \"\") or \"\"),\n            # Remove: username, name, profile_image, etc.\n        }\n\n    def _bucket_followers(self, count: int) -&gt; str:\n        \"\"\"Bucket follower count for privacy.\"\"\"\n        if count &lt; 100:\n            return \"&lt;100\"\n        elif count &lt; 1000:\n            return \"100-999\"\n        elif count &lt; 10000:\n            return \"1K-10K\"\n        elif count &lt; 100000:\n            return \"10K-100K\"\n        else:\n            return \"&gt;100K\"\n\n    def _categorize_age(self, created_at) -&gt; str:\n        \"\"\"Categorize account age.\"\"\"\n        if not created_at:\n            return \"unknown\"\n\n        age_days = (datetime.now() - created_at).days\n\n        if age_days &lt; 90:\n            return \"new (&lt;3mo)\"\n        elif age_days &lt; 365:\n            return \"recent (3mo-1y)\"\n        elif age_days &lt; 365 * 3:\n            return \"established (1-3y)\"\n        else:\n            return \"veteran (&gt;3y)\"\n\n    async def collect_conversation_network(\n        self,\n        seed_tweets: list[str],\n        depth: int = 2,\n        anonymize: bool = True\n    ) -&gt; dict:\n        \"\"\"Collect conversation network from seed tweets.\"\"\"\n        network = {\n            \"nodes\": [],\n            \"edges\": [],\n            \"tweets\": []\n        }\n\n        visited = set()\n        queue = [(url, 0) for url in seed_tweets]\n\n        while queue:\n            tweet_url, current_depth = queue.pop(0)\n\n            if tweet_url in visited or current_depth &gt; depth:\n                continue\n\n            visited.add(tweet_url)\n\n            try:\n                replies = await self.x.scrape.replies(tweet_url, limit=100)\n\n                for reply in replies.items:\n                    # Process tweet\n                    tweet_data = {\n                        \"id_hash\": hashlib.sha256(reply.id.encode()).hexdigest()[:12],\n                        \"text\": reply.text if not anonymize else self._anonymize_text(reply.text),\n                        \"created_at\": reply.created_at.isoformat(),\n                        \"engagement\": {\n                            \"likes\": reply.likes or 0,\n                            \"retweets\": reply.retweets or 0,\n                            \"replies\": reply.replies or 0\n                        }\n                    }\n\n                    if anonymize:\n                        tweet_data[\"author\"] = self._anonymize_user(asdict(reply.author))\n\n                    network[\"tweets\"].append(tweet_data)\n\n                    # Add to queue for further exploration\n                    if current_depth &lt; depth:\n                        queue.append((reply.url, current_depth + 1))\n\n            except Exception as e:\n                self._log_error(\"collect_conversation\", str(e))\n\n        self._log_collection(\"conversation_network\", {\n            \"seed_tweets\": len(seed_tweets),\n            \"depth\": depth\n        }, len(network[\"tweets\"]))\n\n        return network\n\n    def _anonymize_text(self, text: str) -&gt; str:\n        \"\"\"Basic text anonymization.\"\"\"\n        import re\n\n        # Remove @mentions\n        text = re.sub(r'@\\w+', '@[USER]', text)\n\n        # Remove URLs\n        text = re.sub(r'https?://\\S+', '[URL]', text)\n\n        # Remove potential names (basic)\n        # Note: More sophisticated NER would be needed for production\n\n        return text\n\n    async def collect_hashtag_sample(\n        self,\n        hashtag: str,\n        sample_size: int,\n        sampling_method: str = \"chronological\"\n    ) -&gt; list[dict]:\n        \"\"\"Collect stratified sample from hashtag.\"\"\"\n        tweets = await self.x.scrape.hashtag(\n            f\"#{hashtag}\",\n            limit=sample_size * 2  # Oversample for filtering\n        )\n\n        # Apply sampling method\n        if sampling_method == \"chronological\":\n            sample = tweets.items[:sample_size]\n        elif sampling_method == \"random\":\n            import random\n            sample = random.sample(\n                tweets.items,\n                min(sample_size, len(tweets.items))\n            )\n        elif sampling_method == \"stratified_engagement\":\n            # Stratify by engagement level\n            sorted_tweets = sorted(\n                tweets.items,\n                key=lambda t: (t.likes or 0) + (t.retweets or 0)\n            )\n\n            # Take from each quartile\n            quartile_size = len(sorted_tweets) // 4\n            sample = []\n            for i in range(4):\n                start = i * quartile_size\n                end = start + (sample_size // 4)\n                sample.extend(sorted_tweets[start:end])\n        else:\n            sample = tweets.items[:sample_size]\n\n        self._log_collection(\"hashtag_sample\", {\n            \"hashtag\": hashtag,\n            \"method\": sampling_method\n        }, len(sample))\n\n        return [self._process_tweet_for_research(t) for t in sample]\n\n    def _process_tweet_for_research(self, tweet) -&gt; dict:\n        \"\"\"Process tweet for research use.\"\"\"\n        return {\n            \"id_hash\": hashlib.sha256(tweet.id.encode()).hexdigest()[:12],\n            \"text\": tweet.text,\n            \"created_at\": tweet.created_at.isoformat() if tweet.created_at else None,\n            \"likes\": tweet.likes or 0,\n            \"retweets\": tweet.retweets or 0,\n            \"replies\": tweet.replies or 0,\n            \"has_media\": bool(getattr(tweet, 'media', None)),\n            \"has_url\": 'http' in tweet.text,\n            \"has_hashtags\": '#' in tweet.text,\n            \"tweet_length\": len(tweet.text),\n            \"author_anonymized\": self._anonymize_user(\n                asdict(tweet.author) if hasattr(tweet.author, '__dict__') else {}\n            )\n        }\n\n    def _log_collection(self, operation: str, params: dict, count: int):\n        \"\"\"Log data collection operation.\"\"\"\n        log = DataCollectionLog(\n            timestamp=datetime.now(),\n            operation=operation,\n            parameters=params,\n            records_collected=count,\n            errors=[],\n            checksum=hashlib.sha256(\n                json.dumps(params, sort_keys=True).encode()\n            ).hexdigest()[:16]\n        )\n        self.collection_logs.append(log)\n\n    def _log_error(self, operation: str, error: str):\n        \"\"\"Log collection error.\"\"\"\n        if self.collection_logs:\n            self.collection_logs[-1].errors.append(error)\n\n    def export_dataset(self, data: list, filename: str):\n        \"\"\"Export dataset with metadata.\"\"\"\n        export = {\n            \"metadata\": asdict(self.metadata),\n            \"collection_logs\": [\n                {\n                    \"timestamp\": log.timestamp.isoformat(),\n                    \"operation\": log.operation,\n                    \"parameters\": log.parameters,\n                    \"records_collected\": log.records_collected,\n                    \"errors\": log.errors,\n                    \"checksum\": log.checksum\n                }\n                for log in self.collection_logs\n            ],\n            \"data\": data\n        }\n\n        filepath = self.data_dir / \"processed\" / filename\n        with open(filepath, 'w') as f:\n            json.dump(export, f, indent=2, default=str)\n\n        print(f\"\u2705 Dataset exported to {filepath}\")\n        print(f\"   Records: {len(data)}\")\n        print(f\"   Checksum: {hashlib.sha256(json.dumps(data, sort_keys=True, default=str).encode()).hexdigest()[:16]}\")\n\n\n# Usage\nasync def run_research():\n    from xeepy import Xeepy\n\n    metadata = ResearchMetadata(\n        study_id=\"twitter_discourse_2024\",\n        researcher=\"Dr. Jane Smith\",\n        institution=\"University Example\",\n        irb_approval=\"IRB-2024-001\",\n        research_question=\"How do conversation dynamics differ across topic communities?\",\n        data_collection_start=datetime.now(),\n        data_collection_end=None,\n        sampling_method=\"stratified_engagement\",\n        ethical_considerations=[\n            \"Data anonymized before analysis\",\n            \"No individual identification attempted\",\n            \"Public tweets only\"\n        ],\n        data_processing_notes=[]\n    )\n\n    async with Xeepy() as x:\n        researcher = AcademicResearcher(x, metadata)\n\n        # Collect data\n        sample = await researcher.collect_hashtag_sample(\n            \"academictwitter\",\n            sample_size=500,\n            sampling_method=\"stratified_engagement\"\n        )\n\n        # Export\n        researcher.export_dataset(sample, \"hashtag_sample.json\")\n\nasyncio.run(run_research())\n</code></pre>"},{"location":"cookbook/research/#discourse-analysis-toolkit","title":"\ud83d\udcca Discourse Analysis Toolkit","text":"<p>Analyze public discourse and narratives.</p> <pre><code>\"\"\"\nDiscourse Analysis Toolkit\nStudy narratives, framing, and public discourse on Twitter\n\"\"\"\n\nimport asyncio\nfrom collections import Counter, defaultdict\nfrom datetime import datetime\nimport re\n\nclass DiscourseAnalyzer:\n    \"\"\"Analyze Twitter discourse patterns.\"\"\"\n\n    def __init__(self, xeepy):\n        self.x = xeepy\n\n    async def analyze_framing(\n        self,\n        topic: str,\n        sample_size: int = 500\n    ) -&gt; dict:\n        \"\"\"Analyze how a topic is framed in discourse.\"\"\"\n        tweets = await self.x.scrape.search(topic, limit=sample_size)\n\n        analysis = {\n            \"topic\": topic,\n            \"sample_size\": len(tweets.items),\n            \"frames\": defaultdict(list),\n            \"sentiment_distribution\": {\"positive\": 0, \"negative\": 0, \"neutral\": 0},\n            \"key_terms\": Counter(),\n            \"hashtag_clusters\": defaultdict(int),\n            \"source_types\": Counter()\n        }\n\n        for tweet in tweets.items:\n            # Extract frames\n            frames = self._identify_frames(tweet.text)\n            for frame in frames:\n                analysis[\"frames\"][frame].append(tweet.text[:100])\n\n            # Sentiment\n            sentiment = self._simple_sentiment(tweet.text)\n            analysis[\"sentiment_distribution\"][sentiment] += 1\n\n            # Key terms (bigrams)\n            terms = self._extract_key_terms(tweet.text)\n            analysis[\"key_terms\"].update(terms)\n\n            # Hashtag clusters\n            hashtags = re.findall(r'#\\w+', tweet.text.lower())\n            for tag in hashtags:\n                analysis[\"hashtag_clusters\"][tag] += 1\n\n            # Source type\n            source = self._categorize_source(tweet.author)\n            analysis[\"source_types\"][source] += 1\n\n        # Convert to serializable\n        analysis[\"frames\"] = dict(analysis[\"frames\"])\n        analysis[\"key_terms\"] = dict(analysis[\"key_terms\"].most_common(50))\n        analysis[\"hashtag_clusters\"] = dict(analysis[\"hashtag_clusters\"])\n        analysis[\"source_types\"] = dict(analysis[\"source_types\"])\n\n        return analysis\n\n    def _identify_frames(self, text: str) -&gt; list[str]:\n        \"\"\"Identify framing patterns in text.\"\"\"\n        frames = []\n        text_lower = text.lower()\n\n        frame_patterns = {\n            \"economic\": [\"cost\", \"price\", \"money\", \"economy\", \"jobs\", \"business\", \"market\"],\n            \"moral\": [\"right\", \"wrong\", \"should\", \"must\", \"duty\", \"responsible\"],\n            \"conflict\": [\"fight\", \"battle\", \"war\", \"attack\", \"defend\", \"opposition\"],\n            \"human_interest\": [\"story\", \"people\", \"family\", \"children\", \"personal\"],\n            \"scientific\": [\"study\", \"research\", \"data\", \"evidence\", \"expert\", \"science\"],\n            \"political\": [\"government\", \"policy\", \"politician\", \"vote\", \"election\"],\n            \"security\": [\"safe\", \"danger\", \"threat\", \"protect\", \"risk\", \"security\"]\n        }\n\n        for frame, keywords in frame_patterns.items():\n            if any(kw in text_lower for kw in keywords):\n                frames.append(frame)\n\n        return frames if frames else [\"unclassified\"]\n\n    def _simple_sentiment(self, text: str) -&gt; str:\n        \"\"\"Simple sentiment classification.\"\"\"\n        positive = [\"good\", \"great\", \"excellent\", \"amazing\", \"love\", \"best\", \"happy\", \"wonderful\"]\n        negative = [\"bad\", \"terrible\", \"awful\", \"hate\", \"worst\", \"sad\", \"angry\", \"disappointed\"]\n\n        text_lower = text.lower()\n        pos = sum(1 for w in positive if w in text_lower)\n        neg = sum(1 for w in negative if w in text_lower)\n\n        if pos &gt; neg:\n            return \"positive\"\n        elif neg &gt; pos:\n            return \"negative\"\n        return \"neutral\"\n\n    def _extract_key_terms(self, text: str) -&gt; list[str]:\n        \"\"\"Extract meaningful terms.\"\"\"\n        # Remove URLs, mentions\n        text = re.sub(r'https?://\\S+', '', text)\n        text = re.sub(r'@\\w+', '', text)\n        text = re.sub(r'#', '', text)\n\n        # Simple tokenization\n        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', text.lower())\n\n        # Remove common stopwords\n        stopwords = {'the', 'and', 'for', 'that', 'this', 'with', 'are', 'was', 'but', 'have', 'has'}\n        words = [w for w in words if w not in stopwords]\n\n        # Create bigrams\n        bigrams = [f\"{words[i]} {words[i+1]}\" for i in range(len(words)-1)]\n\n        return bigrams\n\n    def _categorize_source(self, author) -&gt; str:\n        \"\"\"Categorize tweet author type.\"\"\"\n        if hasattr(author, 'verified') and author.verified:\n            return \"verified\"\n        elif hasattr(author, 'followers_count'):\n            if author.followers_count &gt; 100000:\n                return \"influencer\"\n            elif author.followers_count &gt; 10000:\n                return \"micro_influencer\"\n            elif author.followers_count &gt; 1000:\n                return \"active_user\"\n        return \"regular_user\"\n\n    async def track_narrative_evolution(\n        self,\n        topic: str,\n        days: int = 7,\n        samples_per_day: int = 100\n    ) -&gt; dict:\n        \"\"\"Track how narratives evolve over time.\"\"\"\n        from datetime import timedelta\n\n        evolution = {\n            \"topic\": topic,\n            \"period_days\": days,\n            \"daily_analysis\": []\n        }\n\n        # Note: Would need date-filtered search\n        # This is a simplified version\n\n        for day_offset in range(days):\n            day_data = {\n                \"day\": day_offset,\n                \"frames\": Counter(),\n                \"sentiment\": Counter(),\n                \"top_terms\": []\n            }\n\n            # In practice, you'd filter by date\n            tweets = await self.x.scrape.search(\n                topic,\n                limit=samples_per_day\n            )\n\n            for tweet in tweets.items:\n                frames = self._identify_frames(tweet.text)\n                day_data[\"frames\"].update(frames)\n                day_data[\"sentiment\"][self._simple_sentiment(tweet.text)] += 1\n\n            day_data[\"frames\"] = dict(day_data[\"frames\"])\n            day_data[\"sentiment\"] = dict(day_data[\"sentiment\"])\n\n            evolution[\"daily_analysis\"].append(day_data)\n\n        return evolution\n\n\n# Usage\nasync def analyze_discourse():\n    from xeepy import Xeepy\n\n    async with Xeepy() as x:\n        analyzer = DiscourseAnalyzer(x)\n\n        # Analyze framing\n        analysis = await analyzer.analyze_framing(\n            \"climate change\",\n            sample_size=500\n        )\n\n        print(\"\ud83d\udcca Discourse Analysis Results\")\n        print(f\"\\nSentiment Distribution:\")\n        for sent, count in analysis[\"sentiment_distribution\"].items():\n            pct = (count / analysis[\"sample_size\"]) * 100\n            print(f\"  {sent}: {pct:.1f}%\")\n\n        print(f\"\\nTop Frames:\")\n        for frame, examples in list(analysis[\"frames\"].items())[:5]:\n            print(f\"  {frame}: {len(examples)} tweets\")\n\n        print(f\"\\nTop Terms:\")\n        for term, count in list(analysis[\"key_terms\"].items())[:10]:\n            print(f\"  '{term}': {count}\")\n\nasyncio.run(analyze_discourse())\n</code></pre>"},{"location":"cookbook/research/#network-analysis","title":"\ud83c\udf10 Network Analysis","text":"<p>Study social network structures and influence.</p> <pre><code>\"\"\"\nSocial Network Analysis\nStudy network structures, influence, and information flow\n\"\"\"\n\nimport asyncio\nfrom collections import defaultdict\nfrom dataclasses import dataclass\nimport json\n\n@dataclass\nclass NetworkNode:\n    \"\"\"Node in social network.\"\"\"\n    id: str\n    label: str\n    followers: int = 0\n    following: int = 0\n    degree: int = 0\n    in_degree: int = 0\n    out_degree: int = 0\n    betweenness: float = 0.0\n    community: int = -1\n\n\nclass NetworkAnalyzer:\n    \"\"\"Analyze Twitter social networks.\"\"\"\n\n    def __init__(self, xeepy):\n        self.x = xeepy\n        self.nodes: dict[str, NetworkNode] = {}\n        self.edges: list[tuple] = []\n\n    async def build_follower_network(\n        self,\n        seed_users: list[str],\n        depth: int = 1,\n        sample_per_user: int = 100\n    ) -&gt; dict:\n        \"\"\"Build follower network from seed users.\"\"\"\n        visited = set()\n        queue = [(user, 0) for user in seed_users]\n\n        while queue:\n            username, current_depth = queue.pop(0)\n\n            if username in visited or current_depth &gt; depth:\n                continue\n\n            visited.add(username)\n\n            try:\n                # Get profile\n                profile = await self.x.scrape.profile(username)\n\n                # Add node\n                self.nodes[username] = NetworkNode(\n                    id=username,\n                    label=profile.name or username,\n                    followers=profile.followers_count,\n                    following=profile.following_count\n                )\n\n                # Get followers\n                followers = await self.x.scrape.followers(\n                    username,\n                    limit=sample_per_user\n                )\n\n                for follower in followers.items:\n                    # Add edge (follower -&gt; user)\n                    self.edges.append((follower.username, username))\n\n                    # Add follower node if not exists\n                    if follower.username not in self.nodes:\n                        self.nodes[follower.username] = NetworkNode(\n                            id=follower.username,\n                            label=follower.name or follower.username,\n                            followers=getattr(follower, 'followers_count', 0),\n                            following=getattr(follower, 'following_count', 0)\n                        )\n\n                    # Queue for expansion\n                    if current_depth &lt; depth:\n                        queue.append((follower.username, current_depth + 1))\n\n            except Exception as e:\n                print(f\"Error processing @{username}: {e}\")\n\n        # Calculate network metrics\n        self._calculate_degrees()\n\n        return {\n            \"nodes\": [vars(n) for n in self.nodes.values()],\n            \"edges\": [{\"source\": e[0], \"target\": e[1]} for e in self.edges],\n            \"stats\": self._network_stats()\n        }\n\n    def _calculate_degrees(self):\n        \"\"\"Calculate node degrees.\"\"\"\n        in_degrees = defaultdict(int)\n        out_degrees = defaultdict(int)\n\n        for source, target in self.edges:\n            out_degrees[source] += 1\n            in_degrees[target] += 1\n\n        for username, node in self.nodes.items():\n            node.in_degree = in_degrees.get(username, 0)\n            node.out_degree = out_degrees.get(username, 0)\n            node.degree = node.in_degree + node.out_degree\n\n    def _network_stats(self) -&gt; dict:\n        \"\"\"Calculate network-level statistics.\"\"\"\n        if not self.nodes:\n            return {}\n\n        degrees = [n.degree for n in self.nodes.values()]\n\n        return {\n            \"num_nodes\": len(self.nodes),\n            \"num_edges\": len(self.edges),\n            \"avg_degree\": sum(degrees) / len(degrees),\n            \"max_degree\": max(degrees),\n            \"density\": len(self.edges) / (len(self.nodes) * (len(self.nodes) - 1)) if len(self.nodes) &gt; 1 else 0\n        }\n\n    def find_key_influencers(self, top_n: int = 10) -&gt; list[dict]:\n        \"\"\"Find most influential nodes.\"\"\"\n        # Combine metrics for influence score\n        scored = []\n        for username, node in self.nodes.items():\n            score = (\n                node.in_degree * 0.4 +\n                (node.followers / 1000) * 0.3 +\n                node.degree * 0.3\n            )\n            scored.append({\n                \"username\": username,\n                \"influence_score\": score,\n                \"in_degree\": node.in_degree,\n                \"followers\": node.followers\n            })\n\n        return sorted(scored, key=lambda x: x[\"influence_score\"], reverse=True)[:top_n]\n\n    def find_bridges(self) -&gt; list[str]:\n        \"\"\"Find bridge nodes connecting communities.\"\"\"\n        # Simple heuristic: nodes with high degree but connecting to many unique nodes\n        bridges = []\n\n        for username, node in self.nodes.items():\n            if node.out_degree &gt; 5 and node.in_degree &gt; 5:\n                # Check if connects to diverse set\n                connections = set()\n                for src, tgt in self.edges:\n                    if src == username:\n                        connections.add(tgt)\n                    if tgt == username:\n                        connections.add(src)\n\n                if len(connections) &gt; node.degree * 0.7:\n                    bridges.append(username)\n\n        return bridges\n\n    def export_for_gephi(self, filename: str):\n        \"\"\"Export network for Gephi visualization.\"\"\"\n        # Export nodes\n        nodes_csv = \"Id,Label,Followers,Following,Degree\\n\"\n        for username, node in self.nodes.items():\n            nodes_csv += f\"{username},{node.label},{node.followers},{node.following},{node.degree}\\n\"\n\n        with open(f\"{filename}_nodes.csv\", \"w\") as f:\n            f.write(nodes_csv)\n\n        # Export edges\n        edges_csv = \"Source,Target\\n\"\n        for source, target in self.edges:\n            edges_csv += f\"{source},{target}\\n\"\n\n        with open(f\"{filename}_edges.csv\", \"w\") as f:\n            f.write(edges_csv)\n\n        print(f\"\u2705 Exported to {filename}_nodes.csv and {filename}_edges.csv\")\n\n\n# Usage\nasync def analyze_network():\n    from xeepy import Xeepy\n\n    async with Xeepy() as x:\n        analyzer = NetworkAnalyzer(x)\n\n        # Build network\n        network = await analyzer.build_follower_network(\n            [\"user1\", \"user2\"],\n            depth=1,\n            sample_per_user=50\n        )\n\n        print(f\"\ud83d\udcca Network Statistics:\")\n        print(f\"   Nodes: {network['stats']['num_nodes']}\")\n        print(f\"   Edges: {network['stats']['num_edges']}\")\n        print(f\"   Density: {network['stats']['density']:.4f}\")\n\n        # Find influencers\n        influencers = analyzer.find_key_influencers(10)\n        print(f\"\\n\ud83c\udf1f Top Influencers:\")\n        for inf in influencers:\n            print(f\"   @{inf['username']}: score={inf['influence_score']:.2f}\")\n\n        # Export for visualization\n        analyzer.export_for_gephi(\"twitter_network\")\n\nasyncio.run(analyze_network())\n</code></pre>"},{"location":"cookbook/research/#trend-detection-analysis","title":"\ud83d\udcc8 Trend Detection &amp; Analysis","text":"<p>Detect and analyze emerging trends.</p> <pre><code>\"\"\"\nTrend Detection System\nIdentify and analyze emerging trends in real-time\n\"\"\"\n\nimport asyncio\nfrom collections import Counter, defaultdict\nfrom datetime import datetime, timedelta\nimport re\n\nclass TrendDetector:\n    \"\"\"Detect emerging trends on Twitter.\"\"\"\n\n    def __init__(self, xeepy):\n        self.x = xeepy\n        self.baseline_terms = Counter()\n        self.current_terms = Counter()\n        self.trend_history = []\n\n    async def capture_baseline(\n        self,\n        topics: list[str],\n        sample_size: int = 500\n    ):\n        \"\"\"Capture baseline term frequencies.\"\"\"\n        all_terms = Counter()\n\n        for topic in topics:\n            tweets = await self.x.scrape.search(topic, limit=sample_size)\n\n            for tweet in tweets.items:\n                terms = self._extract_terms(tweet.text)\n                all_terms.update(terms)\n\n        self.baseline_terms = all_terms\n        print(f\"\ud83d\udcca Baseline captured: {len(all_terms)} unique terms\")\n\n    async def detect_emerging(\n        self,\n        topics: list[str],\n        sample_size: int = 500,\n        threshold: float = 2.0\n    ) -&gt; list[dict]:\n        \"\"\"Detect terms emerging above baseline.\"\"\"\n        current = Counter()\n\n        for topic in topics:\n            tweets = await self.x.scrape.search(topic, limit=sample_size)\n\n            for tweet in tweets.items:\n                terms = self._extract_terms(tweet.text)\n                current.update(terms)\n\n        self.current_terms = current\n\n        # Find emerging terms\n        emerging = []\n        for term, count in current.most_common(100):\n            baseline_count = self.baseline_terms.get(term, 1)\n            ratio = count / baseline_count\n\n            if ratio &gt;= threshold and count &gt;= 5:\n                emerging.append({\n                    \"term\": term,\n                    \"current_count\": count,\n                    \"baseline_count\": baseline_count,\n                    \"emergence_ratio\": ratio\n                })\n\n        # Sort by emergence ratio\n        emerging.sort(key=lambda x: x[\"emergence_ratio\"], reverse=True)\n\n        return emerging[:20]\n\n    def _extract_terms(self, text: str) -&gt; list[str]:\n        \"\"\"Extract meaningful terms from text.\"\"\"\n        # Clean\n        text = re.sub(r'https?://\\S+', '', text)\n        text = re.sub(r'@\\w+', '', text)\n\n        # Extract hashtags\n        hashtags = re.findall(r'#(\\w+)', text.lower())\n\n        # Extract words (3+ chars)\n        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', text.lower())\n\n        # Simple stopword filter\n        stopwords = {'the', 'and', 'for', 'that', 'this', 'with', 'are', 'was', 'but', 'have', 'has', 'you', 'your'}\n        words = [w for w in words if w not in stopwords]\n\n        # Combine\n        return hashtags + words\n\n    async def analyze_trend(self, trend_term: str) -&gt; dict:\n        \"\"\"Deep analysis of a specific trend.\"\"\"\n        tweets = await self.x.scrape.search(trend_term, limit=200)\n\n        analysis = {\n            \"term\": trend_term,\n            \"sample_size\": len(tweets.items),\n            \"sentiment\": {\"positive\": 0, \"negative\": 0, \"neutral\": 0},\n            \"co_occurring_hashtags\": Counter(),\n            \"top_users\": Counter(),\n            \"engagement_stats\": {\n                \"total_likes\": 0,\n                \"total_retweets\": 0,\n                \"avg_engagement\": 0\n            },\n            \"sample_tweets\": []\n        }\n\n        total_engagement = 0\n\n        for tweet in tweets.items:\n            # Sentiment\n            sentiment = self._simple_sentiment(tweet.text)\n            analysis[\"sentiment\"][sentiment] += 1\n\n            # Co-occurring hashtags\n            hashtags = re.findall(r'#\\w+', tweet.text.lower())\n            for tag in hashtags:\n                if tag.lower() != f\"#{trend_term.lower()}\":\n                    analysis[\"co_occurring_hashtags\"][tag] += 1\n\n            # Top users\n            analysis[\"top_users\"][tweet.author.username] += 1\n\n            # Engagement\n            engagement = (tweet.likes or 0) + (tweet.retweets or 0)\n            total_engagement += engagement\n            analysis[\"engagement_stats\"][\"total_likes\"] += tweet.likes or 0\n            analysis[\"engagement_stats\"][\"total_retweets\"] += tweet.retweets or 0\n\n        if tweets.items:\n            analysis[\"engagement_stats\"][\"avg_engagement\"] = total_engagement / len(tweets.items)\n\n        # Get top examples\n        sorted_tweets = sorted(\n            tweets.items,\n            key=lambda t: (t.likes or 0) + (t.retweets or 0),\n            reverse=True\n        )\n        analysis[\"sample_tweets\"] = [\n            {\"text\": t.text[:200], \"engagement\": (t.likes or 0) + (t.retweets or 0)}\n            for t in sorted_tweets[:5]\n        ]\n\n        # Convert counters\n        analysis[\"co_occurring_hashtags\"] = dict(\n            analysis[\"co_occurring_hashtags\"].most_common(10)\n        )\n        analysis[\"top_users\"] = dict(\n            analysis[\"top_users\"].most_common(10)\n        )\n\n        return analysis\n\n    def _simple_sentiment(self, text: str) -&gt; str:\n        \"\"\"Simple sentiment classification.\"\"\"\n        positive = [\"good\", \"great\", \"love\", \"amazing\", \"best\", \"happy\", \"excited\"]\n        negative = [\"bad\", \"terrible\", \"hate\", \"worst\", \"sad\", \"angry\", \"disappointed\"]\n\n        text_lower = text.lower()\n        pos = sum(1 for w in positive if w in text_lower)\n        neg = sum(1 for w in negative if w in text_lower)\n\n        if pos &gt; neg:\n            return \"positive\"\n        elif neg &gt; pos:\n            return \"negative\"\n        return \"neutral\"\n\n\n# Usage\nasync def detect_trends():\n    from xeepy import Xeepy\n\n    async with Xeepy() as x:\n        detector = TrendDetector(x)\n\n        # Capture baseline\n        topics = [\"technology\", \"AI\", \"startup\"]\n        await detector.capture_baseline(topics, sample_size=200)\n\n        # Wait and detect emerging\n        await asyncio.sleep(5)\n\n        emerging = await detector.detect_emerging(topics, threshold=1.5)\n\n        print(\"\ud83d\udcc8 Emerging Trends:\")\n        for trend in emerging[:10]:\n            print(f\"   '{trend['term']}': {trend['emergence_ratio']:.1f}x increase\")\n\n        # Analyze top trend\n        if emerging:\n            analysis = await detector.analyze_trend(emerging[0][\"term\"])\n            print(f\"\\n\ud83d\udd0d Trend Analysis: {analysis['term']}\")\n            print(f\"   Sentiment: {analysis['sentiment']}\")\n            print(f\"   Avg Engagement: {analysis['engagement_stats']['avg_engagement']:.1f}\")\n\nasyncio.run(detect_trends())\n</code></pre>"},{"location":"cookbook/research/#next-steps","title":"Next Steps","text":"<ul> <li>Business Recipes - Business applications</li> <li>Data Science Recipes - ML and analytics</li> <li>Academic Resources - Community support</li> </ul>"},{"location":"cookbook/research/academic-scraping/","title":"Academic Research Methodology","text":"<p>A comprehensive guide for conducting ethical academic research using social media data, with proper methodology, IRB compliance, and reproducibility standards.</p>"},{"location":"cookbook/research/academic-scraping/#overview","title":"Overview","text":"<p>This guide covers academic research best practices including:</p> <ul> <li>IRB considerations - Ethical review requirements</li> <li>Sampling strategies - Statistical sampling methods</li> <li>Rate limit compliance - Responsible data collection</li> <li>Data anonymization - Privacy protection pipeline</li> <li>Reproducibility - Standards for replication</li> <li>Publication requirements - Citation and disclosure</li> </ul>"},{"location":"cookbook/research/academic-scraping/#ethical-framework","title":"Ethical Framework","text":""},{"location":"cookbook/research/academic-scraping/#irb-considerations","title":"IRB Considerations","text":"<p>Institutional Review Board</p> <p>Most academic institutions require IRB approval for research involving human subjects, including social media data. Consult your IRB before beginning data collection.</p> <pre><code># research_ethics.py\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import Optional\n\n@dataclass\nclass ResearchProtocol:\n    \"\"\"Document your research protocol for IRB submission.\"\"\"\n\n    # Study Information\n    title: str\n    principal_investigator: str\n    institution: str\n    irb_protocol_number: Optional[str] = None\n    approval_date: Optional[datetime] = None\n\n    # Research Design\n    research_questions: list[str] = None\n    methodology: str = \"\"\n\n    # Data Collection\n    data_types: list[str] = None  # tweets, profiles, etc.\n    estimated_sample_size: int = 0\n    collection_period: str = \"\"\n\n    # Privacy Protections\n    anonymization_method: str = \"\"\n    data_storage: str = \"\"\n    data_retention_period: str = \"\"\n\n    # Risk Assessment\n    risks_to_subjects: list[str] = None\n    risk_mitigation: list[str] = None\n\n    def generate_irb_summary(self) -&gt; str:\n        \"\"\"Generate summary for IRB application.\"\"\"\n        return f\"\"\"\n# IRB Protocol Summary\n\n## Study Title\n{self.title}\n\n## Principal Investigator\n{self.principal_investigator}\nInstitution: {self.institution}\n\n## Research Questions\n{chr(10).join(f\"- {q}\" for q in (self.research_questions or []))}\n\n## Methodology\n{self.methodology}\n\n## Data Collection\n- Data Types: {', '.join(self.data_types or [])}\n- Sample Size: {self.estimated_sample_size:,}\n- Collection Period: {self.collection_period}\n\n## Privacy Protections\n- Anonymization: {self.anonymization_method}\n- Storage: {self.data_storage}\n- Retention: {self.data_retention_period}\n\n## Risk Assessment\nRisks:\n{chr(10).join(f\"- {r}\" for r in (self.risks_to_subjects or []))}\n\nMitigation:\n{chr(10).join(f\"- {m}\" for m in (self.risk_mitigation or []))}\n\"\"\"\n</code></pre>"},{"location":"cookbook/research/academic-scraping/#ethical-guidelines","title":"Ethical Guidelines","text":"<pre><code># ethical_guidelines.py\n\nclass EthicalGuidelines:\n    \"\"\"Guidelines for ethical social media research.\"\"\"\n\n    PRINCIPLES = [\n        \"Respect for persons - Protect autonomy and vulnerable populations\",\n        \"Beneficence - Maximize benefits, minimize harm\",\n        \"Justice - Fair distribution of research benefits and burdens\",\n    ]\n\n    REQUIREMENTS = {\n        \"public_data\": {\n            \"description\": \"Publicly posted content\",\n            \"irb_required\": \"Usually - depends on research questions\",\n            \"consent_required\": \"Generally not for public posts\",\n            \"restrictions\": [\n                \"No contact with users without consent\",\n                \"Cannot use for identifying individuals\",\n                \"Must anonymize before publication\"\n            ]\n        },\n        \"private_data\": {\n            \"description\": \"DMs, protected accounts, private groups\",\n            \"irb_required\": \"Always\",\n            \"consent_required\": \"Always - explicit opt-in\",\n            \"restrictions\": [\n                \"Requires explicit consent from all parties\",\n                \"Must explain data use clearly\",\n                \"Right to withdraw at any time\"\n            ]\n        }\n    }\n\n    @staticmethod\n    def assess_risk(research_type: str) -&gt; dict:\n        \"\"\"Assess risk level for research type.\"\"\"\n\n        risk_levels = {\n            \"aggregate_analysis\": {\n                \"level\": \"minimal\",\n                \"justification\": \"No individual identification possible\",\n                \"recommended_safeguards\": [\n                    \"Aggregate results only\",\n                    \"Minimum cell size of 10\",\n                    \"No demographic details that could identify\"\n                ]\n            },\n            \"content_analysis\": {\n                \"level\": \"low\",\n                \"justification\": \"Public content, but quotes could identify\",\n                \"recommended_safeguards\": [\n                    \"Paraphrase instead of direct quotes\",\n                    \"Remove usernames and identifying info\",\n                    \"Do not include profile images\"\n                ]\n            },\n            \"network_analysis\": {\n                \"level\": \"moderate\",\n                \"justification\": \"Network structure could reveal identity\",\n                \"recommended_safeguards\": [\n                    \"Aggregate network metrics only\",\n                    \"Do not publish individual connections\",\n                    \"Apply k-anonymity to network data\"\n                ]\n            },\n            \"longitudinal_tracking\": {\n                \"level\": \"elevated\",\n                \"justification\": \"Tracking individuals over time\",\n                \"recommended_safeguards\": [\n                    \"Strong justification required\",\n                    \"Robust anonymization\",\n                    \"Secure data storage with audit logs\"\n                ]\n            }\n        }\n\n        return risk_levels.get(research_type, {\n            \"level\": \"unknown\",\n            \"justification\": \"Consult IRB\",\n            \"recommended_safeguards\": [\"Full IRB review recommended\"]\n        })\n</code></pre>"},{"location":"cookbook/research/academic-scraping/#sampling-strategies","title":"Sampling Strategies","text":""},{"location":"cookbook/research/academic-scraping/#statistical-sampling-methods","title":"Statistical Sampling Methods","text":"<pre><code># sampling_strategies.py\nimport random\nfrom datetime import datetime, timedelta\nfrom typing import Optional, Generator\nimport hashlib\n\nclass SamplingStrategy:\n    \"\"\"Implement various sampling strategies for research.\"\"\"\n\n    def __init__(self, seed: int = None):\n        \"\"\"Initialize with optional random seed for reproducibility.\"\"\"\n        self.seed = seed or int(datetime.now().timestamp())\n        random.seed(self.seed)\n\n    def simple_random_sample(\n        self,\n        population: list,\n        sample_size: int\n    ) -&gt; list:\n        \"\"\"Simple random sampling.\"\"\"\n\n        if sample_size &gt;= len(population):\n            return population\n\n        return random.sample(population, sample_size)\n\n    def stratified_sample(\n        self,\n        population: list,\n        strata_key: callable,\n        sample_size: int,\n        proportional: bool = True\n    ) -&gt; list:\n        \"\"\"Stratified sampling by a grouping key.\"\"\"\n\n        # Group by strata\n        strata = {}\n        for item in population:\n            key = strata_key(item)\n            if key not in strata:\n                strata[key] = []\n            strata[key].append(item)\n\n        if proportional:\n            # Sample proportionally to strata size\n            sample = []\n            for key, items in strata.items():\n                stratum_size = int(len(items) / len(population) * sample_size)\n                stratum_size = max(1, stratum_size)  # At least 1 per stratum\n                sample.extend(random.sample(items, min(stratum_size, len(items))))\n        else:\n            # Equal sample from each stratum\n            per_stratum = sample_size // len(strata)\n            sample = []\n            for items in strata.values():\n                sample.extend(random.sample(items, min(per_stratum, len(items))))\n\n        return sample\n\n    def systematic_sample(\n        self,\n        population: list,\n        sample_size: int\n    ) -&gt; list:\n        \"\"\"Systematic sampling (every nth item).\"\"\"\n\n        n = len(population) // sample_size\n        start = random.randint(0, n - 1)\n\n        return [population[i] for i in range(start, len(population), n)]\n\n    def time_stratified_sample(\n        self,\n        items: list,\n        time_key: callable,\n        sample_per_period: int,\n        period: str = \"day\"  # hour, day, week, month\n    ) -&gt; list:\n        \"\"\"Sample stratified by time period.\"\"\"\n\n        # Group by time period\n        def get_period(dt: datetime) -&gt; str:\n            if period == \"hour\":\n                return dt.strftime(\"%Y-%m-%d-%H\")\n            elif period == \"day\":\n                return dt.strftime(\"%Y-%m-%d\")\n            elif period == \"week\":\n                return dt.strftime(\"%Y-W%W\")\n            elif period == \"month\":\n                return dt.strftime(\"%Y-%m\")\n            return dt.strftime(\"%Y-%m-%d\")\n\n        periods = {}\n        for item in items:\n            p = get_period(time_key(item))\n            if p not in periods:\n                periods[p] = []\n            periods[p].append(item)\n\n        # Sample from each period\n        sample = []\n        for period_items in periods.values():\n            n = min(sample_per_period, len(period_items))\n            sample.extend(random.sample(period_items, n))\n\n        return sample\n\n    def quota_sample(\n        self,\n        population: list,\n        quotas: dict[str, int],\n        group_key: callable\n    ) -&gt; list:\n        \"\"\"Quota sampling based on predefined quotas.\"\"\"\n\n        sample = []\n        remaining_quotas = quotas.copy()\n\n        # Shuffle for randomness\n        shuffled = population.copy()\n        random.shuffle(shuffled)\n\n        for item in shuffled:\n            group = group_key(item)\n\n            if group in remaining_quotas and remaining_quotas[group] &gt; 0:\n                sample.append(item)\n                remaining_quotas[group] -= 1\n\n            # Check if all quotas filled\n            if all(q &lt;= 0 for q in remaining_quotas.values()):\n                break\n\n        return sample\n</code></pre>"},{"location":"cookbook/research/academic-scraping/#reproducible-sampling","title":"Reproducible Sampling","text":"<pre><code># reproducible_sampling.py\n\nclass ReproducibleSampler:\n    \"\"\"Sampler that ensures reproducibility.\"\"\"\n\n    def __init__(\n        self,\n        seed: int,\n        study_id: str\n    ):\n        self.seed = seed\n        self.study_id = study_id\n        self.sample_log = []\n\n    def deterministic_id_sample(\n        self,\n        ids: list[str],\n        sample_rate: float\n    ) -&gt; list[str]:\n        \"\"\"\n        Sample based on hash of ID.\n        Same IDs always produce same sample.\n        \"\"\"\n\n        sampled = []\n\n        for id in ids:\n            # Create deterministic hash\n            hash_input = f\"{self.study_id}:{self.seed}:{id}\"\n            hash_value = int(hashlib.md5(hash_input.encode()).hexdigest(), 16)\n\n            # Normalize to 0-1\n            normalized = (hash_value % 10000) / 10000\n\n            if normalized &lt; sample_rate:\n                sampled.append(id)\n\n        return sampled\n\n    def log_sample(\n        self,\n        sample_name: str,\n        method: str,\n        population_size: int,\n        sample_size: int,\n        parameters: dict\n    ):\n        \"\"\"Log sampling decisions for reproducibility.\"\"\"\n\n        self.sample_log.append({\n            'timestamp': datetime.now().isoformat(),\n            'sample_name': sample_name,\n            'method': method,\n            'population_size': population_size,\n            'sample_size': sample_size,\n            'parameters': parameters,\n            'seed': self.seed\n        })\n\n    def export_log(self) -&gt; str:\n        \"\"\"Export sampling log for publication.\"\"\"\n\n        log = \"# Sampling Methodology Log\\n\\n\"\n        log += f\"Study ID: {self.study_id}\\n\"\n        log += f\"Random Seed: {self.seed}\\n\\n\"\n\n        for entry in self.sample_log:\n            log += f\"## {entry['sample_name']}\\n\"\n            log += f\"- Method: {entry['method']}\\n\"\n            log += f\"- Population: {entry['population_size']:,}\\n\"\n            log += f\"- Sample: {entry['sample_size']:,}\\n\"\n            log += f\"- Parameters: {entry['parameters']}\\n\\n\"\n\n        return log\n</code></pre>"},{"location":"cookbook/research/academic-scraping/#rate-limit-compliance","title":"Rate Limit Compliance","text":""},{"location":"cookbook/research/academic-scraping/#responsible-collection","title":"Responsible Collection","text":"<pre><code># rate_compliant_collector.py\nimport asyncio\nfrom datetime import datetime, timedelta\nfrom typing import AsyncGenerator\nimport logging\n\nfrom xeepy import Xeepy\n\nclass RateCompliantCollector:\n    \"\"\"Collect data while respecting rate limits.\"\"\"\n\n    def __init__(\n        self,\n        requests_per_15min: int = 50,  # Conservative limit\n        daily_limit: int = 1000,\n        log_file: str = \"collection_log.txt\"\n    ):\n        self.requests_per_15min = requests_per_15min\n        self.daily_limit = daily_limit\n        self.request_times: list[datetime] = []\n        self.daily_count = 0\n        self.last_reset = datetime.now().date()\n\n        # Setup logging\n        logging.basicConfig(\n            filename=log_file,\n            level=logging.INFO,\n            format='%(asctime)s - %(message)s'\n        )\n        self.logger = logging.getLogger(__name__)\n\n    async def wait_if_needed(self):\n        \"\"\"Wait if approaching rate limits.\"\"\"\n\n        now = datetime.now()\n\n        # Reset daily count if new day\n        if now.date() != self.last_reset:\n            self.daily_count = 0\n            self.last_reset = now.date()\n\n        # Check daily limit\n        if self.daily_count &gt;= self.daily_limit:\n            wait_until = datetime.combine(\n                now.date() + timedelta(days=1),\n                datetime.min.time()\n            )\n            wait_seconds = (wait_until - now).total_seconds()\n            self.logger.info(f\"Daily limit reached. Waiting {wait_seconds/3600:.1f} hours\")\n            await asyncio.sleep(wait_seconds)\n            self.daily_count = 0\n\n        # Check 15-minute window\n        cutoff = now - timedelta(minutes=15)\n        self.request_times = [t for t in self.request_times if t &gt; cutoff]\n\n        if len(self.request_times) &gt;= self.requests_per_15min:\n            oldest = min(self.request_times)\n            wait_until = oldest + timedelta(minutes=15)\n            wait_seconds = (wait_until - now).total_seconds()\n\n            if wait_seconds &gt; 0:\n                self.logger.info(f\"Rate limit approaching. Waiting {wait_seconds:.0f}s\")\n                await asyncio.sleep(wait_seconds)\n\n    async def collect_with_backoff(\n        self,\n        collect_func,\n        *args,\n        max_retries: int = 3,\n        **kwargs\n    ):\n        \"\"\"Collect with exponential backoff on failure.\"\"\"\n\n        for attempt in range(max_retries):\n            try:\n                await self.wait_if_needed()\n\n                result = await collect_func(*args, **kwargs)\n\n                self.request_times.append(datetime.now())\n                self.daily_count += 1\n\n                return result\n\n            except Exception as e:\n                wait_time = (2 ** attempt) * 60  # Exponential backoff\n                self.logger.warning(f\"Request failed: {e}. Retrying in {wait_time}s\")\n                await asyncio.sleep(wait_time)\n\n        raise Exception(f\"Failed after {max_retries} retries\")\n\n    async def collect_batch(\n        self,\n        items: list,\n        collect_func,\n        batch_size: int = 10,\n        delay_between: float = 2.0\n    ) -&gt; AsyncGenerator:\n        \"\"\"Collect items in batches with delays.\"\"\"\n\n        total = len(items)\n\n        for i in range(0, total, batch_size):\n            batch = items[i:i + batch_size]\n\n            for item in batch:\n                result = await self.collect_with_backoff(collect_func, item)\n                yield result\n\n                await asyncio.sleep(delay_between)\n\n            # Progress logging\n            progress = min(i + batch_size, total)\n            self.logger.info(f\"Progress: {progress}/{total} ({progress/total*100:.1f}%)\")\n\n    def get_collection_stats(self) -&gt; dict:\n        \"\"\"Get collection statistics.\"\"\"\n\n        return {\n            'daily_count': self.daily_count,\n            'daily_limit': self.daily_limit,\n            'requests_last_15min': len(self.request_times),\n            'rate_limit_15min': self.requests_per_15min\n        }\n</code></pre>"},{"location":"cookbook/research/academic-scraping/#data-anonymization-pipeline","title":"Data Anonymization Pipeline","text":"<pre><code># anonymization_pipeline.py\nimport hashlib\nimport re\nfrom datetime import datetime\nfrom typing import Optional\n\nclass AnonymizationPipeline:\n    \"\"\"Pipeline for anonymizing social media data.\"\"\"\n\n    def __init__(self, salt: str = None):\n        \"\"\"Initialize with optional salt for hashing.\"\"\"\n        self.salt = salt or datetime.now().isoformat()\n        self.id_mapping: dict[str, str] = {}\n\n    def hash_id(self, original_id: str) -&gt; str:\n        \"\"\"Create consistent anonymous ID.\"\"\"\n\n        if original_id in self.id_mapping:\n            return self.id_mapping[original_id]\n\n        hash_input = f\"{self.salt}:{original_id}\"\n        hashed = hashlib.sha256(hash_input.encode()).hexdigest()[:16]\n\n        self.id_mapping[original_id] = hashed\n        return hashed\n\n    def anonymize_text(self, text: str) -&gt; str:\n        \"\"\"Remove identifying information from text.\"\"\"\n\n        # Remove @mentions\n        text = re.sub(r'@\\w+', '@[USER]', text)\n\n        # Remove URLs\n        text = re.sub(r'https?://\\S+', '[URL]', text)\n\n        # Remove email addresses\n        text = re.sub(r'\\S+@\\S+\\.\\S+', '[EMAIL]', text)\n\n        # Remove phone numbers\n        text = re.sub(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', '[PHONE]', text)\n\n        return text\n\n    def anonymize_profile(self, profile: dict) -&gt; dict:\n        \"\"\"Anonymize user profile.\"\"\"\n\n        return {\n            'anonymous_id': self.hash_id(profile.get('user_id', '')),\n            'followers_bucket': self._bucket_followers(profile.get('followers', 0)),\n            'account_age_years': self._calculate_age_years(profile.get('created_at')),\n            'is_verified': profile.get('verified', False),\n            # Do NOT include: username, display_name, bio, location, profile_image\n        }\n\n    def anonymize_tweet(self, tweet: dict) -&gt; dict:\n        \"\"\"Anonymize tweet data.\"\"\"\n\n        return {\n            'anonymous_tweet_id': self.hash_id(tweet.get('tweet_id', '')),\n            'anonymous_author_id': self.hash_id(tweet.get('author_id', '')),\n            'text_anonymized': self.anonymize_text(tweet.get('text', '')),\n            'timestamp_hour': self._round_timestamp(tweet.get('created_at')),\n            'engagement_bucket': self._bucket_engagement(\n                tweet.get('likes', 0) + tweet.get('retweets', 0)\n            ),\n            'has_media': bool(tweet.get('media')),\n            'language': tweet.get('language'),\n            # Do NOT include: exact timestamp, exact engagement counts\n        }\n\n    def _bucket_followers(self, count: int) -&gt; str:\n        \"\"\"Bucket follower counts to prevent identification.\"\"\"\n\n        if count &lt; 100:\n            return \"&lt;100\"\n        elif count &lt; 1000:\n            return \"100-1K\"\n        elif count &lt; 10000:\n            return \"1K-10K\"\n        elif count &lt; 100000:\n            return \"10K-100K\"\n        else:\n            return \"&gt;100K\"\n\n    def _bucket_engagement(self, count: int) -&gt; str:\n        \"\"\"Bucket engagement counts.\"\"\"\n\n        if count &lt; 10:\n            return \"&lt;10\"\n        elif count &lt; 100:\n            return \"10-100\"\n        elif count &lt; 1000:\n            return \"100-1K\"\n        else:\n            return \"&gt;1K\"\n\n    def _round_timestamp(\n        self,\n        dt: Optional[datetime]\n    ) -&gt; Optional[str]:\n        \"\"\"Round timestamp to hour for privacy.\"\"\"\n\n        if dt is None:\n            return None\n\n        return dt.replace(minute=0, second=0, microsecond=0).isoformat()\n\n    def _calculate_age_years(\n        self,\n        created_at: Optional[datetime]\n    ) -&gt; Optional[int]:\n        \"\"\"Calculate account age in years.\"\"\"\n\n        if created_at is None:\n            return None\n\n        age = datetime.now() - created_at\n        return age.days // 365\n\n    def generate_anonymization_report(self) -&gt; str:\n        \"\"\"Generate report of anonymization applied.\"\"\"\n\n        return f\"\"\"\n# Anonymization Report\n\n## Methods Applied\n\n### ID Anonymization\n- Method: SHA-256 hashing with salt\n- Mapping preserved: Yes (for linking)\n- IDs anonymized: {len(self.id_mapping)}\n\n### Text Anonymization\n- @mentions: Replaced with [USER]\n- URLs: Replaced with [URL]\n- Emails: Replaced with [EMAIL]\n- Phone numbers: Replaced with [PHONE]\n\n### Numeric Anonymization\n- Follower counts: Bucketed (5 ranges)\n- Engagement counts: Bucketed (4 ranges)\n- Timestamps: Rounded to hour\n\n### Fields Removed\n- Usernames\n- Display names\n- Bios\n- Locations\n- Profile images\n- Exact counts\n- Exact timestamps\n\n## Compliance\n- GDPR: Personal data pseudonymized\n- Research ethics: Re-identification risk minimized\n\"\"\"\n</code></pre>"},{"location":"cookbook/research/academic-scraping/#reproducibility-guidelines","title":"Reproducibility Guidelines","text":""},{"location":"cookbook/research/academic-scraping/#code-repository-structure","title":"Code Repository Structure","text":"<pre><code>research-project/\n\u251c\u2500\u2500 README.md              # Project overview\n\u251c\u2500\u2500 LICENSE                # Open source license\n\u251c\u2500\u2500 CITATION.cff           # Citation file\n\u251c\u2500\u2500 requirements.txt       # Python dependencies\n\u251c\u2500\u2500 environment.yml        # Conda environment\n\u2502\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/              # Original collected data (DO NOT COMMIT)\n\u2502   \u251c\u2500\u2500 processed/        # Anonymized data\n\u2502   \u2514\u2500\u2500 README.md         # Data documentation\n\u2502\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 collection/       # Data collection scripts\n\u2502   \u251c\u2500\u2500 processing/       # Data processing\n\u2502   \u251c\u2500\u2500 analysis/         # Analysis code\n\u2502   \u2514\u2500\u2500 visualization/    # Visualization code\n\u2502\n\u251c\u2500\u2500 notebooks/\n\u2502   \u251c\u2500\u2500 01_data_exploration.ipynb\n\u2502   \u251c\u2500\u2500 02_analysis.ipynb\n\u2502   \u2514\u2500\u2500 03_visualization.ipynb\n\u2502\n\u251c\u2500\u2500 results/\n\u2502   \u251c\u2500\u2500 figures/          # Generated figures\n\u2502   \u2514\u2500\u2500 tables/           # Generated tables\n\u2502\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 methodology.md    # Detailed methodology\n\u2502   \u251c\u2500\u2500 codebook.md       # Variable definitions\n\u2502   \u2514\u2500\u2500 ethics.md         # Ethics documentation\n\u2502\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 test_analysis.py  # Unit tests\n</code></pre>"},{"location":"cookbook/research/academic-scraping/#documentation-template","title":"Documentation Template","text":"<pre><code># documentation_generator.py\n\nclass ResearchDocumentation:\n    \"\"\"Generate research documentation.\"\"\"\n\n    def generate_methodology(\n        self,\n        collection_params: dict,\n        sampling_log: str,\n        anonymization_report: str\n    ) -&gt; str:\n        \"\"\"Generate methodology documentation.\"\"\"\n\n        return f\"\"\"\n# Methodology Documentation\n\n## Data Collection\n\n### Source\n- Platform: X/Twitter\n- Collection Tool: Xeepy v{collection_params.get('version', 'N/A')}\n- Collection Period: {collection_params.get('period', 'N/A')}\n\n### Parameters\n- Total queries: {collection_params.get('total_queries', 0)}\n- Rate limiting: {collection_params.get('rate_limit', 'N/A')}\n\n## Sampling\n\n{sampling_log}\n\n## Anonymization\n\n{anonymization_report}\n\n## Reproducibility\n\n### Random Seeds\n- Sampling seed: {collection_params.get('seed', 'N/A')}\n\n### Software Versions\n- Python: {collection_params.get('python_version', 'N/A')}\n- Xeepy: {collection_params.get('xeepy_version', 'N/A')}\n\n### Code Availability\n- Repository: {collection_params.get('repo_url', 'N/A')}\n- Commit: {collection_params.get('commit_hash', 'N/A')}\n\"\"\"\n\n    def generate_codebook(self, variables: list[dict]) -&gt; str:\n        \"\"\"Generate variable codebook.\"\"\"\n\n        codebook = \"# Codebook\\n\\n\"\n        codebook += \"| Variable | Type | Description | Values |\\n\"\n        codebook += \"|----------|------|-------------|--------|\\n\"\n\n        for var in variables:\n            codebook += f\"| {var['name']} | {var['type']} | {var['description']} | {var.get('values', 'N/A')} |\\n\"\n\n        return codebook\n\n    def generate_citation(self, metadata: dict) -&gt; str:\n        \"\"\"Generate CITATION.cff file.\"\"\"\n\n        return f\"\"\"\ncff-version: 1.2.0\nmessage: \"If you use this dataset, please cite it as below.\"\nauthors:\n  - family-names: {metadata.get('author_last', '')}\n    given-names: {metadata.get('author_first', '')}\n    orcid: {metadata.get('orcid', '')}\ntitle: \"{metadata.get('title', '')}\"\nversion: {metadata.get('version', '1.0.0')}\ndoi: {metadata.get('doi', '')}\ndate-released: {metadata.get('date', '')}\n\"\"\"\n</code></pre>"},{"location":"cookbook/research/academic-scraping/#publication-checklist","title":"Publication Checklist","text":"<pre><code># publication_checklist.py\n\nPUBLICATION_CHECKLIST = {\n    \"methodology\": [\n        \"\u2610 Research questions clearly stated\",\n        \"\u2610 Data collection methods documented\",\n        \"\u2610 Sampling strategy explained and justified\",\n        \"\u2610 Sample size and power analysis reported\",\n        \"\u2610 Time period of data collection specified\",\n        \"\u2610 Rate limiting approach documented\",\n    ],\n    \"ethics\": [\n        \"\u2610 IRB approval obtained (if required)\",\n        \"\u2610 IRB protocol number cited\",\n        \"\u2610 Consent procedures documented\",\n        \"\u2610 Anonymization methods described\",\n        \"\u2610 Risk assessment completed\",\n        \"\u2610 Data storage and security addressed\",\n    ],\n    \"reproducibility\": [\n        \"\u2610 Code available in public repository\",\n        \"\u2610 Data available (or explanation why not)\",\n        \"\u2610 Random seeds documented\",\n        \"\u2610 Software versions specified\",\n        \"\u2610 Environment files provided\",\n        \"\u2610 Analysis pipeline documented\",\n    ],\n    \"citation\": [\n        \"\u2610 Platform (X/Twitter) properly cited\",\n        \"\u2610 Collection tool (Xeepy) cited\",\n        \"\u2610 Previous related work cited\",\n        \"\u2610 CITATION.cff file created\",\n        \"\u2610 DOI obtained for dataset\",\n    ],\n    \"disclosure\": [\n        \"\u2610 Limitations clearly stated\",\n        \"\u2610 Potential biases acknowledged\",\n        \"\u2610 Platform terms of service addressed\",\n        \"\u2610 Conflicts of interest disclosed\",\n    ]\n}\n\ndef print_checklist():\n    \"\"\"Print publication checklist.\"\"\"\n\n    print(\"=\" * 60)\n    print(\"ACADEMIC PUBLICATION CHECKLIST\")\n    print(\"=\" * 60)\n\n    for category, items in PUBLICATION_CHECKLIST.items():\n        print(f\"\\n{category.upper()}\")\n        print(\"-\" * 40)\n        for item in items:\n            print(f\"  {item}\")\n\n    print(\"\\n\" + \"=\" * 60)\n</code></pre>"},{"location":"cookbook/research/academic-scraping/#complete-example","title":"Complete Example","text":"<pre><code># academic_research_example.py\nimport asyncio\nfrom datetime import datetime\n\nfrom xeepy import Xeepy\nfrom research_ethics import ResearchProtocol\nfrom sampling_strategies import SamplingStrategy, ReproducibleSampler\nfrom rate_compliant_collector import RateCompliantCollector\nfrom anonymization_pipeline import AnonymizationPipeline\nfrom documentation_generator import ResearchDocumentation\n\nasync def main():\n    # 1. Document research protocol\n    protocol = ResearchProtocol(\n        title=\"Sentiment Analysis of Tech Industry Discussions\",\n        principal_investigator=\"Dr. Jane Smith\",\n        institution=\"University of Example\",\n        research_questions=[\n            \"How does sentiment vary across tech topics?\",\n            \"What factors predict engagement?\"\n        ],\n        data_types=[\"tweets\", \"user_profiles\"],\n        estimated_sample_size=10000,\n        collection_period=\"2024-01-01 to 2024-03-31\",\n        anonymization_method=\"SHA-256 hashing with bucketing\",\n        data_storage=\"Encrypted institutional server\",\n        data_retention_period=\"5 years\",\n        risks_to_subjects=[\"Minimal - public data only\"],\n        risk_mitigation=[\"Anonymization\", \"Aggregate reporting\"]\n    )\n\n    print(protocol.generate_irb_summary())\n\n    # 2. Setup reproducible sampling\n    SEED = 42\n    sampler = ReproducibleSampler(seed=SEED, study_id=\"TECH-SENT-2024\")\n\n    # 3. Initialize collector\n    collector = RateCompliantCollector(\n        requests_per_15min=30,\n        daily_limit=500\n    )\n\n    # 4. Collect data\n    async with Xeepy() as x:\n        # Search for relevant tweets\n        search_queries = [\"python programming\", \"javascript\", \"machine learning\"]\n\n        all_tweets = []\n        for query in search_queries:\n            tweets = await collector.collect_with_backoff(\n                x.scrape.search,\n                query=query,\n                limit=1000\n            )\n            all_tweets.extend(tweets)\n\n    print(f\"Collected {len(all_tweets)} tweets\")\n\n    # 5. Sample\n    strategy = SamplingStrategy(seed=SEED)\n    sampled = strategy.stratified_sample(\n        population=all_tweets,\n        strata_key=lambda t: t.created_at.date().month,\n        sample_size=1000\n    )\n\n    sampler.log_sample(\n        sample_name=\"main_sample\",\n        method=\"stratified_by_month\",\n        population_size=len(all_tweets),\n        sample_size=len(sampled),\n        parameters={'strata': 'month'}\n    )\n\n    # 6. Anonymize\n    pipeline = AnonymizationPipeline(salt=f\"TECH-SENT-{SEED}\")\n\n    anonymized = []\n    for tweet in sampled:\n        anon_tweet = pipeline.anonymize_tweet({\n            'tweet_id': tweet.id,\n            'author_id': tweet.author.id,\n            'text': tweet.text,\n            'created_at': tweet.created_at,\n            'likes': tweet.like_count,\n            'retweets': tweet.retweet_count,\n            'media': tweet.media\n        })\n        anonymized.append(anon_tweet)\n\n    # 7. Generate documentation\n    docs = ResearchDocumentation()\n\n    methodology = docs.generate_methodology(\n        collection_params={\n            'version': '1.0.0',\n            'period': '2024-01-01 to 2024-03-31',\n            'total_queries': len(search_queries),\n            'rate_limit': '30 requests/15min',\n            'seed': SEED,\n            'python_version': '3.11',\n            'xeepy_version': '1.0.0'\n        },\n        sampling_log=sampler.export_log(),\n        anonymization_report=pipeline.generate_anonymization_report()\n    )\n\n    with open(\"methodology.md\", \"w\") as f:\n        f.write(methodology)\n\n    print(\"Research documentation generated!\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"cookbook/research/academic-scraping/#best-practices","title":"Best Practices","text":"<p>Always Consult IRB</p> <p>Even for \"public\" data, consult your institution's IRB. Requirements vary by institution and jurisdiction.</p> <p>Reproducibility First</p> <p>Set random seeds, document everything, and use version control from day one.</p> <p>Privacy Matters</p> <p>Even public tweets can identify individuals. Always anonymize before publication.</p>"},{"location":"cookbook/research/academic-scraping/#resources","title":"Resources","text":"<ul> <li>ACM Ethics Guidelines</li> <li>AoIR Ethics Guidelines</li> <li>GDPR for Researchers</li> <li>Twitter Research API Terms</li> </ul>"},{"location":"cookbook/research/academic-scraping/#related-recipes","title":"Related Recipes","text":"<ul> <li>Brand Monitoring - Data collection patterns</li> <li>Sentiment Analysis - Analysis methods</li> <li>Data Export - Data export</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome to Xeepy! This guide will take you from zero to automating X/Twitter in under 5 minutes.</p>"},{"location":"getting-started/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li> <p> Installation</p> <p>Install Xeepy and its dependencies with a single command</p> <p> Install Guide</p> </li> <li> <p> Quick Start</p> <p>Run your first scraping script in under 2 minutes</p> <p> Quick Start</p> </li> <li> <p> Authentication</p> <p>Learn how to authenticate without API keys</p> <p> Auth Guide</p> </li> <li> <p> Configuration</p> <p>Customize Xeepy for your use case</p> <p> Config Guide</p> </li> <li> <p> First Script</p> <p>Build a complete automation workflow</p> <p> First Script</p> </li> </ul>"},{"location":"getting-started/#the-60-second-version","title":"The 60-Second Version","text":"<pre><code># Install Xeepy\npip install xeepy\n\n# Install browser\nplaywright install chromium\n\n# Run your first scrape\nxeepy scrape replies https://x.com/elonmusk/status/123456789\n</code></pre> <p>That's it! You're now scraping X/Twitter without paying for API access.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"Requirement Version Why Python 3.10+ Modern async/await support Playwright Latest Browser automation engine Chromium Auto-installed Browser for scraping <p>No API Keys Required</p> <p>Unlike Tweepy or the official Twitter API, Xeepy uses browser automation. This means:</p> <ul> <li>\u2705 No $100/month API fees</li> <li>\u2705 No rate limit anxiety</li> <li>\u2705 No approval process</li> <li>\u2705 Access to all features</li> </ul>"},{"location":"getting-started/#learning-path","title":"Learning Path","text":"<pre><code>graph LR\n    A[Install] --&gt; B[Authenticate]\n    B --&gt; C[First Scrape]\n    C --&gt; D[Export Data]\n    D --&gt; E[Advanced]\n\n    style A fill:#1da1f2\n    style B fill:#1da1f2\n    style C fill:#1da1f2\n    style D fill:#1da1f2\n    style E fill:#1da1f2</code></pre>"},{"location":"getting-started/#recommended-reading-order","title":"Recommended Reading Order","text":"<ol> <li>Installation - Get Xeepy running (2 min)</li> <li>Authentication - Set up your session (3 min)</li> <li>Quick Start - Run example scripts (5 min)</li> <li>First Script - Build something useful (10 min)</li> <li>Configuration - Customize behavior (optional)</li> </ol>"},{"location":"getting-started/#choose-your-path","title":"Choose Your Path","text":"I want to scrape data fast I want to grow my following I want analytics &amp; insights I want AI-powered features <p>Perfect! Jump straight to the Scraping Guide.</p> <pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    # Get 1000 replies to any tweet\n    replies = await x.scrape.replies(tweet_url, limit=1000)\n    x.export.to_csv(replies, \"replies.csv\")\n</code></pre> <p>Check out our Growth Automation Cookbook.</p> <pre><code>async with Xeepy() as x:\n    # Unfollow non-followers, follow by hashtag\n    await x.unfollow.non_followers(max_unfollows=100)\n    await x.follow.by_hashtag(\"#buildinpublic\", limit=50)\n</code></pre> <p>Head to the Analytics Guide.</p> <pre><code>async with Xeepy() as x:\n    # Track growth, find best posting times\n    growth = await x.analytics.track_growth(\"7d\")\n    best_times = await x.analytics.best_time_to_post()\n</code></pre> <p>Explore AI Features.</p> <pre><code>from xeepy.ai import ContentGenerator\n\nai = ContentGenerator(provider=\"openai\")\nreply = await ai.generate_reply(tweet_text, style=\"witty\")\n</code></pre>"},{"location":"getting-started/#need-help","title":"Need Help?","text":"<ul> <li> FAQ - Common questions answered</li> <li> GitHub Issues - Bug reports</li> <li>:material-discord: Discord - Community support</li> <li> Email - Direct support</li> </ul> <p>Ready? Let's install Xeepy \u2192</p>"},{"location":"getting-started/authentication/","title":"Authentication","text":"<p>Xeepy uses browser-based authentication\u2014no API keys required. This guide covers all authentication methods.</p>"},{"location":"getting-started/authentication/#why-browser-auth","title":"Why Browser Auth?","text":"Traditional API Xeepy Browser Auth $100+/month fees Free Rate limited Natural pacing Approval required Instant access Limited features Full access Can be revoked Your browser, your rules"},{"location":"getting-started/authentication/#quick-start","title":"Quick Start","text":"<p>The fastest way to authenticate:</p> <pre><code># Interactive login (opens browser)\nxeepy auth login\n</code></pre> <p>This opens a browser window where you log in normally. Xeepy saves your session for future use.</p>"},{"location":"getting-started/authentication/#authentication-methods","title":"Authentication Methods","text":""},{"location":"getting-started/authentication/#method-1-interactive-login-recommended","title":"Method 1: Interactive Login (Recommended)","text":"<pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    # Opens browser for manual login\n    await x.auth.login()\n\n    # Your session is now saved automatically\n    # Future runs will use the saved session\n</code></pre> <p>When to use: First-time setup, session expired, or switching accounts.</p>"},{"location":"getting-started/authentication/#method-2-load-saved-session","title":"Method 2: Load Saved Session","text":"<pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    # Load previously saved session\n    await x.auth.load_session()\n\n    # Or from a specific file\n    await x.auth.load_session(\"my_session.json\")\n</code></pre> <p>When to use: Automated scripts, CI/CD, scheduled tasks.</p>"},{"location":"getting-started/authentication/#method-3-cookie-import","title":"Method 3: Cookie Import","text":"<p>Export cookies from your browser and import them:</p> <pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    # Import from exported cookie file\n    await x.auth.import_cookies(\"cookies.json\")\n\n    # Or from browser extension export\n    await x.auth.import_cookies(\"cookies.txt\", format=\"netscape\")\n</code></pre> <p>When to use: When you can't use interactive login (headless servers).</p>"},{"location":"getting-started/authentication/#method-4-environment-variables","title":"Method 4: Environment Variables","text":"<pre><code># Set in your environment\nexport XEEPY_SESSION_FILE=\"/path/to/session.json\"\n</code></pre> <pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    # Automatically loads from XEEPY_SESSION_FILE\n    await x.auth.auto_login()\n</code></pre> <p>When to use: Production deployments, Docker containers.</p>"},{"location":"getting-started/authentication/#session-management","title":"Session Management","text":""},{"location":"getting-started/authentication/#save-session","title":"Save Session","text":"<pre><code>async with Xeepy() as x:\n    await x.auth.login()\n\n    # Save for later use\n    await x.auth.save_session(\"my_session.json\")\n</code></pre>"},{"location":"getting-started/authentication/#session-file-location","title":"Session File Location","text":"<p>Default locations:</p> OS Path Linux <code>~/.config/xeepy/session.json</code> macOS <code>~/Library/Application Support/xeepy/session.json</code> Windows <code>%APPDATA%\\xeepy\\session.json</code>"},{"location":"getting-started/authentication/#check-session-status","title":"Check Session Status","text":"<pre><code>async with Xeepy() as x:\n    # Check if session is valid\n    if await x.auth.is_authenticated():\n        print(\"\u2713 Logged in\")\n        print(f\"Username: {await x.auth.get_username()}\")\n    else:\n        print(\"\u2717 Not logged in\")\n        await x.auth.login()\n</code></pre>"},{"location":"getting-started/authentication/#refresh-session","title":"Refresh Session","text":"<p>Sessions can expire. Refresh them:</p> <pre><code>async with Xeepy() as x:\n    # Try to use existing session, refresh if needed\n    await x.auth.ensure_authenticated()\n</code></pre>"},{"location":"getting-started/authentication/#multiple-accounts","title":"Multiple Accounts","text":"<p>Manage multiple X/Twitter accounts:</p> <pre><code>from xeepy import Xeepy\n\n# Account 1\nasync with Xeepy(profile=\"personal\") as x:\n    await x.auth.login()  # Login as personal account\n\n# Account 2\nasync with Xeepy(profile=\"business\") as x:\n    await x.auth.login()  # Login as business account\n\n# Later, switch between them\nasync with Xeepy(profile=\"personal\") as x:\n    await x.auth.load_session()  # Loads personal session\n</code></pre>"},{"location":"getting-started/authentication/#account-profiles","title":"Account Profiles","text":"<pre><code>from xeepy import Xeepy\n\n# Create named profiles\nprofiles = {\n    \"main\": \"sessions/main_session.json\",\n    \"backup\": \"sessions/backup_session.json\",\n    \"research\": \"sessions/research_session.json\"\n}\n\nasync def use_account(profile_name: str):\n    async with Xeepy() as x:\n        await x.auth.load_session(profiles[profile_name])\n        return x\n</code></pre>"},{"location":"getting-started/authentication/#cli-authentication","title":"CLI Authentication","text":""},{"location":"getting-started/authentication/#login","title":"Login","text":"<pre><code># Interactive login (default)\nxeepy auth login\n\n# Login with specific profile\nxeepy auth login --profile business\n\n# Login with browser visible (debugging)\nxeepy auth login --headful\n</code></pre>"},{"location":"getting-started/authentication/#status","title":"Status","text":"<pre><code># Check current auth status\nxeepy auth status\n\n# Output:\n# \u2713 Authenticated\n# Username: @yourhandle\n# Session age: 2 days\n# Expires: ~28 days\n</code></pre>"},{"location":"getting-started/authentication/#logout","title":"Logout","text":"<pre><code># Clear saved session\nxeepy auth logout\n\n# Clear specific profile\nxeepy auth logout --profile business\n\n# Clear all sessions\nxeepy auth logout --all\n</code></pre>"},{"location":"getting-started/authentication/#exportimport","title":"Export/Import","text":"<pre><code># Export session for backup\nxeepy auth export backup_session.json\n\n# Import from backup\nxeepy auth import backup_session.json\n\n# Import from browser cookie export\nxeepy auth import cookies.txt --format netscape\n</code></pre>"},{"location":"getting-started/authentication/#exporting-cookies-from-browser","title":"Exporting Cookies from Browser","text":""},{"location":"getting-started/authentication/#chromebrave","title":"Chrome/Brave","text":"<ol> <li>Install \"EditThisCookie\" extension</li> <li>Go to x.com and log in</li> <li>Click the extension icon</li> <li>Click \"Export\" (copies JSON to clipboard)</li> <li>Save to <code>cookies.json</code></li> </ol>"},{"location":"getting-started/authentication/#firefox","title":"Firefox","text":"<ol> <li>Install \"Cookie Quick Manager\" extension</li> <li>Go to x.com and log in</li> <li>Click extension \u2192 Export \u2192 JSON</li> <li>Save the file</li> </ol>"},{"location":"getting-started/authentication/#using-browser-devtools","title":"Using Browser DevTools","text":"<pre><code>// In browser console on x.com\ncopy(document.cookie.split('; ').map(c =&gt; {\n    const [name, value] = c.split('=');\n    return {name, value, domain: '.x.com', path: '/'};\n}));\n</code></pre>"},{"location":"getting-started/authentication/#security-best-practices","title":"Security Best Practices","text":"<p>Never share session files</p> <p>Session files contain authentication tokens. Treat them like passwords.</p>"},{"location":"getting-started/authentication/#recommended-practices","title":"Recommended Practices","text":"<ol> <li>Use environment variables for session paths in production</li> <li>Encrypt session files at rest</li> <li>Rotate sessions periodically</li> <li>Use separate accounts for automation vs personal use</li> <li>Enable 2FA on your X account (Xeepy handles it)</li> </ol>"},{"location":"getting-started/authentication/#secure-session-storage","title":"Secure Session Storage","text":"<pre><code>import os\nfrom cryptography.fernet import Fernet\n\n# Generate key (save this securely!)\nkey = Fernet.generate_key()\ncipher = Fernet(key)\n\nasync with Xeepy() as x:\n    await x.auth.login()\n\n    # Get session data\n    session_data = await x.auth.export_session()\n\n    # Encrypt before saving\n    encrypted = cipher.encrypt(session_data.encode())\n    with open(\"session.enc\", \"wb\") as f:\n        f.write(encrypted)\n</code></pre>"},{"location":"getting-started/authentication/#gitignore-sessions","title":"Gitignore Sessions","text":"<p>Add to your <code>.gitignore</code>:</p> <pre><code># Xeepy sessions\n*.session.json\nsession.json\nsessions/\ncookies.json\n*.enc\n</code></pre>"},{"location":"getting-started/authentication/#two-factor-authentication-2fa","title":"Two-Factor Authentication (2FA)","text":"<p>Xeepy handles 2FA during interactive login:</p> <pre><code>async with Xeepy() as x:\n    # If 2FA is enabled, you'll be prompted in the browser\n    await x.auth.login()  # Complete 2FA in browser window\n</code></pre> <p>For automated 2FA (advanced):</p> <pre><code>async with Xeepy() as x:\n    await x.auth.login(\n        totp_secret=\"YOUR_2FA_SECRET\"  # From authenticator setup\n    )\n</code></pre>"},{"location":"getting-started/authentication/#troubleshooting","title":"Troubleshooting","text":"Session expired unexpectedly <p>Sessions typically last 30 days. To auto-refresh: <pre><code>async with Xeepy(auto_refresh_session=True) as x:\n    await x.auth.load_session()\n</code></pre></p> Login loop / Can't complete login <p>Try clearing browser state: <pre><code>xeepy auth logout --clear-browser\nxeepy auth login --headful\n</code></pre></p> Captcha during login <p>Use headful mode to solve manually: <pre><code>async with Xeepy(headless=False) as x:\n    await x.auth.login()  # Solve captcha in browser\n</code></pre></p> Account locked after automation <p>This usually means rate limits were hit. See Rate Limiting.</p> Session works locally but not on server <p>Sessions are tied to IP/fingerprint. Options:</p> <ol> <li>Login on the server directly</li> <li>Use residential proxies</li> <li>Use the same browser fingerprint</li> </ol>"},{"location":"getting-started/authentication/#api-reference","title":"API Reference","text":"<p>::: xeepy.core.auth.Auth     options:       show_source: false       members:         - login         - logout         - load_session         - save_session         - import_cookies         - export_session         - is_authenticated         - ensure_authenticated         - get_username</p> <p>Next: Quick Start Guide \u2192</p>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>Xeepy is highly configurable. This guide covers all configuration options and best practices.</p>"},{"location":"getting-started/configuration/#configuration-methods","title":"Configuration Methods","text":"<p>Xeepy supports multiple configuration methods (in order of precedence):</p> <ol> <li>Code - Direct parameters in your script</li> <li>Environment variables - For secrets and deployment</li> <li>Config file - <code>xeepy.toml</code> or <code>xeepy.yaml</code></li> <li>Defaults - Sensible built-in defaults</li> </ol>"},{"location":"getting-started/configuration/#quick-configuration","title":"Quick Configuration","text":""},{"location":"getting-started/configuration/#in-code","title":"In Code","text":"<pre><code>from xeepy import Xeepy\n\nasync with Xeepy(\n    headless=True,           # Run browser invisibly\n    timeout=30000,           # 30 second timeout\n    rate_limit=20,           # Max 20 requests/minute\n    session_file=\"session.json\"\n) as x:\n    # Your code here\n    pass\n</code></pre>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<pre><code># Authentication\nexport XEEPY_SESSION_FILE=\"/path/to/session.json\"\n\n# Browser\nexport XEEPY_HEADLESS=true\nexport XEEPY_TIMEOUT=30000\n\n# Rate limiting\nexport XEEPY_RATE_LIMIT=20\n\n# Proxy\nexport XEEPY_PROXY_URL=\"http://user:pass@proxy:8080\"\n\n# AI Features\nexport OPENAI_API_KEY=\"sk-...\"\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\n\n# Notifications\nexport DISCORD_WEBHOOK=\"https://discord.com/api/webhooks/...\"\nexport TELEGRAM_BOT_TOKEN=\"123456:ABC...\"\nexport TELEGRAM_CHAT_ID=\"123456789\"\n</code></pre>"},{"location":"getting-started/configuration/#config-file","title":"Config File","text":"<p>Create <code>xeepy.toml</code> in your project root:</p> <pre><code>[xeepy]\n# ============================================\n# CORE SETTINGS\n# ============================================\n\n# Browser mode: true = invisible, false = visible\nheadless = true\n\n# Page load timeout (milliseconds)\ntimeout = 30000\n\n# Slow down operations by X ms (helps avoid detection)\nslow_mo = 100\n\n# Default session file location\nsession_file = \"~/.config/xeepy/session.json\"\n\n# ============================================\n# RATE LIMITING\n# ============================================\n\n[xeepy.rate_limit]\n# Global rate limit (requests per minute)\nrequests_per_minute = 20\n\n# Operation-specific limits\nfollows_per_hour = 30\nunfollows_per_hour = 50\nlikes_per_hour = 100\ntweets_per_day = 300\n\n# Cooldown after hitting limits (seconds)\ncooldown_duration = 300\n\n# ============================================\n# PROXY SETTINGS\n# ============================================\n\n[xeepy.proxy]\nenabled = false\nurl = \"http://user:pass@proxy:8080\"\n\n# Rotate proxies (requires proxy list)\nrotate = false\nproxy_file = \"proxies.txt\"\n\n# ============================================\n# BROWSER FINGERPRINT\n# ============================================\n\n[xeepy.browser]\n# User agent (leave empty for default)\nuser_agent = \"\"\n\n# Viewport size\nviewport_width = 1920\nviewport_height = 1080\n\n# Locale and timezone\nlocale = \"en-US\"\ntimezone = \"America/New_York\"\n\n# ============================================\n# STORAGE &amp; CACHING\n# ============================================\n\n[xeepy.storage]\n# Enable caching\ncache_enabled = true\n\n# Cache location\ncache_dir = \"~/.cache/xeepy\"\n\n# Cache TTL (seconds) - how long to keep cached data\ncache_ttl = 3600\n\n# Database for persistent storage\ndatabase_url = \"sqlite:///~/.local/share/xeepy/data.db\"\n\n# ============================================\n# EXPORT DEFAULTS\n# ============================================\n\n[xeepy.export]\n# Default format: csv, json, excel, parquet\ndefault_format = \"csv\"\n\n# Default output directory\noutput_dir = \"./exports\"\n\n# Include timestamp in filenames\ntimestamp_filenames = true\n\n# ============================================\n# AI FEATURES\n# ============================================\n\n[xeepy.ai]\n# Default provider: openai, anthropic, ollama\ndefault_provider = \"openai\"\n\n# Model settings per provider\n[xeepy.ai.openai]\nmodel = \"gpt-4-turbo\"\ntemperature = 0.7\nmax_tokens = 500\n\n[xeepy.ai.anthropic]\nmodel = \"claude-3-sonnet\"\ntemperature = 0.7\nmax_tokens = 500\n\n[xeepy.ai.ollama]\nmodel = \"llama3\"\nbase_url = \"http://localhost:11434\"\n\n# ============================================\n# NOTIFICATIONS\n# ============================================\n\n[xeepy.notifications]\n# Enable notifications\nenabled = true\n\n# Notification triggers\nnotify_on_error = true\nnotify_on_complete = false\nnotify_daily_report = true\n\n# Discord\ndiscord_webhook = \"\"\n\n# Telegram\ntelegram_bot_token = \"\"\ntelegram_chat_id = \"\"\n\n# Email\nsmtp_host = \"smtp.gmail.com\"\nsmtp_port = 587\nsmtp_user = \"\"\nsmtp_password = \"\"\nemail_to = \"\"\n\n# ============================================\n# LOGGING\n# ============================================\n\n[xeepy.logging]\n# Log level: DEBUG, INFO, WARNING, ERROR\nlevel = \"INFO\"\n\n# Log file (leave empty for console only)\nfile = \"~/.local/share/xeepy/xeepy.log\"\n\n# Log format\nformat = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n\n# Rotate logs\nmax_size_mb = 10\nbackup_count = 5\n\n# ============================================\n# SAFETY SETTINGS\n# ============================================\n\n[xeepy.safety]\n# Dry run mode (no actual actions)\ndry_run = false\n\n# Confirm destructive operations\nconfirm_unfollow = false\nconfirm_mass_operations = true\n\n# Daily limits (0 = unlimited)\nmax_follows_per_day = 100\nmax_unfollows_per_day = 200\nmax_likes_per_day = 500\nmax_tweets_per_day = 50\n\n# Whitelist (never unfollow these)\nwhitelist_file = \"whitelist.txt\"\n</code></pre>"},{"location":"getting-started/configuration/#yaml-format","title":"YAML Format","text":"<p>If you prefer YAML, create <code>xeepy.yaml</code>:</p> <pre><code>xeepy:\n  headless: true\n  timeout: 30000\n  session_file: ~/.config/xeepy/session.json\n\n  rate_limit:\n    requests_per_minute: 20\n    follows_per_hour: 30\n\n  proxy:\n    enabled: false\n    url: http://user:pass@proxy:8080\n\n  ai:\n    default_provider: openai\n    openai:\n      model: gpt-4-turbo\n      temperature: 0.7\n\n  notifications:\n    enabled: true\n    discord_webhook: ${DISCORD_WEBHOOK}\n</code></pre>"},{"location":"getting-started/configuration/#environment-specific-config","title":"Environment-Specific Config","text":""},{"location":"getting-started/configuration/#development","title":"Development","text":"<pre><code># xeepy.dev.toml\n[xeepy]\nheadless = false  # See the browser\nslow_mo = 500     # Slow for debugging\nrate_limit.requests_per_minute = 5  # Conservative\n\n[xeepy.logging]\nlevel = \"DEBUG\"\n\n[xeepy.safety]\ndry_run = true    # Don't actually perform actions\n</code></pre>"},{"location":"getting-started/configuration/#production","title":"Production","text":"<pre><code># xeepy.prod.toml\n[xeepy]\nheadless = true\nslow_mo = 50\n\n[xeepy.rate_limit]\nrequests_per_minute = 30\n\n[xeepy.logging]\nlevel = \"INFO\"\nfile = \"/var/log/xeepy/xeepy.log\"\n\n[xeepy.notifications]\nenabled = true\nnotify_on_error = true\n</code></pre>"},{"location":"getting-started/configuration/#load-environment-config","title":"Load Environment Config","text":"<pre><code>import os\nfrom xeepy import Xeepy\n\n# Load based on environment\nenv = os.getenv(\"XEEPY_ENV\", \"dev\")\nconfig_file = f\"xeepy.{env}.toml\"\n\nasync with Xeepy(config_file=config_file) as x:\n    pass\n</code></pre>"},{"location":"getting-started/configuration/#programmatic-configuration","title":"Programmatic Configuration","text":""},{"location":"getting-started/configuration/#using-config-class","title":"Using Config Class","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.core.config import Config\n\n# Create config programmatically\nconfig = Config(\n    headless=True,\n    rate_limit=Config.RateLimit(\n        requests_per_minute=25,\n        follows_per_hour=40\n    ),\n    proxy=Config.Proxy(\n        enabled=True,\n        url=\"http://proxy:8080\"\n    )\n)\n\nasync with Xeepy(config=config) as x:\n    pass\n</code></pre>"},{"location":"getting-started/configuration/#runtime-configuration","title":"Runtime Configuration","text":"<pre><code>async with Xeepy() as x:\n    # Change settings at runtime\n    x.config.rate_limit.requests_per_minute = 10\n    x.config.headless = False\n\n    # Reload config from file\n    x.config.reload()\n\n    # Get current config\n    print(x.config.to_dict())\n</code></pre>"},{"location":"getting-started/configuration/#profile-system","title":"Profile System","text":"<p>Manage multiple configurations:</p> <pre><code>from xeepy import Xeepy\n\n# Development profile\nasync with Xeepy(profile=\"dev\") as x:\n    pass  # Uses xeepy.dev.toml + session_dev.json\n\n# Production profile\nasync with Xeepy(profile=\"prod\") as x:\n    pass  # Uses xeepy.prod.toml + session_prod.json\n\n# Custom profile\nasync with Xeepy(profile=\"client_abc\") as x:\n    pass  # Uses xeepy.client_abc.toml\n</code></pre>"},{"location":"getting-started/configuration/#configuration-validation","title":"Configuration Validation","text":"<p>Xeepy validates your configuration on startup:</p> <pre><code>from xeepy.core.config import Config, validate_config\n\n# Validate config file\nerrors = validate_config(\"xeepy.toml\")\nif errors:\n    for error in errors:\n        print(f\"Config error: {error}\")\nelse:\n    print(\"\u2713 Configuration valid\")\n</code></pre>"},{"location":"getting-started/configuration/#secrets-management","title":"Secrets Management","text":""},{"location":"getting-started/configuration/#using-environment-variables","title":"Using Environment Variables","text":"<pre><code># xeepy.toml - Reference env vars\n[xeepy.notifications]\ndiscord_webhook = \"${DISCORD_WEBHOOK}\"\ntelegram_bot_token = \"${TELEGRAM_TOKEN}\"\n\n[xeepy.ai.openai]\napi_key = \"${OPENAI_API_KEY}\"\n</code></pre>"},{"location":"getting-started/configuration/#using-env-files","title":"Using .env Files","text":"<pre><code># .env file\nXEEPY_SESSION_FILE=/secure/path/session.json\nDISCORD_WEBHOOK=https://discord.com/api/webhooks/...\nOPENAI_API_KEY=sk-...\n</code></pre> <pre><code>from dotenv import load_dotenv\nfrom xeepy import Xeepy\n\nload_dotenv()  # Load .env file\n\nasync with Xeepy() as x:\n    pass  # Uses env vars automatically\n</code></pre>"},{"location":"getting-started/configuration/#using-secret-managers","title":"Using Secret Managers","text":"<pre><code>import boto3\nfrom xeepy import Xeepy\n\n# AWS Secrets Manager example\ndef get_secret(name):\n    client = boto3.client('secretsmanager')\n    response = client.get_secret_value(SecretId=name)\n    return response['SecretString']\n\nasync with Xeepy(\n    session_file=get_secret(\"xeepy/session\"),\n    proxy_url=get_secret(\"xeepy/proxy\")\n) as x:\n    pass\n</code></pre>"},{"location":"getting-started/configuration/#configuration-reference","title":"Configuration Reference","text":""},{"location":"getting-started/configuration/#all-options","title":"All Options","text":"Option Type Default Description <code>headless</code> bool <code>True</code> Run browser invisibly <code>timeout</code> int <code>30000</code> Page timeout (ms) <code>slow_mo</code> int <code>0</code> Slow down operations (ms) <code>session_file</code> str Auto Path to session file <code>config_file</code> str Auto Path to config file <code>profile</code> str None Named profile to use"},{"location":"getting-started/configuration/#rate-limit-options","title":"Rate Limit Options","text":"Option Type Default Description <code>requests_per_minute</code> int <code>20</code> Global rate limit <code>follows_per_hour</code> int <code>30</code> Max follows per hour <code>unfollows_per_hour</code> int <code>50</code> Max unfollows per hour <code>likes_per_hour</code> int <code>100</code> Max likes per hour <code>cooldown_duration</code> int <code>300</code> Cooldown seconds"},{"location":"getting-started/configuration/#proxy-options","title":"Proxy Options","text":"Option Type Default Description <code>enabled</code> bool <code>False</code> Enable proxy <code>url</code> str None Proxy URL <code>rotate</code> bool <code>False</code> Rotate proxies <code>proxy_file</code> str None File with proxy list"},{"location":"getting-started/configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Use environment variables for secrets - Never commit API keys or tokens</li> <li>Use profiles for different environments - dev, staging, prod</li> <li>Start with conservative rate limits - Increase gradually</li> <li>Enable dry_run when testing - Avoid accidental actions</li> <li>Set up notifications - Know when things go wrong</li> <li>Use a whitelist - Protect important follows from unfollowing</li> </ol> <p>Next: Build Your First Script \u2192</p>"},{"location":"getting-started/first-script/","title":"Your First Complete Script","text":"<p>Let's build a real-world automation script from scratch. By the end, you'll have a working \"Account Health Dashboard\" that:</p> <ul> <li>\u2705 Tracks follower changes</li> <li>\u2705 Identifies unfollowers</li> <li>\u2705 Analyzes engagement</li> <li>\u2705 Finds optimal posting times</li> <li>\u2705 Generates a report</li> <li>\u2705 Sends notifications</li> </ul>"},{"location":"getting-started/first-script/#project-setup","title":"Project Setup","text":"<p>Create a new directory for your project:</p> <pre><code>mkdir xeepy-dashboard\ncd xeepy-dashboard\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # or `venv\\Scripts\\activate` on Windows\n\n# Install dependencies\npip install \"xeepy[all]\"\nplaywright install chromium\n\n# Authenticate\nxeepy auth login\n</code></pre> <p>Create the project structure:</p> <pre><code>xeepy-dashboard/\n\u251c\u2500\u2500 dashboard.py         # Main script\n\u251c\u2500\u2500 config.py           # Configuration\n\u251c\u2500\u2500 xeepy.toml         # Xeepy config\n\u251c\u2500\u2500 whitelist.txt       # Users to never unfollow\n\u2514\u2500\u2500 .env                # Secrets (don't commit!)\n</code></pre>"},{"location":"getting-started/first-script/#step-1-configuration","title":"Step 1: Configuration","text":"<p><code>.env</code> - Store secrets here: <pre><code>DISCORD_WEBHOOK=https://discord.com/api/webhooks/your/webhook\nOPENAI_API_KEY=sk-your-key-here\n</code></pre></p> <p><code>xeepy.toml</code> - Xeepy configuration: <pre><code>[xeepy]\nheadless = true\ntimeout = 30000\n\n[xeepy.rate_limit]\nrequests_per_minute = 20\nfollows_per_hour = 25\nunfollows_per_hour = 40\n\n[xeepy.storage]\ndatabase_url = \"sqlite:///dashboard.db\"\n\n[xeepy.notifications]\nenabled = true\n\n[xeepy.safety]\ndry_run = false\nmax_unfollows_per_day = 100\nwhitelist_file = \"whitelist.txt\"\n</code></pre></p> <p><code>whitelist.txt</code> - Users to protect: <pre><code>naval\npaulg\nelonmusk\nyour_best_friend\nimportant_client\n</code></pre></p> <p><code>config.py</code> - Python configuration: <pre><code>\"\"\"Dashboard configuration\"\"\"\nimport os\nfrom dataclasses import dataclass\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n@dataclass\nclass DashboardConfig:\n    # Your username\n    username: str = \"your_twitter_handle\"\n\n    # Analysis settings\n    tweet_analysis_count: int = 100\n    follower_sample_size: int = 500\n\n    # Automation settings\n    auto_unfollow_enabled: bool = True\n    max_daily_unfollows: int = 50\n    auto_engage_enabled: bool = True\n    max_daily_likes: int = 100\n\n    # Report settings\n    report_output: str = \"reports/\"\n\n    # Notifications\n    discord_webhook: str = os.getenv(\"DISCORD_WEBHOOK\", \"\")\n\n    # AI features\n    ai_enabled: bool = True\n    openai_api_key: str = os.getenv(\"OPENAI_API_KEY\", \"\")\n\nconfig = DashboardConfig()\n</code></pre></p>"},{"location":"getting-started/first-script/#step-2-the-dashboard-script","title":"Step 2: The Dashboard Script","text":"<p><code>dashboard.py</code>:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nXeepy Account Health Dashboard\nRun daily to track, analyze, and optimize your X/Twitter presence.\n\"\"\"\nimport asyncio\nfrom datetime import datetime\nfrom pathlib import Path\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nfrom xeepy import Xeepy\nfrom xeepy.ai import ContentGenerator\nfrom config import config\n\n\n@dataclass\nclass DashboardReport:\n    \"\"\"Container for all dashboard metrics\"\"\"\n    timestamp: datetime = field(default_factory=datetime.now)\n\n    # Follower metrics\n    followers_count: int = 0\n    following_count: int = 0\n    new_followers: list = field(default_factory=list)\n    unfollowers: list = field(default_factory=list)\n    net_change: int = 0\n\n    # Engagement metrics\n    avg_likes: float = 0\n    avg_retweets: float = 0\n    avg_replies: float = 0\n    engagement_rate: float = 0\n    top_tweet: Optional[dict] = None\n\n    # Timing insights\n    best_day: str = \"\"\n    best_hour: int = 0\n\n    # Audience insights\n    top_follower_locations: list = field(default_factory=list)\n    common_interests: list = field(default_factory=list)\n\n    # Actions taken\n    unfollowed_count: int = 0\n    liked_count: int = 0\n\n    # AI insights\n    content_suggestions: list = field(default_factory=list)\n\n\nasync def run_dashboard():\n    \"\"\"Main dashboard routine\"\"\"\n    print(\"\"\"\n    \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n    \u2551       \ud83d\udc26 Xeepy Account Health Dashboard \ud83d\udc26           \u2551\n    \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n    \"\"\")\n\n    report = DashboardReport()\n\n    async with Xeepy() as x:\n        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n        # SECTION 1: FOLLOWER TRACKING\n        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n        print(\"\\n\ud83d\udcca [1/6] Analyzing follower changes...\")\n\n        # Get current follower/following counts\n        profile = await x.scrape.profile(config.username)\n        report.followers_count = profile.followers_count\n        report.following_count = profile.following_count\n\n        # Detect unfollowers since last check\n        unfollower_report = await x.monitor.unfollowers()\n        report.unfollowers = unfollower_report.unfollowers\n        report.new_followers = unfollower_report.new_followers\n        report.net_change = len(report.new_followers) - len(report.unfollowers)\n\n        print(f\"   \u2713 Followers: {report.followers_count:,}\")\n        print(f\"   \u2713 Following: {report.following_count:,}\")\n        print(f\"   \u2713 New followers: +{len(report.new_followers)}\")\n        print(f\"   \u2713 Unfollowers: -{len(report.unfollowers)}\")\n        print(f\"   \u2713 Net change: {report.net_change:+d}\")\n\n        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n        # SECTION 2: ENGAGEMENT ANALYSIS\n        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n        print(\"\\n\ud83d\udcac [2/6] Analyzing engagement...\")\n\n        # Get recent tweets for analysis\n        my_tweets = await x.scrape.tweets(\n            config.username,\n            limit=config.tweet_analysis_count\n        )\n\n        if my_tweets:\n            # Calculate averages\n            report.avg_likes = sum(t.likes for t in my_tweets) / len(my_tweets)\n            report.avg_retweets = sum(t.retweets for t in my_tweets) / len(my_tweets)\n            report.avg_replies = sum(t.replies for t in my_tweets) / len(my_tweets)\n\n            # Engagement rate = (likes + retweets + replies) / followers * 100\n            total_engagement = report.avg_likes + report.avg_retweets + report.avg_replies\n            report.engagement_rate = (total_engagement / report.followers_count) * 100\n\n            # Find top performing tweet\n            top_tweet = max(my_tweets, key=lambda t: t.likes + t.retweets * 2)\n            report.top_tweet = {\n                \"text\": top_tweet.text[:100] + \"...\" if len(top_tweet.text) &gt; 100 else top_tweet.text,\n                \"likes\": top_tweet.likes,\n                \"retweets\": top_tweet.retweets,\n                \"url\": top_tweet.url\n            }\n\n        print(f\"   \u2713 Avg likes: {report.avg_likes:.1f}\")\n        print(f\"   \u2713 Avg retweets: {report.avg_retweets:.1f}\")\n        print(f\"   \u2713 Engagement rate: {report.engagement_rate:.2f}%\")\n\n        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n        # SECTION 3: OPTIMAL TIMING\n        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n        print(\"\\n\u23f0 [3/6] Finding optimal posting times...\")\n\n        timing_analysis = await x.analytics.best_time_to_post()\n        report.best_day = timing_analysis.best_day\n        report.best_hour = timing_analysis.best_hour\n\n        print(f\"   \u2713 Best day: {report.best_day}\")\n        print(f\"   \u2713 Best hour: {report.best_hour}:00\")\n\n        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n        # SECTION 4: AUDIENCE INSIGHTS\n        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n        print(\"\\n\ud83d\udc65 [4/6] Analyzing audience...\")\n\n        audience = await x.analytics.audience_insights(\n            sample_size=config.follower_sample_size\n        )\n        report.top_follower_locations = audience.top_locations[:5]\n        report.common_interests = audience.common_interests[:5]\n\n        print(f\"   \u2713 Top locations: {', '.join(report.top_follower_locations)}\")\n        print(f\"   \u2713 Common interests: {', '.join(report.common_interests)}\")\n\n        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n        # SECTION 5: AUTOMATED ACTIONS\n        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n        print(\"\\n\ud83e\udd16 [5/6] Running automated actions...\")\n\n        # Unfollow non-followers\n        if config.auto_unfollow_enabled:\n            unfollow_result = await x.unfollow.non_followers(\n                max_unfollows=config.max_daily_unfollows,\n                whitelist_file=\"whitelist.txt\"\n            )\n            report.unfollowed_count = unfollow_result.unfollowed_count\n            print(f\"   \u2713 Unfollowed {report.unfollowed_count} non-followers\")\n\n        # Auto-engage with niche content\n        if config.auto_engage_enabled:\n            liked_tweets = await x.engage.auto_like(\n                keywords=report.common_interests[:3],  # Use top interests\n                limit=min(30, config.max_daily_likes)\n            )\n            report.liked_count = len(liked_tweets)\n            print(f\"   \u2713 Liked {report.liked_count} tweets in your niche\")\n\n        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n        # SECTION 6: AI CONTENT SUGGESTIONS\n        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n        if config.ai_enabled and config.openai_api_key:\n            print(\"\\n\ud83e\udde0 [6/6] Generating AI insights...\")\n\n            ai = ContentGenerator(\n                provider=\"openai\",\n                api_key=config.openai_api_key\n            )\n\n            # Analyze top tweet and suggest similar content\n            if report.top_tweet:\n                suggestions = await ai.suggest_content(\n                    based_on=report.top_tweet[\"text\"],\n                    audience_interests=report.common_interests,\n                    count=3\n                )\n                report.content_suggestions = suggestions\n                print(\"   \u2713 Generated content suggestions\")\n        else:\n            print(\"\\n\u23ed\ufe0f  [6/6] Skipping AI insights (not configured)\")\n\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    # GENERATE REPORT\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    await generate_report(report)\n\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    # SEND NOTIFICATIONS\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    if config.discord_webhook:\n        await send_discord_notification(report)\n\n    return report\n\n\nasync def generate_report(report: DashboardReport):\n    \"\"\"Generate and save a markdown report\"\"\"\n    print(\"\\n\ud83d\udcdd Generating report...\")\n\n    # Create reports directory\n    Path(config.report_output).mkdir(exist_ok=True)\n\n    # Generate filename with date\n    filename = f\"report_{report.timestamp.strftime('%Y-%m-%d')}.md\"\n    filepath = Path(config.report_output) / filename\n\n    # Build report content\n    content = f\"\"\"# \ud83d\udcca Account Health Report\n\n**Date:** {report.timestamp.strftime('%Y-%m-%d %H:%M')}  \n**Account:** @{config.username}\n\n---\n\n## \ud83d\udcc8 Follower Metrics\n\n| Metric | Value |\n|--------|-------|\n| Total Followers | {report.followers_count:,} |\n| Total Following | {report.following_count:,} |\n| Ratio | {report.followers_count / max(report.following_count, 1):.2f} |\n| New Followers | +{len(report.new_followers)} |\n| Unfollowers | -{len(report.unfollowers)} |\n| **Net Change** | **{report.net_change:+d}** |\n\n### New Followers\n{chr(10).join(f'- @{u}' for u in report.new_followers[:10]) or '- None'}\n\n### Unfollowers\n{chr(10).join(f'- @{u}' for u in report.unfollowers[:10]) or '- None'}\n\n---\n\n## \ud83d\udcac Engagement Metrics\n\n| Metric | Value |\n|--------|-------|\n| Avg Likes | {report.avg_likes:.1f} |\n| Avg Retweets | {report.avg_retweets:.1f} |\n| Avg Replies | {report.avg_replies:.1f} |\n| Engagement Rate | {report.engagement_rate:.2f}% |\n\n### Top Performing Tweet\n&gt; {report.top_tweet['text'] if report.top_tweet else 'N/A'}\n\n\u2764\ufe0f {report.top_tweet['likes'] if report.top_tweet else 0} likes | \n\ud83d\udd01 {report.top_tweet['retweets'] if report.top_tweet else 0} retweets\n\n---\n\n## \u23f0 Optimal Posting Times\n\n| Best Day | Best Hour |\n|----------|-----------|\n| {report.best_day} | {report.best_hour}:00 |\n\n---\n\n## \ud83d\udc65 Audience Insights\n\n### Top Locations\n{chr(10).join(f'{i+1}. {loc}' for i, loc in enumerate(report.top_follower_locations)) or 'N/A'}\n\n### Common Interests\n{chr(10).join(f'- {interest}' for interest in report.common_interests) or 'N/A'}\n\n---\n\n## \ud83e\udd16 Automated Actions\n\n| Action | Count |\n|--------|-------|\n| Users Unfollowed | {report.unfollowed_count} |\n| Tweets Liked | {report.liked_count} |\n\n---\n\n## \ud83d\udca1 AI Content Suggestions\n\n{chr(10).join(f'{i+1}. {s}' for i, s in enumerate(report.content_suggestions)) or 'AI suggestions disabled'}\n\n---\n\n*Generated by Xeepy Dashboard*\n\"\"\"\n\n    # Save report\n    filepath.write_text(content)\n    print(f\"   \u2713 Report saved to {filepath}\")\n\n    # Also save as latest\n    (Path(config.report_output) / \"latest.md\").write_text(content)\n\n\nasync def send_discord_notification(report: DashboardReport):\n    \"\"\"Send summary to Discord\"\"\"\n    from xeepy.notifications import DiscordNotifier\n\n    discord = DiscordNotifier(webhook_url=config.discord_webhook)\n\n    # Create embed\n    embed = {\n        \"title\": \"\ud83d\udcca Daily Account Health Report\",\n        \"color\": 0x1DA1F2,  # Twitter blue\n        \"fields\": [\n            {\n                \"name\": \"\ud83d\udcc8 Followers\",\n                \"value\": f\"{report.followers_count:,} ({report.net_change:+d})\",\n                \"inline\": True\n            },\n            {\n                \"name\": \"\ud83d\udcac Engagement\",\n                \"value\": f\"{report.engagement_rate:.2f}%\",\n                \"inline\": True\n            },\n            {\n                \"name\": \"\u23f0 Best Time\",\n                \"value\": f\"{report.best_day} {report.best_hour}:00\",\n                \"inline\": True\n            },\n            {\n                \"name\": \"\ud83e\udd16 Actions\",\n                \"value\": f\"Unfollowed: {report.unfollowed_count}\\nLiked: {report.liked_count}\",\n                \"inline\": True\n            }\n        ],\n        \"timestamp\": report.timestamp.isoformat()\n    }\n\n    await discord.send_embed(embed)\n    print(\"   \u2713 Discord notification sent\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_dashboard())\n</code></pre>"},{"location":"getting-started/first-script/#step-3-run-your-dashboard","title":"Step 3: Run Your Dashboard","text":"<pre><code># Run the dashboard\npython dashboard.py\n</code></pre> <p>Expected output:</p> <pre><code>\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551       \ud83d\udc26 Xeepy Account Health Dashboard \ud83d\udc26           \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\n\ud83d\udcca [1/6] Analyzing follower changes...\n   \u2713 Followers: 12,456\n   \u2713 Following: 1,234\n   \u2713 New followers: +23\n   \u2713 Unfollowers: -5\n   \u2713 Net change: +18\n\n\ud83d\udcac [2/6] Analyzing engagement...\n   \u2713 Avg likes: 45.2\n   \u2713 Avg retweets: 8.3\n   \u2713 Engagement rate: 0.43%\n\n\u23f0 [3/6] Finding optimal posting times...\n   \u2713 Best day: Tuesday\n   \u2713 Best hour: 14:00\n\n\ud83d\udc65 [4/6] Analyzing audience...\n   \u2713 Top locations: USA, UK, India, Germany, Canada\n   \u2713 Common interests: tech, startups, python, ai\n\n\ud83e\udd16 [5/6] Running automated actions...\n   \u2713 Unfollowed 25 non-followers\n   \u2713 Liked 30 tweets in your niche\n\n\ud83e\udde0 [6/6] Generating AI insights...\n   \u2713 Generated content suggestions\n\n\ud83d\udcdd Generating report...\n   \u2713 Report saved to reports/report_2024-01-15.md\n   \u2713 Discord notification sent\n</code></pre>"},{"location":"getting-started/first-script/#step-4-schedule-it","title":"Step 4: Schedule It","text":""},{"location":"getting-started/first-script/#using-cron-linuxmacos","title":"Using Cron (Linux/macOS)","text":"<pre><code># Edit crontab\ncrontab -e\n\n# Run daily at 9 AM\n0 9 * * * cd /path/to/xeepy-dashboard &amp;&amp; /path/to/venv/bin/python dashboard.py &gt;&gt; logs/dashboard.log 2&gt;&amp;1\n</code></pre>"},{"location":"getting-started/first-script/#using-task-scheduler-windows","title":"Using Task Scheduler (Windows)","text":"<p>Create a batch file <code>run_dashboard.bat</code>: <pre><code>@echo off\ncd C:\\path\\to\\xeepy-dashboard\ncall venv\\Scripts\\activate\npython dashboard.py\n</code></pre></p> <p>Then add to Task Scheduler to run daily.</p>"},{"location":"getting-started/first-script/#using-systemd-timer-linux","title":"Using systemd Timer (Linux)","text":"<pre><code># /etc/systemd/system/xeepy-dashboard.timer\n[Unit]\nDescription=Run Xeepy Dashboard Daily\n\n[Timer]\nOnCalendar=*-*-* 09:00:00\nPersistent=true\n\n[Install]\nWantedBy=timers.target\n</code></pre> <pre><code># /etc/systemd/system/xeepy-dashboard.service\n[Unit]\nDescription=Xeepy Dashboard\n\n[Service]\nType=oneshot\nWorkingDirectory=/path/to/xeepy-dashboard\nExecStart=/path/to/venv/bin/python dashboard.py\nUser=youruser\n</code></pre>"},{"location":"getting-started/first-script/#step-5-extend-it","title":"Step 5: Extend It","text":"<p>Add more features to your dashboard:</p>"},{"location":"getting-started/first-script/#add-competitor-tracking","title":"Add Competitor Tracking","text":"<pre><code>async def track_competitors(x: Xeepy):\n    \"\"\"Track competitor growth and engagement\"\"\"\n    competitors = [\"competitor1\", \"competitor2\"]\n\n    for comp in competitors:\n        profile = await x.scrape.profile(comp)\n        tweets = await x.scrape.tweets(comp, limit=50)\n\n        print(f\"\\n\ud83d\udcca @{comp} Analysis:\")\n        print(f\"   Followers: {profile.followers_count:,}\")\n\n        avg_engagement = sum(t.likes for t in tweets) / len(tweets)\n        print(f\"   Avg engagement: {avg_engagement:.0f}\")\n</code></pre>"},{"location":"getting-started/first-script/#add-trending-topics","title":"Add Trending Topics","text":"<pre><code>async def analyze_trends(x: Xeepy):\n    \"\"\"Find trending topics in your niche\"\"\"\n    hashtags = [\"#buildinpublic\", \"#indiehackers\", \"#startup\"]\n\n    for tag in hashtags:\n        tweets = await x.scrape.hashtag(tag, limit=100)\n\n        # Extract common themes\n        # Identify viral tweets\n        # Suggest content angles\n</code></pre>"},{"location":"getting-started/first-script/#add-sentiment-tracking","title":"Add Sentiment Tracking","text":"<pre><code>async def track_sentiment(x: Xeepy, ai: ContentGenerator):\n    \"\"\"Track sentiment of mentions and replies\"\"\"\n    mentions = await x.scrape.mentions(config.username, limit=50)\n\n    for mention in mentions:\n        sentiment = await ai.analyze_sentiment(mention.text)\n\n        if sentiment.score &lt; -0.5:\n            print(f\"\u26a0\ufe0f Negative mention: {mention.text[:50]}...\")\n</code></pre>"},{"location":"getting-started/first-script/#whats-next","title":"What's Next?","text":"<p>You've built a complete automation dashboard! Next steps:</p> <ul> <li> <p>Growth Strategies</p> <p>Advanced techniques for follower growth</p> </li> <li> <p>AI-Powered Engagement</p> <p>Use AI to generate replies and analyze content</p> </li> <li> <p>Advanced Monitoring</p> <p>Real-time alerts and tracking</p> </li> <li> <p>Data Science Recipes</p> <p>Analyze your data with pandas and visualization</p> </li> </ul> <p>\ud83c\udf89 Congratulations! You've completed the Getting Started guide. You're now ready to explore the full power of Xeepy.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Get Xeepy up and running in under 2 minutes.</p>"},{"location":"getting-started/installation/#quick-install","title":"Quick Install","text":"pip (Recommended)pipx (Isolated)PoetryFrom Source <pre><code>pip install xeepy\n</code></pre> <pre><code>pipx install xeepy\n</code></pre> <pre><code>poetry add xeepy\n</code></pre> <pre><code>git clone https://github.com/xeepy/xeepy.git\ncd xeepy\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#install-browser","title":"Install Browser","text":"<p>Xeepy uses Playwright for browser automation. Install the browser:</p> <pre><code>playwright install chromium\n</code></pre> <p>First-time setup</p> <p>This downloads a Chromium browser (~150MB). It's a one-time operation.</p>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code># Check version\nxeepy --version\n\n# Run diagnostics\nxeepy doctor\n</code></pre> <p>Expected output: <pre><code>Xeepy v1.0.0\n\u2713 Python 3.10+\n\u2713 Playwright installed\n\u2713 Chromium browser ready\n\u2713 All systems operational\n</code></pre></p>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>Install additional features based on your needs:</p> AI FeaturesData ScienceNotificationsAll Features <pre><code>pip install \"xeepy[ai]\"\n</code></pre> <p>Includes: OpenAI, Anthropic, and Ollama integrations</p> <pre><code>pip install \"xeepy[data]\"\n</code></pre> <p>Includes: pandas, numpy, matplotlib for analytics</p> <pre><code>pip install \"xeepy[notify]\"\n</code></pre> <p>Includes: Discord, Telegram, email integrations</p> <pre><code>pip install \"xeepy[all]\"\n</code></pre> <p>Includes everything above</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":""},{"location":"getting-started/installation/#minimum-requirements","title":"Minimum Requirements","text":"Component Requirement Python 3.10 or higher RAM 512MB available Disk 500MB for browser OS Linux, macOS, Windows"},{"location":"getting-started/installation/#recommended-setup","title":"Recommended Setup","text":"Component Recommendation Python 3.11+ (faster async) RAM 2GB+ for heavy scraping Disk SSD for database caching Network Stable connection"},{"location":"getting-started/installation/#platform-specific-notes","title":"Platform-Specific Notes","text":"Linux macOS Windows Docker <pre><code># Install system dependencies (Ubuntu/Debian)\nsudo apt-get update\nsudo apt-get install -y python3-pip\n\n# Install Xeepy\npip3 install xeepy\nplaywright install chromium\n\n# Install browser dependencies\nplaywright install-deps chromium\n</code></pre> <p>Headless servers</p> <p>Xeepy works perfectly on headless Linux servers. Use <code>headless=True</code> (default) in your scripts.</p> <pre><code># Using Homebrew Python\nbrew install python@3.11\n\n# Install Xeepy\npip3 install xeepy\nplaywright install chromium\n</code></pre> <p>Apple Silicon</p> <p>Xeepy fully supports M1/M2/M3 Macs natively.</p> <pre><code># Install Python from python.org first\n\n# Install Xeepy\npip install xeepy\nplaywright install chromium\n</code></pre> <p>Windows Defender</p> <p>You may need to allow Playwright through Windows Defender.</p> <pre><code>FROM mcr.microsoft.com/playwright/python:v1.40.0-jammy\n\nRUN pip install xeepy[all]\n\n# Your script\nCOPY script.py .\nCMD [\"python\", \"script.py\"]\n</code></pre> <p>Or use our pre-built image:</p> <pre><code>docker pull xeepy/xeepy:latest\ndocker run -it xeepy/xeepy python your_script.py\n</code></pre>"},{"location":"getting-started/installation/#virtual-environment-setup","title":"Virtual Environment Setup","text":"<p>We recommend using a virtual environment:</p> <pre><code># Create virtual environment\npython -m venv xeepy-env\n\n# Activate it\nsource xeepy-env/bin/activate  # Linux/macOS\n# or\nxeepy-env\\Scripts\\activate     # Windows\n\n# Install Xeepy\npip install xeepy[all]\nplaywright install chromium\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For contributing or modifying Xeepy:</p> <pre><code># Clone the repository\ngit clone https://github.com/xeepy/xeepy.git\ncd xeepy\n\n# Create dev environment\npython -m venv .venv\nsource .venv/bin/activate\n\n# Install in editable mode with dev dependencies\npip install -e \".[dev]\"\n\n# Install pre-commit hooks\npre-commit install\n\n# Run tests\npytest\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":"Error: playwright not found <p>Make sure playwright is installed: <pre><code>pip install playwright\nplaywright install chromium\n</code></pre></p> Error: browser closed unexpectedly <p>Install browser dependencies: <pre><code># Linux only\nplaywright install-deps chromium\n</code></pre></p> Error: Permission denied <p>Use <code>--user</code> flag or a virtual environment: <pre><code>pip install --user xeepy\n</code></pre></p> Slow installation on Linux <p>The browser download can be slow. Use a mirror: <pre><code>PLAYWRIGHT_DOWNLOAD_HOST=https://playwright.azureedge.net playwright install chromium\n</code></pre></p> Import errors after installation <p>Ensure you're using the correct Python: <pre><code>which python  # Should show your venv\npython -c \"import xeepy; print(xeepy.__version__)\"\n</code></pre></p>"},{"location":"getting-started/installation/#upgrading","title":"Upgrading","text":"<pre><code># Upgrade Xeepy\npip install --upgrade xeepy\n\n# Upgrade browser (occasionally needed)\nplaywright install chromium\n</code></pre>"},{"location":"getting-started/installation/#uninstalling","title":"Uninstalling","text":"<pre><code># Remove Xeepy\npip uninstall xeepy\n\n# Remove browser (optional)\nrm -rf ~/.cache/ms-playwright\n</code></pre> <p>Next: Set up authentication \u2192</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get productive with Xeepy in 5 minutes. This guide covers the most common use cases with copy-paste examples.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<pre><code># Make sure you've completed installation\npip install xeepy[all]\nplaywright install chromium\n\n# And authenticated\nxeepy auth login\n</code></pre>"},{"location":"getting-started/quickstart/#your-first-scrape","title":"Your First Scrape","text":"<p>Let's scrape replies to a tweet:</p> <pre><code>import asyncio\nfrom xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        # Scrape replies to any tweet\n        replies = await x.scrape.replies(\n            \"https://x.com/elonmusk/status/1234567890\",\n            limit=100\n        )\n\n        # Print results\n        for reply in replies:\n            print(f\"@{reply.author.username}: {reply.text}\")\n\n        # Export to CSV\n        x.export.to_csv(replies, \"replies.csv\")\n        print(f\"\u2713 Saved {len(replies)} replies to replies.csv\")\n\nasyncio.run(main())\n</code></pre> <p>Run it: <pre><code>python scrape_replies.py\n</code></pre></p>"},{"location":"getting-started/quickstart/#5-essential-operations","title":"5 Essential Operations","text":""},{"location":"getting-started/quickstart/#1-scrape-any-data","title":"1. Scrape Any Data","text":"<pre><code>async with Xeepy() as x:\n    # Get user profile\n    profile = await x.scrape.profile(\"elonmusk\")\n    print(f\"{profile.name} has {profile.followers_count:,} followers\")\n\n    # Get user's tweets\n    tweets = await x.scrape.tweets(\"elonmusk\", limit=50)\n\n    # Get followers list\n    followers = await x.scrape.followers(\"elonmusk\", limit=500)\n\n    # Search tweets\n    results = await x.scrape.search(\"python programming\", limit=100)\n\n    # Get hashtag tweets\n    hashtag_tweets = await x.scrape.hashtag(\"#buildinpublic\", limit=100)\n</code></pre>"},{"location":"getting-started/quickstart/#2-follow-unfollow","title":"2. Follow &amp; Unfollow","text":"<pre><code>async with Xeepy() as x:\n    # Follow a user\n    await x.follow.user(\"naval\")\n\n    # Follow users by hashtag\n    await x.follow.by_hashtag(\"#startup\", limit=20)\n\n    # Unfollow non-followers (most popular feature!)\n    result = await x.unfollow.non_followers(\n        max_unfollows=50,\n        whitelist=[\"important_friend\", \"business_partner\"]\n    )\n    print(f\"Unfollowed {result.unfollowed_count} users\")\n</code></pre>"},{"location":"getting-started/quickstart/#3-monitor-your-account","title":"3. Monitor Your Account","text":"<pre><code>async with Xeepy() as x:\n    # Check who unfollowed you\n    report = await x.monitor.unfollowers()\n    print(f\"Lost {len(report.unfollowers)} followers since last check\")\n\n    # Track growth over time\n    growth = await x.analytics.track_growth(period=\"7d\")\n    print(f\"Net change: {growth.net_followers:+d} followers\")\n\n    # Find best posting times\n    best_times = await x.analytics.best_time_to_post()\n    print(f\"Best time to post: {best_times[0]}\")\n</code></pre>"},{"location":"getting-started/quickstart/#4-engage-with-content","title":"4. Engage with Content","text":"<pre><code>async with Xeepy() as x:\n    # Like a tweet\n    await x.engage.like(\"https://x.com/user/status/123\")\n\n    # Retweet\n    await x.engage.retweet(\"https://x.com/user/status/123\")\n\n    # Reply to a tweet\n    await x.engage.reply(\n        \"https://x.com/user/status/123\",\n        \"Great thread! \ud83d\udd25\"\n    )\n\n    # Auto-like tweets by keyword\n    await x.engage.auto_like(\n        keywords=[\"python\", \"opensource\"],\n        limit=20\n    )\n</code></pre>"},{"location":"getting-started/quickstart/#5-export-data","title":"5. Export Data","text":"<pre><code>async with Xeepy() as x:\n    data = await x.scrape.followers(\"username\", limit=1000)\n\n    # Export options\n    x.export.to_csv(data, \"followers.csv\")\n    x.export.to_json(data, \"followers.json\")\n    x.export.to_excel(data, \"followers.xlsx\")\n\n    # Export to database\n    await x.export.to_database(data, \"sqlite:///xeepy.db\")\n</code></pre>"},{"location":"getting-started/quickstart/#cli-quick-reference","title":"CLI Quick Reference","text":"<p>Don't want to write Python? Use the CLI:</p> <pre><code># Scrape replies\nxeepy scrape replies https://x.com/user/status/123 -o replies.csv\n\n# Get profile info\nxeepy scrape profile elonmusk\n\n# Unfollow non-followers (dry run first!)\nxeepy unfollow non-followers --dry-run\nxeepy unfollow non-followers --max 50\n\n# Check unfollowers\nxeepy monitor unfollowers\n\n# Search tweets\nxeepy scrape search \"python tips\" --limit 100 -o results.csv\n\n# Growth report\nxeepy analytics growth --period 30d\n</code></pre>"},{"location":"getting-started/quickstart/#real-world-examples","title":"Real-World Examples","text":""},{"location":"getting-started/quickstart/#example-1-research-thread-engagement","title":"Example 1: Research Thread Engagement","text":"<pre><code>\"\"\"Find which of your threads performed best\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def analyze_threads():\n    async with Xeepy() as x:\n        # Get your own tweets\n        my_tweets = await x.scrape.tweets(\"yourusername\", limit=200)\n\n        # Filter to threads (tweets with replies to self)\n        threads = [t for t in my_tweets if t.is_thread_start]\n\n        # Sort by engagement\n        threads.sort(key=lambda t: t.engagement_rate, reverse=True)\n\n        print(\"\ud83e\uddf5 Your Top Performing Threads:\")\n        for i, thread in enumerate(threads[:10], 1):\n            print(f\"{i}. {thread.text[:50]}...\")\n            print(f\"   \u2764\ufe0f {thread.likes} | \ud83d\udd01 {thread.retweets} | \ud83d\udcac {thread.replies}\")\n            print()\n\nasyncio.run(analyze_threads())\n</code></pre>"},{"location":"getting-started/quickstart/#example-2-competitor-analysis","title":"Example 2: Competitor Analysis","text":"<pre><code>\"\"\"Analyze what content works for competitors\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def analyze_competitor():\n    async with Xeepy() as x:\n        competitor = \"competitor_handle\"\n\n        # Get their recent tweets\n        tweets = await x.scrape.tweets(competitor, limit=100)\n\n        # Analyze engagement patterns\n        analysis = await x.analytics.engagement_analysis(tweets)\n\n        print(f\"\ud83d\udcca @{competitor} Analysis:\")\n        print(f\"Avg likes: {analysis.avg_likes:.0f}\")\n        print(f\"Avg retweets: {analysis.avg_retweets:.0f}\")\n        print(f\"Best posting day: {analysis.best_day}\")\n        print(f\"Best posting hour: {analysis.best_hour}:00\")\n        print(f\"Top hashtags: {', '.join(analysis.top_hashtags[:5])}\")\n\n        # Their best performing tweet\n        top_tweet = max(tweets, key=lambda t: t.likes)\n        print(f\"\\n\ud83c\udfc6 Top Tweet ({top_tweet.likes:,} likes):\")\n        print(f\"   {top_tweet.text[:100]}...\")\n\nasyncio.run(analyze_competitor())\n</code></pre>"},{"location":"getting-started/quickstart/#example-3-daily-automation-script","title":"Example 3: Daily Automation Script","text":"<pre><code>\"\"\"Daily automation: clean up follows, engage, report\"\"\"\nimport asyncio\nfrom xeepy import Xeepy\n\nasync def daily_routine():\n    async with Xeepy() as x:\n        print(\"\ud83c\udf05 Starting daily routine...\")\n\n        # 1. Check for new unfollowers\n        unfollower_report = await x.monitor.unfollowers()\n        print(f\"\ud83d\udcc9 {len(unfollower_report.unfollowers)} new unfollowers\")\n\n        # 2. Unfollow non-followers (limit 25/day to be safe)\n        unfollow_result = await x.unfollow.non_followers(max_unfollows=25)\n        print(f\"\ud83d\udc4b Unfollowed {unfollow_result.unfollowed_count} non-followers\")\n\n        # 3. Follow people from target hashtag\n        await x.follow.by_hashtag(\"#buildinpublic\", limit=10)\n        print(f\"\u2795 Followed 10 people from #buildinpublic\")\n\n        # 4. Like tweets in your niche\n        liked = await x.engage.auto_like(\n            keywords=[\"indie hacker\", \"saas\", \"startup\"],\n            limit=20\n        )\n        print(f\"\u2764\ufe0f Liked {len(liked)} tweets\")\n\n        # 5. Generate growth report\n        growth = await x.analytics.track_growth(period=\"24h\")\n        print(f\"\\n\ud83d\udcca 24h Summary:\")\n        print(f\"   Followers: {growth.followers_count:,} ({growth.net_followers:+d})\")\n        print(f\"   Following: {growth.following_count:,}\")\n\n        print(\"\\n\u2705 Daily routine complete!\")\n\nasyncio.run(daily_routine())\n</code></pre>"},{"location":"getting-started/quickstart/#configuration","title":"Configuration","text":"<p>Create <code>xeepy.toml</code> in your project:</p> <pre><code>[xeepy]\n# Browser settings\nheadless = true\ntimeout = 30000\n\n# Rate limiting (requests per minute)\nrate_limit = 20\n\n# Default export format\nexport_format = \"csv\"\n\n# Session file\nsession_file = \"~/.config/xeepy/session.json\"\n\n[xeepy.proxy]\n# Optional: Use proxy\nenabled = false\nurl = \"http://user:pass@proxy:8080\"\n\n[xeepy.notifications]\n# Get notified on errors\ndiscord_webhook = \"https://discord.com/api/webhooks/...\"\n</code></pre>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<p>Now that you've got the basics, explore:</p> <ul> <li> <p>Scraping Guide</p> <p>Master all scraping features: replies, followers, search, and more</p> </li> <li> <p>Growth Cookbook</p> <p>Proven strategies for growing your X presence</p> </li> <li> <p>AI Features</p> <p>Generate replies, analyze sentiment, detect bots</p> </li> <li> <p>Analytics</p> <p>Deep insights into your account performance</p> </li> </ul> <p>Next: Build Your First Complete Script \u2192</p>"},{"location":"guides/","title":"Guides","text":"<p>Comprehensive guides for every Xeepy feature.</p> <ul> <li> <p> Scraping</p> <p>Extract data from X/Twitter without API limits</p> <p> Scraping Guide</p> </li> <li> <p> Actions</p> <p>Follow, unfollow, like, retweet, and engage</p> <p> Actions Guide</p> </li> <li> <p> Monitoring</p> <p>Track followers, keywords, and account changes</p> <p> Monitoring Guide</p> </li> <li> <p> Analytics</p> <p>Growth metrics, engagement analysis, and insights</p> <p> Analytics Guide</p> </li> <li> <p> AI Features</p> <p>Content generation, sentiment analysis, and more</p> <p> AI Guide</p> </li> <li> <p> Notifications</p> <p>Discord, Telegram, email, and webhook alerts</p> <p> Notifications Guide</p> </li> <li> <p> Data Export</p> <p>CSV, JSON, Excel, and database exports</p> <p> Export Guide</p> </li> </ul>"},{"location":"guides/#quick-reference","title":"Quick Reference","text":""},{"location":"guides/#most-common-tasks","title":"Most Common Tasks","text":"Task Guide Code Scrape tweet replies Replies <code>await x.scrape.replies(url)</code> Unfollow non-followers Unfollow <code>await x.unfollow.non_followers()</code> Follow by keyword Follow <code>await x.follow.by_keyword([...])</code> Track unfollowers Monitoring <code>await x.monitor.unfollowers()</code> Auto-like tweets Engagement <code>await x.engage.auto_like([...])</code> Generate AI replies AI <code>await ai.generate_reply(text)</code>"},{"location":"guides/#learning-path","title":"Learning Path","text":"BeginnerIntermediateAdvanced <ol> <li>Installation</li> <li>Quick Start</li> <li>Scraping Basics</li> <li>Follow/Unfollow</li> </ol> <ol> <li>Rate Limiting</li> <li>Monitoring</li> <li>Analytics</li> <li>Notifications</li> </ol> <ol> <li>AI Features</li> <li>Mass Operations</li> <li>Multi-Account</li> <li>Custom Scrapers</li> </ol>"},{"location":"guides/actions/","title":"Follow &amp; Unfollow Actions","text":"<p>Xeepy provides smart, safe follow and unfollow operations with built-in rate limiting and protection features.</p>"},{"location":"guides/actions/#overview","title":"Overview","text":"<ul> <li> <p> Follow Operations</p> <p>Follow users by username, hashtag, or list</p> </li> <li> <p> Unfollow Operations</p> <p>Unfollow non-followers, inactive users, or everyone</p> </li> <li> <p> Safety Features</p> <p>Whitelist, dry run, daily limits</p> </li> </ul>"},{"location":"guides/actions/#follow-operations","title":"Follow Operations","text":""},{"location":"guides/actions/#follow-a-single-user","title":"Follow a Single User","text":"<pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    # Follow by username\n    await x.follow.user(\"naval\")\n\n    # Follow multiple users\n    users = [\"paulg\", \"sama\", \"naval\"]\n    for user in users:\n        await x.follow.user(user)\n        print(f\"\u2713 Followed @{user}\")\n</code></pre>"},{"location":"guides/actions/#follow-by-hashtag","title":"Follow by Hashtag","text":"<p>Find and follow users who tweet about specific topics:</p> <pre><code>async with Xeepy() as x:\n    # Follow people tweeting about #buildinpublic\n    result = await x.follow.by_hashtag(\n        \"#buildinpublic\",\n        limit=20,               # Max users to follow\n        min_followers=100,      # Min followers to qualify\n        max_followers=50000,    # Max followers (avoid big accounts)\n        must_have_bio=True,     # Skip accounts without bio\n        exclude_verified=False  # Include verified accounts\n    )\n\n    print(f\"Followed {result.followed_count} users\")\n    for user in result.followed_users:\n        print(f\"  \u2713 @{user.username} ({user.followers_count} followers)\")\n</code></pre>"},{"location":"guides/actions/#follow-from-search","title":"Follow from Search","text":"<pre><code>async with Xeepy() as x:\n    # Follow users from search results\n    result = await x.follow.from_search(\n        query=\"python developer\",\n        limit=15,\n        filters={\n            \"min_followers\": 500,\n            \"has_website\": True,\n            \"account_age_days\": 365  # At least 1 year old\n        }\n    )\n</code></pre>"},{"location":"guides/actions/#follow-followers-of-account","title":"Follow Followers of Account","text":"<pre><code>async with Xeepy() as x:\n    # Follow followers of a similar account\n    result = await x.follow.followers_of(\n        \"competitor_account\",\n        limit=30,\n        skip_if_following=True,  # Don't follow if already following\n        filters={\n            \"min_followers\": 100,\n            \"active_in_days\": 30  # Active in last 30 days\n        }\n    )\n</code></pre>"},{"location":"guides/actions/#follow-list-members","title":"Follow List Members","text":"<pre><code>async with Xeepy() as x:\n    # Follow members of a curated list\n    result = await x.follow.list_members(\n        \"username/list-name\",\n        limit=20\n    )\n</code></pre>"},{"location":"guides/actions/#unfollow-operations","title":"Unfollow Operations","text":""},{"location":"guides/actions/#unfollow-non-followers","title":"Unfollow Non-Followers","text":"<p>The most popular feature\u2014clean up accounts that don't follow you back:</p> <pre><code>async with Xeepy() as x:\n    # Preview first (recommended!)\n    preview = await x.unfollow.non_followers(dry_run=True)\n    print(f\"Would unfollow {len(preview.would_unfollow)} users\")\n\n    # Actually unfollow\n    result = await x.unfollow.non_followers(\n        max_unfollows=50,  # Limit per run\n        whitelist=[\"friend1\", \"friend2\"],  # Never unfollow these\n    )\n\n    print(f\"Unfollowed {result.unfollowed_count} non-followers\")\n</code></pre>"},{"location":"guides/actions/#unfollow-with-whitelist-file","title":"Unfollow with Whitelist File","text":"<pre><code>async with Xeepy() as x:\n    result = await x.unfollow.non_followers(\n        max_unfollows=100,\n        whitelist_file=\"whitelist.txt\"  # One username per line\n    )\n</code></pre> <p>whitelist.txt: <pre><code>naval\npaulg\nbestfriend\nimportant_client\n</code></pre></p>"},{"location":"guides/actions/#smart-unfollow","title":"Smart Unfollow","text":"<p>Unfollow based on multiple criteria:</p> <pre><code>async with Xeepy() as x:\n    result = await x.unfollow.smart(\n        max_unfollows=50,\n        criteria={\n            \"not_following_back\": True,     # Don't follow me\n            \"inactive_days\": 90,            # No tweets in 90 days\n            \"no_profile_pic\": True,         # Default profile pic\n            \"no_bio\": True,                 # Empty bio\n            \"follower_ratio_below\": 0.1,    # Very low follower ratio\n        },\n        whitelist_file=\"whitelist.txt\"\n    )\n</code></pre>"},{"location":"guides/actions/#unfollow-everyone","title":"Unfollow Everyone","text":"<p>Nuclear option\u2014unfollow all accounts:</p> <pre><code>async with Xeepy() as x:\n    # ALWAYS dry run first!\n    preview = await x.unfollow.everyone(dry_run=True)\n    print(f\"Would unfollow {preview.total_count} users\")\n\n    # Confirm before proceeding\n    if input(\"Type 'yes' to confirm: \") == \"yes\":\n        result = await x.unfollow.everyone(\n            whitelist_file=\"whitelist.txt\",\n            batch_size=50,        # Unfollow in batches\n            delay_between=300     # 5 min between batches\n        )\n</code></pre>"},{"location":"guides/actions/#unfollow-inactive-accounts","title":"Unfollow Inactive Accounts","text":"<pre><code>async with Xeepy() as x:\n    result = await x.unfollow.inactive(\n        inactive_days=180,  # No tweets in 6 months\n        max_unfollows=30,\n        whitelist_file=\"whitelist.txt\"\n    )\n</code></pre>"},{"location":"guides/actions/#unfollow-by-criteria","title":"Unfollow by Criteria","text":"<pre><code>async with Xeepy() as x:\n    # Unfollow based on custom filters\n    result = await x.unfollow.by_criteria(\n        criteria={\n            \"followers_below\": 50,      # Very small accounts\n            \"following_above\": 5000,    # Follow spam accounts\n            \"tweets_below\": 10,         # Low activity\n        },\n        max_unfollows=25\n    )\n</code></pre>"},{"location":"guides/actions/#safety-features","title":"Safety Features","text":""},{"location":"guides/actions/#dry-run-mode","title":"Dry Run Mode","text":"<p>Always preview before executing:</p> <pre><code>async with Xeepy() as x:\n    # See what would happen without doing it\n    result = await x.unfollow.non_followers(dry_run=True)\n\n    print(\"Would unfollow:\")\n    for user in result.would_unfollow:\n        print(f\"  - @{user}\")\n\n    # Then run for real if satisfied\n    await x.unfollow.non_followers(dry_run=False)\n</code></pre>"},{"location":"guides/actions/#whitelist-protection","title":"Whitelist Protection","text":"<pre><code>async with Xeepy() as x:\n    # Method 1: Inline list\n    result = await x.unfollow.non_followers(\n        whitelist=[\"vip1\", \"vip2\", \"friend\"]\n    )\n\n    # Method 2: File (better for many users)\n    result = await x.unfollow.non_followers(\n        whitelist_file=\"whitelist.txt\"\n    )\n\n    # Method 3: Pattern matching\n    result = await x.unfollow.non_followers(\n        whitelist_patterns=[\".*_official\", \"team_.*\"]\n    )\n</code></pre>"},{"location":"guides/actions/#daily-limits","title":"Daily Limits","text":"<pre><code>async with Xeepy() as x:\n    # Configure safety limits\n    x.config.safety.max_follows_per_day = 50\n    x.config.safety.max_unfollows_per_day = 100\n\n    # Operations will stop when limit reached\n    result = await x.unfollow.non_followers(max_unfollows=200)\n    # Will only do 100 due to daily limit\n</code></pre>"},{"location":"guides/actions/#rate-limiting","title":"Rate Limiting","text":"<pre><code>async with Xeepy() as x:\n    # Default: 30 follows/hour, 50 unfollows/hour\n    # Customize if needed\n    x.config.rate_limit.follows_per_hour = 20\n    x.config.rate_limit.unfollows_per_hour = 40\n</code></pre>"},{"location":"guides/actions/#cli-commands","title":"CLI Commands","text":""},{"location":"guides/actions/#follow-commands","title":"Follow Commands","text":"<pre><code># Follow a user\nxeepy follow user naval\n\n# Follow by hashtag\nxeepy follow hashtag \"#buildinpublic\" --limit 20 --min-followers 100\n\n# Follow from search\nxeepy follow search \"python developer\" --limit 15\n\n# Follow followers of account\nxeepy follow followers-of elonmusk --limit 30\n</code></pre>"},{"location":"guides/actions/#unfollow-commands","title":"Unfollow Commands","text":"<pre><code># Preview non-followers (dry run)\nxeepy unfollow non-followers --dry-run\n\n# Unfollow non-followers\nxeepy unfollow non-followers --max 50 --whitelist-file whitelist.txt\n\n# Unfollow inactive\nxeepy unfollow inactive --days 180 --max 30\n\n# Smart unfollow\nxeepy unfollow smart --criteria inactive,no-bio,not-following --max 25\n\n# Nuclear option (be careful!)\nxeepy unfollow everyone --whitelist-file whitelist.txt --confirm\n</code></pre>"},{"location":"guides/actions/#scheduling-followunfollow","title":"Scheduling Follow/Unfollow","text":""},{"location":"guides/actions/#daily-cleanup-script","title":"Daily Cleanup Script","text":"<pre><code>import asyncio\nfrom datetime import datetime\nfrom xeepy import Xeepy\n\nasync def daily_cleanup():\n    \"\"\"Run daily to maintain healthy following list\"\"\"\n    async with Xeepy() as x:\n        print(f\"\ud83e\uddf9 Daily cleanup - {datetime.now()}\")\n\n        # 1. Unfollow non-followers (conservative)\n        unfollow_result = await x.unfollow.non_followers(\n            max_unfollows=25,\n            whitelist_file=\"whitelist.txt\"\n        )\n        print(f\"   Unfollowed {unfollow_result.unfollowed_count} non-followers\")\n\n        # 2. Follow people from target hashtag\n        follow_result = await x.follow.by_hashtag(\n            \"#buildinpublic\",\n            limit=15,\n            min_followers=100\n        )\n        print(f\"   Followed {follow_result.followed_count} new users\")\n\n        # 3. Log results\n        print(f\"\u2713 Net change: {follow_result.followed_count - unfollow_result.unfollowed_count:+d}\")\n\nasyncio.run(daily_cleanup())\n</code></pre>"},{"location":"guides/actions/#growth-campaign","title":"Growth Campaign","text":"<pre><code>async def growth_campaign(target_hashtags: list, days: int = 7):\n    \"\"\"Run a multi-day growth campaign\"\"\"\n    from datetime import datetime, timedelta\n\n    async with Xeepy() as x:\n        end_date = datetime.now() + timedelta(days=days)\n\n        while datetime.now() &lt; end_date:\n            # Rotate through hashtags\n            for hashtag in target_hashtags:\n                await x.follow.by_hashtag(\n                    hashtag,\n                    limit=10,\n                    min_followers=100,\n                    max_followers=10000\n                )\n\n            # Clean up non-followers\n            await x.unfollow.non_followers(max_unfollows=20)\n\n            # Wait before next cycle (respect rate limits)\n            await asyncio.sleep(3600)  # 1 hour\n\n# Run campaign\nasyncio.run(growth_campaign(\n    target_hashtags=[\"#startup\", \"#indiehackers\", \"#buildinpublic\"],\n    days=7\n))\n</code></pre>"},{"location":"guides/actions/#best-practices","title":"Best Practices","text":"<p>Golden Rules</p> <ol> <li>Always dry-run first - Preview before executing</li> <li>Use whitelists - Protect important connections</li> <li>Start conservative - Begin with low limits</li> <li>Monitor results - Check for unexpected unfollows</li> <li>Respect rate limits - Don't disable protections</li> </ol> <p>Avoid Account Flags</p> <ul> <li>Don't follow/unfollow too fast</li> <li>Don't follow then immediately unfollow</li> <li>Mix automated actions with organic activity</li> <li>Keep follow ratios reasonable (&lt; 1.5 following/followers)</li> </ul>"},{"location":"guides/actions/#troubleshooting","title":"Troubleshooting","text":"Why didn't it unfollow everyone I expected? <p>Check: - Whitelist matches (exact username) - Daily limits reached - Rate limits hit - Protected/private accounts</p> Follow operation failed <p>Common causes: - Account is private - You're already following - Account blocked you - Rate limit reached</p> How do I undo an unfollow? <p>The result object contains unfollowed usernames: <pre><code>result = await x.unfollow.non_followers()\n# Re-follow if needed\nfor user in result.unfollowed_users[:5]:\n    await x.follow.user(user)\n</code></pre></p>"},{"location":"guides/actions/engagement/","title":"Engagement Automation","text":"<p>Automate likes, retweets, comments, and bookmarks to grow your presence.</p>"},{"location":"guides/actions/engagement/#auto-like","title":"Auto Like","text":"<p>Automatically like tweets matching your criteria:</p> PythonCLI <pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    result = await x.engage.auto_like(\n        keywords=[\"python\", \"programming\", \"tech\"],\n        max_likes=50,\n        filters={\n            \"min_likes\": 10,        # Already has some engagement\n            \"max_likes\": 1000,      # Not too viral (author won't notice)\n            \"min_followers\": 100,   # Real accounts\n            \"max_followers\": 50000, # They'll notice your like\n            \"exclude_replies\": True,\n            \"exclude_retweets\": True,\n        },\n        delay_range=(2, 5),\n        on_like=lambda t: print(f\"\u2764\ufe0f Liked: {t.text[:50]}...\")\n    )\n\n    print(f\"\\n\u2705 Liked {result.success_count} tweets\")\n</code></pre> <pre><code>xeepy engage like --keywords \"python,programming\" --max 50\n</code></pre>"},{"location":"guides/actions/engagement/#smart-targeting","title":"Smart Targeting","text":"<p>Target tweets that maximize your visibility:</p> <pre><code># Like tweets from accounts likely to follow back\nresult = await x.engage.auto_like(\n    keywords=[\"python developer\", \"learning to code\"],\n    filters={\n        \"min_followers\": 100,\n        \"max_followers\": 5000,    # Small accounts notice likes\n        \"min_likes\": 5,\n        \"max_likes\": 100,         # Not viral yet\n        \"account_age_days\": 30,   # Established accounts\n        \"has_bio\": True,\n    },\n    max_likes=30,\n)\n\n# Like tweets with questions (high engagement opportunity)\nresult = await x.engage.auto_like(\n    keywords=[\"how to python\", \"python help\", \"python question\"],\n    search_type=\"latest\",  # Fresh tweets\n    filters={\n        \"max_likes\": 10,   # Unanswered questions\n        \"max_age_hours\": 2, # Recent\n    },\n    max_likes=20,\n)\n</code></pre>"},{"location":"guides/actions/engagement/#engagement-chains","title":"Engagement Chains","text":"<p>Like multiple tweets from same author for better visibility:</p> <pre><code>async def engagement_chain(username, count=5):\n    \"\"\"Like multiple tweets from one user\"\"\"\n    async with Xeepy() as x:\n        tweets = await x.scrape.tweets(username, limit=count)\n\n        for tweet in tweets:\n            await x.engage.like(tweet.url)\n            await asyncio.sleep(random.uniform(3, 8))\n\n        print(f\"Liked {count} tweets from @{username}\")\n</code></pre>"},{"location":"guides/actions/engagement/#auto-retweet","title":"Auto Retweet","text":"<p>Amplify content while building relationships:</p> <pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    result = await x.engage.auto_retweet(\n        keywords=[\"python tips\", \"coding advice\"],\n        max_retweets=10,  # Be conservative with RTs\n        filters={\n            \"min_likes\": 100,      # Quality content\n            \"min_followers\": 1000,  # Credible source\n            \"exclude_replies\": True,\n        },\n        add_comment=False,  # Quote RT with comment (see below)\n    )\n</code></pre>"},{"location":"guides/actions/engagement/#quote-retweets-with-ai","title":"Quote Retweets with AI","text":"<p>Add intelligent comments to your retweets:</p> <pre><code>from xeepy import Xeepy\nfrom xeepy.ai import ContentGenerator\n\nasync with Xeepy() as x:\n    ai = ContentGenerator(provider=\"openai\", model=\"gpt-4o-mini\")\n\n    # Find tweets to quote\n    tweets = await x.scrape.search(\n        \"python tutorial\",\n        min_likes=50,\n        limit=10\n    )\n\n    for tweet in tweets[:5]:\n        # Generate contextual comment\n        comment = await ai.generate_quote_comment(\n            tweet_text=tweet.text,\n            style=\"insightful\",\n            max_length=200\n        )\n\n        await x.engage.quote_retweet(\n            tweet_url=tweet.url,\n            comment=comment\n        )\n\n        print(f\"Quoted: {tweet.text[:50]}...\")\n        print(f\"Comment: {comment}\\n\")\n</code></pre>"},{"location":"guides/actions/engagement/#auto-comment","title":"Auto Comment","text":"<p>Reply to tweets automatically with AI-generated comments:</p> <pre><code>from xeepy import Xeepy\nfrom xeepy.ai import ContentGenerator\n\nasync with Xeepy() as x:\n    ai = ContentGenerator(\n        provider=\"openai\",\n        model=\"gpt-4o-mini\",\n        temperature=0.7\n    )\n\n    # Find tweets to reply to\n    tweets = await x.scrape.search(\n        \"learning python\",\n        search_type=\"latest\",\n        limit=20\n    )\n\n    for tweet in tweets[:10]:\n        # Generate helpful reply\n        reply = await ai.generate_reply(\n            tweet_text=tweet.text,\n            author_bio=tweet.author.bio,\n            style=\"helpful\",\n            tone=\"friendly\",\n            max_length=280,\n            include_question=True,  # Encourage response\n        )\n\n        # Optional: Review before posting\n        print(f\"\\nTweet: {tweet.text[:100]}...\")\n        print(f\"Reply: {reply}\")\n\n        if input(\"Post? (y/n): \").lower() == 'y':\n            await x.engage.reply(tweet.url, reply)\n            print(\"\u2705 Posted!\")\n</code></pre>"},{"location":"guides/actions/engagement/#reply-templates","title":"Reply Templates","text":"<p>Use templates for consistent engagement:</p> <pre><code>from xeepy.engage import ReplyTemplates\n\ntemplates = ReplyTemplates(\n    greetings=[\"Great point!\", \"Love this!\", \"So true!\"],\n    questions=[\n        \"What's your experience with {topic}?\",\n        \"Have you tried {alternative}?\",\n        \"What would you add to this?\",\n    ],\n    value_adds=[\n        \"I'd also add that {insight}\",\n        \"Another tip: {tip}\",\n        \"Building on this: {addition}\",\n    ],\n    closings=[\n        \"Thanks for sharing! \ud83d\ude4f\",\n        \"Following for more! \ud83d\udc40\",\n        \"Bookmarked! \ud83d\udccc\",\n    ]\n)\n\nasync with Xeepy() as x:\n    result = await x.engage.auto_reply(\n        keywords=[\"python tips\"],\n        templates=templates,\n        max_replies=10,\n        require_review=True,  # Show before posting\n    )\n</code></pre>"},{"location":"guides/actions/engagement/#conversation-threading","title":"Conversation Threading","text":"<p>Engage in multi-turn conversations:</p> <pre><code>async def conversation_bot():\n    \"\"\"Monitor and respond to replies to your tweets\"\"\"\n\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\")\n\n        # Get replies to your tweets\n        my_tweets = await x.scrape.tweets(\"me\", limit=10)\n\n        for tweet in my_tweets:\n            replies = await x.scrape.replies(tweet.url, limit=5)\n\n            for reply in replies:\n                # Skip if already replied\n                if reply.author.username == \"me\":\n                    continue\n\n                # Generate response\n                response = await ai.generate_reply(\n                    tweet_text=reply.text,\n                    context=tweet.text,  # Original tweet\n                    style=\"conversational\",\n                )\n\n                print(f\"Reply from @{reply.author.username}: {reply.text}\")\n                print(f\"Response: {response}\")\n\n                if input(\"Send? (y/n): \").lower() == 'y':\n                    await x.engage.reply(reply.url, response)\n</code></pre>"},{"location":"guides/actions/engagement/#bookmarks","title":"Bookmarks","text":"<p>Organize content with smart bookmarking:</p> <pre><code>async with Xeepy() as x:\n    # Auto-bookmark valuable content\n    result = await x.engage.auto_bookmark(\n        keywords=[\"python tutorial\", \"coding tips\", \"tech insights\"],\n        filters={\n            \"min_likes\": 500,\n            \"min_bookmarks\": 50,  # Already bookmarked by others\n        },\n        max_bookmarks=20,\n    )\n\n    # Get bookmarked tweets\n    bookmarks = await x.scrape.bookmarks(limit=100)\n\n    # Export for reference\n    x.export.to_csv(bookmarks, \"my_bookmarks.csv\")\n</code></pre>"},{"location":"guides/actions/engagement/#bookmark-folders-conceptual","title":"Bookmark Folders (Conceptual)","text":"<p>Organize bookmarks by category:</p> <pre><code>from xeepy.storage import BookmarkManager\n\nbm = BookmarkManager(\"bookmarks.db\")\n\n# Categorize bookmarks\ncategories = {\n    \"tutorials\": [\"python tutorial\", \"how to\", \"guide\"],\n    \"tools\": [\"tool\", \"library\", \"framework\"],\n    \"inspiration\": [\"thread\", \"story\", \"journey\"],\n    \"save_for_later\": [\"interesting\", \"read later\"],\n}\n\nasync with Xeepy() as x:\n    bookmarks = await x.scrape.bookmarks(limit=200)\n\n    for bookmark in bookmarks:\n        for category, keywords in categories.items():\n            if any(kw in bookmark.text.lower() for kw in keywords):\n                bm.add_to_folder(bookmark.id, category)\n                break\n\n    # Get by folder\n    tutorials = bm.get_folder(\"tutorials\")\n    print(f\"Tutorial bookmarks: {len(tutorials)}\")\n</code></pre>"},{"location":"guides/actions/engagement/#engagement-pods","title":"Engagement Pods","text":"<p>Coordinate engagement with a group (use ethically!):</p> <pre><code>from xeepy import Xeepy\nimport asyncio\n\nclass EngagementPod:\n    \"\"\"Coordinate mutual engagement among members\"\"\"\n\n    def __init__(self, members: list):\n        self.members = members\n\n    async def engage_with_member(self, x, username):\n        \"\"\"Like recent tweets from pod member\"\"\"\n        tweets = await x.scrape.tweets(username, limit=3)\n\n        for tweet in tweets:\n            await x.engage.like(tweet.url)\n            await asyncio.sleep(2)\n\n    async def run_pod_session(self):\n        \"\"\"Engage with all pod members\"\"\"\n        async with Xeepy() as x:\n            for member in self.members:\n                if member != \"my_username\":  # Skip self\n                    await self.engage_with_member(x, member)\n                    print(f\"\u2705 Engaged with @{member}\")\n\n# Usage\npod = EngagementPod([\"friend1\", \"friend2\", \"friend3\"])\nasyncio.run(pod.run_pod_session())\n</code></pre>"},{"location":"guides/actions/engagement/#engagement-analytics","title":"Engagement Analytics","text":"<p>Track your engagement effectiveness:</p> <pre><code>from xeepy.storage import EngagementTracker\n\ntracker = EngagementTracker(\"engagement.db\")\n\n# Log engagement\ntracker.record_like(tweet_id=\"123\", author=\"username\")\ntracker.record_reply(tweet_id=\"456\", author=\"username\", reply_text=\"...\")\n\n# Get stats\nstats = tracker.get_stats()\nprint(f\"Total likes: {stats.total_likes}\")\nprint(f\"Total replies: {stats.total_replies}\")\nprint(f\"Reply rate: {stats.reply_rate}%\")  # % of likes that turned into convos\n\n# Engagement effectiveness\neffectiveness = tracker.get_effectiveness()\nprint(f\"\\nEngagement Effectiveness:\")\nprint(f\"  Likes \u2192 Follows: {effectiveness.likes_to_follows}%\")\nprint(f\"  Replies \u2192 Follows: {effectiveness.replies_to_follows}%\")\nprint(f\"  Best keywords: {effectiveness.best_keywords}\")\n</code></pre>"},{"location":"guides/actions/engagement/#scheduled-engagement","title":"Scheduled Engagement","text":"<p>Run engagement at optimal times:</p> <pre><code>from xeepy import Xeepy\nfrom xeepy.scheduling import EngagementScheduler\n\nscheduler = EngagementScheduler(\n    active_hours=(9, 22),  # 9 AM to 10 PM\n    peak_hours=[(12, 14), (18, 21)],  # Lunch and evening\n)\n\nasync def scheduled_engagement():\n    async with Xeepy() as x:\n        schedule = scheduler.get_today_schedule(\n            likes_per_day=50,\n            replies_per_day=10,\n        )\n\n        for task in schedule:\n            if task.type == \"like\":\n                await x.engage.auto_like(\n                    keywords=task.keywords,\n                    max_likes=task.count,\n                )\n            elif task.type == \"reply\":\n                await x.engage.auto_reply(\n                    keywords=task.keywords,\n                    max_replies=task.count,\n                )\n\n            print(f\"Completed: {task.type} x{task.count}\")\n            await asyncio.sleep(task.delay_until_next)\n</code></pre>"},{"location":"guides/actions/engagement/#best-practices","title":"Best Practices","text":"<p>Do's</p> <ul> <li>\u2705 Engage genuinely - add value with comments</li> <li>\u2705 Use AI to generate helpful, contextual replies</li> <li>\u2705 Target smaller accounts (they notice engagement)</li> <li>\u2705 Space out engagement naturally</li> <li>\u2705 Track what works and optimize</li> </ul> <p>Don'ts</p> <ul> <li>\u274c Mass-like without reading tweets</li> <li>\u274c Post generic \"Great post!\" comments</li> <li>\u274c Engage more than 100 times/day</li> <li>\u274c Reply to controversial topics</li> <li>\u274c Spam the same message repeatedly</li> </ul>"},{"location":"guides/actions/engagement/#safety-settings","title":"Safety Settings","text":"<pre><code># Conservative engagement settings\nconfig = {\n    \"daily_likes\": 50,\n    \"daily_retweets\": 10,\n    \"daily_replies\": 10,\n    \"min_delay\": 3,\n    \"max_delay\": 10,\n    \"require_review\": True,  # Review AI comments before posting\n    \"blocked_keywords\": [\"politics\", \"controversial\", \"nsfw\"],\n}\n</code></pre>"},{"location":"guides/actions/engagement/#next-steps","title":"Next Steps","text":"<p> Mass Operations - Scale your actions</p> <p> AI Replies - Generate better content</p> <p> Analytics - Measure effectiveness</p>"},{"location":"guides/actions/follow/","title":"Follow Users Guide","text":"<p>Master the art of strategic following to grow your X/Twitter presence.</p>"},{"location":"guides/actions/follow/#overview","title":"Overview","text":"<p>Xeepy provides 6 powerful ways to follow users:</p> Method Use Case Follow-back Rate <code>follow_user</code> Single user N/A <code>follow_by_keyword</code> Niche targeting 15-25% <code>follow_by_hashtag</code> Community targeting 20-30% <code>follow_followers</code> Competitor's audience 25-35% <code>follow_engagers</code> High-intent users 30-40% <code>auto_follow</code> Automated growth Varies"},{"location":"guides/actions/follow/#single-user-follow","title":"Single User Follow","text":"<p>The simplest operation - follow one user:</p> PythonCLI <pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    result = await x.follow.user(\"elonmusk\")\n\n    if result.success:\n        print(f\"\u2713 Now following @elonmusk\")\n    else:\n        print(f\"\u2717 Failed: {result.error}\")\n</code></pre> <pre><code>xeepy follow user elonmusk\n</code></pre>"},{"location":"guides/actions/follow/#batch-follow","title":"Batch Follow","text":"<p>Follow multiple users efficiently:</p> <pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    usernames = [\"user1\", \"user2\", \"user3\", \"user4\", \"user5\"]\n\n    result = await x.follow.users(\n        usernames=usernames,\n        delay_range=(3, 8),  # Random 3-8 second delay\n        skip_if_following=True,\n        on_follow=lambda u, s: print(f\"{'\u2713' if s else '\u2717'} @{u}\")\n    )\n\n    print(f\"\\nResults:\")\n    print(f\"  Followed: {result.success_count}\")\n    print(f\"  Failed: {result.failed_count}\")\n    print(f\"  Already following: {result.skipped_count}\")\n</code></pre>"},{"location":"guides/actions/follow/#follow-by-keyword","title":"Follow by Keyword","text":"<p>Find and follow users tweeting about topics you care about:</p> <pre><code>from xeepy import Xeepy\nfrom xeepy.actions.base import FollowFilters\n\nasync with Xeepy() as x:\n    # Quality filters are CRITICAL for good follow-back rates\n    filters = FollowFilters(\n        min_followers=100,        # Not too small\n        max_followers=50000,      # Not too big (won't notice you)\n        min_tweets=50,            # Active users\n        must_have_bio=True,       # Real people have bios\n        must_have_profile_pic=True,\n        min_account_age_days=30,  # Not brand new\n        exclude_default_pic=True,\n        keywords_in_bio=[\"developer\", \"engineer\", \"founder\"],  # Optional\n    )\n\n    result = await x.follow.by_keyword(\n        keywords=[\"python programming\", \"machine learning\", \"startup\"],\n        max_follows=50,\n        search_type=\"users\",  # or \"tweets\" to find tweet authors\n        filters=filters,\n        dry_run=False,\n        on_progress=lambda c, t, m: print(f\"[{c}/{t}] {m}\")\n    )\n\n    print(f\"\\n\ud83c\udfaf Followed {result.success_count} users matching your niche\")\n</code></pre>"},{"location":"guides/actions/follow/#search-types","title":"Search Types","text":"Type Description Best For <code>users</code> Search user profiles Finding people in your industry <code>tweets</code> Search tweets, follow authors Finding active discussions <code>top</code> Top results only High-quality matches <code>latest</code> Most recent Trending conversations"},{"location":"guides/actions/follow/#keyword-strategies","title":"Keyword Strategies","text":"<pre><code># Strategy 1: Broad to narrow\nkeywords_broad = [\"python\", \"programming\", \"coding\"]\nkeywords_narrow = [\"python asyncio\", \"fastapi tutorial\", \"django rest\"]\n\n# Strategy 2: Problem-based (high intent)\nkeywords_problems = [\n    \"how to learn python\",\n    \"python help needed\",\n    \"struggling with python\",\n]\n\n# Strategy 3: Tool-based (specific communities)\nkeywords_tools = [\n    \"pytorch\", \"tensorflow\", \"scikit-learn\",\n    \"pandas\", \"numpy\", \"jupyter\"\n]\n\n# Strategy 4: Event-based (timely)\nkeywords_events = [\n    \"PyCon 2026\", \"Python conference\",\n    \"#100DaysOfCode\", \"#CodeNewbie\"\n]\n</code></pre>"},{"location":"guides/actions/follow/#follow-by-hashtag","title":"Follow by Hashtag","text":"<p>Target specific communities through hashtags:</p> <pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    result = await x.follow.by_hashtag(\n        hashtags=[\n            \"#Python\",\n            \"#MachineLearning\", \n            \"#DataScience\",\n            \"#100DaysOfCode\"\n        ],\n        max_follows=30,\n        filters=filters,\n        include_recent=True,   # Recent posts\n        include_top=True,      # Top posts\n        min_engagement=5,      # Tweets with 5+ likes\n    )\n</code></pre>"},{"location":"guides/actions/follow/#high-performing-hashtags","title":"High-Performing Hashtags","text":"<pre><code># Tech/Developer hashtags (high follow-back)\ntech_hashtags = [\n    \"#100DaysOfCode\",  # Learners (very engaged)\n    \"#CodeNewbie\",     # Beginners (supportive community)\n    \"#DevCommunity\",   # General developers\n    \"#WomenInTech\",    # Inclusive community\n    \"#BuildInPublic\",  # Makers (very engaged)\n]\n\n# Niche-specific (targeted)\nai_hashtags = [\n    \"#MachineLearning\",\n    \"#DeepLearning\", \n    \"#AIArt\",\n    \"#GenerativeAI\",\n    \"#LLM\",\n]\n\n# Engagement hashtags (high activity)\nengagement_hashtags = [\n    \"#FollowFriday\",\n    \"#WritingCommunity\",\n    \"#SmallStreamersConnect\",\n]\n</code></pre>"},{"location":"guides/actions/follow/#follow-targets-followers","title":"Follow Target's Followers","text":"<p>The highest ROI strategy - follow your competitor's audience:</p> <pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    # Follow followers of a competitor/influencer\n    result = await x.follow.target_followers(\n        target=\"competitor_account\",\n        max_follows=50,\n        mode=\"followers\",  # Their followers\n        filters=FollowFilters(\n            min_followers=100,\n            max_followers=10000,  # Lower = higher follow-back\n            must_have_bio=True,\n        ),\n        skip_mutual=True,  # Skip if you already follow them\n        randomize=True,    # Don't follow sequentially\n    )\n\n    print(f\"Followed {result.success_count} of @competitor's followers\")\n</code></pre>"},{"location":"guides/actions/follow/#targeting-strategies","title":"Targeting Strategies","text":"<pre><code># Strategy 1: Competitor's followers\nawait x.follow.target_followers(\"direct_competitor\", mode=\"followers\")\n\n# Strategy 2: Competitor's following (who they trust)\nawait x.follow.target_followers(\"industry_leader\", mode=\"following\")\n\n# Strategy 3: Multiple targets\ntargets = [\"competitor1\", \"competitor2\", \"industry_influencer\"]\nfor target in targets:\n    await x.follow.target_followers(\n        target,\n        max_follows=25,  # Spread across targets\n        filters=filters\n    )\n</code></pre>"},{"location":"guides/actions/follow/#follow-engagers","title":"Follow Engagers","text":"<p>Highest quality leads - follow users who engage with specific tweets:</p> <pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    # Follow people who liked/retweeted a viral tweet in your niche\n    result = await x.follow.engagers(\n        tweet_urls=[\n            \"https://x.com/user/status/1234567890\",  # Viral tweet\n            \"https://x.com/user/status/0987654321\",  # Another popular one\n        ],\n        engagement_type=\"likers\",  # likers, retweeters, commenters, all\n        max_follows=30,\n        filters=filters,\n    )\n</code></pre>"},{"location":"guides/actions/follow/#engagement-types","title":"Engagement Types","text":"Type Quality Volume Best For <code>likers</code> Medium High Quick growth <code>retweeters</code> High Medium Engaged audience <code>commenters</code> Highest Low Quality connections <code>all</code> Mixed Highest Maximum reach"},{"location":"guides/actions/follow/#finding-high-value-tweets","title":"Finding High-Value Tweets","text":"<pre><code># 1. Search for viral tweets in your niche\ntweets = await x.scrape.search(\n    \"python tips\",\n    min_likes=1000,\n    min_retweets=100,\n    limit=10\n)\n\n# 2. Follow engagers of each\nfor tweet in tweets:\n    await x.follow.engagers(\n        tweet_urls=[tweet.url],\n        engagement_type=\"retweeters\",  # Higher quality\n        max_follows=20,\n        filters=filters\n    )\n</code></pre>"},{"location":"guides/actions/follow/#auto-follow","title":"Auto Follow","text":"<p>Set up automated following with rules:</p> <pre><code>from xeepy import Xeepy\nfrom xeepy.actions.follow.auto_follow import (\n    AutoFollowConfig,\n    FollowRule,\n    FollowStrategy\n)\n\nconfig = AutoFollowConfig(\n    daily_follow_limit=150,\n    hourly_follow_limit=25,\n    min_interval_minutes=30,\n    max_interval_minutes=90,\n    active_hours=(9, 22),  # 9 AM to 10 PM\n    filters=FollowFilters(\n        min_followers=100,\n        max_followers=50000,\n        must_have_bio=True\n    ),\n    rules=[\n        # 50% weight on keyword following\n        FollowRule(\n            strategy=FollowStrategy.KEYWORD,\n            params={\"keywords\": [\"python\", \"ai\", \"startup\"]},\n            weight=2.0,\n            daily_limit=75\n        ),\n        # 30% weight on competitor followers\n        FollowRule(\n            strategy=FollowStrategy.TARGET_FOLLOWERS,\n            params={\"targets\": [\"competitor1\", \"competitor2\"]},\n            weight=1.5,\n            daily_limit=50\n        ),\n        # 20% weight on hashtags\n        FollowRule(\n            strategy=FollowStrategy.HASHTAG,\n            params={\"hashtags\": [\"#BuildInPublic\", \"#IndieHacker\"]},\n            weight=1.0,\n            daily_limit=25\n        ),\n    ]\n)\n\nasync with Xeepy() as x:\n    action = AutoFollow(x.browser, x.rate_limiter, x.tracker, config)\n\n    # Run for 8 hours\n    result = await action.execute(\n        duration_hours=8,\n        on_run_complete=lambda r: print(f\"Run: +{r.success_count} follows\")\n    )\n\n    # Daily summary\n    stats = action.get_auto_stats()\n    print(f\"\\n\ud83d\udcca Today's Stats:\")\n    print(f\"  Total followed: {stats['total_followed']}\")\n    print(f\"  By keyword: {stats['by_strategy']['keyword']}\")\n    print(f\"  By followers: {stats['by_strategy']['target_followers']}\")\n</code></pre>"},{"location":"guides/actions/follow/#quality-filters-deep-dive","title":"Quality Filters Deep Dive","text":"<p>Filters are critical for good follow-back rates:</p> <pre><code>from xeepy.actions.base import FollowFilters\n\n# Conservative (high quality, lower volume)\nconservative = FollowFilters(\n    min_followers=500,\n    max_followers=20000,\n    min_tweets=100,\n    min_account_age_days=180,\n    must_have_bio=True,\n    must_have_profile_pic=True,\n    keywords_in_bio=[\"founder\", \"developer\", \"creator\"],\n    min_follower_ratio=0.3,  # Following/follower ratio\n)\n\n# Moderate (balanced)\nmoderate = FollowFilters(\n    min_followers=100,\n    max_followers=50000,\n    min_tweets=25,\n    min_account_age_days=30,\n    must_have_bio=True,\n    must_have_profile_pic=True,\n)\n\n# Aggressive (high volume, lower quality)\naggressive = FollowFilters(\n    min_followers=10,\n    max_followers=100000,\n    min_tweets=5,\n    must_have_profile_pic=True,\n)\n</code></pre>"},{"location":"guides/actions/follow/#testing-filters","title":"Testing Filters","text":"<pre><code># Test your filters before running\nprofile = await x.scrape.profile(\"test_user\")\n\nmatches, reason = filters.matches({\n    \"followers_count\": profile.followers_count,\n    \"following_count\": profile.following_count,\n    \"tweets_count\": profile.tweets_count,\n    \"bio\": profile.bio,\n    \"has_profile_pic\": profile.has_profile_pic,\n    \"verified\": profile.verified,\n    \"created_at\": profile.created_at,\n})\n\nprint(f\"Matches: {matches}\")\nprint(f\"Reason: {reason}\")\n</code></pre>"},{"location":"guides/actions/follow/#tracking-analytics","title":"Tracking &amp; Analytics","text":"<p>All follows are tracked automatically:</p> <pre><code>from xeepy.storage import FollowTracker\n\ntracker = FollowTracker(\"xeepy.db\")\n\n# Get stats\nstats = tracker.get_stats()\nprint(f\"Total follows: {stats.total_follows}\")\nprint(f\"Follow-back rate: {stats.follow_back_rate}%\")\nprint(f\"Best source: {stats.best_source}\")\n\n# Get by source\nkeyword_stats = tracker.get_stats_by_source(\"keyword:python\")\nprint(f\"Python keyword: {keyword_stats.follow_back_rate}% follow-back\")\n\n# Export\ntracker.export_history(\"follow_history.csv\")\n</code></pre>"},{"location":"guides/actions/follow/#best-practices","title":"Best Practices","text":"<p>Do's</p> <ul> <li>\u2705 Use quality filters (min 100 followers, must have bio)</li> <li>\u2705 Start slow (25/day for new accounts)</li> <li>\u2705 Diversify strategies (keywords + hashtags + followers)</li> <li>\u2705 Track which sources perform best</li> <li>\u2705 Add important accounts to whitelist</li> <li>\u2705 Use dry-run first</li> </ul> <p>Don'ts</p> <ul> <li>\u274c Follow more than 100-200/day</li> <li>\u274c Follow without filters (spam accounts)</li> <li>\u274c Follow and immediately unfollow</li> <li>\u274c Follow the same accounts repeatedly</li> <li>\u274c Ignore rate limit warnings</li> </ul>"},{"location":"guides/actions/follow/#next-steps","title":"Next Steps","text":"<p> Unfollow Guide - Learn to clean up non-followers</p> <p> Auto Follow Setup - AI-powered targeting</p> <p> Analytics - Track your growth</p>"},{"location":"guides/actions/mass-operations/","title":"Mass Operations","text":"<p>Execute large-scale operations safely and efficiently.</p>"},{"location":"guides/actions/mass-operations/#overview","title":"Overview","text":"<p>Mass operations require careful planning to avoid account restrictions:</p> Operation Safe Daily Limit Aggressive Limit Recovery Time Follows 100-200 400 24-48 hours Unfollows 100-200 400 24-48 hours Likes 200-500 1000 12-24 hours DMs 50-100 200 48-72 hours"},{"location":"guides/actions/mass-operations/#safe-mass-following","title":"Safe Mass Following","text":"<p>Follow thousands over multiple days:</p> <pre><code>from xeepy import Xeepy\nfrom xeepy.actions.base import MassOperationConfig\nimport asyncio\n\nconfig = MassOperationConfig(\n    total_target=1000,\n    daily_limit=150,\n    hourly_limit=25,\n    batch_size=25,\n    batch_delay=300,  # 5 min between batches\n    session_duration_hours=8,\n    active_hours=(9, 22),\n)\n\nasync def mass_follow_campaign():\n    \"\"\"Execute a large-scale follow campaign over multiple days\"\"\"\n\n    async with Xeepy() as x:\n        keywords = [\"python developer\", \"tech founder\", \"startup\"]\n        completed = 0\n\n        while completed &lt; config.total_target:\n            # Daily session\n            daily_count = 0\n\n            while daily_count &lt; config.daily_limit:\n                result = await x.follow.by_keyword(\n                    keywords=keywords,\n                    max_follows=config.batch_size,\n                    filters=x.config.default_follow_filters,\n                )\n\n                completed += result.success_count\n                daily_count += result.success_count\n\n                print(f\"Batch complete: +{result.success_count}\")\n                print(f\"Daily: {daily_count}/{config.daily_limit}\")\n                print(f\"Total: {completed}/{config.total_target}\")\n\n                if daily_count &gt;= config.daily_limit:\n                    break\n\n                # Batch delay\n                print(f\"Waiting {config.batch_delay}s...\")\n                await asyncio.sleep(config.batch_delay)\n\n            # Wait until tomorrow\n            if completed &lt; config.total_target:\n                print(\"\\n\ud83d\udcc5 Daily limit reached. Resuming tomorrow...\")\n                await asyncio.sleep(24 * 60 * 60)\n\nasyncio.run(mass_follow_campaign())\n</code></pre>"},{"location":"guides/actions/mass-operations/#safe-mass-unfollowing","title":"Safe Mass Unfollowing","text":"<p>Clean up thousands of non-followers:</p> <pre><code>from xeepy import Xeepy\n\nasync def mass_unfollow_cleanup():\n    \"\"\"Unfollow non-followers over multiple days\"\"\"\n\n    async with Xeepy() as x:\n        # First, get total count\n        preview = await x.unfollow.non_followers(\n            max_unfollows=10000,\n            dry_run=True\n        )\n\n        total = preview.success_count\n        daily_limit = 150\n        days = (total // daily_limit) + 1\n\n        print(f\"\ud83d\udcca Found {total} non-followers\")\n        print(f\"\ud83d\udcc5 Will take ~{days} days at {daily_limit}/day\")\n\n        if input(\"\\nStart campaign? (y/n): \").lower() != 'y':\n            return\n\n        completed = 0\n        day = 0\n\n        while completed &lt; total:\n            day += 1\n            print(f\"\\n\ud83d\uddd3\ufe0f  Day {day}\")\n\n            result = await x.unfollow.non_followers(\n                max_unfollows=daily_limit,\n                whitelist=[\"important1\", \"important2\"],\n                min_following_days=7,\n                exclude_verified=True,\n                on_progress=lambda c, t, m: print(f\"  [{c}/{t}] {m}\")\n            )\n\n            completed += result.success_count\n            print(f\"  \u2705 Unfollowed: {result.success_count}\")\n            print(f\"  \ud83d\udcc8 Total progress: {completed}/{total}\")\n\n            if completed &lt; total:\n                print(\"  \u23f3 Waiting until tomorrow...\")\n                await asyncio.sleep(24 * 60 * 60)\n\n        print(f\"\\n\ud83c\udf89 Campaign complete! Unfollowed {completed} users\")\n\nasyncio.run(mass_unfollow_cleanup())\n</code></pre>"},{"location":"guides/actions/mass-operations/#resumable-operations","title":"Resumable Operations","text":"<p>Handle interruptions gracefully:</p> <pre><code>from xeepy import Xeepy\nfrom xeepy.storage import FollowTracker\n\nasync def resumable_follow_campaign(campaign_name: str):\n    \"\"\"Campaign that can be resumed after interruption\"\"\"\n\n    tracker = FollowTracker(\"xeepy.db\")\n\n    # Check for existing session\n    session = tracker.get_pending_session(campaign_name)\n\n    if session:\n        print(f\"\ud83d\udcc2 Found existing session: {session['id']}\")\n        pending = tracker.get_pending_session_items(session['id'])\n        print(f\"   {len(pending)} items remaining\")\n\n        if input(\"Resume? (y/n): \").lower() != 'y':\n            tracker.complete_session(session['id'])\n            session = None\n\n    async with Xeepy() as x:\n        if not session:\n            # Create new campaign\n            print(\"\ud83c\udd95 Creating new campaign...\")\n\n            # Get target users\n            targets = await x.scrape.search_users(\n                \"python developer\",\n                limit=500\n            )\n\n            usernames = [t.username for t in targets]\n\n            # Create session\n            session_id = tracker.create_session(\n                campaign_name,\n                usernames\n            )\n            pending = usernames\n            print(f\"   Created session with {len(pending)} targets\")\n        else:\n            session_id = session['id']\n\n        # Process pending items\n        for username in pending:\n            try:\n                result = await x.follow.user(username)\n                status = \"success\" if result.success else \"failed\"\n            except Exception as e:\n                status = f\"error: {str(e)}\"\n\n            tracker.update_session_item(\n                session_id, \n                username, \n                status\n            )\n\n            # Rate limiting\n            await asyncio.sleep(random.uniform(3, 8))\n\n        # Complete session\n        tracker.complete_session(session_id)\n        print(\"\ud83c\udf89 Campaign complete!\")\n</code></pre>"},{"location":"guides/actions/mass-operations/#parallel-processing","title":"Parallel Processing","text":"<p>Speed up read-only operations:</p> <pre><code>from xeepy import Xeepy\nimport asyncio\n\nasync def parallel_scrape(usernames: list, max_concurrent: int = 5):\n    \"\"\"Scrape multiple profiles in parallel\"\"\"\n\n    semaphore = asyncio.Semaphore(max_concurrent)\n    results = []\n\n    async def scrape_one(x, username):\n        async with semaphore:\n            try:\n                profile = await x.scrape.profile(username)\n                return {\"username\": username, \"profile\": profile}\n            except Exception as e:\n                return {\"username\": username, \"error\": str(e)}\n\n    async with Xeepy() as x:\n        tasks = [scrape_one(x, u) for u in usernames]\n        results = await asyncio.gather(*tasks)\n\n    return results\n\n# Scrape 100 profiles with 5 concurrent requests\nusernames = [f\"user{i}\" for i in range(100)]\nresults = asyncio.run(parallel_scrape(usernames, max_concurrent=5))\n</code></pre> <p>Parallel Limits</p> <p>Only use parallel processing for read operations (scraping).</p> <p>Never parallelize: - Follow/unfollow - Like/retweet - Comments/DMs</p>"},{"location":"guides/actions/mass-operations/#batch-processing","title":"Batch Processing","text":"<p>Process in controlled batches:</p> <pre><code>from xeepy import Xeepy\n\nasync def batch_operation(items: list, batch_size: int = 25):\n    \"\"\"Process items in batches with delays\"\"\"\n\n    async with Xeepy() as x:\n        total = len(items)\n\n        for i in range(0, total, batch_size):\n            batch = items[i:i + batch_size]\n            batch_num = (i // batch_size) + 1\n            total_batches = (total // batch_size) + 1\n\n            print(f\"\\n\ud83d\udce6 Batch {batch_num}/{total_batches}\")\n\n            for item in batch:\n                # Process item\n                await x.follow.user(item)\n                await asyncio.sleep(random.uniform(2, 5))\n\n            # Longer pause between batches\n            if i + batch_size &lt; total:\n                pause = 60 * 5  # 5 minutes\n                print(f\"\u23f8\ufe0f  Pausing {pause}s before next batch...\")\n                await asyncio.sleep(pause)\n</code></pre>"},{"location":"guides/actions/mass-operations/#progress-tracking","title":"Progress Tracking","text":"<p>Track and display progress:</p> <pre><code>from xeepy import Xeepy\nfrom tqdm import tqdm\nimport time\n\nasync def tracked_mass_operation():\n    \"\"\"Mass operation with progress bar\"\"\"\n\n    async with Xeepy() as x:\n        # Get targets\n        targets = await x.scrape.followers(\"competitor\", limit=500)\n\n        with tqdm(total=len(targets), desc=\"Following\") as pbar:\n            success = 0\n            failed = 0\n\n            for user in targets:\n                result = await x.follow.user(user.username)\n\n                if result.success:\n                    success += 1\n                    pbar.set_postfix({\"\u2713\": success, \"\u2717\": failed})\n                else:\n                    failed += 1\n\n                pbar.update(1)\n                await asyncio.sleep(random.uniform(3, 8))\n\n            print(f\"\\n\u2705 Complete: {success} followed, {failed} failed\")\n</code></pre>"},{"location":"guides/actions/mass-operations/#error-recovery","title":"Error Recovery","text":"<p>Handle errors without losing progress:</p> <pre><code>from xeepy import Xeepy\nimport json\n\nasync def resilient_mass_operation():\n    \"\"\"Mass operation with error recovery\"\"\"\n\n    progress_file = \"mass_op_progress.json\"\n\n    # Load progress\n    try:\n        with open(progress_file) as f:\n            progress = json.load(f)\n    except FileNotFoundError:\n        progress = {\"completed\": [], \"failed\": [], \"pending\": []}\n\n    async with Xeepy() as x:\n        # Get targets (or use pending from progress)\n        if not progress[\"pending\"]:\n            targets = await x.scrape.followers(\"competitor\", limit=500)\n            progress[\"pending\"] = [t.username for t in targets]\n\n        while progress[\"pending\"]:\n            username = progress[\"pending\"][0]\n\n            try:\n                result = await x.follow.user(username)\n\n                if result.success:\n                    progress[\"completed\"].append(username)\n                else:\n                    progress[\"failed\"].append(username)\n\n            except Exception as e:\n                print(f\"Error: {e}\")\n                progress[\"failed\"].append(username)\n\n            finally:\n                # Remove from pending regardless of result\n                progress[\"pending\"].pop(0)\n\n                # Save progress\n                with open(progress_file, \"w\") as f:\n                    json.dump(progress, f)\n\n            await asyncio.sleep(random.uniform(3, 8))\n\n        print(f\"\u2705 Complete!\")\n        print(f\"   Succeeded: {len(progress['completed'])}\")\n        print(f\"   Failed: {len(progress['failed'])}\")\n</code></pre>"},{"location":"guides/actions/mass-operations/#rate-limit-handling","title":"Rate Limit Handling","text":"<p>Automatically handle rate limits:</p> <pre><code>from xeepy import Xeepy\nfrom xeepy.actions.base import RateLimitHandler\n\nhandler = RateLimitHandler(\n    initial_delay=3,\n    max_delay=300,\n    backoff_multiplier=2,\n    max_retries=3,\n)\n\nasync def rate_limited_operation():\n    async with Xeepy() as x:\n        for username in usernames:\n            success = False\n            retries = 0\n\n            while not success and retries &lt; handler.max_retries:\n                try:\n                    result = await x.follow.user(username)\n                    success = result.success\n                    handler.reset()\n\n                except RateLimitError:\n                    delay = handler.get_backoff_delay()\n                    print(f\"\u23f3 Rate limited. Waiting {delay}s...\")\n                    await asyncio.sleep(delay)\n                    retries += 1\n\n            await asyncio.sleep(handler.get_delay())\n</code></pre>"},{"location":"guides/actions/mass-operations/#multi-account-operations","title":"Multi-Account Operations","text":"<p>Distribute load across accounts:</p> <pre><code>from xeepy import Xeepy, AccountRotator\n\nasync def multi_account_mass_follow():\n    \"\"\"Distribute follows across multiple accounts\"\"\"\n\n    accounts = [\"account1\", \"account2\", \"account3\"]\n    targets_per_account = 50\n\n    rotator = AccountRotator(\n        profiles=accounts,\n        strategy=\"round-robin\",\n        cooldown_minutes=60\n    )\n\n    targets = [\"target1\", \"target2\", ...]  # 150 targets\n\n    for i, target in enumerate(targets):\n        # Get next available account\n        profile = rotator.get_next()\n\n        async with Xeepy(profile=profile) as x:\n            await x.follow.user(target)\n            print(f\"[{profile}] Followed @{target}\")\n\n        await asyncio.sleep(random.uniform(3, 8))\n</code></pre>"},{"location":"guides/actions/mass-operations/#campaign-templates","title":"Campaign Templates","text":""},{"location":"guides/actions/mass-operations/#follower-growth-campaign","title":"Follower Growth Campaign","text":"<pre><code>async def growth_campaign(days: int = 7):\n    \"\"\"7-day follower growth campaign\"\"\"\n\n    daily_follows = 100\n    daily_unfollows = 50\n\n    async with Xeepy() as x:\n        for day in range(days):\n            print(f\"\\n\ud83d\udcc5 Day {day + 1}/{days}\")\n\n            # Morning: Follow new users\n            print(\"\ud83c\udf05 Morning session: Following...\")\n            await x.follow.by_keyword(\n                keywords=[\"your niche\"],\n                max_follows=daily_follows // 2\n            )\n\n            # Afternoon: Engage with existing\n            print(\"\u2600\ufe0f Afternoon session: Engaging...\")\n            await x.engage.auto_like(\n                keywords=[\"your niche\"],\n                max_likes=50\n            )\n\n            # Evening: Unfollow non-followers\n            print(\"\ud83c\udf19 Evening session: Cleanup...\")\n            await x.unfollow.non_followers(\n                max_unfollows=daily_unfollows,\n                min_following_days=7\n            )\n\n            # Report\n            me = await x.scrape.profile(\"me\")\n            print(f\"\ud83d\udcca Stats: {me.followers_count} followers, {me.following_count} following\")\n\n            if day &lt; days - 1:\n                await asyncio.sleep(24 * 60 * 60)\n</code></pre>"},{"location":"guides/actions/mass-operations/#cleanup-campaign","title":"Cleanup Campaign","text":"<pre><code>async def cleanup_campaign():\n    \"\"\"Aggressive cleanup of low-quality follows\"\"\"\n\n    async with Xeepy() as x:\n        # Phase 1: Non-followers\n        print(\"Phase 1: Non-followers\")\n        await x.unfollow.non_followers(max_unfollows=200)\n\n        # Phase 2: Inactive accounts\n        print(\"Phase 2: Inactive accounts\")\n        await x.unfollow.by_criteria(\n            criteria=UnfollowCriteria(inactive_days=180),\n            max_unfollows=100\n        )\n\n        # Phase 3: Spam accounts\n        print(\"Phase 3: Spam accounts\")\n        await x.unfollow.by_criteria(\n            criteria=UnfollowCriteria(\n                bio_contains=[\"follow back\", \"f4f\"],\n                max_followers=50\n            ),\n            max_unfollows=100\n        )\n\n        print(\"\u2705 Cleanup complete!\")\n</code></pre>"},{"location":"guides/actions/mass-operations/#best-practices","title":"Best Practices","text":"<p>Safety Checklist</p> <ul> <li>\u2705 Always test with small batches first</li> <li>\u2705 Use session tracking for resumability</li> <li>\u2705 Implement exponential backoff</li> <li>\u2705 Spread operations over multiple days</li> <li>\u2705 Monitor account for warnings</li> <li>\u2705 Keep detailed logs</li> </ul> <p>Avoid</p> <ul> <li>\u274c Running multiple mass operations simultaneously</li> <li>\u274c Exceeding daily limits</li> <li>\u274c Ignoring rate limit warnings</li> <li>\u274c Operations without progress tracking</li> <li>\u274c Running during account cooldowns</li> </ul>"},{"location":"guides/actions/mass-operations/#monitoring","title":"Monitoring","text":"<p>Watch for account restrictions:</p> <pre><code>async def check_account_health():\n    \"\"\"Monitor account for restriction signs\"\"\"\n\n    async with Xeepy() as x:\n        health = await x.account.health_check()\n\n        if health.warnings:\n            print(\"\u26a0\ufe0f Warnings detected:\")\n            for warning in health.warnings:\n                print(f\"  - {warning}\")\n            return False\n\n        if health.rate_limited:\n            print(f\"\u23f3 Rate limited until {health.reset_time}\")\n            return False\n\n        print(\"\u2705 Account healthy\")\n        return True\n</code></pre>"},{"location":"guides/actions/mass-operations/#next-steps","title":"Next Steps","text":"<p> Rate Limiting - Configure safety limits</p> <p> Scheduling - Automate campaigns</p> <p> Multi-Account - Scale with multiple accounts</p>"},{"location":"guides/actions/unfollow/","title":"Unfollow Users Guide","text":"<p>Clean up your following list strategically to improve your follower ratio and engagement.</p>"},{"location":"guides/actions/unfollow/#why-unfollowing-matters","title":"Why Unfollowing Matters","text":"Following:Follower Ratio Perception 0.5 or lower Authority/Influencer 0.5 - 1.0 Balanced/Professional 1.0 - 2.0 Normal user 2.0+ Looks like spam <p>A healthy ratio improves: - Profile credibility - Follow-back rates on new follows - Algorithm recommendations</p>"},{"location":"guides/actions/unfollow/#unfollow-non-followers","title":"Unfollow Non-Followers","text":"<p>The #1 most requested feature. Unfollow everyone who doesn't follow you back:</p> PythonCLI <pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    # Always preview first!\n    preview = await x.unfollow.non_followers(\n        max_unfollows=100,\n        whitelist=[\"friend1\", \"mentor\", \"boss\"],\n        min_following_days=7,     # Grace period\n        exclude_verified=True,     # Keep verified accounts\n        min_followers=10000,       # Keep if they have 10k+ followers\n        dry_run=True               # Preview mode\n    )\n\n    print(f\"Would unfollow {preview.success_count} users:\")\n    for user in preview.unfollowed_users[:20]:\n        print(f\"  @{user}\")\n\n    # If satisfied, run for real\n    if input(\"\\nProceed? (y/n): \").lower() == 'y':\n        result = await x.unfollow.non_followers(\n            max_unfollows=100,\n            whitelist=[\"friend1\", \"mentor\", \"boss\"],\n            min_following_days=7,\n            exclude_verified=True,\n            min_followers=10000,\n            dry_run=False,\n            on_unfollow=lambda u, s: print(f\"{'\u2713' if s else '\u2717'} @{u}\")\n        )\n        print(f\"\\n\u2705 Unfollowed {result.success_count} users\")\n</code></pre> <pre><code># Preview first\nxeepy unfollow non-followers --max 100 --grace-days 7 --exclude-verified --dry-run\n\n# Execute\nxeepy unfollow non-followers --max 100 --grace-days 7 --exclude-verified\n</code></pre>"},{"location":"guides/actions/unfollow/#statistics-callback","title":"Statistics Callback","text":"<p>Get detailed stats before unfollowing:</p> <pre><code>async def show_stats(stats):\n    print(f\"\\n\ud83d\udcca Account Analysis\")\n    print(f\"{'='*40}\")\n    print(f\"Following:     {stats.total_following:,}\")\n    print(f\"Followers:     {stats.total_followers:,}\")\n    print(f\"Ratio:         {stats.ratio:.2f}\")\n    print(f\"Non-followers: {stats.non_followers_count:,}\")\n    print(f\"Mutual:        {stats.mutual_count:,}\")\n    print(f\"{'='*40}\\n\")\n\nresult = await x.unfollow.non_followers(\n    max_unfollows=100,\n    on_stats=show_stats,\n    dry_run=True\n)\n</code></pre>"},{"location":"guides/actions/unfollow/#whitelist-management","title":"Whitelist Management","text":"<p>Protect important accounts:</p> <pre><code>from xeepy.storage import FollowTracker\n\ntracker = FollowTracker(\"xeepy.db\")\n\n# Add to whitelist\ntracker.add_to_whitelist(\"best_friend\", reason=\"Personal friend\")\ntracker.add_to_whitelist(\"mentor_account\", reason=\"Mentor\")\ntracker.add_to_whitelist(\"boss_twitter\", reason=\"Work contact\")\ntracker.add_to_whitelist(\"favorite_creator\", reason=\"Fan\")\n\n# Bulk add\nfriends = [\"friend1\", \"friend2\", \"friend3\"]\nfor friend in friends:\n    tracker.add_to_whitelist(friend, reason=\"Friend\")\n\n# View whitelist\nprint(\"Protected accounts:\")\nfor user in tracker.get_whitelist():\n    print(f\"  @{user}\")\n\n# Remove from whitelist\ntracker.remove_from_whitelist(\"old_friend\")\n</code></pre> <p>CLI whitelist management:</p> <pre><code># Add\nxeepy whitelist add friend1 friend2 --reason \"Friends\"\n\n# List\nxeepy whitelist list\n\n# Remove  \nxeepy whitelist remove old_friend\n\n# Import from file\nxeepy whitelist import whitelist.txt\n</code></pre>"},{"location":"guides/actions/unfollow/#smart-unfollow","title":"Smart Unfollow","text":"<p>Uses tracking data to make intelligent decisions:</p> <pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    # Check eligibility\n    eligible = await x.unfollow.smart_eligible(days_threshold=7)\n    print(f\"{eligible} users haven't followed back in 7+ days\")\n\n    # Smart unfollow with engagement check\n    result = await x.unfollow.smart(\n        days_threshold=7,           # Followed 7+ days ago\n        max_unfollows=50,\n        check_engagement=True,      # Keep if they've engaged with you\n        engagement_types=[\"like\", \"reply\", \"retweet\"],\n        whitelist=[\"friend1\"],\n        exclude_verified=True,\n        dry_run=False\n    )\n\n    print(f\"Unfollowed {result.success_count} dormant follows\")\n</code></pre>"},{"location":"guides/actions/unfollow/#how-it-works","title":"How It Works","text":"<pre><code>flowchart TD\n    A[Get users you follow] --&gt; B{Followed &gt; X days ago?}\n    B --&gt;|No| C[Skip - grace period]\n    B --&gt;|Yes| D{Did they follow back?}\n    D --&gt;|Yes| E[Skip - mutual]\n    D --&gt;|No| F{On whitelist?}\n    F --&gt;|Yes| G[Skip - protected]\n    F --&gt;|No| H{Check engagement?}\n    H --&gt;|Yes| I{Have they engaged?}\n    I --&gt;|Yes| J[Skip - engaged]\n    I --&gt;|No| K[UNFOLLOW]\n    H --&gt;|No| K</code></pre>"},{"location":"guides/actions/unfollow/#engagement-detection","title":"Engagement Detection","text":"<pre><code># Keep users who have engaged with you, even if they don't follow\nresult = await x.unfollow.smart(\n    days_threshold=7,\n    check_engagement=True,\n    engagement_types=[\n        \"like\",      # Liked your tweets\n        \"reply\",     # Replied to you\n        \"retweet\",   # Retweeted you\n        \"quote\",     # Quote tweeted you\n        \"mention\",   # Mentioned you\n    ],\n    engagement_lookback_days=30,  # Check last 30 days\n    min_engagements=1,            # At least 1 interaction\n)\n</code></pre>"},{"location":"guides/actions/unfollow/#unfollow-by-criteria","title":"Unfollow by Criteria","text":"<p>Flexible unfollowing based on custom criteria:</p> <pre><code>from xeepy import Xeepy\nfrom xeepy.actions.unfollow.unfollow_by_criteria import UnfollowCriteria\n\n# Define criteria\ncriteria = UnfollowCriteria(\n    # Unfollow accounts with:\n    max_followers=50,           # Less than 50 followers\n    min_ratio=0.1,              # Following/follower ratio &lt; 0.1\n    inactive_days=90,           # No tweets in 90 days\n    has_bio=False,              # No bio\n    has_profile_pic=False,      # Default profile pic\n    bio_contains=[\"spam\", \"bot\", \"follow back\", \"f4f\"],\n    from_source=\"keyword:crypto\",  # Only from this source\n)\n\nasync with Xeepy() as x:\n    result = await x.unfollow.by_criteria(\n        criteria=criteria,\n        max_unfollows=50,\n        must_be_non_follower=True,  # Only non-followers\n        dry_run=True\n    )\n\n    print(f\"Would unfollow {result.success_count} low-quality accounts\")\n</code></pre>"},{"location":"guides/actions/unfollow/#common-criteria-patterns","title":"Common Criteria Patterns","text":"<pre><code># Pattern 1: Spam accounts\nspam_criteria = UnfollowCriteria(\n    bio_contains=[\"follow back\", \"f4f\", \"follow 4 follow\", \"dm for promo\"],\n    max_followers=100,\n)\n\n# Pattern 2: Inactive accounts\ninactive_criteria = UnfollowCriteria(\n    inactive_days=180,  # 6 months\n)\n\n# Pattern 3: Low-quality accounts\nlow_quality = UnfollowCriteria(\n    max_followers=50,\n    has_bio=False,\n    has_profile_pic=False,\n)\n\n# Pattern 4: Suspicious accounts\nsuspicious = UnfollowCriteria(\n    min_following=5000,      # Following 5000+\n    max_followers=100,       # But only 100 followers\n    max_tweets=10,           # And barely any tweets\n)\n\n# Pattern 5: From specific campaign\ncampaign_cleanup = UnfollowCriteria(\n    from_source=\"keyword:crypto\",  # Only from crypto campaign\n    must_be_non_follower=True,\n)\n</code></pre>"},{"location":"guides/actions/unfollow/#mass-unfollow","title":"Mass Unfollow","text":"<p>Unfollow everyone (use with caution!):</p> <pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    def confirm(count):\n        print(f\"\\n\u26a0\ufe0f  WARNING: About to unfollow {count} users!\")\n        return input(\"Type 'CONFIRM' to proceed: \") == \"CONFIRM\"\n\n    result = await x.unfollow.all(\n        max_unfollows=500,        # Safety limit\n        whitelist=[\"keeper1\", \"keeper2\"],\n        require_confirmation=True,\n        on_confirm=confirm,\n        batch_size=50,            # Process in batches\n        batch_delay=60,           # Pause between batches\n        on_progress=lambda p, t, m: print(f\"[{p}/{t}] {m}\")\n    )\n\n    print(f\"\\n\u2705 Unfollowed {result.success_count} users\")\n    print(f\"\u23ed\ufe0f  Skipped (whitelist): {result.skipped_count}\")\n</code></pre> <p>Use Mass Unfollow Carefully</p> <ul> <li>Always use <code>dry_run=True</code> first</li> <li>Set reasonable <code>max_unfollows</code> limits</li> <li>Add important accounts to whitelist</li> <li>Consider doing in batches over multiple days</li> </ul>"},{"location":"guides/actions/unfollow/#single-user-unfollow","title":"Single User Unfollow","text":"<p>Simple single-user unfollow:</p> <pre><code>async with Xeepy() as x:\n    result = await x.unfollow.user(\"someuser\")\n\n    if result.success:\n        print(f\"\u2713 Unfollowed @someuser\")\n</code></pre> <p>Batch unfollow:</p> <pre><code>users_to_unfollow = [\"user1\", \"user2\", \"user3\"]\n\nresult = await x.unfollow.users(\n    usernames=users_to_unfollow,\n    delay_range=(2, 5),\n    on_unfollow=lambda u, s: print(f\"{'\u2713' if s else '\u2717'} @{u}\")\n)\n</code></pre>"},{"location":"guides/actions/unfollow/#scheduling-unfollows","title":"Scheduling Unfollows","text":"<p>Spread unfollows over time:</p> <pre><code>from xeepy import Xeepy\nimport asyncio\nfrom datetime import datetime, timedelta\n\nasync def scheduled_cleanup():\n    \"\"\"Run unfollow in batches over multiple days\"\"\"\n\n    async with Xeepy() as x:\n        # Get total non-followers\n        preview = await x.unfollow.non_followers(\n            max_unfollows=1000,\n            dry_run=True\n        )\n\n        total = preview.success_count\n        per_day = 100\n        days_needed = (total // per_day) + 1\n\n        print(f\"\ud83d\udcca Found {total} non-followers\")\n        print(f\"\ud83d\udcc5 Will take {days_needed} days at {per_day}/day\")\n\n        for day in range(days_needed):\n            print(f\"\\n\ud83d\uddd3\ufe0f  Day {day + 1}/{days_needed}\")\n\n            result = await x.unfollow.non_followers(\n                max_unfollows=per_day,\n                whitelist=[\"friend1\"],\n                min_following_days=7,\n                exclude_verified=True,\n            )\n\n            print(f\"  Unfollowed: {result.success_count}\")\n\n            if day &lt; days_needed - 1:\n                # Wait until tomorrow (or use task scheduler)\n                print(f\"  \u23f3 Waiting until tomorrow...\")\n                await asyncio.sleep(24 * 60 * 60)\n\nasyncio.run(scheduled_cleanup())\n</code></pre>"},{"location":"guides/actions/unfollow/#analytics-reporting","title":"Analytics &amp; Reporting","text":"<p>Track your unfollow campaigns:</p> <pre><code>from xeepy.storage import FollowTracker\n\ntracker = FollowTracker(\"xeepy.db\")\n\n# Get unfollow stats\nstats = tracker.get_stats()\nprint(f\"Total unfollowed: {stats.total_unfollows}\")\nprint(f\"Unfollows this week: {stats.unfollows_this_week}\")\n\n# Get by reason\nby_reason = tracker.get_unfollows_by_reason()\nfor reason, count in by_reason.items():\n    print(f\"  {reason}: {count}\")\n\n# Export history\ntracker.export_unfollowed(\"unfollowed_users.csv\")\n</code></pre>"},{"location":"guides/actions/unfollow/#daily-report","title":"Daily Report","text":"<pre><code>async def daily_report():\n    tracker = FollowTracker(\"xeepy.db\")\n\n    stats = tracker.get_daily_stats()\n\n    report = f\"\"\"\n\ud83d\udcca Daily Unfollow Report\n{'='*40}\n\nToday's Activity:\n  - Unfollowed: {stats.today_unfollows}\n  - New followers: {stats.today_new_followers}\n  - Lost followers: {stats.today_lost_followers}\n\nRunning Totals:\n  - Following: {stats.total_following}\n  - Followers: {stats.total_followers}\n  - Ratio: {stats.ratio:.2f}\n\nTop Unfollow Reasons:\n{chr(10).join(f'  - {r}: {c}' for r, c in stats.top_reasons.items())}\n\n{'='*40}\n\"\"\"\n    print(report)\n\n    # Or send via webhook\n    # await send_discord_webhook(report)\n</code></pre>"},{"location":"guides/actions/unfollow/#best-practices","title":"Best Practices","text":"<p>Do's</p> <ul> <li>\u2705 Always use <code>dry_run=True</code> first</li> <li>\u2705 Set up whitelist before any mass operations</li> <li>\u2705 Use 7+ day grace period for non-followers</li> <li>\u2705 Keep verified accounts and high-follower accounts</li> <li>\u2705 Spread large unfollows over multiple days</li> <li>\u2705 Track everything for analytics</li> </ul> <p>Don'ts</p> <ul> <li>\u274c Unfollow more than 100-200/day</li> <li>\u274c Follow/unfollow the same users repeatedly</li> <li>\u274c Unfollow immediately after following</li> <li>\u274c Skip the whitelist</li> <li>\u274c Ignore rate limit warnings</li> </ul>"},{"location":"guides/actions/unfollow/#troubleshooting","title":"Troubleshooting","text":"Rate limited while unfollowing <p>Twitter limits unfollows too. Solutions:</p> <ol> <li>Reduce <code>max_unfollows</code> to 50-100/day</li> <li>Increase delay: <code>delay_range=(5, 10)</code></li> <li>Spread over multiple sessions</li> <li>Wait 24 hours before retrying</li> </ol> Wrong users unfollowed <p>Always use <code>dry_run=True</code> first. To recover:</p> <ol> <li>Export unfollowed list: <code>tracker.export_unfollowed(\"recovery.csv\")</code></li> <li>Re-follow important ones: <code>await x.follow.users(important_users)</code></li> <li>Add to whitelist to prevent future issues</li> </ol> Whitelist not working <p>Check whitelist is loaded:</p> <pre><code>tracker = FollowTracker(\"xeepy.db\")\nprint(tracker.get_whitelist())  # Should show your protected users\n</code></pre>"},{"location":"guides/actions/unfollow/#next-steps","title":"Next Steps","text":"<p> Engagement Automation - Auto-like and comment</p> <p> Growth Analytics - Track your progress</p> <p> Smart Targeting - AI-powered targeting</p>"},{"location":"guides/ai/","title":"AI Features Guide","text":"<p>Xeepy integrates with leading AI providers to supercharge your X/Twitter automation with intelligent content generation, analysis, and insights.</p>"},{"location":"guides/ai/#overview","title":"Overview","text":"<ul> <li> <p> Content Generation</p> <p>Generate tweets, threads, and replies with AI</p> </li> <li> <p> Smart Replies</p> <p>Context-aware reply generation</p> </li> <li> <p> Sentiment Analysis</p> <p>Understand the tone of conversations</p> </li> <li> <p> Bot Detection</p> <p>Identify fake accounts and bots</p> </li> <li> <p> Smart Targeting</p> <p>Find the right accounts to engage with</p> </li> </ul>"},{"location":"guides/ai/#supported-providers","title":"Supported Providers","text":"Provider Models Best For OpenAI GPT-4, GPT-4 Turbo, GPT-3.5 Best quality, most versatile Anthropic Claude 3 Opus, Sonnet, Haiku Nuanced content, safety Ollama Llama 3, Mistral, etc. Free, local, privacy"},{"location":"guides/ai/#quick-start","title":"Quick Start","text":"<pre><code>from xeepy.ai import ContentGenerator\n\n# Initialize with your preferred provider\nai = ContentGenerator(\n    provider=\"openai\",\n    api_key=\"sk-your-api-key\",  # or use OPENAI_API_KEY env var\n    model=\"gpt-4-turbo\"\n)\n\n# Generate a reply\nreply = await ai.generate_reply(\n    tweet_text=\"Just shipped my first SaaS product! \ud83d\ude80\",\n    style=\"supportive\"\n)\nprint(reply)  # \"Congrats on the launch! \ud83c\udf89 What problem does it solve?\"\n</code></pre>"},{"location":"guides/ai/#content-generation","title":"Content Generation","text":""},{"location":"guides/ai/#generate-tweets","title":"Generate Tweets","text":"<pre><code>from xeepy.ai import ContentGenerator\n\nai = ContentGenerator(provider=\"openai\")\n\n# Generate a tweet about a topic\ntweet = await ai.generate_tweet(\n    topic=\"Python programming tips\",\n    style=\"educational\",\n    include_hashtags=True,\n    max_length=280\n)\n\nprint(tweet)\n# \"\ud83d\udc0d Python tip: Use list comprehensions instead of loops for cleaner code.\n#\n# # Instead of this:\n# result = []\n# for x in items:\n#     result.append(x * 2)\n#\n# # Do this:\n# result = [x * 2 for x in items]\n#\n# #Python #CodingTips\"\n</code></pre>"},{"location":"guides/ai/#generate-threads","title":"Generate Threads","text":"<pre><code>ai = ContentGenerator(provider=\"anthropic\", model=\"claude-3-opus\")\n\nthread = await ai.generate_thread(\n    topic=\"How I grew from 0 to 10k followers\",\n    key_points=[\n        \"Consistency is key\",\n        \"Engage with your community\",\n        \"Provide value, not self-promotion\",\n        \"Use threads effectively\"\n    ],\n    style=\"storytelling\",\n    thread_length=8  # Number of tweets\n)\n\nfor i, tweet in enumerate(thread.tweets, 1):\n    print(f\"Tweet {i}/{len(thread.tweets)}:\")\n    print(tweet)\n    print()\n</code></pre>"},{"location":"guides/ai/#content-styles","title":"Content Styles","text":"<pre><code># Available styles\nstyles = [\n    \"professional\",    # Business-appropriate\n    \"casual\",          # Friendly, conversational\n    \"witty\",           # Clever, humorous\n    \"supportive\",      # Encouraging, positive\n    \"educational\",     # Teaching, informative\n    \"controversial\",   # Thought-provoking (use carefully)\n    \"storytelling\",    # Narrative format\n    \"crypto\",          # Web3/crypto community style\n    \"tech\",            # Developer community style\n]\n\n# Use different styles\ntweet_pro = await ai.generate_tweet(topic, style=\"professional\")\ntweet_casual = await ai.generate_tweet(topic, style=\"casual\")\ntweet_witty = await ai.generate_tweet(topic, style=\"witty\")\n</code></pre>"},{"location":"guides/ai/#smart-replies","title":"Smart Replies","text":""},{"location":"guides/ai/#context-aware-replies","title":"Context-Aware Replies","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import ContentGenerator\n\nasync with Xeepy() as x:\n    ai = ContentGenerator(provider=\"openai\")\n\n    # Get a tweet to reply to\n    tweet_url = \"https://x.com/user/status/123456789\"\n    tweet = await x.scrape.tweet(tweet_url)\n\n    # Generate contextual reply\n    reply = await ai.generate_reply(\n        tweet_text=tweet.text,\n        author_bio=tweet.author.bio,\n        style=\"supportive\",\n        max_length=280\n    )\n\n    print(f\"Suggested reply: {reply}\")\n</code></pre>"},{"location":"guides/ai/#reply-variations","title":"Reply Variations","text":"<pre><code># Get multiple reply options\nreplies = await ai.generate_reply(\n    tweet_text=\"What's the best programming language to learn in 2024?\",\n    style=\"educational\",\n    variations=3  # Generate 3 different replies\n)\n\nfor i, reply in enumerate(replies, 1):\n    print(f\"Option {i}: {reply}\")\n</code></pre>"},{"location":"guides/ai/#auto-reply-system","title":"Auto-Reply System","text":"<pre><code>async with Xeepy() as x:\n    ai = ContentGenerator(provider=\"openai\")\n\n    # Get mentions\n    mentions = await x.scrape.mentions(\"yourusername\", limit=10)\n\n    for mention in mentions:\n        # Skip if already replied\n        if mention.has_my_reply:\n            continue\n\n        # Generate reply\n        reply = await ai.generate_reply(\n            tweet_text=mention.text,\n            style=\"supportive\"\n        )\n\n        # Review before posting (recommended!)\n        print(f\"Tweet: {mention.text}\")\n        print(f\"Suggested: {reply}\")\n\n        if input(\"Post? (y/n): \").lower() == 'y':\n            await x.engage.reply(mention.url, reply)\n</code></pre>"},{"location":"guides/ai/#sentiment-analysis","title":"Sentiment Analysis","text":""},{"location":"guides/ai/#analyze-tweet-sentiment","title":"Analyze Tweet Sentiment","text":"<pre><code>from xeepy.ai import SentimentAnalyzer\n\nanalyzer = SentimentAnalyzer(provider=\"openai\")\n\n# Single tweet\nresult = await analyzer.analyze(\"This product is amazing! Best purchase ever \ud83d\ude4c\")\nprint(f\"Sentiment: {result.label}\")  # \"positive\"\nprint(f\"Score: {result.score}\")      # 0.95\nprint(f\"Confidence: {result.confidence}\")  # 0.98\n\n# Batch analysis\ntweets = [\"Great work!\", \"This is terrible\", \"It's okay I guess\"]\nresults = await analyzer.analyze_batch(tweets)\n\nfor tweet, result in zip(tweets, results):\n    print(f\"{tweet}: {result.label} ({result.score:.2f})\")\n</code></pre>"},{"location":"guides/ai/#monitor-sentiment-trends","title":"Monitor Sentiment Trends","text":"<pre><code>async with Xeepy() as x:\n    analyzer = SentimentAnalyzer(provider=\"openai\")\n\n    # Get replies to your tweet\n    replies = await x.scrape.replies(my_tweet_url, limit=100)\n\n    # Analyze sentiment\n    sentiments = await analyzer.analyze_batch([r.text for r in replies])\n\n    positive = sum(1 for s in sentiments if s.label == \"positive\")\n    negative = sum(1 for s in sentiments if s.label == \"negative\")\n    neutral = sum(1 for s in sentiments if s.label == \"neutral\")\n\n    print(f\"\"\"\n    Sentiment Breakdown:\n    \u2705 Positive: {positive} ({positive/len(sentiments):.0%})\n    \u274c Negative: {negative} ({negative/len(sentiments):.0%})\n    \ud83d\ude10 Neutral: {neutral} ({neutral/len(sentiments):.0%})\n    \"\"\")\n</code></pre>"},{"location":"guides/ai/#bot-detection","title":"Bot Detection","text":""},{"location":"guides/ai/#detect-bot-accounts","title":"Detect Bot Accounts","text":"<pre><code>from xeepy.ai import BotDetector\n\ndetector = BotDetector(provider=\"openai\")\n\n# Check single account\nresult = await detector.analyze(\"suspicious_account\")\n\nprint(f\"Bot probability: {result.bot_probability:.0%}\")\nprint(f\"Signals: {', '.join(result.signals)}\")\n# Signals: \"High posting frequency\", \"Generic bio\", \"Recent account\"\n\n# Classification\nif result.bot_probability &gt; 0.7:\n    print(\"\u26a0\ufe0f Likely bot\")\nelif result.bot_probability &gt; 0.4:\n    print(\"\ud83e\udd14 Possibly bot\")\nelse:\n    print(\"\u2705 Likely human\")\n</code></pre>"},{"location":"guides/ai/#filter-bot-followers","title":"Filter Bot Followers","text":"<pre><code>async with Xeepy() as x:\n    detector = BotDetector(provider=\"openai\")\n\n    # Get followers\n    followers = await x.scrape.followers(\"yourusername\", limit=500)\n\n    # Analyze for bots\n    real_followers = []\n    bot_followers = []\n\n    for follower in followers:\n        result = await detector.analyze_profile(follower)\n\n        if result.bot_probability &gt; 0.7:\n            bot_followers.append(follower)\n        else:\n            real_followers.append(follower)\n\n    print(f\"Real followers: {len(real_followers)}\")\n    print(f\"Likely bots: {len(bot_followers)}\")\n</code></pre>"},{"location":"guides/ai/#smart-targeting","title":"Smart Targeting","text":""},{"location":"guides/ai/#find-ideal-accounts-to-engage","title":"Find Ideal Accounts to Engage","text":"<pre><code>from xeepy.ai import SmartTargeting\n\ntargeting = SmartTargeting(provider=\"openai\")\n\n# Define your ideal audience\nideal_audience = await targeting.find_accounts(\n    criteria={\n        \"interests\": [\"startups\", \"SaaS\", \"indie hacking\"],\n        \"follower_range\": (1000, 50000),\n        \"engagement_rate_min\": 0.02,\n        \"active_days\": 7,\n        \"language\": \"en\"\n    },\n    sample_from=\"followers_of:competitor_account\",\n    limit=100\n)\n\nfor account in ideal_audience:\n    print(f\"@{account.username}\")\n    print(f\"  Relevance: {account.relevance_score:.0%}\")\n    print(f\"  Why: {account.relevance_reason}\")\n</code></pre>"},{"location":"guides/ai/#content-recommendations","title":"Content Recommendations","text":"<pre><code>targeting = SmartTargeting(provider=\"anthropic\")\n\n# Get content recommendations based on your audience\nrecommendations = await targeting.content_recommendations(\n    based_on=\"my_top_tweets\",\n    audience_analysis=True\n)\n\nprint(\"\ud83d\udcdd Content Recommendations:\")\nfor rec in recommendations:\n    print(f\"  - {rec.topic}\")\n    print(f\"    Predicted engagement: {rec.predicted_engagement}\")\n    print(f\"    Best time to post: {rec.best_time}\")\n</code></pre>"},{"location":"guides/ai/#local-ai-with-ollama","title":"Local AI with Ollama","text":"<p>Use AI features without cloud APIs:</p> <pre><code># Install Ollama first: https://ollama.ai\nollama pull llama3\nollama pull mistral\n</code></pre> <pre><code>from xeepy.ai import ContentGenerator\n\n# Use local Ollama\nai = ContentGenerator(\n    provider=\"ollama\",\n    model=\"llama3\",\n    base_url=\"http://localhost:11434\"  # Default Ollama URL\n)\n\n# Works the same as cloud providers\ntweet = await ai.generate_tweet(\n    topic=\"Python automation\",\n    style=\"educational\"\n)\n</code></pre>"},{"location":"guides/ai/#configuration","title":"Configuration","text":""},{"location":"guides/ai/#environment-variables","title":"Environment Variables","text":"<pre><code># OpenAI\nexport OPENAI_API_KEY=\"sk-...\"\n\n# Anthropic\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\n\n# Ollama (optional)\nexport OLLAMA_BASE_URL=\"http://localhost:11434\"\n</code></pre>"},{"location":"guides/ai/#in-code","title":"In Code","text":"<pre><code>from xeepy.ai import ContentGenerator\n\n# Full configuration\nai = ContentGenerator(\n    provider=\"openai\",\n    api_key=\"sk-...\",\n    model=\"gpt-4-turbo\",\n    temperature=0.7,      # Creativity (0-1)\n    max_tokens=500,       # Response length\n    timeout=30,           # Request timeout\n)\n</code></pre>"},{"location":"guides/ai/#config-file","title":"Config File","text":"<pre><code># xeepy.toml\n[xeepy.ai]\ndefault_provider = \"openai\"\n\n[xeepy.ai.openai]\nmodel = \"gpt-4-turbo\"\ntemperature = 0.7\nmax_tokens = 500\n\n[xeepy.ai.anthropic]\nmodel = \"claude-3-sonnet\"\ntemperature = 0.7\n\n[xeepy.ai.ollama]\nmodel = \"llama3\"\nbase_url = \"http://localhost:11434\"\n</code></pre>"},{"location":"guides/ai/#cli-commands","title":"CLI Commands","text":"<pre><code># Generate tweet\nxeepy ai tweet \"Python tips\" --style educational\n\n# Generate thread\nxeepy ai thread \"My startup journey\" --length 5\n\n# Generate reply\nxeepy ai reply \"https://x.com/user/status/123\" --style supportive\n\n# Analyze sentiment\nxeepy ai sentiment \"This is amazing!\"\n\n# Detect bot\nxeepy ai bot-check suspicious_username\n</code></pre>"},{"location":"guides/ai/#best-practices","title":"Best Practices","text":"<ol> <li>Review AI content - Always review before posting</li> <li>Add personal touch - Edit AI suggestions to match your voice</li> <li>Don't over-automate - Mix AI and human content</li> <li>Use appropriate models - GPT-4 for quality, GPT-3.5 for speed</li> <li>Consider local AI - Ollama for privacy and cost savings</li> <li>Test styles - Experiment to find what works for your audience</li> </ol>"},{"location":"guides/ai/bot-detection/","title":"Bot and Spam Detection","text":"<p>Identify automated accounts, spam bots, and inauthentic behavior using AI-powered analysis of user profiles and activity patterns.</p>"},{"location":"guides/ai/bot-detection/#overview","title":"Overview","text":"<p>Bot detection analyzes user profiles, posting patterns, and content characteristics to identify automated or suspicious accounts. This helps maintain community quality, filter engagement data, and protect against spam attacks.</p>"},{"location":"guides/ai/bot-detection/#use-cases","title":"Use Cases","text":"<ul> <li>Follower Quality Audit: Identify bot followers inflating counts</li> <li>Engagement Authenticity: Filter fake engagement from analytics</li> <li>Community Protection: Block spam accounts proactively</li> <li>Influencer Vetting: Verify influencer audience authenticity</li> <li>Research Data Quality: Clean datasets of bot-generated content</li> </ul>"},{"location":"guides/ai/bot-detection/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom xeepy.ai import ContentGenerator\n\nasync def detect_bot():\n    async with Xeepy() as x:\n        ai = ContentGenerator(\n            provider=\"openai\",\n            api_key=\"your-api-key\",\n            model=\"gpt-4\"\n        )\n\n        # Get user profile\n        profile = await x.scrape.profile(\"suspicious_account\")\n\n        # Analyze for bot indicators\n        result = await ai.detect_bot(profile)\n\n        print(f\"Bot probability: {result.bot_score:.1%}\")\n        print(f\"Classification: {result.classification}\")  # bot, human, suspicious\n        print(f\"\\nRisk factors:\")\n        for factor in result.risk_factors:\n            print(f\"  - {factor}\")\n\nasyncio.run(detect_bot())\n</code></pre>"},{"location":"guides/ai/bot-detection/#detailed-bot-analysis","title":"Detailed Bot Analysis","text":"<pre><code>async def detailed_bot_analysis():\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n        # Get comprehensive profile data\n        profile = await x.scrape.profile(\"account_to_check\")\n        tweets = await x.scrape.tweets(\"account_to_check\", limit=50)\n\n        # Full analysis with tweet patterns\n        result = await ai.detect_bot(\n            profile=profile,\n            tweets=tweets,\n            analyze_patterns=True  # Check posting patterns\n        )\n\n        print(f\"Account: @{profile.username}\")\n        print(f\"Bot Score: {result.bot_score:.1%}\")\n        print(f\"\\nIndicators:\")\n        print(f\"  Profile completeness: {result.indicators['profile_score']:.1%}\")\n        print(f\"  Activity patterns: {result.indicators['activity_score']:.1%}\")\n        print(f\"  Content originality: {result.indicators['content_score']:.1%}\")\n        print(f\"  Network authenticity: {result.indicators['network_score']:.1%}\")\n\nasyncio.run(detailed_bot_analysis())\n</code></pre>"},{"location":"guides/ai/bot-detection/#batch-follower-audit","title":"Batch Follower Audit","text":"<pre><code>async def audit_followers():\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n        # Get followers\n        followers = await x.scrape.followers(\"your_account\", limit=500)\n\n        # Analyze each follower\n        bots = []\n        suspicious = []\n        humans = []\n\n        for follower in followers:\n            result = await ai.detect_bot(follower)\n\n            if result.bot_score &gt; 0.8:\n                bots.append(follower)\n            elif result.bot_score &gt; 0.5:\n                suspicious.append(follower)\n            else:\n                humans.append(follower)\n\n        total = len(followers)\n        print(f\"Follower Audit Results:\")\n        print(f\"  Likely bots: {len(bots)} ({len(bots)/total*100:.1f}%)\")\n        print(f\"  Suspicious: {len(suspicious)} ({len(suspicious)/total*100:.1f}%)\")\n        print(f\"  Likely humans: {len(humans)} ({len(humans)/total*100:.1f}%)\")\n\n        # Export bot list\n        x.export.to_csv(bots, \"potential_bots.csv\")\n\nasyncio.run(audit_followers())\n</code></pre>"},{"location":"guides/ai/bot-detection/#heuristic-detection-no-ai","title":"Heuristic Detection (No AI)","text":"<pre><code>async def heuristic_bot_detection():\n    async with Xeepy() as x:\n        profile = await x.scrape.profile(\"account_to_check\")\n\n        # Rule-based bot indicators\n        score = 0\n        factors = []\n\n        # Check profile age vs tweet count\n        account_age_days = (datetime.now() - profile.created_at).days\n        tweets_per_day = profile.tweet_count / max(account_age_days, 1)\n\n        if tweets_per_day &gt; 50:\n            score += 0.3\n            factors.append(f\"High posting rate: {tweets_per_day:.1f} tweets/day\")\n\n        # Check follower/following ratio\n        if profile.followers_count &gt; 0:\n            ratio = profile.following_count / profile.followers_count\n            if ratio &gt; 10:\n                score += 0.2\n                factors.append(f\"Suspicious ratio: following {ratio:.1f}x more\")\n\n        # Check default profile\n        if profile.default_profile_image:\n            score += 0.2\n            factors.append(\"Default profile image\")\n\n        # Check bio length\n        if len(profile.bio or \"\") &lt; 10:\n            score += 0.1\n            factors.append(\"Minimal or no bio\")\n\n        # Check username pattern\n        import re\n        if re.search(r'\\d{6,}$', profile.username):\n            score += 0.2\n            factors.append(\"Username ends with many numbers\")\n\n        print(f\"Heuristic bot score: {min(score, 1.0):.1%}\")\n        print(f\"Factors: {factors}\")\n\nasyncio.run(heuristic_bot_detection())\n</code></pre>"},{"location":"guides/ai/bot-detection/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>profile</code> User required User profile object <code>tweets</code> list None Recent tweets for pattern analysis <code>analyze_patterns</code> bool True Check posting time patterns <code>threshold</code> float 0.7 Bot classification threshold <code>include_network</code> bool False Analyze follower network <p>Combine Methods</p> <p>For best results, combine AI analysis with heuristic rules. Use fast heuristics for initial filtering, then AI for borderline cases.</p> <p>False Positives</p> <p>Automated accounts aren't always malicious. Brand accounts, news bots, and utility accounts may trigger bot detection. Consider context.</p>"},{"location":"guides/ai/bot-detection/#posting-pattern-analysis","title":"Posting Pattern Analysis","text":"<pre><code>async def analyze_posting_patterns():\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n        tweets = await x.scrape.tweets(\"account_to_check\", limit=100)\n\n        # Analyze timing patterns\n        from collections import Counter\n\n        hours = Counter(t.created_at.hour for t in tweets)\n        days = Counter(t.created_at.weekday() for t in tweets)\n\n        # Calculate variance (bots often have low variance)\n        hour_variance = max(hours.values()) / (sum(hours.values()) / 24) if hours else 0\n\n        print(\"Posting time distribution:\")\n        print(f\"  Peak hour concentration: {hour_variance:.2f}x average\")\n\n        # Check for precise intervals\n        if len(tweets) &gt; 1:\n            intervals = []\n            for i in range(1, len(tweets)):\n                delta = (tweets[i-1].created_at - tweets[i].created_at).total_seconds()\n                intervals.append(delta)\n\n            avg_interval = sum(intervals) / len(intervals)\n            variance = sum((i - avg_interval)**2 for i in intervals) / len(intervals)\n\n            if variance &lt; 100:  # Very consistent timing\n                print(\"  \u26a0\ufe0f Suspiciously consistent posting intervals\")\n\nasyncio.run(analyze_posting_patterns())\n</code></pre>"},{"location":"guides/ai/bot-detection/#content-similarity-detection","title":"Content Similarity Detection","text":"<pre><code>async def detect_content_spam():\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n        tweets = await x.scrape.tweets(\"account_to_check\", limit=50)\n\n        # Check for repeated content\n        texts = [t.text for t in tweets]\n\n        # Simple duplicate check\n        unique_ratio = len(set(texts)) / len(texts)\n\n        if unique_ratio &lt; 0.5:\n            print(f\"\u26a0\ufe0f Low content uniqueness: {unique_ratio:.1%}\")\n\n        # AI-based similarity analysis\n        result = await ai.analyze_content_patterns(texts)\n\n        print(f\"\\nContent Analysis:\")\n        print(f\"  Template usage detected: {result.template_score:.1%}\")\n        print(f\"  Promotional content ratio: {result.promo_ratio:.1%}\")\n        print(f\"  Repetitive phrases: {len(result.repeated_phrases)}\")\n\nasyncio.run(detect_content_spam())\n</code></pre>"},{"location":"guides/ai/bot-detection/#influencer-audience-audit","title":"Influencer Audience Audit","text":"<pre><code>async def audit_influencer():\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n        influencer = \"influencer_username\"\n\n        # Sample followers\n        followers = await x.scrape.followers(influencer, limit=200)\n\n        # Analyze sample\n        bot_count = 0\n        for follower in followers:\n            result = await ai.detect_bot(follower)\n            if result.bot_score &gt; 0.7:\n                bot_count += 1\n\n        fake_percentage = bot_count / len(followers) * 100\n\n        print(f\"Influencer Audit: @{influencer}\")\n        print(f\"  Sampled followers: {len(followers)}\")\n        print(f\"  Likely fake: {bot_count} ({fake_percentage:.1f}%)\")\n        print(f\"  Estimated real followers: {int((100 - fake_percentage) / 100 * await x.scrape.profile(influencer).followers_count):,}\")\n\nasyncio.run(audit_influencer())\n</code></pre>"},{"location":"guides/ai/bot-detection/#export-detection-results","title":"Export Detection Results","text":"<pre><code>async def export_bot_report():\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n        followers = await x.scrape.followers(\"your_account\", limit=500)\n\n        report = []\n        for follower in followers:\n            result = await ai.detect_bot(follower)\n            report.append({\n                \"username\": follower.username,\n                \"followers\": follower.followers_count,\n                \"bot_score\": result.bot_score,\n                \"classification\": result.classification,\n                \"risk_factors\": \", \".join(result.risk_factors)\n            })\n\n        x.export.to_csv(report, \"bot_detection_report.csv\")\n        print(f\"Exported analysis of {len(report)} accounts\")\n\nasyncio.run(export_bot_report())\n</code></pre>"},{"location":"guides/ai/bot-detection/#best-practices","title":"Best Practices","text":"<ol> <li>Sample Appropriately: For large follower bases, use representative samples</li> <li>Set Thresholds Carefully: Too strict catches false positives; too loose misses bots</li> <li>Consider Context: News aggregators and brand accounts may appear bot-like</li> <li>Use Multiple Signals: Combine profile, content, and behavior analysis</li> <li>Regular Audits: Bot tactics evolve; audit periodically</li> <li>Document Decisions: Keep records of why accounts were flagged</li> </ol>"},{"location":"guides/ai/bot-detection/#related-guides","title":"Related Guides","text":"<ul> <li>AI-Powered Replies</li> <li>Sentiment Analysis</li> <li>Audience Insights</li> </ul>"},{"location":"guides/ai/content/","title":"AI Content Generation","text":"<p>Generate high-quality tweets, threads, and content with AI.</p>"},{"location":"guides/ai/content/#setup","title":"Setup","text":"<pre><code>from xeepy.ai import ContentGenerator\n\n# OpenAI\nai = ContentGenerator(\n    provider=\"openai\",\n    api_key=\"sk-...\",\n    model=\"gpt-4\"  # or \"gpt-3.5-turbo\"\n)\n\n# Anthropic\nai = ContentGenerator(\n    provider=\"anthropic\",\n    api_key=\"sk-ant-...\",\n    model=\"claude-3-opus-20240229\"\n)\n\n# Ollama (local, free)\nai = ContentGenerator(\n    provider=\"ollama\",\n    model=\"llama2\"\n)\n</code></pre>"},{"location":"guides/ai/content/#generate-tweets","title":"Generate Tweets","text":""},{"location":"guides/ai/content/#basic-tweet-generation","title":"Basic Tweet Generation","text":"<pre><code># Simple tweet\ntweet = await ai.generate_tweet(\n    topic=\"Python productivity tips\"\n)\n# Output: \"\ud83d\udc0d Python tip: Use list comprehensions instead of loops for cleaner, \n#          faster code. [x**2 for x in range(10)] &gt; traditional loops every time!\"\n\n# With style\ntweet = await ai.generate_tweet(\n    topic=\"New feature announcement\",\n    style=\"excited\"\n)\n\n# With constraints\ntweet = await ai.generate_tweet(\n    topic=\"AI in healthcare\",\n    max_length=200,  # Leave room for link\n    include_emoji=True,\n    include_hashtags=False\n)\n</code></pre>"},{"location":"guides/ai/content/#style-options","title":"Style Options","text":"Style Description Use Case <code>professional</code> Formal, authoritative B2B, announcements <code>casual</code> Friendly, conversational Community building <code>excited</code> Energetic, enthusiastic Launches, wins <code>educational</code> Informative, teaching Tips, how-tos <code>witty</code> Clever, humorous Engagement bait <code>supportive</code> Encouraging, helpful Replies, community <code>controversial</code> Bold, thought-provoking Engagement, debates <pre><code># Professional announcement\ntweet = await ai.generate_tweet(\n    topic=\"Q4 earnings exceeded expectations\",\n    style=\"professional\",\n    tone_modifiers=[\"confident\", \"data-driven\"]\n)\n\n# Casual engagement\ntweet = await ai.generate_tweet(\n    topic=\"Friday mood\",\n    style=\"casual\",\n    include_emoji=True\n)\n\n# Controversial take\ntweet = await ai.generate_tweet(\n    topic=\"Remote work vs office\",\n    style=\"controversial\",\n    avoid=[\"offensive\", \"political\"]\n)\n</code></pre>"},{"location":"guides/ai/content/#generate-threads","title":"Generate Threads","text":""},{"location":"guides/ai/content/#basic-thread","title":"Basic Thread","text":"<pre><code>thread = await ai.generate_thread(\n    topic=\"10 Python tips every developer should know\",\n    thread_length=10,\n    style=\"educational\"\n)\n\n# Returns list of tweets\nfor i, tweet in enumerate(thread, 1):\n    print(f\"{i}. {tweet}\")\n\n# Post the thread\nasync with Xeepy() as x:\n    await x.post.thread(thread)\n</code></pre>"},{"location":"guides/ai/content/#structured-thread","title":"Structured Thread","text":"<pre><code># Thread with specific structure\nthread = await ai.generate_thread(\n    topic=\"How I grew from 0 to 100k followers\",\n    thread_length=12,\n    structure={\n        \"hook\": \"Attention-grabbing opener\",\n        \"story\": \"4-5 story tweets\",\n        \"lessons\": \"3-4 key takeaways\",\n        \"cta\": \"Call to action\"\n    },\n    style=\"storytelling\"\n)\n</code></pre>"},{"location":"guides/ai/content/#thread-from-content","title":"Thread from Content","text":"<pre><code># Convert article to thread\nthread = await ai.article_to_thread(\n    content=\"\"\"\n    Your long-form article or blog post content here...\n    Can be multiple paragraphs with detailed information.\n    \"\"\",\n    max_tweets=15,\n    preserve_key_points=True\n)\n\n# Convert video transcript to thread\nthread = await ai.transcript_to_thread(\n    transcript=\"Video transcript text...\",\n    max_tweets=10,\n    highlight_quotes=True\n)\n</code></pre>"},{"location":"guides/ai/content/#generate-replies","title":"Generate Replies","text":""},{"location":"guides/ai/content/#contextual-replies","title":"Contextual Replies","text":"<pre><code># Reply to a tweet\nreply = await ai.generate_reply(\n    tweet_text=\"Just launched my first SaaS product! Nervous but excited \ud83d\ude80\",\n    style=\"supportive\"\n)\n# Output: \"Congratulations on the launch! \ud83c\udf89 The first step is always the hardest. \n#          What problem does it solve? Would love to check it out!\"\n\n# Reply matching energy\nreply = await ai.generate_reply(\n    tweet_text=\"This is the worst take I've seen all week\",\n    style=\"diplomatic\",\n    avoid=[\"confrontational\", \"dismissive\"]\n)\n</code></pre>"},{"location":"guides/ai/content/#reply-with-context","title":"Reply with Context","text":"<pre><code># Use conversation context\nreply = await ai.generate_reply(\n    tweet_text=\"What's your favorite programming language?\",\n    context={\n        \"author_bio\": \"Python developer, ML enthusiast\",\n        \"your_expertise\": [\"Python\", \"AI\", \"Data Science\"],\n        \"conversation_tone\": \"friendly\"\n    },\n    style=\"knowledgeable\"\n)\n\n# Reply to question\nreply = await ai.generate_reply(\n    tweet_text=\"How do I get started with machine learning?\",\n    style=\"helpful\",\n    include_resources=True,\n    max_length=280\n)\n</code></pre>"},{"location":"guides/ai/content/#brand-voice-training","title":"Brand Voice Training","text":""},{"location":"guides/ai/content/#train-on-your-content","title":"Train on Your Content","text":"<pre><code># Provide example tweets\nai.train_voice(\n    example_tweets=[\n        \"Your past tweet 1\",\n        \"Your past tweet 2\", \n        \"Your past tweet 3\",\n        # More examples = better results (10-20 recommended)\n    ]\n)\n\n# Or load from your timeline\nasync with Xeepy() as x:\n    my_tweets = await x.scrape.tweets(\"your_username\", limit=50)\n    ai.train_voice(example_tweets=[t.text for t in my_tweets])\n</code></pre>"},{"location":"guides/ai/content/#brand-guidelines","title":"Brand Guidelines","text":"<pre><code>ai.set_brand_guidelines({\n    \"tone\": \"friendly, professional\",\n    \"personality\": [\"helpful\", \"knowledgeable\", \"approachable\"],\n    \"vocabulary\": {\n        \"prefer\": [\"innovative\", \"streamlined\", \"powerful\"],\n        \"avoid\": [\"revolutionary\", \"disrupting\", \"game-changing\"]\n    },\n    \"formatting\": {\n        \"use_emoji\": True,\n        \"emoji_style\": \"minimal\",  # or \"expressive\"\n        \"hashtags\": False,\n        \"threads_over_long_tweets\": True\n    },\n    \"topics\": {\n        \"focus\": [\"AI\", \"productivity\", \"development\"],\n        \"avoid\": [\"politics\", \"controversy\"]\n    }\n})\n\n# Generate with brand voice\ntweet = await ai.generate_tweet(\n    topic=\"New feature release\",\n    use_brand_voice=True\n)\n</code></pre>"},{"location":"guides/ai/content/#batch-generation","title":"Batch Generation","text":"<pre><code># Generate multiple tweets at once\ntopics = [\n    \"Python tip of the day\",\n    \"Remote work productivity\",\n    \"AI industry news\",\n    \"Developer tools recommendation\"\n]\n\ntweets = await ai.generate_batch(\n    topics=topics,\n    style=\"educational\",\n    deduplicate=True  # Ensure variety\n)\n\n# Generate week of content\ncontent_calendar = await ai.generate_content_calendar(\n    themes=[\"Python\", \"AI\", \"Productivity\"],\n    days=7,\n    posts_per_day=3,\n    styles=[\"educational\", \"engaging\", \"promotional\"]\n)\n</code></pre>"},{"location":"guides/ai/content/#advanced-options","title":"Advanced Options","text":""},{"location":"guides/ai/content/#temperature-control","title":"Temperature Control","text":"<pre><code># Low temperature = more consistent, predictable\nai_consistent = ContentGenerator(\n    provider=\"openai\",\n    api_key=\"...\",\n    temperature=0.3\n)\n\n# High temperature = more creative, varied\nai_creative = ContentGenerator(\n    provider=\"openai\", \n    api_key=\"...\",\n    temperature=0.9\n)\n</code></pre>"},{"location":"guides/ai/content/#ab-testing","title":"A/B Testing","text":"<pre><code># Generate variations for testing\nvariations = await ai.generate_variations(\n    topic=\"New product launch\",\n    count=5,\n    vary_by=[\"style\", \"emoji\", \"length\"]\n)\n\n# Test which performs best\nfor i, tweet in enumerate(variations):\n    print(f\"Variation {i+1}: {tweet}\")\n</code></pre>"},{"location":"guides/ai/content/#content-hooks","title":"Content Hooks","text":"<pre><code># Generate attention-grabbing hooks\nhooks = await ai.generate_hooks(\n    topic=\"Why Python is perfect for beginners\",\n    hook_types=[\n        \"question\",      # \"Ever wondered why...?\"\n        \"statistic\",     # \"90% of developers...\"\n        \"controversial\", # \"Unpopular opinion:...\"\n        \"story\",         # \"Last year I...\"\n        \"bold_claim\"     # \"Python will change...\"\n    ]\n)\n</code></pre>"},{"location":"guides/ai/content/#quality-control","title":"Quality Control","text":""},{"location":"guides/ai/content/#review-system","title":"Review System","text":"<pre><code># Generate with quality check\ntweet = await ai.generate_tweet(\n    topic=\"AI ethics\",\n    quality_check=True  # AI reviews its own output\n)\n\n# Manual approval workflow\nclass ContentApprover:\n    def __init__(self, ai):\n        self.ai = ai\n        self.pending = []\n\n    async def generate_for_review(self, topic, count=5):\n        tweets = await self.ai.generate_batch([topic] * count)\n        self.pending.extend(tweets)\n        return tweets\n\n    def approve(self, index):\n        return self.pending.pop(index)\n\n    def reject_all(self):\n        self.pending.clear()\n</code></pre>"},{"location":"guides/ai/content/#content-filtering","title":"Content Filtering","text":"<pre><code># Built-in safety\nai = ContentGenerator(\n    provider=\"openai\",\n    api_key=\"...\",\n    safety_filter=True,  # Filter inappropriate content\n    fact_check=True,     # Flag potentially false claims\n    brand_safe=True      # Avoid controversial topics\n)\n</code></pre>"},{"location":"guides/ai/content/#cost-optimization","title":"Cost Optimization","text":"<pre><code># Use cheaper model for drafts, expensive for final\ndraft_ai = ContentGenerator(provider=\"openai\", model=\"gpt-3.5-turbo\")\nfinal_ai = ContentGenerator(provider=\"openai\", model=\"gpt-4\")\n\n# Generate drafts cheaply\ndrafts = await draft_ai.generate_batch(topics, count=10)\n\n# Polish best ones with GPT-4\nbest_draft = drafts[0]  # After human selection\nfinal = await final_ai.improve_tweet(best_draft)\n\n# Or use local AI for high volume\nlocal_ai = ContentGenerator(provider=\"ollama\", model=\"llama2\")\nbulk_content = await local_ai.generate_batch(topics, count=100)\n</code></pre>"},{"location":"guides/ai/content/#integration-with-posting","title":"Integration with Posting","text":"<pre><code>async with Xeepy() as x:\n    # Generate and post\n    tweet = await ai.generate_tweet(\"Python tip of the day\")\n    await x.post.tweet(tweet)\n\n    # Generate and schedule\n    tweets = await ai.generate_batch(\n        [\"Monday motivation\", \"Tuesday tip\", \"Wednesday wisdom\"],\n        style=\"inspiring\"\n    )\n\n    for i, tweet in enumerate(tweets):\n        schedule_time = f\"2024-01-{15+i} 09:00\"\n        await x.schedule.tweet(tweet, schedule_time)\n</code></pre>"},{"location":"guides/ai/content/#best-practices","title":"Best Practices","text":"<p>Quality Tips</p> <ul> <li>Provide specific topics, not vague prompts</li> <li>Use brand guidelines for consistency</li> <li>Review all AI content before posting</li> <li>A/B test different styles</li> </ul> <p>Authenticity</p> <ul> <li>Don't rely 100% on AI - add your personal touch</li> <li>Disclose AI usage if required by platform</li> <li>Mix AI content with genuine posts</li> <li>Respond to comments personally</li> </ul>"},{"location":"guides/ai/content/#next-steps","title":"Next Steps","text":"<p> Smart Replies - AI-powered reply generation</p> <p> Sentiment Analysis - Analyze content tone</p> <p> Content Calendar - Automate content scheduling</p>"},{"location":"guides/ai/replies/","title":"AI-Powered Reply Generation","text":"<p>Generate contextual, engaging replies to tweets using AI models from OpenAI, Anthropic, or local Ollama instances.</p>"},{"location":"guides/ai/replies/#overview","title":"Overview","text":"<p>The AI reply generator creates human-like responses to tweets based on context, tone, and your specified style preferences. It supports multiple AI providers and can be fine-tuned for different engagement strategies.</p>"},{"location":"guides/ai/replies/#use-cases","title":"Use Cases","text":"<ul> <li>Community Engagement: Respond thoughtfully to followers at scale</li> <li>Brand Voice Consistency: Maintain consistent tone across all replies</li> <li>Time Savings: Draft replies quickly for human review</li> <li>Multilingual Support: Generate replies in multiple languages</li> <li>A/B Testing: Test different response styles</li> </ul>"},{"location":"guides/ai/replies/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom xeepy.ai import ContentGenerator\n\nasync def generate_reply():\n    ai = ContentGenerator(\n        provider=\"openai\",\n        api_key=\"your-openai-api-key\",\n        model=\"gpt-4\"\n    )\n\n    # Generate a reply to a tweet\n    reply = await ai.generate_reply(\n        tweet_text=\"Just launched my first Python package! \ud83c\udf89\",\n        style=\"supportive\"\n    )\n\n    print(f\"Generated reply: {reply}\")\n\nasyncio.run(generate_reply())\n</code></pre>"},{"location":"guides/ai/replies/#provider-configuration","title":"Provider Configuration","text":"<pre><code>from xeepy.ai import ContentGenerator\n\n# OpenAI Configuration\nopenai_ai = ContentGenerator(\n    provider=\"openai\",\n    api_key=\"sk-...\",\n    model=\"gpt-4-turbo\"  # or gpt-3.5-turbo\n)\n\n# Anthropic (Claude) Configuration\nanthropic_ai = ContentGenerator(\n    provider=\"anthropic\",\n    api_key=\"sk-ant-...\",\n    model=\"claude-3-opus-20240229\"  # or claude-3-sonnet\n)\n\n# Ollama (Local) Configuration\nollama_ai = ContentGenerator(\n    provider=\"ollama\",\n    base_url=\"http://localhost:11434\",\n    model=\"llama2\"  # or mistral, codellama\n)\n</code></pre>"},{"location":"guides/ai/replies/#reply-styles","title":"Reply Styles","text":"<pre><code>async def styled_replies():\n    ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n    tweet = \"Struggling to learn machine learning. Any tips?\"\n\n    # Different style options\n    styles = [\"supportive\", \"professional\", \"witty\", \"educational\", \"casual\"]\n\n    for style in styles:\n        reply = await ai.generate_reply(\n            tweet_text=tweet,\n            style=style,\n            max_length=280\n        )\n        print(f\"\\n[{style.upper()}]: {reply}\")\n\nasyncio.run(styled_replies())\n</code></pre>"},{"location":"guides/ai/replies/#context-aware-replies","title":"Context-Aware Replies","text":"<pre><code>async def contextual_reply():\n    ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n    # Provide additional context for better replies\n    reply = await ai.generate_reply(\n        tweet_text=\"What's the best programming language for beginners?\",\n        style=\"educational\",\n        context={\n            \"your_expertise\": \"Python developer with 10 years experience\",\n            \"brand_voice\": \"helpful, encouraging, practical\",\n            \"target_audience\": \"coding beginners\",\n            \"avoid_topics\": [\"language wars\", \"negative comparisons\"]\n        },\n        max_length=280\n    )\n\n    print(f\"Reply: {reply}\")\n\nasyncio.run(contextual_reply())\n</code></pre>"},{"location":"guides/ai/replies/#batch-reply-generation","title":"Batch Reply Generation","text":"<pre><code>async def batch_replies():\n    from xeepy import Xeepy\n    from xeepy.ai import ContentGenerator\n\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n        # Get tweets to reply to\n        mentions = await x.scrape.search(\"@yourusername\", limit=20)\n\n        replies = []\n        for tweet in mentions:\n            reply = await ai.generate_reply(\n                tweet_text=tweet.text,\n                style=\"professional\",\n                max_length=280\n            )\n            replies.append({\n                \"original_tweet\": tweet.text,\n                \"author\": tweet.author.username,\n                \"generated_reply\": reply\n            })\n            print(f\"@{tweet.author.username}: {reply[:50]}...\")\n\n        # Export for review before posting\n        x.export.to_json(replies, \"draft_replies.json\")\n\nasyncio.run(batch_replies())\n</code></pre>"},{"location":"guides/ai/replies/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>provider</code> str required openai, anthropic, ollama <code>api_key</code> str required* API key for provider <code>model</code> str required Model identifier <code>base_url</code> str None Custom API endpoint (Ollama) <code>temperature</code> float 0.7 Creativity level (0-1) <code>max_tokens</code> int 150 Maximum response length <p>Temperature Settings</p> <ul> <li>0.3-0.5: More consistent, predictable replies</li> <li>0.6-0.8: Balanced creativity and coherence</li> <li>0.9-1.0: More creative, varied responses</li> </ul> <p>Human Review</p> <p>Always review AI-generated content before posting. AI can produce inappropriate or off-brand responses that require editing.</p>"},{"location":"guides/ai/replies/#thread-reply-generation","title":"Thread Reply Generation","text":"<pre><code>async def generate_thread_reply():\n    ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n    # Generate reply with thread context\n    reply = await ai.generate_reply(\n        tweet_text=\"This is the 5th tweet in the thread\",\n        thread_context=[\n            \"First tweet introducing the topic\",\n            \"Second tweet with more details\",\n            \"Third tweet with examples\",\n            \"Fourth tweet addressing concerns\"\n        ],\n        style=\"educational\",\n        max_length=280\n    )\n\n    print(f\"Thread-aware reply: {reply}\")\n\nasyncio.run(generate_thread_reply())\n</code></pre>"},{"location":"guides/ai/replies/#custom-prompts","title":"Custom Prompts","text":"<pre><code>async def custom_prompt_reply():\n    ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n    # Use a completely custom prompt\n    reply = await ai.generate_reply(\n        tweet_text=\"What's your favorite productivity tool?\",\n        custom_prompt=\"\"\"\n        You are a tech productivity expert. Generate a reply that:\n        1. Recommends one specific tool\n        2. Gives a brief reason why\n        3. Keeps it under 200 characters\n        4. Ends with an engaging question\n\n        Tweet to reply to: {tweet_text}\n        \"\"\",\n        max_length=280\n    )\n\n    print(f\"Custom reply: {reply}\")\n\nasyncio.run(custom_prompt_reply())\n</code></pre>"},{"location":"guides/ai/replies/#quality-filtering","title":"Quality Filtering","text":"<pre><code>async def quality_filtered_replies():\n    ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n    tweet = \"Just shipped my first feature!\"\n\n    # Generate multiple options and score them\n    options = []\n    for _ in range(3):\n        reply = await ai.generate_reply(\n            tweet_text=tweet,\n            style=\"supportive\",\n            temperature=0.8  # Higher variance\n        )\n\n        # Score the reply\n        score = await ai.score_reply(\n            original_tweet=tweet,\n            reply=reply,\n            criteria=[\"relevance\", \"engagement_potential\", \"brand_alignment\"]\n        )\n\n        options.append({\"reply\": reply, \"score\": score})\n\n    # Pick the best one\n    best = max(options, key=lambda x: x[\"score\"])\n    print(f\"Best reply (score {best['score']:.2f}): {best['reply']}\")\n\nasyncio.run(quality_filtered_replies())\n</code></pre>"},{"location":"guides/ai/replies/#best-practices","title":"Best Practices","text":"<ol> <li>Review Before Posting: Never auto-post without human review</li> <li>Define Brand Voice: Create clear guidelines for the AI to follow</li> <li>Use Temperature Wisely: Lower for consistency, higher for creativity</li> <li>Provide Context: More context leads to better, more relevant replies</li> <li>A/B Test Styles: Try different styles to see what resonates</li> <li>Monitor Quality: Regularly audit generated content for accuracy</li> <li>Respect Rate Limits: AI API calls have costs and limits</li> </ol>"},{"location":"guides/ai/replies/#related-guides","title":"Related Guides","text":"<ul> <li>Sentiment Analysis</li> <li>Bot Detection</li> <li>Engagement Analysis</li> </ul>"},{"location":"guides/ai/sentiment/","title":"Sentiment Analysis Guide","text":"<p>Analyze the emotional tone and sentiment of tweets to understand audience reactions, monitor brand perception, and identify trends.</p>"},{"location":"guides/ai/sentiment/#overview","title":"Overview","text":"<p>Sentiment analysis uses AI to classify tweets as positive, negative, or neutral, and can extract more nuanced emotions like excitement, frustration, or curiosity. This enables data-driven understanding of how your content and brand are perceived.</p>"},{"location":"guides/ai/sentiment/#use-cases","title":"Use Cases","text":"<ul> <li>Brand Monitoring: Track sentiment around brand mentions</li> <li>Campaign Analysis: Measure emotional response to campaigns</li> <li>Crisis Detection: Identify negative sentiment spikes early</li> <li>Content Optimization: Understand what content generates positive reactions</li> <li>Competitor Comparison: Compare sentiment across competing brands</li> </ul>"},{"location":"guides/ai/sentiment/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom xeepy.ai import ContentGenerator\n\nasync def analyze_sentiment():\n    ai = ContentGenerator(\n        provider=\"openai\",\n        api_key=\"your-api-key\",\n        model=\"gpt-4\"\n    )\n\n    # Analyze a single tweet\n    result = await ai.analyze_sentiment(\n        \"This new product update is absolutely amazing! Love the new features \ud83c\udf89\"\n    )\n\n    print(f\"Sentiment: {result.sentiment}\")      # positive, negative, neutral\n    print(f\"Confidence: {result.confidence:.2%}\")\n    print(f\"Emotions: {result.emotions}\")        # joy, excitement, etc.\n\nasyncio.run(analyze_sentiment())\n</code></pre>"},{"location":"guides/ai/sentiment/#batch-sentiment-analysis","title":"Batch Sentiment Analysis","text":"<pre><code>async def batch_sentiment_analysis():\n    from xeepy import Xeepy\n    from xeepy.ai import ContentGenerator\n\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n        # Get tweets mentioning your brand\n        tweets = await x.scrape.search(\"@yourbrand\", limit=100)\n\n        # Analyze sentiment for all tweets\n        results = await ai.analyze_sentiment_batch([t.text for t in tweets])\n\n        # Aggregate results\n        positive = sum(1 for r in results if r.sentiment == \"positive\")\n        negative = sum(1 for r in results if r.sentiment == \"negative\")\n        neutral = sum(1 for r in results if r.sentiment == \"neutral\")\n\n        print(f\"Sentiment Distribution:\")\n        print(f\"  Positive: {positive} ({positive/len(results)*100:.1f}%)\")\n        print(f\"  Negative: {negative} ({negative/len(results)*100:.1f}%)\")\n        print(f\"  Neutral: {neutral} ({neutral/len(results)*100:.1f}%)\")\n\nasyncio.run(batch_sentiment_analysis())\n</code></pre>"},{"location":"guides/ai/sentiment/#detailed-emotion-analysis","title":"Detailed Emotion Analysis","text":"<pre><code>async def emotion_analysis():\n    ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n    text = \"I've been waiting for this feature for months! Finally it's here!\"\n\n    # Get detailed emotional breakdown\n    result = await ai.analyze_sentiment(\n        text,\n        detailed_emotions=True\n    )\n\n    print(f\"Overall: {result.sentiment} ({result.confidence:.2%})\")\n    print(f\"\\nEmotion scores:\")\n    for emotion, score in result.emotion_scores.items():\n        bar = \"\u2588\" * int(score * 20)\n        print(f\"  {emotion:12}: {bar} {score:.2f}\")\n\nasyncio.run(emotion_analysis())\n</code></pre>"},{"location":"guides/ai/sentiment/#time-series-sentiment-tracking","title":"Time-Series Sentiment Tracking","text":"<pre><code>async def sentiment_over_time():\n    from xeepy import Xeepy\n    from xeepy.ai import ContentGenerator\n    from collections import defaultdict\n\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n        # Get tweets from the past week\n        tweets = await x.scrape.search(\n            \"@yourbrand\",\n            limit=500,\n            since=\"2024-01-01\",\n            until=\"2024-01-07\"\n        )\n\n        # Group by date and analyze\n        daily_sentiment = defaultdict(lambda: {\"positive\": 0, \"negative\": 0, \"neutral\": 0})\n\n        for tweet in tweets:\n            result = await ai.analyze_sentiment(tweet.text)\n            date = tweet.created_at.strftime(\"%Y-%m-%d\")\n            daily_sentiment[date][result.sentiment] += 1\n\n        # Report\n        print(\"Daily Sentiment Breakdown:\")\n        for date in sorted(daily_sentiment.keys()):\n            counts = daily_sentiment[date]\n            total = sum(counts.values())\n            pos_pct = counts[\"positive\"] / total * 100\n            print(f\"  {date}: {pos_pct:.1f}% positive ({total} tweets)\")\n\nasyncio.run(sentiment_over_time())\n</code></pre>"},{"location":"guides/ai/sentiment/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>provider</code> str required AI provider (openai, anthropic, ollama) <code>model</code> str required Model to use for analysis <code>detailed_emotions</code> bool False Return detailed emotion scores <code>language</code> str \"auto\" Language hint for analysis <code>threshold</code> float 0.6 Confidence threshold for classification <p>Model Selection</p> <p>For sentiment analysis, <code>gpt-3.5-turbo</code> often provides sufficient accuracy at lower cost. Reserve <code>gpt-4</code> for nuanced cases requiring deeper understanding.</p> <p>Language Support</p> <p>Sentiment analysis works best with English text. For other languages, specify the <code>language</code> parameter for better accuracy.</p>"},{"location":"guides/ai/sentiment/#comparative-brand-analysis","title":"Comparative Brand Analysis","text":"<pre><code>async def compare_brand_sentiment():\n    from xeepy import Xeepy\n    from xeepy.ai import ContentGenerator\n\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n        brands = [\"@yourbrand\", \"@competitor1\", \"@competitor2\"]\n        results = {}\n\n        for brand in brands:\n            tweets = await x.scrape.search(brand, limit=100)\n            sentiments = await ai.analyze_sentiment_batch([t.text for t in tweets])\n\n            positive = sum(1 for s in sentiments if s.sentiment == \"positive\")\n            results[brand] = positive / len(sentiments) * 100\n\n        print(\"Brand Sentiment Comparison (% Positive):\")\n        for brand, score in sorted(results.items(), key=lambda x: -x[1]):\n            bar = \"\u2588\" * int(score / 2)\n            print(f\"  {brand:15}: {bar} {score:.1f}%\")\n\nasyncio.run(compare_brand_sentiment())\n</code></pre>"},{"location":"guides/ai/sentiment/#alert-on-negative-sentiment","title":"Alert on Negative Sentiment","text":"<pre><code>async def sentiment_alerts():\n    from xeepy import Xeepy\n    from xeepy.ai import ContentGenerator\n\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n        # Monitor for negative sentiment\n        tweets = await x.scrape.search(\"@yourbrand\", limit=50)\n\n        alerts = []\n        for tweet in tweets:\n            result = await ai.analyze_sentiment(tweet.text)\n\n            if result.sentiment == \"negative\" and result.confidence &gt; 0.8:\n                alerts.append({\n                    \"author\": tweet.author.username,\n                    \"text\": tweet.text,\n                    \"confidence\": result.confidence,\n                    \"url\": tweet.url\n                })\n\n        if alerts:\n            print(f\"\u26a0\ufe0f {len(alerts)} high-confidence negative mentions:\")\n            for alert in alerts:\n                print(f\"\\n@{alert['author']} ({alert['confidence']:.0%} negative)\")\n                print(f\"  {alert['text'][:100]}...\")\n                print(f\"  {alert['url']}\")\n\nasyncio.run(sentiment_alerts())\n</code></pre>"},{"location":"guides/ai/sentiment/#sentiment-by-topic","title":"Sentiment by Topic","text":"<pre><code>async def sentiment_by_topic():\n    from xeepy import Xeepy\n    from xeepy.ai import ContentGenerator\n\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n        topics = [\"pricing\", \"support\", \"features\", \"performance\"]\n        results = {}\n\n        for topic in topics:\n            tweets = await x.scrape.search(f\"@yourbrand {topic}\", limit=50)\n\n            if tweets:\n                sentiments = await ai.analyze_sentiment_batch([t.text for t in tweets])\n                positive = sum(1 for s in sentiments if s.sentiment == \"positive\")\n                results[topic] = positive / len(sentiments) * 100\n\n        print(\"Sentiment by Topic:\")\n        for topic, score in sorted(results.items(), key=lambda x: -x[1]):\n            indicator = \"\u2705\" if score &gt; 60 else \"\u26a0\ufe0f\" if score &gt; 40 else \"\u274c\"\n            print(f\"  {indicator} {topic}: {score:.1f}% positive\")\n\nasyncio.run(sentiment_by_topic())\n</code></pre>"},{"location":"guides/ai/sentiment/#export-sentiment-data","title":"Export Sentiment Data","text":"<pre><code>async def export_sentiment_report():\n    from xeepy import Xeepy\n    from xeepy.ai import ContentGenerator\n\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n        tweets = await x.scrape.search(\"@yourbrand\", limit=200)\n\n        report_data = []\n        for tweet in tweets:\n            result = await ai.analyze_sentiment(tweet.text, detailed_emotions=True)\n            report_data.append({\n                \"tweet_id\": tweet.id,\n                \"author\": tweet.author.username,\n                \"text\": tweet.text,\n                \"sentiment\": result.sentiment,\n                \"confidence\": result.confidence,\n                \"primary_emotion\": max(result.emotion_scores, key=result.emotion_scores.get),\n                \"created_at\": tweet.created_at\n            })\n\n        x.export.to_csv(report_data, \"sentiment_report.csv\")\n        print(f\"Exported {len(report_data)} analyzed tweets\")\n\nasyncio.run(export_sentiment_report())\n</code></pre>"},{"location":"guides/ai/sentiment/#best-practices","title":"Best Practices","text":"<ol> <li>Use Sufficient Sample Size: Analyze at least 100 tweets for reliable trends</li> <li>Set Confidence Thresholds: Filter low-confidence classifications</li> <li>Consider Context: Sarcasm and irony can confuse sentiment analysis</li> <li>Track Over Time: Monitor sentiment trends, not just snapshots</li> <li>Combine with Engagement: High engagement + negative sentiment = urgent attention</li> <li>Segment by Topic: Break down sentiment by specific topics or features</li> <li>Validate Regularly: Spot-check AI classifications for accuracy</li> </ol>"},{"location":"guides/ai/sentiment/#related-guides","title":"Related Guides","text":"<ul> <li>AI-Powered Replies</li> <li>Bot Detection</li> <li>Engagement Analysis</li> </ul>"},{"location":"guides/ai/targeting/","title":"Smart Audience Targeting","text":"<p>Use AI to identify and reach your ideal audience by analyzing user profiles, interests, and engagement patterns for precision targeting.</p>"},{"location":"guides/ai/targeting/#overview","title":"Overview","text":"<p>Smart targeting leverages AI to build audience segments based on sophisticated criteria beyond simple follower counts. Analyze interests, engagement quality, content preferences, and network connections to find users most likely to engage with your content.</p>"},{"location":"guides/ai/targeting/#use-cases","title":"Use Cases","text":"<ul> <li>Content Distribution: Find users interested in specific topics</li> <li>Growth Targeting: Identify high-value accounts to engage with</li> <li>Campaign Optimization: Target users most likely to convert</li> <li>Community Building: Find potential community members</li> <li>Influencer Discovery: Identify micro-influencers in your niche</li> </ul>"},{"location":"guides/ai/targeting/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom xeepy.ai import ContentGenerator\n\nasync def basic_targeting():\n    async with Xeepy() as x:\n        ai = ContentGenerator(\n            provider=\"openai\",\n            api_key=\"your-api-key\",\n            model=\"gpt-4\"\n        )\n\n        # Define target criteria\n        targets = await ai.find_audience(\n            interests=[\"machine learning\", \"Python\", \"data science\"],\n            min_followers=500,\n            max_followers=50000,\n            engagement_rate_min=0.02,\n            limit=100\n        )\n\n        for user in targets:\n            print(f\"@{user.username}: {user.match_score:.0%} match\")\n            print(f\"  Followers: {user.followers_count}\")\n            print(f\"  Interests: {', '.join(user.detected_interests)}\\n\")\n\nasyncio.run(basic_targeting())\n</code></pre>"},{"location":"guides/ai/targeting/#interest-based-targeting","title":"Interest-Based Targeting","text":"<pre><code>async def interest_targeting():\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n        # Analyze users from a seed account's followers\n        seed_followers = await x.scrape.followers(\"industry_leader\", limit=500)\n\n        # Filter by interests\n        matching_users = []\n\n        for user in seed_followers:\n            # Get user's recent tweets for interest analysis\n            try:\n                tweets = await x.scrape.tweets(user.username, limit=20)\n\n                # AI analyzes interests from content\n                interests = await ai.analyze_interests(\n                    bio=user.bio,\n                    tweets=[t.text for t in tweets]\n                )\n\n                # Check for target interests\n                target_interests = {\"AI\", \"startup\", \"technology\"}\n                if interests.primary &amp; target_interests:\n                    matching_users.append({\n                        \"user\": user,\n                        \"interests\": interests.primary,\n                        \"confidence\": interests.confidence\n                    })\n            except:\n                continue\n\n        print(f\"Found {len(matching_users)} matching users\")\n        for match in matching_users[:10]:\n            print(f\"@{match['user'].username}: {match['interests']}\")\n\nasyncio.run(interest_targeting())\n</code></pre>"},{"location":"guides/ai/targeting/#engagement-quality-scoring","title":"Engagement Quality Scoring","text":"<pre><code>async def engagement_quality_targeting():\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n        # Get potential targets\n        candidates = await x.scrape.search_users(\"python developer\", limit=200)\n\n        scored_users = []\n\n        for user in candidates:\n            # Calculate engagement quality score\n            tweets = await x.scrape.tweets(user.username, limit=20)\n\n            if not tweets:\n                continue\n\n            # Engagement rate\n            total_engagement = sum(t.likes + t.retweets + t.reply_count for t in tweets)\n            avg_engagement = total_engagement / len(tweets)\n            engagement_rate = avg_engagement / max(user.followers_count, 1)\n\n            # Reply ratio (shows active engagement)\n            reply_tweets = [t for t in tweets if t.is_reply]\n            reply_ratio = len(reply_tweets) / len(tweets)\n\n            # Quality score\n            quality_score = (\n                0.4 * min(engagement_rate * 50, 1) +  # Engagement rate\n                0.3 * min(reply_ratio * 2, 1) +       # Reply activity\n                0.3 * (1 - min(user.followers_count / 100000, 1))  # Not too big\n            )\n\n            scored_users.append({\n                \"user\": user,\n                \"quality_score\": quality_score,\n                \"engagement_rate\": engagement_rate\n            })\n\n        # Sort by quality\n        scored_users.sort(key=lambda x: x[\"quality_score\"], reverse=True)\n\n        print(\"Top quality targets:\")\n        for item in scored_users[:20]:\n            print(f\"@{item['user'].username}: {item['quality_score']:.2f} score, {item['engagement_rate']:.2%} ER\")\n\nasyncio.run(engagement_quality_targeting())\n</code></pre>"},{"location":"guides/ai/targeting/#lookalike-audience","title":"Lookalike Audience","text":"<pre><code>async def find_lookalike_audience():\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n        # Define seed users (your best followers/customers)\n        seed_users = [\"user1\", \"user2\", \"user3\", \"user4\", \"user5\"]\n\n        # Analyze seed user characteristics\n        seed_profiles = []\n        for username in seed_users:\n            profile = await x.scrape.profile(username)\n            tweets = await x.scrape.tweets(username, limit=30)\n\n            characteristics = await ai.analyze_user_characteristics(\n                profile=profile,\n                tweets=[t.text for t in tweets]\n            )\n            seed_profiles.append(characteristics)\n\n        # Build composite profile\n        composite = await ai.build_audience_profile(seed_profiles)\n\n        print(\"Ideal Audience Profile:\")\n        print(f\"  Interests: {composite.interests}\")\n        print(f\"  Follower range: {composite.follower_range}\")\n        print(f\"  Content style: {composite.content_style}\")\n        print(f\"  Activity level: {composite.activity_level}\")\n\n        # Find similar users\n        search_terms = composite.search_keywords[:3]\n        candidates = []\n\n        for term in search_terms:\n            users = await x.scrape.search_users(term, limit=100)\n            candidates.extend(users)\n\n        # Score against composite\n        lookalikes = []\n        for user in candidates:\n            similarity = await ai.calculate_similarity(user, composite)\n            if similarity &gt; 0.7:\n                lookalikes.append({\"user\": user, \"similarity\": similarity})\n\n        lookalikes.sort(key=lambda x: x[\"similarity\"], reverse=True)\n        print(f\"\\nFound {len(lookalikes)} lookalike users\")\n\nasyncio.run(find_lookalike_audience())\n</code></pre>"},{"location":"guides/ai/targeting/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>interests</code> list required Target interest keywords <code>min_followers</code> int 0 Minimum follower count <code>max_followers</code> int None Maximum follower count <code>engagement_rate_min</code> float 0 Minimum engagement rate <code>verified_only</code> bool False Only verified accounts <code>language</code> str None Filter by primary language <code>location</code> str None Filter by location <p>Micro-Influencers</p> <p>For engagement, target accounts with 1k-50k followers. They often have higher engagement rates and more genuine interactions than larger accounts.</p> <p>Targeting Ethics</p> <p>Use targeting responsibly. Don't spam targeted users. Focus on providing value through genuine engagement.</p>"},{"location":"guides/ai/targeting/#geographic-targeting","title":"Geographic Targeting","text":"<pre><code>async def geographic_targeting():\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n        # Search for users in specific locations\n        locations = [\"San Francisco\", \"New York\", \"Austin\"]\n\n        targeted_users = []\n\n        for location in locations:\n            users = await x.scrape.search_users(\n                f\"software engineer {location}\",\n                limit=50\n            )\n\n            for user in users:\n                # Verify location from profile\n                if user.location and location.lower() in user.location.lower():\n                    targeted_users.append({\n                        \"user\": user,\n                        \"location\": location\n                    })\n\n        print(f\"Found {len(targeted_users)} location-verified users:\")\n        for item in targeted_users[:20]:\n            print(f\"@{item['user'].username} - {item['location']}\")\n\nasyncio.run(geographic_targeting())\n</code></pre>"},{"location":"guides/ai/targeting/#industry-targeting","title":"Industry Targeting","text":"<pre><code>async def industry_targeting():\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n        # Define industry keywords\n        industry_keywords = {\n            \"fintech\": [\"fintech\", \"payments\", \"banking tech\", \"neobank\"],\n            \"healthtech\": [\"healthtech\", \"digital health\", \"medtech\", \"biotech\"],\n            \"edtech\": [\"edtech\", \"e-learning\", \"online education\"]\n        }\n\n        industry_users = {}\n\n        for industry, keywords in industry_keywords.items():\n            industry_users[industry] = []\n\n            for keyword in keywords:\n                users = await x.scrape.search_users(keyword, limit=30)\n\n                for user in users:\n                    # AI validates industry relevance\n                    relevance = await ai.check_industry_relevance(\n                        user_bio=user.bio,\n                        target_industry=industry\n                    )\n\n                    if relevance &gt; 0.7:\n                        industry_users[industry].append(user)\n\n        for industry, users in industry_users.items():\n            print(f\"\\n{industry.upper()}: {len(users)} users found\")\n\nasyncio.run(industry_targeting())\n</code></pre>"},{"location":"guides/ai/targeting/#engagement-potential-scoring","title":"Engagement Potential Scoring","text":"<pre><code>async def score_engagement_potential():\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n        candidates = await x.scrape.search_users(\"AI enthusiast\", limit=100)\n\n        scored = []\n        for user in candidates:\n            # AI predicts engagement potential\n            potential = await ai.predict_engagement_potential(\n                profile=user,\n                your_content_topics=[\"machine learning\", \"tutorials\", \"Python\"]\n            )\n\n            scored.append({\n                \"user\": user,\n                \"potential\": potential.score,\n                \"reasoning\": potential.reasoning\n            })\n\n        # Top engagement potential\n        scored.sort(key=lambda x: x[\"potential\"], reverse=True)\n\n        print(\"Highest engagement potential:\")\n        for item in scored[:10]:\n            print(f\"\\n@{item['user'].username}: {item['potential']:.0%}\")\n            print(f\"  {item['reasoning']}\")\n\nasyncio.run(score_engagement_potential())\n</code></pre>"},{"location":"guides/ai/targeting/#export-targeting-data","title":"Export Targeting Data","text":"<pre><code>async def export_targets():\n    async with Xeepy() as x:\n        ai = ContentGenerator(provider=\"openai\", api_key=\"...\", model=\"gpt-4\")\n\n        targets = await ai.find_audience(\n            interests=[\"startup\", \"SaaS\", \"B2B\"],\n            min_followers=1000,\n            max_followers=20000,\n            limit=500\n        )\n\n        export_data = [{\n            \"username\": t.username,\n            \"name\": t.name,\n            \"followers\": t.followers_count,\n            \"bio\": t.bio,\n            \"match_score\": t.match_score,\n            \"interests\": \", \".join(t.detected_interests)\n        } for t in targets]\n\n        x.export.to_csv(export_data, \"target_audience.csv\")\n        print(f\"Exported {len(export_data)} targeted users\")\n\nasyncio.run(export_targets())\n</code></pre>"},{"location":"guides/ai/targeting/#best-practices","title":"Best Practices","text":"<ol> <li>Quality Over Quantity: Target fewer high-quality accounts over many low-quality ones</li> <li>Validate Interests: Verify interest signals from multiple sources (bio, tweets, engagement)</li> <li>Respect Boundaries: Don't spam targeted users; provide genuine value</li> <li>Test and Iterate: Refine targeting criteria based on engagement results</li> <li>Segment Audiences: Create different targeting strategies for different goals</li> <li>Track Results: Measure conversion from targeted outreach</li> </ol>"},{"location":"guides/ai/targeting/#related-guides","title":"Related Guides","text":"<ul> <li>Audience Insights</li> <li>Bot Detection</li> <li>Engagement Analysis</li> </ul>"},{"location":"guides/analytics/","title":"Analytics Guide","text":"<p>Xeepy provides deep analytics to understand your X/Twitter performance, audience, and optimal strategies.</p>"},{"location":"guides/analytics/#overview","title":"Overview","text":"<ul> <li> <p> Growth Analytics</p> <p>Track follower trends and growth patterns</p> </li> <li> <p> Engagement Analytics</p> <p>Analyze likes, retweets, and interactions</p> </li> <li> <p> Best Time to Post</p> <p>Find when your audience is most active</p> </li> <li> <p> Audience Insights</p> <p>Understand who follows you</p> </li> <li> <p> Competitor Analysis</p> <p>Benchmark against competitors</p> </li> <li> <p> Reports</p> <p>Generate comprehensive reports</p> </li> </ul>"},{"location":"guides/analytics/#quick-start","title":"Quick Start","text":"<pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    # Get comprehensive analytics\n    analytics = await x.analytics.dashboard()\n\n    print(f\"\"\"\n    \ud83d\udcca Analytics Dashboard\n    \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    Followers: {analytics.followers:,} ({analytics.follower_change:+d} this week)\n    Engagement Rate: {analytics.engagement_rate:.2%}\n    Best Day to Post: {analytics.best_day}\n    Best Hour: {analytics.best_hour}:00\n\n    Top Content Type: {analytics.top_content_type}\n    Avg Likes per Post: {analytics.avg_likes:.1f}\n    \"\"\")\n</code></pre>"},{"location":"guides/analytics/#growth-analytics","title":"Growth Analytics","text":""},{"location":"guides/analytics/#track-growth-over-time","title":"Track Growth Over Time","text":"<pre><code>async with Xeepy() as x:\n    growth = await x.analytics.track_growth(period=\"30d\")\n\n    print(f\"\"\"\n    \ud83d\udcc8 30-Day Growth Report\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    Starting: {growth.start_followers:,}\n    Current: {growth.end_followers:,}\n    Net Change: {growth.net_change:+,}\n    Growth Rate: {growth.growth_rate:.1%}\n    Avg Daily: {growth.avg_daily:+.1f}\n    \"\"\")\n\n    # Daily breakdown\n    for day in growth.daily_data:\n        print(f\"  {day.date}: {day.followers:,} ({day.change:+d})\")\n</code></pre>"},{"location":"guides/analytics/#growth-projections","title":"Growth Projections","text":"<pre><code>async with Xeepy() as x:\n    projection = await x.analytics.growth_projection(\n        based_on=\"90d\",  # Historical data to use\n        project_to=\"1y\"   # Project 1 year ahead\n    )\n\n    print(f\"Projected followers in 1 year: {projection.projected_count:,}\")\n    print(f\"Confidence: {projection.confidence:.0%}\")\n\n    # Milestones\n    for milestone in projection.milestones:\n        print(f\"  {milestone.count:,} followers: {milestone.estimated_date}\")\n</code></pre>"},{"location":"guides/analytics/#engagement-analytics","title":"Engagement Analytics","text":""},{"location":"guides/analytics/#analyze-your-engagement","title":"Analyze Your Engagement","text":"<pre><code>async with Xeepy() as x:\n    engagement = await x.analytics.engagement_analysis(period=\"30d\")\n\n    print(f\"\"\"\n    \ud83d\udcac Engagement Analysis\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    Total Tweets: {engagement.total_tweets}\n    Total Likes: {engagement.total_likes:,}\n    Total Retweets: {engagement.total_retweets:,}\n    Total Replies: {engagement.total_replies:,}\n\n    Engagement Rate: {engagement.rate:.2%}\n    Avg Likes/Tweet: {engagement.avg_likes:.1f}\n    Avg Retweets: {engagement.avg_retweets:.1f}\n\n    Top Performing Tweet:\n    \"{engagement.top_tweet.text[:100]}...\"\n    ({engagement.top_tweet.likes:,} likes)\n    \"\"\")\n</code></pre>"},{"location":"guides/analytics/#content-performance-by-type","title":"Content Performance by Type","text":"<pre><code>async with Xeepy() as x:\n    by_type = await x.analytics.engagement_by_type()\n\n    print(\"\ud83d\udcdd Content Type Performance:\")\n    print(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\")\n\n    for content_type in by_type.types:\n        print(f\"\"\"\n    {content_type.name}:\n      Posts: {content_type.count}\n      Avg Engagement: {content_type.avg_engagement:.1f}\n      Best: {content_type.engagement_rate:.2%}\n        \"\"\")\n</code></pre>"},{"location":"guides/analytics/#hashtag-performance","title":"Hashtag Performance","text":"<pre><code>async with Xeepy() as x:\n    hashtags = await x.analytics.hashtag_performance(period=\"90d\")\n\n    print(\"# Hashtag Performance:\")\n    for tag in hashtags.top_hashtags[:10]:\n        print(f\"  {tag.name}: {tag.avg_engagement:.1f} avg engagement ({tag.uses} uses)\")\n</code></pre>"},{"location":"guides/analytics/#best-time-to-post","title":"Best Time to Post","text":""},{"location":"guides/analytics/#find-optimal-posting-times","title":"Find Optimal Posting Times","text":"<pre><code>async with Xeepy() as x:\n    best_times = await x.analytics.best_time_to_post()\n\n    print(f\"\"\"\n    \u23f0 Best Times to Post\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n    Best Day: {best_times.best_day}\n    Best Hour: {best_times.best_hour}:00 (your timezone)\n\n    Top 5 Time Slots:\n    \"\"\")\n\n    for slot in best_times.top_slots[:5]:\n        print(f\"  {slot.day} {slot.hour}:00 - {slot.engagement_score:.1f} score\")\n</code></pre>"},{"location":"guides/analytics/#heatmap-data","title":"Heatmap Data","text":"<pre><code>async with Xeepy() as x:\n    heatmap = await x.analytics.engagement_heatmap()\n\n    # Get engagement score for each hour of each day\n    for day in heatmap.days:\n        print(f\"\\n{day.name}:\")\n        for hour in day.hours:\n            bar = \"\u2588\" * int(hour.score / 10)\n            print(f\"  {hour.hour:02d}:00 {bar} {hour.score:.0f}\")\n</code></pre>"},{"location":"guides/analytics/#audience-insights","title":"Audience Insights","text":""},{"location":"guides/analytics/#understand-your-audience","title":"Understand Your Audience","text":"<pre><code>async with Xeepy() as x:\n    audience = await x.analytics.audience_insights(sample_size=1000)\n\n    print(f\"\"\"\n    \ud83d\udc65 Audience Insights\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n    Demographics:\n      Location: {', '.join(audience.top_locations[:5])}\n      Primary Language: {audience.primary_language}\n\n    Interests:\n      {', '.join(audience.top_interests[:10])}\n\n    Account Types:\n      Creators: {audience.creator_percentage:.0%}\n      Businesses: {audience.business_percentage:.0%}\n      Personal: {audience.personal_percentage:.0%}\n\n    Activity:\n      Active (last 7d): {audience.active_percentage:.0%}\n      Avg Followers: {audience.avg_follower_count:,.0f}\n    \"\"\")\n</code></pre>"},{"location":"guides/analytics/#influencer-analysis","title":"Influencer Analysis","text":"<pre><code>async with Xeepy() as x:\n    influencers = await x.analytics.follower_influencers(min_followers=10000)\n\n    print(\"\ud83c\udf1f Your Most Influential Followers:\")\n    for inf in influencers[:20]:\n        print(f\"  @{inf.username} - {inf.followers_count:,} followers\")\n        print(f\"    Niche: {inf.niche}\")\n        print(f\"    Engagement Rate: {inf.engagement_rate:.2%}\")\n</code></pre>"},{"location":"guides/analytics/#competitor-analysis","title":"Competitor Analysis","text":""},{"location":"guides/analytics/#compare-to-competitors","title":"Compare to Competitors","text":"<pre><code>async with Xeepy() as x:\n    comparison = await x.analytics.competitor_analysis(\n        competitors=[\"competitor1\", \"competitor2\", \"competitor3\"],\n        metrics=[\"followers\", \"engagement\", \"posting_frequency\"]\n    )\n\n    print(\"\ud83d\udcca Competitor Comparison\")\n    print(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\")\n\n    for comp in comparison.competitors:\n        print(f\"\"\"\n    @{comp.username}:\n      Followers: {comp.followers:,}\n      Engagement Rate: {comp.engagement_rate:.2%}\n      Posts/Week: {comp.posts_per_week:.1f}\n      Top Hashtags: {', '.join(comp.top_hashtags[:3])}\n        \"\"\")\n</code></pre>"},{"location":"guides/analytics/#content-gap-analysis","title":"Content Gap Analysis","text":"<pre><code>async with Xeepy() as x:\n    gaps = await x.analytics.content_gaps(competitors=[\"comp1\", \"comp2\"])\n\n    print(\"\ud83d\udca1 Content Opportunities:\")\n    for gap in gaps.opportunities:\n        print(f\"  - {gap.topic}: {gap.competitor_engagement:.0f} avg engagement\")\n        print(f\"    You haven't posted about this recently\")\n</code></pre>"},{"location":"guides/analytics/#generating-reports","title":"Generating Reports","text":""},{"location":"guides/analytics/#weekly-report","title":"Weekly Report","text":"<pre><code>async with Xeepy() as x:\n    report = await x.analytics.generate_report(\n        period=\"7d\",\n        format=\"markdown\",\n        include=[\"growth\", \"engagement\", \"top_content\", \"audience\"]\n    )\n\n    # Save report\n    with open(\"weekly_report.md\", \"w\") as f:\n        f.write(report.content)\n\n    print(f\"\u2713 Report saved: weekly_report.md\")\n</code></pre>"},{"location":"guides/analytics/#pdf-report","title":"PDF Report","text":"<pre><code>async with Xeepy() as x:\n    report = await x.analytics.generate_report(\n        period=\"30d\",\n        format=\"pdf\",\n        template=\"professional\"  # or \"minimal\", \"detailed\"\n    )\n\n    report.save(\"monthly_report.pdf\")\n</code></pre>"},{"location":"guides/analytics/#scheduled-reports","title":"Scheduled Reports","text":"<pre><code>async with Xeepy() as x:\n    # Configure automatic reports\n    await x.analytics.schedule_report(\n        frequency=\"weekly\",\n        day=\"monday\",\n        time=\"09:00\",\n        recipients=[\"you@email.com\"],\n        format=\"pdf\"\n    )\n</code></pre>"},{"location":"guides/analytics/#cli-commands","title":"CLI Commands","text":"<pre><code># Growth analytics\nxeepy analytics growth --period 30d\n\n# Engagement analysis\nxeepy analytics engagement --period 7d\n\n# Best time to post\nxeepy analytics best-time\n\n# Audience insights\nxeepy analytics audience --sample 1000\n\n# Competitor analysis\nxeepy analytics competitors comp1,comp2,comp3\n\n# Generate report\nxeepy analytics report --period 30d --format pdf --output report.pdf\n</code></pre>"},{"location":"guides/analytics/#data-export","title":"Data Export","text":"<pre><code>async with Xeepy() as x:\n    # Export raw analytics data\n    growth_data = await x.analytics.track_growth(\"90d\")\n    x.export.to_csv(growth_data.daily_data, \"growth_data.csv\")\n\n    # Export for data science\n    x.export.to_parquet(growth_data.daily_data, \"growth_data.parquet\")\n\n    # Export to database\n    await x.export.to_database(\n        growth_data,\n        \"postgresql://user:pass@host/db\",\n        table=\"growth_metrics\"\n    )\n</code></pre>"},{"location":"guides/analytics/#best-practices","title":"Best Practices","text":"<ol> <li>Analyze trends, not snapshots - Look at data over time</li> <li>Compare fairly - Use similar time periods for comparisons</li> <li>Account for seasonality - Some periods are naturally slower</li> <li>Focus on actionable insights - Optimize based on data</li> <li>Regular reviews - Check analytics weekly at minimum</li> </ol>"},{"location":"guides/analytics/audience/","title":"Audience Insights","text":"<p>Understand your Twitter audience through demographic analysis, interest mapping, and behavioral patterns to create more resonant content.</p>"},{"location":"guides/analytics/audience/#overview","title":"Overview","text":"<p>Audience insights reveal who your followers are, what they care about, and how they interact with content. This knowledge enables targeted content creation, better engagement strategies, and more effective community building.</p>"},{"location":"guides/analytics/audience/#use-cases","title":"Use Cases","text":"<ul> <li>Content Strategy: Create content that resonates with your audience</li> <li>Persona Development: Build detailed audience personas</li> <li>Partnership Targeting: Find alignment for collaborations</li> <li>Growth Strategy: Identify similar audiences to target</li> <li>Product Development: Understand customer segments</li> </ul>"},{"location":"guides/analytics/audience/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom xeepy.analytics import AudienceAnalyzer\n\nasync def analyze_audience():\n    async with Xeepy() as x:\n        analyzer = AudienceAnalyzer(x)\n\n        # Get audience overview\n        insights = await analyzer.get_insights(\n            username=\"your_username\",\n            sample_size=500\n        )\n\n        print(\"Audience Overview:\")\n        print(f\"  Total followers: {insights.total_followers:,}\")\n        print(f\"  Verified followers: {insights.verified_count} ({insights.verified_pct:.1%})\")\n        print(f\"  Avg follower count: {insights.avg_follower_count:,.0f}\")\n        print(f\"  Median follower count: {insights.median_follower_count:,}\")\n\nasyncio.run(analyze_audience())\n</code></pre>"},{"location":"guides/analytics/audience/#interest-analysis","title":"Interest Analysis","text":"<pre><code>async def analyze_interests():\n    async with Xeepy() as x:\n        analyzer = AudienceAnalyzer(x)\n\n        # Analyze follower interests from bios and tweets\n        interests = await analyzer.interest_analysis(\n            username=\"your_username\",\n            sample_size=300\n        )\n\n        print(\"Top Audience Interests:\")\n        print(\"-\" * 50)\n\n        for interest in interests.top_interests[:15]:\n            bar = \"\u2588\" * int(interest.percentage * 50)\n            print(f\"{interest.name:20} {bar} {interest.percentage:.1%}\")\n\n        print(f\"\\nInterest categories:\")\n        for category, pct in interests.categories.items():\n            print(f\"  {category}: {pct:.1%}\")\n\nasyncio.run(analyze_interests())\n</code></pre>"},{"location":"guides/analytics/audience/#follower-size-distribution","title":"Follower Size Distribution","text":"<pre><code>async def follower_distribution():\n    async with Xeepy() as x:\n        analyzer = AudienceAnalyzer(x)\n\n        followers = await x.scrape.followers(\"your_username\", limit=1000)\n\n        # Categorize by follower size\n        categories = {\n            \"Nano (0-1k)\": 0,\n            \"Micro (1k-10k)\": 0,\n            \"Mid (10k-100k)\": 0,\n            \"Macro (100k-1M)\": 0,\n            \"Mega (1M+)\": 0\n        }\n\n        for f in followers:\n            count = f.followers_count\n            if count &lt; 1000:\n                categories[\"Nano (0-1k)\"] += 1\n            elif count &lt; 10000:\n                categories[\"Micro (1k-10k)\"] += 1\n            elif count &lt; 100000:\n                categories[\"Mid (10k-100k)\"] += 1\n            elif count &lt; 1000000:\n                categories[\"Macro (100k-1M)\"] += 1\n            else:\n                categories[\"Mega (1M+)\"] += 1\n\n        print(\"Follower Size Distribution:\")\n        print(\"-\" * 50)\n\n        total = len(followers)\n        for category, count in categories.items():\n            pct = count / total * 100\n            bar = \"\u2588\" * int(pct / 2)\n            print(f\"{category:18} {bar} {pct:.1f}% ({count:,})\")\n\nasyncio.run(follower_distribution())\n</code></pre>"},{"location":"guides/analytics/audience/#geographic-distribution","title":"Geographic Distribution","text":"<pre><code>async def geographic_analysis():\n    async with Xeepy() as x:\n        analyzer = AudienceAnalyzer(x)\n\n        # Analyze location data from profiles\n        geo = await analyzer.geographic_distribution(\n            username=\"your_username\",\n            sample_size=500\n        )\n\n        print(\"Geographic Distribution:\")\n        print(\"-\" * 50)\n\n        print(\"\\nTop Countries:\")\n        for country in geo.top_countries[:10]:\n            bar = \"\u2588\" * int(country.percentage * 30)\n            print(f\"{country.name:20} {bar} {country.percentage:.1%}\")\n\n        print(\"\\nTop Cities:\")\n        for city in geo.top_cities[:10]:\n            print(f\"  {city.name}: {city.percentage:.1%}\")\n\n        print(f\"\\nPrimary language: {geo.primary_language}\")\n\nasyncio.run(geographic_analysis())\n</code></pre>"},{"location":"guides/analytics/audience/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>username</code> str required Account to analyze <code>sample_size</code> int 500 Followers to sample <code>include_tweets</code> bool True Analyze follower tweets <code>deep_analysis</code> bool False Full profile analysis <p>Sample Size Trade-offs</p> <p>Larger samples provide more accurate insights but take longer. For accounts with 10k+ followers, a 500-1000 sample usually provides reliable data.</p> <p>Location Data Limitations</p> <p>Only ~30% of Twitter users share location. Results represent users with location data, which may skew toward certain demographics.</p>"},{"location":"guides/analytics/audience/#activity-level-analysis","title":"Activity Level Analysis","text":"<pre><code>async def activity_analysis():\n    async with Xeepy() as x:\n        analyzer = AudienceAnalyzer(x)\n\n        followers = await x.scrape.followers(\"your_username\", limit=500)\n\n        # Categorize by activity\n        active_today = 0\n        active_week = 0\n        active_month = 0\n        dormant = 0\n\n        from datetime import datetime, timedelta\n        now = datetime.now()\n\n        for f in followers:\n            if f.last_tweet_date:\n                days_ago = (now - f.last_tweet_date).days\n                if days_ago &lt;= 1:\n                    active_today += 1\n                elif days_ago &lt;= 7:\n                    active_week += 1\n                elif days_ago &lt;= 30:\n                    active_month += 1\n                else:\n                    dormant += 1\n\n        total = len(followers)\n        print(\"Follower Activity Levels:\")\n        print(f\"  Active today: {active_today} ({active_today/total*100:.1f}%)\")\n        print(f\"  Active this week: {active_week} ({active_week/total*100:.1f}%)\")\n        print(f\"  Active this month: {active_month} ({active_month/total*100:.1f}%)\")\n        print(f\"  Dormant (30+ days): {dormant} ({dormant/total*100:.1f}%)\")\n\nasyncio.run(activity_analysis())\n</code></pre>"},{"location":"guides/analytics/audience/#influential-followers","title":"Influential Followers","text":"<pre><code>async def find_influential_followers():\n    async with Xeepy() as x:\n        analyzer = AudienceAnalyzer(x)\n\n        # Get most influential followers\n        influential = await analyzer.influential_followers(\n            username=\"your_username\",\n            limit=100,\n            min_followers=10000\n        )\n\n        print(\"Most Influential Followers:\")\n        print(\"-\" * 60)\n\n        for user in influential[:20]:\n            verified = \"\u2713\" if user.verified else \" \"\n            print(f\"{verified} @{user.username:20} {user.followers_count:&gt;10,} followers\")\n            if user.bio:\n                print(f\"   {user.bio[:60]}...\")\n            print()\n\nasyncio.run(find_influential_followers())\n</code></pre>"},{"location":"guides/analytics/audience/#audience-overlap-analysis","title":"Audience Overlap Analysis","text":"<pre><code>async def audience_overlap():\n    async with Xeepy() as x:\n        analyzer = AudienceAnalyzer(x)\n\n        # Compare your audience with another account\n        overlap = await analyzer.audience_overlap(\n            username1=\"your_username\",\n            username2=\"similar_account\",\n            sample_size=500\n        )\n\n        print(\"Audience Overlap Analysis:\")\n        print(\"-\" * 50)\n        print(f\"Your followers sampled: {overlap.sample1_size}\")\n        print(f\"Their followers sampled: {overlap.sample2_size}\")\n        print(f\"Overlap: {overlap.overlap_count} ({overlap.overlap_percentage:.1%})\")\n        print(f\"\\nShared followers:\")\n\n        for user in overlap.shared_followers[:10]:\n            print(f\"  @{user.username} ({user.followers_count:,} followers)\")\n\nasyncio.run(audience_overlap())\n</code></pre>"},{"location":"guides/analytics/audience/#persona-development","title":"Persona Development","text":"<pre><code>async def develop_personas():\n    async with Xeepy() as x:\n        analyzer = AudienceAnalyzer(x)\n\n        # AI-powered persona generation\n        personas = await analyzer.generate_personas(\n            username=\"your_username\",\n            num_personas=3,\n            sample_size=500\n        )\n\n        print(\"Audience Personas:\")\n        print(\"=\" * 60)\n\n        for i, persona in enumerate(personas, 1):\n            print(f\"\\nPersona {i}: {persona.name}\")\n            print(\"-\" * 40)\n            print(f\"Description: {persona.description}\")\n            print(f\"Percentage: {persona.percentage:.1%} of audience\")\n            print(f\"Interests: {', '.join(persona.interests)}\")\n            print(f\"Goals: {', '.join(persona.goals)}\")\n            print(f\"Content preferences: {', '.join(persona.content_preferences)}\")\n            print(f\"Best engagement times: {persona.best_times}\")\n\nasyncio.run(develop_personas())\n</code></pre>"},{"location":"guides/analytics/audience/#engagement-segmentation","title":"Engagement Segmentation","text":"<pre><code>async def segment_by_engagement():\n    async with Xeepy() as x:\n        analyzer = AudienceAnalyzer(x)\n\n        # Segment followers by their engagement with you\n        segments = await analyzer.engagement_segments(\n            username=\"your_username\",\n            days=30\n        )\n\n        print(\"Follower Engagement Segments:\")\n        print(\"-\" * 50)\n\n        print(f\"\\n\ud83d\udd25 Super Fans (engaged 10+ times): {len(segments.super_fans)}\")\n        for user in segments.super_fans[:5]:\n            print(f\"   @{user.username}: {user.engagement_count} interactions\")\n\n        print(f\"\\n\ud83d\udc9a Regular Engagers (3-9 times): {len(segments.regulars)}\")\n        print(f\"\\n\ud83d\udc4b Occasional (1-2 times): {len(segments.occasional)}\")\n        print(f\"\\n\ud83d\ude34 Silent followers (0 times): {len(segments.silent)}\")\n\nasyncio.run(segment_by_engagement())\n</code></pre>"},{"location":"guides/analytics/audience/#export-audience-data","title":"Export Audience Data","text":"<pre><code>async def export_audience_report():\n    async with Xeepy() as x:\n        analyzer = AudienceAnalyzer(x)\n\n        insights = await analyzer.comprehensive_report(\n            username=\"your_username\",\n            sample_size=1000\n        )\n\n        report_data = {\n            \"overview\": {\n                \"total_followers\": insights.total_followers,\n                \"verified_pct\": insights.verified_pct,\n                \"avg_follower_count\": insights.avg_follower_count\n            },\n            \"interests\": [\n                {\"name\": i.name, \"percentage\": i.percentage}\n                for i in insights.interests.top_interests\n            ],\n            \"geography\": {\n                \"countries\": insights.geography.top_countries,\n                \"cities\": insights.geography.top_cities\n            },\n            \"activity\": insights.activity_levels,\n            \"influential\": [\n                {\"username\": u.username, \"followers\": u.followers_count}\n                for u in insights.influential[:50]\n            ]\n        }\n\n        x.export.to_json(report_data, \"audience_report.json\")\n        print(\"Audience report exported to audience_report.json\")\n\nasyncio.run(export_audience_report())\n</code></pre>"},{"location":"guides/analytics/audience/#best-practices","title":"Best Practices","text":"<ol> <li>Regular Updates: Re-analyze quarterly as your audience evolves</li> <li>Combine Signals: Use bio, tweets, and engagement for complete picture</li> <li>Focus on Active: Weight analysis toward engaged followers</li> <li>Validate Personas: Check personas against actual engagement data</li> <li>Act on Insights: Use findings to inform content strategy</li> <li>Respect Privacy: Use aggregate data, not individual tracking</li> </ol>"},{"location":"guides/analytics/audience/#related-guides","title":"Related Guides","text":"<ul> <li>Growth Tracking</li> <li>Engagement Analysis</li> <li>Smart Targeting</li> </ul>"},{"location":"guides/analytics/best-time/","title":"Best Time to Post Analysis","text":"<p>Discover optimal posting times by analyzing when your audience is most active and engaged for maximum reach and interaction.</p>"},{"location":"guides/analytics/best-time/#overview","title":"Overview","text":"<p>Posting at the right time can dramatically increase your content's visibility and engagement. This guide shows how to analyze your historical data and audience behavior to find your personal best posting times.</p>"},{"location":"guides/analytics/best-time/#use-cases","title":"Use Cases","text":"<ul> <li>Content Scheduling: Plan posts for maximum engagement</li> <li>Global Audiences: Optimize for multiple time zones</li> <li>Campaign Timing: Launch campaigns at peak activity times</li> <li>A/B Testing: Test timing hypotheses with data</li> <li>Automation: Feed insights into scheduling tools</li> </ul>"},{"location":"guides/analytics/best-time/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom xeepy.analytics import TimingAnalyzer\n\nasync def find_best_times():\n    async with Xeepy() as x:\n        analyzer = TimingAnalyzer(x)\n\n        # Analyze your best posting times\n        best_times = await analyzer.best_times(\n            username=\"your_username\",\n            days=90,\n            metric=\"engagement\"  # engagement, likes, retweets, replies\n        )\n\n        print(\"Best Times to Post:\")\n        print(\"-\" * 40)\n\n        for slot in best_times.top_slots[:5]:\n            print(f\"{slot.day} at {slot.hour}:00 UTC\")\n            print(f\"  Avg engagement: {slot.avg_engagement:.1f}\")\n            print(f\"  Sample size: {slot.tweet_count} tweets\\n\")\n\nasyncio.run(find_best_times())\n</code></pre>"},{"location":"guides/analytics/best-time/#hourly-engagement-analysis","title":"Hourly Engagement Analysis","text":"<pre><code>async def hourly_analysis():\n    async with Xeepy() as x:\n        analyzer = TimingAnalyzer(x)\n\n        # Get hourly engagement patterns\n        hourly = await analyzer.hourly_breakdown(\"your_username\", days=60)\n\n        print(\"Engagement by Hour (UTC):\")\n        print(\"-\" * 50)\n\n        max_engagement = max(h.avg_engagement for h in hourly)\n\n        for hour_data in hourly:\n            bar_length = int((hour_data.avg_engagement / max_engagement) * 30)\n            bar = \"\u2588\" * bar_length\n            indicator = \"\u2b50\" if hour_data.avg_engagement == max_engagement else \"  \"\n\n            print(f\"{hour_data.hour:02d}:00 {indicator} {bar} {hour_data.avg_engagement:.1f}\")\n\nasyncio.run(hourly_analysis())\n</code></pre>"},{"location":"guides/analytics/best-time/#day-of-week-analysis","title":"Day-of-Week Analysis","text":"<pre><code>async def daily_analysis():\n    async with Xeepy() as x:\n        analyzer = TimingAnalyzer(x)\n\n        # Get daily patterns\n        daily = await analyzer.daily_breakdown(\"your_username\", days=90)\n\n        days = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n\n        print(\"Engagement by Day of Week:\")\n        print(\"-\" * 50)\n\n        for day_data in daily:\n            day_name = days[day_data.day_index]\n            bar = \"\u2588\" * int(day_data.avg_engagement / 5)\n            print(f\"{day_name:10} {bar} {day_data.avg_engagement:.1f} avg ({day_data.tweet_count} tweets)\")\n\nasyncio.run(daily_analysis())\n</code></pre>"},{"location":"guides/analytics/best-time/#heatmap-data-generation","title":"Heatmap Data Generation","text":"<pre><code>async def generate_heatmap_data():\n    async with Xeepy() as x:\n        analyzer = TimingAnalyzer(x)\n\n        # Get full heatmap data (day x hour)\n        heatmap = await analyzer.engagement_heatmap(\"your_username\", days=90)\n\n        days = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n\n        print(\"Engagement Heatmap (Hour vs Day):\")\n        print(\"-\" * 80)\n        print(\"     \" + \" \".join(f\"{h:02d}\" for h in range(24)))\n\n        for day_idx, day_name in enumerate(days):\n            row = heatmap.data[day_idx]\n            # Normalize to 0-9 scale for display\n            max_val = max(max(r) for r in heatmap.data) or 1\n            normalized = [int((v / max_val) * 9) for v in row]\n            row_str = \" \".join(str(v) for v in normalized)\n            print(f\"{day_name}  {row_str}\")\n\n        # Export for visualization\n        x.export.to_json(heatmap.to_dict(), \"timing_heatmap.json\")\n\nasyncio.run(generate_heatmap_data())\n</code></pre>"},{"location":"guides/analytics/best-time/#audience-timezone-analysis","title":"Audience Timezone Analysis","text":"<pre><code>async def analyze_audience_timezones():\n    async with Xeepy() as x:\n        analyzer = TimingAnalyzer(x)\n\n        # Analyze follower activity patterns to infer timezones\n        timezone_dist = await analyzer.infer_audience_timezones(\n            username=\"your_username\",\n            sample_size=500\n        )\n\n        print(\"Estimated Audience Timezone Distribution:\")\n        print(\"-\" * 50)\n\n        for tz, percentage in timezone_dist.distribution.items():\n            bar = \"\u2588\" * int(percentage * 50)\n            print(f\"{tz:20} {bar} {percentage:.1%}\")\n\n        print(f\"\\nPrimary timezone: {timezone_dist.primary_timezone}\")\n        print(f\"Recommended posting window: {timezone_dist.recommended_window}\")\n\nasyncio.run(analyze_audience_timezones())\n</code></pre>"},{"location":"guides/analytics/best-time/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>username</code> str required Account to analyze <code>days</code> int 30 Historical days to analyze <code>metric</code> str \"engagement\" Metric to optimize <code>timezone</code> str \"UTC\" Output timezone <code>min_tweets</code> int 3 Minimum tweets per slot <p>Sample Size Matters</p> <p>More historical data leads to more reliable insights. Aim for at least 60-90 days of data with consistent posting patterns.</p> <p>Individual Variation</p> <p>Generic \"best times\" rarely apply universally. Your optimal times depend on your specific audience demographics and behavior.</p>"},{"location":"guides/analytics/best-time/#compare-your-times-vs-optimal","title":"Compare Your Times vs Optimal","text":"<pre><code>async def compare_posting_patterns():\n    async with Xeepy() as x:\n        analyzer = TimingAnalyzer(x)\n\n        tweets = await x.scrape.tweets(\"your_username\", limit=200)\n        best_times = await analyzer.best_times(\"your_username\", days=90)\n\n        # Analyze your actual posting times\n        from collections import Counter\n        posting_hours = Counter(t.created_at.hour for t in tweets)\n\n        optimal_hours = [slot.hour for slot in best_times.top_slots[:5]]\n\n        # Calculate alignment\n        total_tweets = len(tweets)\n        optimal_tweets = sum(1 for t in tweets if t.created_at.hour in optimal_hours)\n        alignment = optimal_tweets / total_tweets\n\n        print(f\"Posting Time Analysis:\")\n        print(f\"  Your most common hours: {posting_hours.most_common(3)}\")\n        print(f\"  Optimal hours: {optimal_hours}\")\n        print(f\"  Tweets at optimal times: {alignment:.1%}\")\n\n        if alignment &lt; 0.5:\n            print(f\"\\n\ud83d\udca1 Suggestion: Try posting more during {optimal_hours}\")\n\nasyncio.run(compare_posting_patterns())\n</code></pre>"},{"location":"guides/analytics/best-time/#content-type-specific-timing","title":"Content-Type Specific Timing","text":"<pre><code>async def timing_by_content_type():\n    async with Xeepy() as x:\n        analyzer = TimingAnalyzer(x)\n\n        tweets = await x.scrape.tweets(\"your_username\", limit=300)\n\n        # Separate by content type\n        media_tweets = [t for t in tweets if t.media]\n        link_tweets = [t for t in tweets if t.urls and not t.media]\n        text_tweets = [t for t in tweets if not t.media and not t.urls]\n\n        content_types = {\n            \"Media posts\": media_tweets,\n            \"Link shares\": link_tweets,\n            \"Text only\": text_tweets\n        }\n\n        print(\"Best Times by Content Type:\")\n        print(\"-\" * 50)\n\n        for content_type, type_tweets in content_types.items():\n            if len(type_tweets) &gt;= 10:\n                # Find best hour for this content type\n                from collections import defaultdict\n                hourly = defaultdict(list)\n\n                for tweet in type_tweets:\n                    engagement = tweet.likes + tweet.retweets\n                    hourly[tweet.created_at.hour].append(engagement)\n\n                best_hour = max(hourly.keys(), key=lambda h: sum(hourly[h]) / len(hourly[h]))\n                best_avg = sum(hourly[best_hour]) / len(hourly[best_hour])\n\n                print(f\"{content_type}: Best at {best_hour}:00 UTC (avg {best_avg:.1f} engagement)\")\n\nasyncio.run(timing_by_content_type())\n</code></pre>"},{"location":"guides/analytics/best-time/#schedule-optimization","title":"Schedule Optimization","text":"<pre><code>async def optimize_schedule():\n    async with Xeepy() as x:\n        analyzer = TimingAnalyzer(x)\n\n        # Get optimal schedule for X posts per week\n        posts_per_week = 14  # 2 per day\n\n        schedule = await analyzer.optimal_schedule(\n            username=\"your_username\",\n            posts_per_week=posts_per_week,\n            min_gap_hours=4  # Minimum hours between posts\n        )\n\n        print(f\"Optimal Weekly Schedule ({posts_per_week} posts):\")\n        print(\"-\" * 50)\n\n        days = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n\n        for slot in schedule.slots:\n            print(f\"{days[slot.day_index]} {slot.hour:02d}:00 UTC - Expected ER: {slot.expected_engagement:.2%}\")\n\n        print(f\"\\nTotal expected weekly engagement: {schedule.total_expected:.0f}\")\n\nasyncio.run(optimize_schedule())\n</code></pre>"},{"location":"guides/analytics/best-time/#export-timing-data","title":"Export Timing Data","text":"<pre><code>async def export_timing_analysis():\n    async with Xeepy() as x:\n        analyzer = TimingAnalyzer(x)\n\n        # Comprehensive timing export\n        heatmap = await analyzer.engagement_heatmap(\"your_username\", days=90)\n        best_times = await analyzer.best_times(\"your_username\", days=90)\n\n        export_data = {\n            \"heatmap\": heatmap.to_dict(),\n            \"best_slots\": [\n                {\n                    \"day\": slot.day,\n                    \"hour\": slot.hour,\n                    \"avg_engagement\": slot.avg_engagement,\n                    \"tweet_count\": slot.tweet_count\n                }\n                for slot in best_times.top_slots\n            ],\n            \"analysis_period_days\": 90,\n            \"generated_at\": datetime.now().isoformat()\n        }\n\n        x.export.to_json(export_data, \"timing_analysis.json\")\n        print(\"Timing analysis exported to timing_analysis.json\")\n\nasyncio.run(export_timing_analysis())\n</code></pre>"},{"location":"guides/analytics/best-time/#best-practices","title":"Best Practices","text":"<ol> <li>Use Your Data: Generic advice is less valuable than your own analytics</li> <li>Account for Timezones: Know where your primary audience is located</li> <li>Test Consistently: Post at various times over weeks to gather data</li> <li>Consider Content Type: Different content may perform better at different times</li> <li>Review Regularly: Audience behavior changes; update analysis quarterly</li> <li>Don't Over-Optimize: Consistency matters more than perfect timing</li> </ol>"},{"location":"guides/analytics/best-time/#related-guides","title":"Related Guides","text":"<ul> <li>Engagement Analysis</li> <li>Audience Insights</li> <li>Content Strategy</li> </ul>"},{"location":"guides/analytics/competitors/","title":"Competitor Analysis","text":"<p>Monitor and analyze competitor Twitter accounts to benchmark performance, identify opportunities, and inform your strategy.</p>"},{"location":"guides/analytics/competitors/#overview","title":"Overview","text":"<p>Competitor analysis tracks other accounts' content strategy, engagement patterns, and growth to help you understand your market position and discover winning tactics you can adapt.</p>"},{"location":"guides/analytics/competitors/#use-cases","title":"Use Cases","text":"<ul> <li>Benchmarking: Compare your performance against competitors</li> <li>Content Ideas: Identify successful content themes in your space</li> <li>Gap Analysis: Find opportunities competitors aren't addressing</li> <li>Trend Spotting: Detect emerging trends early through competitor activity</li> <li>Strategic Planning: Inform strategy with competitive intelligence</li> </ul>"},{"location":"guides/analytics/competitors/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom xeepy.analytics import CompetitorAnalyzer\n\nasync def analyze_competitor():\n    async with Xeepy() as x:\n        analyzer = CompetitorAnalyzer(x)\n\n        # Basic competitor analysis\n        report = await analyzer.analyze(\"competitor_username\")\n\n        print(f\"Competitor Analysis: @{report.username}\")\n        print(\"-\" * 50)\n        print(f\"Followers: {report.followers:,}\")\n        print(f\"Following: {report.following:,}\")\n        print(f\"Tweets (30 days): {report.tweet_count}\")\n        print(f\"Avg engagement rate: {report.engagement_rate:.2%}\")\n        print(f\"Growth rate: {report.growth_rate:.2%}\")\n\nasyncio.run(analyze_competitor())\n</code></pre>"},{"location":"guides/analytics/competitors/#multi-competitor-comparison","title":"Multi-Competitor Comparison","text":"<pre><code>async def compare_competitors():\n    async with Xeepy() as x:\n        analyzer = CompetitorAnalyzer(x)\n\n        competitors = [\"competitor1\", \"competitor2\", \"competitor3\", \"your_account\"]\n\n        reports = []\n        for username in competitors:\n            report = await analyzer.analyze(username, days=30)\n            reports.append(report)\n\n        # Sort by engagement rate\n        reports.sort(key=lambda r: r.engagement_rate, reverse=True)\n\n        print(\"Competitor Comparison (30 Days)\")\n        print(\"=\" * 80)\n        print(f\"{'Account':&lt;20} {'Followers':&gt;12} {'Tweets':&gt;8} {'Eng Rate':&gt;10} {'Growth':&gt;10}\")\n        print(\"-\" * 80)\n\n        for r in reports:\n            is_you = \" \u2190 You\" if r.username == \"your_account\" else \"\"\n            print(f\"@{r.username:&lt;19} {r.followers:&gt;12,} {r.tweet_count:&gt;8} {r.engagement_rate:&gt;9.2%} {r.growth_rate:&gt;9.2%}{is_you}\")\n\nasyncio.run(compare_competitors())\n</code></pre>"},{"location":"guides/analytics/competitors/#content-strategy-analysis","title":"Content Strategy Analysis","text":"<pre><code>async def analyze_content_strategy():\n    async with Xeepy() as x:\n        analyzer = CompetitorAnalyzer(x)\n\n        # Analyze competitor's content strategy\n        strategy = await analyzer.content_strategy(\"competitor_username\", days=30)\n\n        print(\"Content Strategy Analysis:\")\n        print(\"-\" * 50)\n\n        print(\"\\nContent Mix:\")\n        for content_type, pct in strategy.content_mix.items():\n            bar = \"\u2588\" * int(pct * 30)\n            print(f\"  {content_type:15} {bar} {pct:.1%}\")\n\n        print(f\"\\nPosting Frequency: {strategy.avg_tweets_per_day:.1f} tweets/day\")\n        print(f\"Most active day: {strategy.most_active_day}\")\n        print(f\"Peak posting hour: {strategy.peak_hour}:00 UTC\")\n\n        print(\"\\nTop Performing Content Types:\")\n        for ct in strategy.top_performing[:3]:\n            print(f\"  {ct.type}: {ct.avg_engagement:.1f} avg engagement\")\n\nasyncio.run(analyze_content_strategy())\n</code></pre>"},{"location":"guides/analytics/competitors/#top-content-discovery","title":"Top Content Discovery","text":"<pre><code>async def find_top_content():\n    async with Xeepy() as x:\n        analyzer = CompetitorAnalyzer(x)\n\n        # Get competitor's best performing content\n        top_content = await analyzer.top_content(\n            username=\"competitor_username\",\n            days=30,\n            limit=10\n        )\n\n        print(\"Top Performing Tweets:\")\n        print(\"-\" * 60)\n\n        for i, tweet in enumerate(top_content, 1):\n            print(f\"\\n{i}. [{tweet.created_at.strftime('%Y-%m-%d')}]\")\n            print(f\"   {tweet.text[:100]}...\")\n            print(f\"   \ud83d\udcca Likes: {tweet.likes:,} | RTs: {tweet.retweets:,} | Replies: {tweet.reply_count:,}\")\n            print(f\"   \ud83d\udcc8 Engagement rate: {tweet.engagement_rate:.2%}\")\n\n            # Identify content characteristics\n            tags = []\n            if tweet.media:\n                tags.append(\"\ud83d\udcf7 Media\")\n            if tweet.urls:\n                tags.append(\"\ud83d\udd17 Links\")\n            if \"?\" in tweet.text:\n                tags.append(\"\u2753 Question\")\n            if tweet.hashtags:\n                tags.append(f\"# {len(tweet.hashtags)} hashtags\")\n\n            if tags:\n                print(f\"   Tags: {' | '.join(tags)}\")\n\nasyncio.run(find_top_content())\n</code></pre>"},{"location":"guides/analytics/competitors/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>username</code> str required Competitor username <code>days</code> int 30 Analysis period <code>include_replies</code> bool False Include reply tweets <code>engagement_calc</code> str \"standard\" Engagement calculation method <p>Competitor Selection</p> <p>Choose competitors at different stages: direct competitors (similar size), aspirational (larger), and emerging (smaller but growing fast).</p> <p>Public Data Only</p> <p>Competitor analysis works only with public accounts. Private accounts cannot be analyzed.</p>"},{"location":"guides/analytics/competitors/#growth-comparison","title":"Growth Comparison","text":"<pre><code>async def compare_growth():\n    async with Xeepy() as x:\n        analyzer = CompetitorAnalyzer(x)\n\n        competitors = [\"competitor1\", \"competitor2\", \"your_account\"]\n\n        print(\"30-Day Growth Comparison:\")\n        print(\"-\" * 60)\n\n        results = []\n        for username in competitors:\n            growth = await analyzer.growth_metrics(username, days=30)\n            results.append({\n                \"username\": username,\n                \"start\": growth.start_followers,\n                \"end\": growth.end_followers,\n                \"change\": growth.net_change,\n                \"rate\": growth.growth_rate\n            })\n\n        # Sort by growth rate\n        results.sort(key=lambda r: r[\"rate\"], reverse=True)\n\n        for r in results:\n            indicator = \"\ud83d\udcc8\" if r[\"rate\"] &gt; 0 else \"\ud83d\udcc9\"\n            print(f\"@{r['username']:20} {indicator} {r['change']:+,} ({r['rate']:+.2%})\")\n\nasyncio.run(compare_growth())\n</code></pre>"},{"location":"guides/analytics/competitors/#hashtag-and-topic-analysis","title":"Hashtag and Topic Analysis","text":"<pre><code>async def analyze_topics():\n    async with Xeepy() as x:\n        analyzer = CompetitorAnalyzer(x)\n\n        # Analyze topics and hashtags used by competitor\n        topics = await analyzer.topic_analysis(\"competitor_username\", days=30)\n\n        print(\"Topic Analysis:\")\n        print(\"-\" * 50)\n\n        print(\"\\nTop Hashtags:\")\n        for tag in topics.hashtags[:10]:\n            print(f\"  #{tag.name}: {tag.count} uses, {tag.avg_engagement:.1f} avg engagement\")\n\n        print(\"\\nKey Topics:\")\n        for topic in topics.topics[:10]:\n            print(f\"  {topic.name}: {topic.percentage:.1%} of content\")\n\n        print(\"\\nMentions (collaborations/conversations):\")\n        for mention in topics.mentions[:5]:\n            print(f\"  @{mention.username}: {mention.count} mentions\")\n\nasyncio.run(analyze_topics())\n</code></pre>"},{"location":"guides/analytics/competitors/#audience-comparison","title":"Audience Comparison","text":"<pre><code>async def compare_audiences():\n    async with Xeepy() as x:\n        analyzer = CompetitorAnalyzer(x)\n\n        # Compare follower characteristics\n        comparison = await analyzer.audience_comparison(\n            your_username=\"your_account\",\n            competitor=\"competitor_username\",\n            sample_size=500\n        )\n\n        print(\"Audience Comparison:\")\n        print(\"-\" * 60)\n\n        print(f\"\\n{'Metric':&lt;25} {'You':&gt;15} {'Competitor':&gt;15}\")\n        print(\"-\" * 60)\n        print(f\"{'Avg follower count':&lt;25} {comparison.you.avg_followers:&gt;15,.0f} {comparison.competitor.avg_followers:&gt;15,.0f}\")\n        print(f\"{'Verified %':&lt;25} {comparison.you.verified_pct:&gt;14.1%} {comparison.competitor.verified_pct:&gt;14.1%}\")\n        print(f\"{'Active % (30 days)':&lt;25} {comparison.you.active_pct:&gt;14.1%} {comparison.competitor.active_pct:&gt;14.1%}\")\n\n        print(f\"\\nAudience overlap: {comparison.overlap_percentage:.1%}\")\n        print(f\"Unique to you: {comparison.unique_to_you:,}\")\n        print(f\"Unique to competitor: {comparison.unique_to_competitor:,}\")\n\nasyncio.run(compare_audiences())\n</code></pre>"},{"location":"guides/analytics/competitors/#engagement-pattern-analysis","title":"Engagement Pattern Analysis","text":"<pre><code>async def analyze_engagement_patterns():\n    async with Xeepy() as x:\n        analyzer = CompetitorAnalyzer(x)\n\n        # Compare engagement patterns\n        patterns = await analyzer.engagement_patterns(\n            usernames=[\"competitor1\", \"competitor2\", \"your_account\"],\n            days=30\n        )\n\n        print(\"Engagement Pattern Analysis:\")\n        print(\"-\" * 70)\n\n        for p in patterns:\n            print(f\"\\n@{p.username}:\")\n            print(f\"  Best day: {p.best_day} ({p.best_day_engagement:.1f} avg)\")\n            print(f\"  Best hour: {p.best_hour}:00 UTC ({p.best_hour_engagement:.1f} avg)\")\n            print(f\"  Reply rate: {p.reply_rate:.1%}\")\n            print(f\"  Retweet ratio: {p.retweet_ratio:.1%}\")\n\nasyncio.run(analyze_engagement_patterns())\n</code></pre>"},{"location":"guides/analytics/competitors/#competitive-dashboard-export","title":"Competitive Dashboard Export","text":"<pre><code>async def export_competitive_dashboard():\n    async with Xeepy() as x:\n        analyzer = CompetitorAnalyzer(x)\n\n        competitors = [\"competitor1\", \"competitor2\", \"competitor3\"]\n\n        dashboard_data = {\n            \"generated_at\": datetime.now().isoformat(),\n            \"competitors\": []\n        }\n\n        for username in competitors:\n            report = await analyzer.comprehensive_report(username, days=30)\n\n            dashboard_data[\"competitors\"].append({\n                \"username\": username,\n                \"followers\": report.followers,\n                \"growth_rate\": report.growth_rate,\n                \"engagement_rate\": report.engagement_rate,\n                \"tweets_per_day\": report.tweets_per_day,\n                \"top_content_themes\": report.top_themes,\n                \"best_posting_times\": report.best_times,\n                \"top_hashtags\": report.top_hashtags\n            })\n\n        x.export.to_json(dashboard_data, \"competitive_dashboard.json\")\n        print(\"Competitive dashboard exported to competitive_dashboard.json\")\n\nasyncio.run(export_competitive_dashboard())\n</code></pre>"},{"location":"guides/analytics/competitors/#competitive-alerts","title":"Competitive Alerts","text":"<pre><code>async def monitor_competitors():\n    async with Xeepy() as x:\n        analyzer = CompetitorAnalyzer(x)\n\n        competitors = [\"competitor1\", \"competitor2\"]\n\n        for username in competitors:\n            # Check for significant changes\n            changes = await analyzer.detect_changes(username)\n\n            if changes.significant_changes:\n                print(f\"\u26a0\ufe0f Alert for @{username}:\")\n\n                for change in changes.significant_changes:\n                    print(f\"  - {change.description}\")\n                    print(f\"    Previous: {change.previous_value}\")\n                    print(f\"    Current: {change.current_value}\")\n                    print(f\"    Change: {change.percentage_change:+.1%}\")\n\nasyncio.run(monitor_competitors())\n</code></pre>"},{"location":"guides/analytics/competitors/#best-practices","title":"Best Practices","text":"<ol> <li>Track Consistently: Monitor competitors on a regular schedule</li> <li>Learn, Don't Copy: Adapt successful tactics, don't duplicate content</li> <li>Multiple Competitors: Track 3-5 competitors across different tiers</li> <li>Focus on Actionable: Prioritize insights you can act on</li> <li>Consider Context: Engagement varies by niche and audience size</li> <li>Ethical Monitoring: Use public data only; don't misrepresent insights</li> </ol>"},{"location":"guides/analytics/competitors/#related-guides","title":"Related Guides","text":"<ul> <li>Growth Tracking</li> <li>Engagement Analysis</li> <li>Audience Insights</li> </ul>"},{"location":"guides/analytics/engagement/","title":"Engagement Analysis","text":"<p>Measure, analyze, and optimize your Twitter engagement with comprehensive metrics on likes, retweets, replies, and overall interaction rates.</p>"},{"location":"guides/analytics/engagement/#overview","title":"Overview","text":"<p>Engagement analysis goes beyond vanity metrics to understand how your audience interacts with your content. Track engagement rates, identify top-performing content, and optimize your strategy based on data.</p>"},{"location":"guides/analytics/engagement/#use-cases","title":"Use Cases","text":"<ul> <li>Content Performance: Identify what content resonates best</li> <li>Audience Understanding: Learn when and how your audience engages</li> <li>ROI Measurement: Measure engagement from campaigns and efforts</li> <li>Strategy Optimization: Data-driven content strategy improvements</li> <li>Benchmarking: Compare performance against competitors</li> </ul>"},{"location":"guides/analytics/engagement/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom xeepy.analytics import EngagementAnalyzer\n\nasync def analyze_engagement():\n    async with Xeepy() as x:\n        analyzer = EngagementAnalyzer(x)\n\n        # Get engagement metrics\n        metrics = await analyzer.get_metrics(\"your_username\", days=30)\n\n        print(f\"30-Day Engagement Summary:\")\n        print(f\"  Total tweets: {metrics.tweet_count}\")\n        print(f\"  Total likes: {metrics.total_likes:,}\")\n        print(f\"  Total retweets: {metrics.total_retweets:,}\")\n        print(f\"  Total replies: {metrics.total_replies:,}\")\n        print(f\"  Engagement rate: {metrics.engagement_rate:.2%}\")\n\nasyncio.run(analyze_engagement())\n</code></pre>"},{"location":"guides/analytics/engagement/#detailed-engagement-breakdown","title":"Detailed Engagement Breakdown","text":"<pre><code>async def detailed_engagement():\n    async with Xeepy() as x:\n        analyzer = EngagementAnalyzer(x)\n\n        # Get detailed breakdown\n        report = await analyzer.detailed_report(\"your_username\", days=30)\n\n        print(\"Engagement Breakdown:\")\n        print(f\"\\nLikes:\")\n        print(f\"  Total: {report.likes.total:,}\")\n        print(f\"  Average per tweet: {report.likes.average:.1f}\")\n        print(f\"  Median: {report.likes.median}\")\n        print(f\"  Max: {report.likes.max} (Tweet ID: {report.likes.max_tweet_id})\")\n\n        print(f\"\\nRetweets:\")\n        print(f\"  Total: {report.retweets.total:,}\")\n        print(f\"  Average per tweet: {report.retweets.average:.1f}\")\n        print(f\"  Quote tweets: {report.retweets.quotes}\")\n\n        print(f\"\\nReplies:\")\n        print(f\"  Total received: {report.replies.total:,}\")\n        print(f\"  Reply rate: {report.replies.reply_rate:.2%}\")\n        print(f\"  Avg thread depth: {report.replies.avg_thread_depth:.1f}\")\n\nasyncio.run(detailed_engagement())\n</code></pre>"},{"location":"guides/analytics/engagement/#top-performing-content","title":"Top Performing Content","text":"<pre><code>async def top_content_analysis():\n    async with Xeepy() as x:\n        analyzer = EngagementAnalyzer(x)\n\n        # Get top performing tweets\n        top_tweets = await analyzer.top_content(\n            username=\"your_username\",\n            metric=\"engagement\",  # likes, retweets, replies, engagement\n            limit=10,\n            days=30\n        )\n\n        print(\"Top 10 Tweets by Engagement:\")\n        print(\"-\" * 60)\n\n        for i, tweet in enumerate(top_tweets, 1):\n            print(f\"\\n{i}. {tweet.text[:80]}...\")\n            print(f\"   Likes: {tweet.likes} | RTs: {tweet.retweets} | Replies: {tweet.reply_count}\")\n            print(f\"   Engagement rate: {tweet.engagement_rate:.2%}\")\n            print(f\"   Posted: {tweet.created_at}\")\n\nasyncio.run(top_content_analysis())\n</code></pre>"},{"location":"guides/analytics/engagement/#engagement-rate-calculation","title":"Engagement Rate Calculation","text":"<pre><code>async def calculate_engagement_rates():\n    async with Xeepy() as x:\n        analyzer = EngagementAnalyzer(x)\n\n        # Get tweets\n        tweets = await x.scrape.tweets(\"your_username\", limit=100)\n        profile = await x.scrape.profile(\"your_username\")\n\n        # Calculate different engagement metrics\n        for tweet in tweets[:10]:\n            # Standard engagement rate\n            standard_er = (tweet.likes + tweet.retweets + tweet.reply_count) / profile.followers_count\n\n            # Engagement rate by impressions (if available)\n            impression_er = (tweet.likes + tweet.retweets) / tweet.views if tweet.views else 0\n\n            # Amplification rate (retweets per follower)\n            amp_rate = tweet.retweets / profile.followers_count\n\n            print(f\"Tweet: {tweet.text[:40]}...\")\n            print(f\"  Standard ER: {standard_er:.3%}\")\n            print(f\"  Impression ER: {impression_er:.3%}\")\n            print(f\"  Amplification: {amp_rate:.4%}\\n\")\n\nasyncio.run(calculate_engagement_rates())\n</code></pre>"},{"location":"guides/analytics/engagement/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>username</code> str required Account to analyze <code>days</code> int 30 Analysis period <code>metric</code> str \"engagement\" Primary metric to sort by <code>include_replies</code> bool True Include reply tweets <code>min_engagement</code> int 0 Minimum engagement filter <p>Engagement Rate Benchmarks</p> <ul> <li>&lt; 1%: Below average</li> <li>1-3%: Average engagement</li> <li>3-6%: Good engagement</li> <li>&gt; 6%: Excellent engagement</li> </ul> <p>Engagement Rate Formula</p> <p>Standard: <code>(Likes + Retweets + Replies) / Followers \u00d7 100</code></p>"},{"location":"guides/analytics/engagement/#content-type-analysis","title":"Content Type Analysis","text":"<pre><code>async def analyze_content_types():\n    async with Xeepy() as x:\n        analyzer = EngagementAnalyzer(x)\n\n        tweets = await x.scrape.tweets(\"your_username\", limit=200)\n\n        # Categorize content\n        categories = {\n            \"with_media\": [],\n            \"with_links\": [],\n            \"questions\": [],\n            \"threads\": [],\n            \"plain_text\": []\n        }\n\n        for tweet in tweets:\n            if tweet.media:\n                categories[\"with_media\"].append(tweet)\n            elif tweet.urls:\n                categories[\"with_links\"].append(tweet)\n            elif \"?\" in tweet.text:\n                categories[\"questions\"].append(tweet)\n            elif tweet.is_thread_start:\n                categories[\"threads\"].append(tweet)\n            else:\n                categories[\"plain_text\"].append(tweet)\n\n        # Calculate engagement by category\n        print(\"Engagement by Content Type:\")\n        print(\"-\" * 50)\n\n        for category, cat_tweets in categories.items():\n            if cat_tweets:\n                avg_engagement = sum(t.likes + t.retweets for t in cat_tweets) / len(cat_tweets)\n                print(f\"{category:15}: {len(cat_tweets):3} tweets, avg engagement: {avg_engagement:.1f}\")\n\nasyncio.run(analyze_content_types())\n</code></pre>"},{"location":"guides/analytics/engagement/#time-based-engagement-patterns","title":"Time-Based Engagement Patterns","text":"<pre><code>async def engagement_by_time():\n    async with Xeepy() as x:\n        analyzer = EngagementAnalyzer(x)\n\n        tweets = await x.scrape.tweets(\"your_username\", limit=200)\n\n        # Group by hour\n        from collections import defaultdict\n        hourly_engagement = defaultdict(list)\n\n        for tweet in tweets:\n            hour = tweet.created_at.hour\n            engagement = tweet.likes + tweet.retweets + tweet.reply_count\n            hourly_engagement[hour].append(engagement)\n\n        print(\"Average Engagement by Hour (UTC):\")\n        print(\"-\" * 40)\n\n        for hour in range(24):\n            if hourly_engagement[hour]:\n                avg = sum(hourly_engagement[hour]) / len(hourly_engagement[hour])\n                bar = \"\u2588\" * int(avg / 10)\n                print(f\"{hour:02d}:00  {bar} {avg:.1f}\")\n\nasyncio.run(engagement_by_time())\n</code></pre>"},{"location":"guides/analytics/engagement/#competitor-engagement-comparison","title":"Competitor Engagement Comparison","text":"<pre><code>async def compare_engagement():\n    async with Xeepy() as x:\n        analyzer = EngagementAnalyzer(x)\n\n        accounts = [\"your_account\", \"competitor1\", \"competitor2\"]\n\n        results = []\n        for account in accounts:\n            metrics = await analyzer.get_metrics(account, days=30)\n            results.append({\n                \"account\": account,\n                \"tweets\": metrics.tweet_count,\n                \"avg_likes\": metrics.avg_likes,\n                \"avg_retweets\": metrics.avg_retweets,\n                \"engagement_rate\": metrics.engagement_rate\n            })\n\n        # Sort by engagement rate\n        results.sort(key=lambda x: x[\"engagement_rate\"], reverse=True)\n\n        print(\"Engagement Comparison (30 Days):\")\n        print(\"-\" * 70)\n        print(f\"{'Account':&lt;20} {'Tweets':&lt;8} {'Avg Likes':&lt;12} {'Avg RTs':&lt;10} {'ER':&lt;8}\")\n        print(\"-\" * 70)\n\n        for r in results:\n            print(f\"@{r['account']:&lt;19} {r['tweets']:&lt;8} {r['avg_likes']:&lt;12.1f} {r['avg_retweets']:&lt;10.1f} {r['engagement_rate']:.2%}\")\n\nasyncio.run(compare_engagement())\n</code></pre>"},{"location":"guides/analytics/engagement/#engagement-trends","title":"Engagement Trends","text":"<pre><code>async def engagement_trends():\n    async with Xeepy() as x:\n        analyzer = EngagementAnalyzer(x)\n\n        # Get weekly engagement trends\n        trends = await analyzer.weekly_trends(\"your_username\", weeks=8)\n\n        print(\"Weekly Engagement Trends:\")\n        print(\"-\" * 50)\n\n        for week in trends:\n            trend_indicator = \"\ud83d\udcc8\" if week.change &gt; 0 else \"\ud83d\udcc9\" if week.change &lt; 0 else \"\u27a1\ufe0f\"\n            print(f\"Week of {week.start_date}:\")\n            print(f\"  {trend_indicator} Engagement: {week.total_engagement:,} ({week.change:+.1%})\")\n            print(f\"  Avg per tweet: {week.avg_engagement:.1f}\")\n            print(f\"  Best performer: {week.best_tweet.text[:40]}...\\n\")\n\nasyncio.run(engagement_trends())\n</code></pre>"},{"location":"guides/analytics/engagement/#export-engagement-data","title":"Export Engagement Data","text":"<pre><code>async def export_engagement():\n    async with Xeepy() as x:\n        analyzer = EngagementAnalyzer(x)\n\n        # Get comprehensive data\n        tweets = await x.scrape.tweets(\"your_username\", limit=500)\n\n        export_data = []\n        for tweet in tweets:\n            export_data.append({\n                \"tweet_id\": tweet.id,\n                \"created_at\": tweet.created_at.isoformat(),\n                \"text\": tweet.text[:100],\n                \"likes\": tweet.likes,\n                \"retweets\": tweet.retweets,\n                \"replies\": tweet.reply_count,\n                \"views\": tweet.views,\n                \"has_media\": bool(tweet.media),\n                \"has_links\": bool(tweet.urls),\n                \"engagement_total\": tweet.likes + tweet.retweets + tweet.reply_count\n            })\n\n        x.export.to_csv(export_data, \"engagement_data.csv\")\n        print(f\"Exported {len(export_data)} tweets with engagement data\")\n\nasyncio.run(export_engagement())\n</code></pre>"},{"location":"guides/analytics/engagement/#best-practices","title":"Best Practices","text":"<ol> <li>Track Consistently: Monitor engagement regularly for trend insights</li> <li>Context Matters: Compare similar content types for fair analysis</li> <li>Look Beyond Likes: Replies and retweets often indicate deeper engagement</li> <li>Test and Iterate: Use data to inform content experiments</li> <li>Benchmark Realistically: Compare against accounts of similar size</li> <li>Consider Reach: High engagement with low reach may indicate niche appeal</li> </ol>"},{"location":"guides/analytics/engagement/#related-guides","title":"Related Guides","text":"<ul> <li>Growth Tracking</li> <li>Best Time to Post</li> <li>Content Analysis</li> </ul>"},{"location":"guides/analytics/growth/","title":"Growth Tracking Analytics","text":"<p>Monitor and analyze your Twitter account growth with detailed metrics, trend analysis, and actionable insights.</p>"},{"location":"guides/analytics/growth/#overview","title":"Overview","text":"<p>Growth tracking provides comprehensive analytics on follower changes, engagement trends, and account performance over time. Understand what drives growth and optimize your strategy with data-driven decisions.</p>"},{"location":"guides/analytics/growth/#use-cases","title":"Use Cases","text":"<ul> <li>Performance Monitoring: Track follower gains and losses daily</li> <li>Campaign ROI: Measure growth impact of marketing campaigns</li> <li>Trend Analysis: Identify growth patterns and seasonality</li> <li>Goal Setting: Set realistic growth targets based on historical data</li> <li>Strategy Optimization: Understand what content drives follower growth</li> </ul>"},{"location":"guides/analytics/growth/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom xeepy.analytics import GrowthTracker\n\nasync def track_growth():\n    async with Xeepy() as x:\n        tracker = GrowthTracker(x)\n\n        # Get current growth metrics\n        metrics = await tracker.get_metrics(\"your_username\")\n\n        print(f\"Current followers: {metrics.followers:,}\")\n        print(f\"30-day change: {metrics.monthly_change:+,}\")\n        print(f\"Daily average: {metrics.daily_average:+.1f}\")\n        print(f\"Growth rate: {metrics.growth_rate:.2%}\")\n\nasyncio.run(track_growth())\n</code></pre>"},{"location":"guides/analytics/growth/#daily-growth-tracking","title":"Daily Growth Tracking","text":"<pre><code>async def daily_tracking():\n    async with Xeepy() as x:\n        tracker = GrowthTracker(x)\n\n        # Track daily changes\n        today = await tracker.daily_snapshot(\"your_username\")\n\n        print(f\"Date: {today.date}\")\n        print(f\"Followers: {today.followers:,}\")\n        print(f\"Following: {today.following:,}\")\n        print(f\"New followers: {today.new_followers:+,}\")\n        print(f\"Lost followers: {today.lost_followers}\")\n        print(f\"Net change: {today.net_change:+,}\")\n\n        # Store in database for historical tracking\n        await tracker.save_snapshot(today)\n\nasyncio.run(daily_tracking())\n</code></pre>"},{"location":"guides/analytics/growth/#historical-growth-analysis","title":"Historical Growth Analysis","text":"<pre><code>async def historical_analysis():\n    async with Xeepy() as x:\n        tracker = GrowthTracker(x)\n\n        # Get growth history\n        history = await tracker.get_history(\n            username=\"your_username\",\n            days=90  # Last 90 days\n        )\n\n        print(\"Growth History (Last 90 Days):\")\n        print(\"-\" * 50)\n\n        # Weekly aggregates\n        for week in history.weekly_aggregates:\n            print(f\"Week of {week.start_date}:\")\n            print(f\"  Gained: +{week.gained:,}\")\n            print(f\"  Lost: -{week.lost:,}\")\n            print(f\"  Net: {week.net_change:+,}\")\n            print(f\"  Growth rate: {week.growth_rate:.2%}\\n\")\n\n        # Overall summary\n        print(f\"Total growth: {history.total_change:+,}\")\n        print(f\"Average daily: {history.daily_average:+.1f}\")\n        print(f\"Best day: {history.best_day.date} (+{history.best_day.change})\")\n        print(f\"Worst day: {history.worst_day.date} ({history.worst_day.change})\")\n\nasyncio.run(historical_analysis())\n</code></pre>"},{"location":"guides/analytics/growth/#follower-change-analysis","title":"Follower Change Analysis","text":"<pre><code>async def analyze_follower_changes():\n    async with Xeepy() as x:\n        tracker = GrowthTracker(x)\n\n        # Get detailed unfollower data\n        report = await tracker.unfollower_report(\"your_username\")\n\n        print(f\"New followers today: {len(report.new_followers)}\")\n        for user in report.new_followers[:5]:\n            print(f\"  + @{user.username} ({user.followers_count:,} followers)\")\n\n        print(f\"\\nUnfollowers today: {len(report.unfollowers)}\")\n        for user in report.unfollowers[:5]:\n            print(f\"  - @{user.username} ({user.followers_count:,} followers)\")\n\n        # Analyze unfollower patterns\n        print(f\"\\nUnfollower analysis:\")\n        print(f\"  Avg follower count: {report.avg_unfollower_size:,.0f}\")\n        print(f\"  Were following back: {report.mutual_unfollows}\")\n\nasyncio.run(analyze_follower_changes())\n</code></pre>"},{"location":"guides/analytics/growth/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>username</code> str required Account to track <code>days</code> int 30 Historical days to analyze <code>include_details</code> bool True Include individual user data <code>store_history</code> bool True Save snapshots to database <p>Consistent Tracking</p> <p>Run daily snapshots at the same time each day for accurate trend analysis. Use cron jobs or scheduled tasks for automation.</p> <p>Historical Data</p> <p>Twitter doesn't provide historical follower data. Start tracking today to build your historical database.</p>"},{"location":"guides/analytics/growth/#growth-rate-calculations","title":"Growth Rate Calculations","text":"<pre><code>async def calculate_growth_rates():\n    async with Xeepy() as x:\n        tracker = GrowthTracker(x)\n\n        history = await tracker.get_history(\"your_username\", days=30)\n\n        # Calculate various growth metrics\n        print(\"Growth Metrics:\")\n        print(f\"  Daily growth rate: {history.daily_growth_rate:.3%}\")\n        print(f\"  Weekly growth rate: {history.weekly_growth_rate:.2%}\")\n        print(f\"  Monthly growth rate: {history.monthly_growth_rate:.2%}\")\n\n        # Compound annual growth rate (CAGR)\n        if history.days &gt;= 30:\n            cagr = ((history.end_followers / history.start_followers) ** (365 / history.days) - 1)\n            print(f\"  Projected annual growth: {cagr:.1%}\")\n\n        # Velocity (acceleration of growth)\n        print(f\"  Growth velocity: {history.velocity:+.2f} (accelerating)\" if history.velocity &gt; 0 \n              else f\"  Growth velocity: {history.velocity:.2f} (slowing)\")\n\nasyncio.run(calculate_growth_rates())\n</code></pre>"},{"location":"guides/analytics/growth/#growth-visualization-data","title":"Growth Visualization Data","text":"<pre><code>async def get_visualization_data():\n    async with Xeepy() as x:\n        tracker = GrowthTracker(x)\n\n        history = await tracker.get_history(\"your_username\", days=60)\n\n        # Prepare data for charts\n        chart_data = {\n            \"dates\": [d.date.isoformat() for d in history.daily_data],\n            \"followers\": [d.followers for d in history.daily_data],\n            \"daily_change\": [d.net_change for d in history.daily_data],\n            \"cumulative_change\": []\n        }\n\n        # Calculate cumulative change\n        cumulative = 0\n        for d in history.daily_data:\n            cumulative += d.net_change\n            chart_data[\"cumulative_change\"].append(cumulative)\n\n        # Export for visualization tools\n        x.export.to_json(chart_data, \"growth_chart_data.json\")\n        print(\"Chart data exported to growth_chart_data.json\")\n\nasyncio.run(get_visualization_data())\n</code></pre>"},{"location":"guides/analytics/growth/#comparative-growth-analysis","title":"Comparative Growth Analysis","text":"<pre><code>async def compare_growth():\n    async with Xeepy() as x:\n        tracker = GrowthTracker(x)\n\n        accounts = [\"your_account\", \"competitor1\", \"competitor2\"]\n\n        print(\"30-Day Growth Comparison:\")\n        print(\"-\" * 60)\n\n        results = []\n        for account in accounts:\n            history = await tracker.get_history(account, days=30)\n            results.append({\n                \"account\": account,\n                \"growth\": history.total_change,\n                \"rate\": history.monthly_growth_rate\n            })\n\n        # Sort by growth rate\n        results.sort(key=lambda x: x[\"rate\"], reverse=True)\n\n        for r in results:\n            bar = \"\u2588\" * int(r[\"rate\"] * 100)\n            print(f\"@{r['account']:20}: {bar} {r['rate']:.2%} ({r['growth']:+,})\")\n\nasyncio.run(compare_growth())\n</code></pre>"},{"location":"guides/analytics/growth/#goal-tracking","title":"Goal Tracking","text":"<pre><code>async def track_goals():\n    async with Xeepy() as x:\n        tracker = GrowthTracker(x)\n\n        # Define growth goal\n        goal = 10000  # Target followers\n        deadline_days = 90  # Days to achieve\n\n        metrics = await tracker.get_metrics(\"your_username\")\n        history = await tracker.get_history(\"your_username\", days=30)\n\n        current = metrics.followers\n        needed = goal - current\n        daily_rate = history.daily_average\n\n        if daily_rate &gt; 0:\n            days_needed = needed / daily_rate\n            on_track = days_needed &lt;= deadline_days\n\n            print(f\"Goal: {goal:,} followers in {deadline_days} days\")\n            print(f\"Current: {current:,}\")\n            print(f\"Needed: {needed:,}\")\n            print(f\"Daily average: {daily_rate:+.1f}\")\n            print(f\"Projected days to goal: {days_needed:.0f}\")\n            print(f\"Status: {'\u2705 On track!' if on_track else '\u26a0\ufe0f Need to increase growth rate'}\")\n\n            if not on_track:\n                required_rate = needed / deadline_days\n                print(f\"Required daily rate: {required_rate:.1f} ({required_rate - daily_rate:+.1f} vs current)\")\n\nasyncio.run(track_goals())\n</code></pre>"},{"location":"guides/analytics/growth/#automated-growth-reports","title":"Automated Growth Reports","text":"<pre><code>async def generate_growth_report():\n    async with Xeepy() as x:\n        tracker = GrowthTracker(x)\n\n        # Weekly growth report\n        report = await tracker.generate_report(\n            username=\"your_username\",\n            period=\"weekly\"\n        )\n\n        print(\"=\" * 60)\n        print(\"WEEKLY GROWTH REPORT\")\n        print(\"=\" * 60)\n        print(f\"Period: {report.start_date} to {report.end_date}\")\n        print(f\"\\nFollowers: {report.end_followers:,} ({report.change:+,})\")\n        print(f\"Growth rate: {report.growth_rate:.2%}\")\n        print(f\"\\nBest performing day: {report.best_day}\")\n        print(f\"Total new followers: {report.total_gained:,}\")\n        print(f\"Total unfollowers: {report.total_lost:,}\")\n        print(f\"\\nEngagement correlation: {report.engagement_correlation:.2f}\")\n\n        # Export full report\n        x.export.to_json(report.to_dict(), \"weekly_report.json\")\n\nasyncio.run(generate_growth_report())\n</code></pre>"},{"location":"guides/analytics/growth/#best-practices","title":"Best Practices","text":"<ol> <li>Track Consistently: Run daily at the same time for accurate data</li> <li>Set Realistic Goals: Base targets on historical performance</li> <li>Correlate with Content: Track which content drives follower growth</li> <li>Monitor Unfollowers: High unfollows may indicate content issues</li> <li>Compare Competitors: Benchmark against similar accounts</li> <li>Long-term View: Focus on trends, not daily fluctuations</li> </ol>"},{"location":"guides/analytics/growth/#related-guides","title":"Related Guides","text":"<ul> <li>Engagement Analysis</li> <li>Audience Insights</li> <li>Report Generation</li> </ul>"},{"location":"guides/analytics/reports/","title":"Report Generation","text":"<p>Create comprehensive, automated reports for Twitter analytics, combining multiple data sources into actionable insights.</p>"},{"location":"guides/analytics/reports/#overview","title":"Overview","text":"<p>Report generation automates the creation of periodic analytics summaries, combining growth, engagement, audience, and competitive data into formatted reports for stakeholders, clients, or personal tracking.</p>"},{"location":"guides/analytics/reports/#use-cases","title":"Use Cases","text":"<ul> <li>Weekly Reports: Automated weekly performance summaries</li> <li>Client Reporting: Professional reports for social media clients</li> <li>Executive Summaries: High-level metrics for leadership</li> <li>Campaign Analysis: Post-campaign performance reports</li> <li>Audit Reports: Comprehensive account audits</li> </ul>"},{"location":"guides/analytics/reports/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\nfrom xeepy.analytics import ReportGenerator\n\nasync def generate_basic_report():\n    async with Xeepy() as x:\n        generator = ReportGenerator(x)\n\n        # Generate a weekly report\n        report = await generator.weekly_report(\"your_username\")\n\n        print(report.summary)\n\n        # Export to different formats\n        report.to_markdown(\"weekly_report.md\")\n        report.to_html(\"weekly_report.html\")\n        report.to_pdf(\"weekly_report.pdf\")\n\nasyncio.run(generate_basic_report())\n</code></pre>"},{"location":"guides/analytics/reports/#comprehensive-performance-report","title":"Comprehensive Performance Report","text":"<pre><code>async def comprehensive_report():\n    async with Xeepy() as x:\n        generator = ReportGenerator(x)\n\n        report = await generator.generate(\n            username=\"your_username\",\n            period_days=30,\n            include_sections=[\n                \"overview\",\n                \"growth\",\n                \"engagement\",\n                \"top_content\",\n                \"audience\",\n                \"recommendations\"\n            ]\n        )\n\n        # Print report sections\n        print(\"=\" * 60)\n        print(\"30-DAY PERFORMANCE REPORT\")\n        print(\"=\" * 60)\n\n        print(\"\\n\ud83d\udcca OVERVIEW\")\n        print(\"-\" * 40)\n        print(f\"Followers: {report.overview.followers:,}\")\n        print(f\"Net change: {report.overview.follower_change:+,}\")\n        print(f\"Tweets published: {report.overview.tweets_published}\")\n        print(f\"Total engagement: {report.overview.total_engagement:,}\")\n\n        print(\"\\n\ud83d\udcc8 GROWTH\")\n        print(\"-\" * 40)\n        print(f\"Growth rate: {report.growth.rate:.2%}\")\n        print(f\"New followers: {report.growth.gained:,}\")\n        print(f\"Lost followers: {report.growth.lost:,}\")\n\n        print(\"\\n\ud83d\udcac ENGAGEMENT\")\n        print(\"-\" * 40)\n        print(f\"Engagement rate: {report.engagement.rate:.2%}\")\n        print(f\"Avg likes: {report.engagement.avg_likes:.1f}\")\n        print(f\"Avg retweets: {report.engagement.avg_retweets:.1f}\")\n        print(f\"Avg replies: {report.engagement.avg_replies:.1f}\")\n\nasyncio.run(comprehensive_report())\n</code></pre>"},{"location":"guides/analytics/reports/#custom-report-templates","title":"Custom Report Templates","text":"<pre><code>async def custom_template_report():\n    async with Xeepy() as x:\n        generator = ReportGenerator(x)\n\n        # Define custom template\n        template = \"\"\"\n        # Twitter Report for @{username}\n        ## Period: {start_date} to {end_date}\n\n        ### Key Metrics\n        | Metric | Value | Change |\n        |--------|-------|--------|\n        | Followers | {followers:,} | {follower_change:+,} |\n        | Engagement Rate | {engagement_rate:.2%} | {er_change:+.2%} |\n        | Tweets | {tweet_count} | - |\n\n        ### Top Performing Tweets\n        {top_tweets}\n\n        ### Recommendations\n        {recommendations}\n        \"\"\"\n\n        report = await generator.generate_from_template(\n            username=\"your_username\",\n            template=template,\n            period_days=7\n        )\n\n        print(report.rendered)\n\nasyncio.run(custom_template_report())\n</code></pre>"},{"location":"guides/analytics/reports/#automated-scheduled-reports","title":"Automated Scheduled Reports","text":"<pre><code>async def scheduled_report_system():\n    from xeepy import Xeepy\n    from xeepy.analytics import ReportGenerator\n    from xeepy.notifications import EmailNotifier\n\n    async with Xeepy() as x:\n        generator = ReportGenerator(x)\n        notifier = EmailNotifier(\n            smtp_host=\"smtp.gmail.com\",\n            smtp_port=587,\n            username=\"your@email.com\",\n            password=\"app_password\"\n        )\n\n        # Generate weekly report\n        report = await generator.weekly_report(\"your_username\")\n\n        # Send via email\n        await notifier.send(\n            to=\"recipient@email.com\",\n            subject=f\"Weekly Twitter Report - {report.period}\",\n            body=report.to_html(),\n            attachments=[\n                (\"report.pdf\", report.to_pdf_bytes()),\n                (\"data.csv\", report.to_csv_bytes())\n            ]\n        )\n\n        print(f\"Report sent for period: {report.period}\")\n\n# Schedule with cron or similar\nasyncio.run(scheduled_report_system())\n</code></pre>"},{"location":"guides/analytics/reports/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>username</code> str required Account to report on <code>period_days</code> int 7 Report period in days <code>include_sections</code> list all Sections to include <code>compare_previous</code> bool True Include period comparison <code>format</code> str \"markdown\" Output format <p>Report Frequency</p> <ul> <li>Daily: High-volume accounts or during campaigns</li> <li>Weekly: Standard for most accounts</li> <li>Monthly: Strategic overview and trend analysis</li> </ul> <p>Data Requirements</p> <p>Some report sections require historical data. Enable tracking before generating comparison reports.</p>"},{"location":"guides/analytics/reports/#multi-account-reports","title":"Multi-Account Reports","text":"<pre><code>async def multi_account_report():\n    async with Xeepy() as x:\n        generator = ReportGenerator(x)\n\n        accounts = [\"account1\", \"account2\", \"account3\"]\n\n        # Generate consolidated report\n        report = await generator.multi_account_report(\n            usernames=accounts,\n            period_days=30\n        )\n\n        print(\"MULTI-ACCOUNT REPORT\")\n        print(\"=\" * 70)\n        print(f\"\\n{'Account':&lt;20} {'Followers':&gt;12} {'Growth':&gt;10} {'Eng Rate':&gt;10}\")\n        print(\"-\" * 70)\n\n        for account in report.accounts:\n            print(f\"@{account.username:&lt;19} {account.followers:&gt;12,} {account.growth_rate:&gt;9.2%} {account.engagement_rate:&gt;9.2%}\")\n\n        print(\"-\" * 70)\n        print(f\"{'TOTALS':&lt;20} {report.total_followers:&gt;12,} {report.avg_growth:&gt;9.2%} {report.avg_engagement:&gt;9.2%}\")\n\nasyncio.run(multi_account_report())\n</code></pre>"},{"location":"guides/analytics/reports/#competitive-report","title":"Competitive Report","text":"<pre><code>async def competitive_report():\n    async with Xeepy() as x:\n        generator = ReportGenerator(x)\n\n        report = await generator.competitive_report(\n            your_account=\"your_username\",\n            competitors=[\"competitor1\", \"competitor2\", \"competitor3\"],\n            period_days=30\n        )\n\n        print(\"COMPETITIVE ANALYSIS REPORT\")\n        print(\"=\" * 60)\n\n        print(\"\\n\ud83d\udcca Market Position\")\n        print(f\"Your rank by followers: #{report.your_rank.followers}\")\n        print(f\"Your rank by engagement: #{report.your_rank.engagement}\")\n        print(f\"Your rank by growth: #{report.your_rank.growth}\")\n\n        print(\"\\n\ud83d\udcc8 Comparative Metrics\")\n        print(f\"Your engagement vs avg: {report.vs_average.engagement:+.2%}\")\n        print(f\"Your growth vs avg: {report.vs_average.growth:+.2%}\")\n\n        print(\"\\n\ud83d\udca1 Competitive Insights\")\n        for insight in report.insights[:5]:\n            print(f\"  \u2022 {insight}\")\n\nasyncio.run(competitive_report())\n</code></pre>"},{"location":"guides/analytics/reports/#campaign-performance-report","title":"Campaign Performance Report","text":"<pre><code>async def campaign_report():\n    async with Xeepy() as x:\n        generator = ReportGenerator(x)\n\n        # Generate campaign-specific report\n        report = await generator.campaign_report(\n            username=\"your_username\",\n            campaign_hashtag=\"#YourCampaign\",\n            start_date=\"2024-01-01\",\n            end_date=\"2024-01-15\"\n        )\n\n        print(\"CAMPAIGN PERFORMANCE REPORT\")\n        print(\"=\" * 60)\n        print(f\"Campaign: {report.hashtag}\")\n        print(f\"Period: {report.start_date} to {report.end_date}\")\n\n        print(\"\\n\ud83d\udcca Campaign Metrics\")\n        print(f\"Total mentions: {report.total_mentions:,}\")\n        print(f\"Unique participants: {report.unique_participants:,}\")\n        print(f\"Total reach: {report.estimated_reach:,}\")\n        print(f\"Total engagement: {report.total_engagement:,}\")\n\n        print(\"\\n\ud83d\udcc8 Your Performance\")\n        print(f\"Your campaign tweets: {report.your_tweets}\")\n        print(f\"Your campaign engagement: {report.your_engagement:,}\")\n        print(f\"Share of voice: {report.share_of_voice:.1%}\")\n\n        print(\"\\n\ud83c\udfc6 Top Campaign Tweets\")\n        for tweet in report.top_tweets[:5]:\n            print(f\"  @{tweet.author}: {tweet.engagement:,} engagement\")\n\nasyncio.run(campaign_report())\n</code></pre>"},{"location":"guides/analytics/reports/#export-options","title":"Export Options","text":"<pre><code>async def export_reports():\n    async with Xeepy() as x:\n        generator = ReportGenerator(x)\n\n        report = await generator.generate(\"your_username\", period_days=30)\n\n        # Export in multiple formats\n\n        # Markdown (for GitHub, docs)\n        report.to_markdown(\"report.md\")\n\n        # HTML (for web, email)\n        report.to_html(\"report.html\", style=\"professional\")\n\n        # PDF (for clients, archival)\n        report.to_pdf(\"report.pdf\", include_charts=True)\n\n        # CSV (for data analysis)\n        report.to_csv(\"report_data.csv\")\n\n        # JSON (for APIs, automation)\n        report.to_json(\"report.json\")\n\n        # Excel (for business users)\n        report.to_excel(\"report.xlsx\", include_charts=True)\n\n        print(\"Reports exported in all formats\")\n\nasyncio.run(export_reports())\n</code></pre>"},{"location":"guides/analytics/reports/#report-with-visualizations","title":"Report with Visualizations","text":"<pre><code>async def report_with_charts():\n    async with Xeepy() as x:\n        generator = ReportGenerator(x)\n\n        report = await generator.generate(\n            username=\"your_username\",\n            period_days=30,\n            include_charts=True\n        )\n\n        # Save charts\n        for chart_name, chart_data in report.charts.items():\n            chart_data.save(f\"charts/{chart_name}.png\")\n\n        # Generate HTML with embedded charts\n        report.to_html(\n            \"visual_report.html\",\n            embed_charts=True,\n            chart_style=\"modern\"\n        )\n\n        print(\"Visual report generated with charts\")\n\nasyncio.run(report_with_charts())\n</code></pre>"},{"location":"guides/analytics/reports/#notification-integration","title":"Notification Integration","text":"<pre><code>async def report_with_notifications():\n    async with Xeepy() as x:\n        generator = ReportGenerator(x)\n\n        report = await generator.weekly_report(\"your_username\")\n\n        # Discord notification\n        await x.notify.discord(\n            webhook_url=\"https://discord.com/api/webhooks/...\",\n            message=report.to_discord_embed()\n        )\n\n        # Telegram notification\n        await x.notify.telegram(\n            bot_token=\"...\",\n            chat_id=\"...\",\n            message=report.summary\n        )\n\n        # Slack notification\n        await x.notify.slack(\n            webhook_url=\"https://hooks.slack.com/...\",\n            message=report.to_slack_blocks()\n        )\n\nasyncio.run(report_with_notifications())\n</code></pre>"},{"location":"guides/analytics/reports/#best-practices","title":"Best Practices","text":"<ol> <li>Consistent Timing: Generate reports at the same time each period</li> <li>Include Context: Add notes about events that affected metrics</li> <li>Actionable Insights: Include recommendations, not just data</li> <li>Visual Clarity: Use charts for trend data, tables for comparisons</li> <li>Archive Reports: Keep historical reports for long-term trend analysis</li> <li>Automate: Set up scheduled generation for consistency</li> </ol>"},{"location":"guides/analytics/reports/#related-guides","title":"Related Guides","text":"<ul> <li>Growth Tracking</li> <li>Engagement Analysis</li> <li>Competitor Analysis</li> </ul>"},{"location":"guides/export/","title":"Data Export Guide","text":"<p>Xeepy makes it easy to export your scraped data and analytics to various formats for analysis, backup, or integration with other tools.</p>"},{"location":"guides/export/#overview","title":"Overview","text":"<ul> <li> <p> CSV</p> <p>Spreadsheet-compatible format</p> </li> <li> <p> JSON</p> <p>Structured data for developers</p> </li> <li> <p> Excel</p> <p>Formatted spreadsheets with sheets</p> </li> <li> <p> Database</p> <p>SQLite, PostgreSQL, MySQL</p> </li> <li> <p> Parquet</p> <p>Efficient columnar format for data science</p> </li> </ul>"},{"location":"guides/export/#quick-start","title":"Quick Start","text":"<pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    # Scrape data\n    followers = await x.scrape.followers(\"username\", limit=1000)\n\n    # Export to different formats\n    x.export.to_csv(followers, \"followers.csv\")\n    x.export.to_json(followers, \"followers.json\")\n    x.export.to_excel(followers, \"followers.xlsx\")\n</code></pre>"},{"location":"guides/export/#csv-export","title":"CSV Export","text":""},{"location":"guides/export/#basic-export","title":"Basic Export","text":"<pre><code>async with Xeepy() as x:\n    tweets = await x.scrape.tweets(\"username\", limit=100)\n\n    # Simple export\n    x.export.to_csv(tweets, \"tweets.csv\")\n</code></pre>"},{"location":"guides/export/#custom-fields","title":"Custom Fields","text":"<pre><code>x.export.to_csv(\n    tweets,\n    \"tweets.csv\",\n    fields=[\"id\", \"text\", \"likes\", \"retweets\", \"created_at\"]\n)\n</code></pre>"},{"location":"guides/export/#nested-fields","title":"Nested Fields","text":"<pre><code># Access nested data with dot notation\nx.export.to_csv(\n    tweets,\n    \"tweets.csv\",\n    fields=[\n        \"id\",\n        \"text\",\n        \"author.username\",      # Nested field\n        \"author.followers_count\",\n        \"likes\",\n        \"created_at\"\n    ]\n)\n</code></pre>"},{"location":"guides/export/#append-to-existing-file","title":"Append to Existing File","text":"<pre><code>async with Xeepy() as x:\n    # First batch\n    batch1 = await x.scrape.followers(\"user1\", limit=500)\n    x.export.to_csv(batch1, \"all_followers.csv\")\n\n    # Append more\n    batch2 = await x.scrape.followers(\"user2\", limit=500)\n    x.export.append_csv(batch2, \"all_followers.csv\")\n</code></pre>"},{"location":"guides/export/#csv-options","title":"CSV Options","text":"<pre><code>x.export.to_csv(\n    data,\n    \"output.csv\",\n    delimiter=\",\",           # Field separator\n    encoding=\"utf-8\",        # File encoding\n    include_header=True,     # Include column names\n    date_format=\"%Y-%m-%d\",  # Date formatting\n    null_value=\"\",           # How to represent None\n)\n</code></pre>"},{"location":"guides/export/#json-export","title":"JSON Export","text":""},{"location":"guides/export/#basic-export_1","title":"Basic Export","text":"<pre><code>async with Xeepy() as x:\n    profile = await x.scrape.profile(\"username\")\n    x.export.to_json(profile, \"profile.json\")\n</code></pre>"},{"location":"guides/export/#pretty-print","title":"Pretty Print","text":"<pre><code>x.export.to_json(\n    data,\n    \"output.json\",\n    indent=2,        # Pretty print with indentation\n    sort_keys=True   # Alphabetize keys\n)\n</code></pre>"},{"location":"guides/export/#json-lines-format","title":"JSON Lines Format","text":"<p>For large datasets, use JSON Lines (one JSON object per line):</p> <pre><code>x.export.to_jsonl(tweets, \"tweets.jsonl\")\n\n# Or standard JSON\nx.export.to_json(tweets, \"tweets.json\", format=\"lines\")\n</code></pre>"},{"location":"guides/export/#custom-serialization","title":"Custom Serialization","text":"<pre><code>from datetime import datetime\n\ndef custom_serializer(obj):\n    if isinstance(obj, datetime):\n        return obj.isoformat()\n    raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n\nx.export.to_json(data, \"output.json\", default=custom_serializer)\n</code></pre>"},{"location":"guides/export/#excel-export","title":"Excel Export","text":""},{"location":"guides/export/#basic-export_2","title":"Basic Export","text":"<pre><code>async with Xeepy() as x:\n    followers = await x.scrape.followers(\"username\", limit=1000)\n    x.export.to_excel(followers, \"followers.xlsx\")\n</code></pre>"},{"location":"guides/export/#multiple-sheets","title":"Multiple Sheets","text":"<pre><code>async with Xeepy() as x:\n    followers = await x.scrape.followers(\"username\")\n    following = await x.scrape.following(\"username\")\n    tweets = await x.scrape.tweets(\"username\", limit=100)\n\n    x.export.to_excel_multi(\n        \"account_data.xlsx\",\n        {\n            \"Followers\": followers,\n            \"Following\": following,\n            \"Tweets\": tweets\n        }\n    )\n</code></pre>"},{"location":"guides/export/#formatted-reports","title":"Formatted Reports","text":"<pre><code>x.export.to_excel(\n    data,\n    \"report.xlsx\",\n    sheet_name=\"Data\",\n    freeze_header=True,      # Freeze top row\n    auto_width=True,         # Auto-fit column widths\n    header_style={\n        \"bold\": True,\n        \"bg_color\": \"#1DA1F2\",\n        \"font_color\": \"white\"\n    }\n)\n</code></pre>"},{"location":"guides/export/#add-charts","title":"Add Charts","text":"<pre><code>x.export.to_excel_with_chart(\n    growth_data,\n    \"growth_report.xlsx\",\n    chart_type=\"line\",\n    x_column=\"date\",\n    y_column=\"followers\",\n    title=\"Follower Growth\"\n)\n</code></pre>"},{"location":"guides/export/#database-export","title":"Database Export","text":""},{"location":"guides/export/#sqlite","title":"SQLite","text":"<pre><code>async with Xeepy() as x:\n    tweets = await x.scrape.tweets(\"username\", limit=1000)\n\n    # Export to SQLite\n    await x.export.to_database(\n        tweets,\n        \"sqlite:///xeepy.db\",\n        table=\"tweets\"\n    )\n</code></pre>"},{"location":"guides/export/#postgresql","title":"PostgreSQL","text":"<pre><code>await x.export.to_database(\n    followers,\n    \"postgresql://user:pass@host:5432/dbname\",\n    table=\"followers\",\n    if_exists=\"append\"  # \"replace\", \"append\", or \"fail\"\n)\n</code></pre>"},{"location":"guides/export/#mysql","title":"MySQL","text":"<pre><code>await x.export.to_database(\n    data,\n    \"mysql://user:pass@host:3306/dbname\",\n    table=\"twitter_data\"\n)\n</code></pre>"},{"location":"guides/export/#custom-schema","title":"Custom Schema","text":"<pre><code>await x.export.to_database(\n    tweets,\n    \"sqlite:///xeepy.db\",\n    table=\"tweets\",\n    dtype={\n        \"id\": \"VARCHAR(50) PRIMARY KEY\",\n        \"text\": \"TEXT\",\n        \"likes\": \"INTEGER\",\n        \"created_at\": \"TIMESTAMP\"\n    }\n)\n</code></pre>"},{"location":"guides/export/#incremental-updates","title":"Incremental Updates","text":"<pre><code>async with Xeepy() as x:\n    # Only insert new records\n    await x.export.to_database(\n        new_followers,\n        \"sqlite:///xeepy.db\",\n        table=\"followers\",\n        if_exists=\"append\",\n        unique_columns=[\"id\"]  # Skip duplicates\n    )\n</code></pre>"},{"location":"guides/export/#parquet-export","title":"Parquet Export","text":"<p>For data science workflows:</p> <pre><code>async with Xeepy() as x:\n    tweets = await x.scrape.tweets(\"username\", limit=10000)\n\n    # Export to Parquet (efficient columnar format)\n    x.export.to_parquet(tweets, \"tweets.parquet\")\n</code></pre>"},{"location":"guides/export/#with-compression","title":"With Compression","text":"<pre><code>x.export.to_parquet(\n    data,\n    \"data.parquet\",\n    compression=\"snappy\"  # or \"gzip\", \"brotli\"\n)\n</code></pre>"},{"location":"guides/export/#partitioned-dataset","title":"Partitioned Dataset","text":"<pre><code>x.export.to_parquet(\n    tweets,\n    \"tweets_partitioned/\",\n    partition_cols=[\"year\", \"month\"]\n)\n</code></pre>"},{"location":"guides/export/#streaming-export","title":"Streaming Export","text":"<p>For large datasets, export as you scrape:</p> <pre><code>async with Xeepy() as x:\n    # Initialize export\n    exporter = x.export.create_stream(\"followers.csv\", format=\"csv\")\n\n    async for batch in x.scrape.followers_batched(\"popular_user\", batch_size=100):\n        # Export each batch immediately\n        exporter.write_batch(batch)\n        print(f\"Exported {exporter.total_rows} rows\")\n\n    exporter.close()\n</code></pre>"},{"location":"guides/export/#custom-export-formats","title":"Custom Export Formats","text":""},{"location":"guides/export/#markdown-table","title":"Markdown Table","text":"<pre><code>md_table = x.export.to_markdown(\n    followers[:10],\n    columns=[\"username\", \"followers_count\", \"bio\"]\n)\nprint(md_table)\n# | username | followers_count | bio |\n# |----------|-----------------|-----|\n# | user1    | 10,000          | ... |\n</code></pre>"},{"location":"guides/export/#html-table","title":"HTML Table","text":"<pre><code>html = x.export.to_html(\n    data,\n    table_class=\"data-table\",\n    include_style=True\n)\n\nwith open(\"table.html\", \"w\") as f:\n    f.write(html)\n</code></pre>"},{"location":"guides/export/#export-transformations","title":"Export Transformations","text":""},{"location":"guides/export/#filter-before-export","title":"Filter Before Export","text":"<pre><code>async with Xeepy() as x:\n    followers = await x.scrape.followers(\"username\", limit=5000)\n\n    # Export only verified users\n    verified = [f for f in followers if f.verified]\n    x.export.to_csv(verified, \"verified_followers.csv\")\n</code></pre>"},{"location":"guides/export/#transform-data","title":"Transform Data","text":"<pre><code>async with Xeepy() as x:\n    tweets = await x.scrape.tweets(\"username\", limit=100)\n\n    # Transform before export\n    transformed = [\n        {\n            \"id\": t.id,\n            \"text\": t.text,\n            \"engagement\": t.likes + t.retweets,\n            \"date\": t.created_at.strftime(\"%Y-%m-%d\")\n        }\n        for t in tweets\n    ]\n\n    x.export.to_json(transformed, \"tweets_transformed.json\")\n</code></pre>"},{"location":"guides/export/#configuration","title":"Configuration","text":""},{"location":"guides/export/#default-settings","title":"Default Settings","text":"<pre><code># xeepy.toml\n[xeepy.export]\ndefault_format = \"csv\"\noutput_dir = \"./exports\"\ntimestamp_filenames = true  # Add timestamp to filenames\nencoding = \"utf-8\"\n\n[xeepy.export.csv]\ndelimiter = \",\"\ninclude_header = true\n\n[xeepy.export.json]\nindent = 2\nsort_keys = false\n\n[xeepy.export.excel]\nfreeze_header = true\nauto_width = true\n</code></pre>"},{"location":"guides/export/#cli-commands","title":"CLI Commands","text":"<pre><code># Export from scrape command\nxeepy scrape followers username --limit 1000 --output followers.csv\nxeepy scrape tweets username --output tweets.json --format json\n\n# Convert formats\nxeepy export convert data.csv data.json\nxeepy export convert data.json data.xlsx\n\n# Export to database\nxeepy export database data.csv sqlite:///data.db --table tweets\n</code></pre>"},{"location":"guides/export/#integration-with-pandas","title":"Integration with pandas","text":"<pre><code>async with Xeepy() as x:\n    tweets = await x.scrape.tweets(\"username\", limit=1000)\n\n    # Convert to DataFrame\n    df = x.export.to_dataframe(tweets)\n\n    # Now use pandas features\n    print(df.describe())\n    print(df.groupby('author.verified')['likes'].mean())\n\n    # Export from DataFrame\n    df.to_csv(\"tweets.csv\")\n    df.to_parquet(\"tweets.parquet\")\n</code></pre>"},{"location":"guides/export/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the right format - CSV for spreadsheets, JSON for APIs, Parquet for data science</li> <li>Stream large exports - Don't load everything into memory</li> <li>Timestamp your files - Avoid overwriting previous exports</li> <li>Verify exports - Check row counts and data integrity</li> <li>Compress large files - Use gzip for CSV, snappy for Parquet</li> <li>Backup to database - For long-term storage and querying</li> </ol>"},{"location":"guides/monitoring/","title":"Monitoring Guide","text":"<p>Xeepy provides comprehensive monitoring capabilities to track your account, detect changes, and stay informed about your X/Twitter presence.</p>"},{"location":"guides/monitoring/#overview","title":"Overview","text":"<ul> <li> <p> Unfollower Detection</p> <p>Know when someone unfollows you</p> </li> <li> <p> Growth Tracking</p> <p>Track follower changes over time</p> </li> <li> <p> Account Monitoring</p> <p>Watch specific accounts for changes</p> </li> <li> <p> Keyword Monitoring</p> <p>Get alerts when keywords are mentioned</p> </li> <li> <p> Engagement Tracking</p> <p>Monitor likes, retweets, replies</p> </li> </ul>"},{"location":"guides/monitoring/#quick-start","title":"Quick Start","text":"<pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    # Check who unfollowed you\n    report = await x.monitor.unfollowers()\n\n    print(f\"New unfollowers: {len(report.unfollowers)}\")\n    print(f\"New followers: {len(report.new_followers)}\")\n\n    for user in report.unfollowers:\n        print(f\"  - @{user} unfollowed you\")\n</code></pre>"},{"location":"guides/monitoring/#unfollower-detection","title":"Unfollower Detection","text":"<p>Track when users unfollow you:</p> <pre><code>async with Xeepy() as x:\n    # Basic unfollower check\n    report = await x.monitor.unfollowers()\n\n    # With history comparison\n    report = await x.monitor.unfollowers(\n        compare_to=\"24h\",  # Compare to 24 hours ago\n        notify=True        # Send notification\n    )\n\n    print(f\"\"\"\n    \ud83d\udcca Follower Report\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    Current followers: {report.current_count:,}\n    New followers: +{len(report.new_followers)}\n    Unfollowers: -{len(report.unfollowers)}\n    Net change: {report.net_change:+d}\n    \"\"\")\n</code></pre>"},{"location":"guides/monitoring/#detailed-unfollower-info","title":"Detailed Unfollower Info","text":"<pre><code>async with Xeepy() as x:\n    report = await x.monitor.unfollowers(include_details=True)\n\n    for unfollower in report.unfollowers_detailed:\n        print(f\"@{unfollower.username}\")\n        print(f\"  Followers: {unfollower.followers_count:,}\")\n        print(f\"  Following: {unfollower.following_count:,}\")\n        print(f\"  Last tweet: {unfollower.last_tweet_date}\")\n        print(f\"  Followed you for: {unfollower.days_following} days\")\n</code></pre>"},{"location":"guides/monitoring/#schedule-unfollower-checks","title":"Schedule Unfollower Checks","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\n\nasync def monitor_unfollowers():\n    \"\"\"Check for unfollowers every hour\"\"\"\n    async with Xeepy() as x:\n        while True:\n            report = await x.monitor.unfollowers()\n\n            if report.unfollowers:\n                # Send alert\n                await x.notify.discord(\n                    f\"\u26a0\ufe0f Lost {len(report.unfollowers)} followers: \" +\n                    \", \".join(f\"@{u}\" for u in report.unfollowers[:5])\n                )\n\n            await asyncio.sleep(3600)  # Check every hour\n</code></pre>"},{"location":"guides/monitoring/#growth-tracking","title":"Growth Tracking","text":"<p>Monitor your account growth over time:</p> <pre><code>async with Xeepy() as x:\n    # Track growth for different periods\n    growth_day = await x.monitor.growth(period=\"24h\")\n    growth_week = await x.monitor.growth(period=\"7d\")\n    growth_month = await x.monitor.growth(period=\"30d\")\n\n    print(f\"\"\"\n    \ud83d\udcc8 Growth Report\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    Last 24h: {growth_day.net_change:+d} followers\n    Last 7d:  {growth_week.net_change:+d} followers\n    Last 30d: {growth_month.net_change:+d} followers\n\n    Average daily: {growth_month.avg_daily_change:+.1f}\n    Growth rate: {growth_month.growth_rate:.2%}\n    \"\"\")\n</code></pre>"},{"location":"guides/monitoring/#growth-trends","title":"Growth Trends","text":"<pre><code>async with Xeepy() as x:\n    trends = await x.monitor.growth_trends(period=\"30d\")\n\n    # Daily breakdown\n    for day in trends.daily_data:\n        print(f\"{day.date}: {day.followers:,} ({day.change:+d})\")\n\n    # Identify best/worst days\n    print(f\"Best day: {trends.best_day.date} (+{trends.best_day.change})\")\n    print(f\"Worst day: {trends.worst_day.date} ({trends.worst_day.change})\")\n</code></pre>"},{"location":"guides/monitoring/#account-monitoring","title":"Account Monitoring","text":"<p>Watch specific accounts for changes:</p> <pre><code>async with Xeepy() as x:\n    # Monitor competitor accounts\n    competitors = [\"competitor1\", \"competitor2\"]\n\n    for account in competitors:\n        changes = await x.monitor.account(account)\n\n        if changes.has_changes:\n            print(f\"@{account} changes:\")\n            if changes.bio_changed:\n                print(f\"  Bio: {changes.old_bio} \u2192 {changes.new_bio}\")\n            if changes.name_changed:\n                print(f\"  Name: {changes.old_name} \u2192 {changes.new_name}\")\n            if changes.followers_changed:\n                print(f\"  Followers: {changes.follower_change:+d}\")\n</code></pre>"},{"location":"guides/monitoring/#watch-for-specific-events","title":"Watch for Specific Events","text":"<pre><code>async with Xeepy() as x:\n    # Get notified when account posts\n    await x.monitor.watch_account(\n        \"target_user\",\n        events=[\"new_tweet\", \"new_thread\", \"pinned_change\"],\n        callback=my_notification_handler\n    )\n</code></pre>"},{"location":"guides/monitoring/#keyword-monitoring","title":"Keyword Monitoring","text":"<p>Track mentions of keywords across X:</p> <pre><code>async with Xeepy() as x:\n    # Monitor keywords in real-time\n    await x.monitor.keywords(\n        keywords=[\"your_brand\", \"your_product\", \"@yourusername\"],\n        callback=handle_mention\n    )\n\nasync def handle_mention(tweet):\n    \"\"\"Called when keyword is found\"\"\"\n    print(f\"Found mention: {tweet.text}\")\n\n    # Auto-respond, like, or notify\n    if tweet.sentiment &gt; 0.5:\n        await x.engage.like(tweet.url)\n</code></pre>"},{"location":"guides/monitoring/#scheduled-keyword-search","title":"Scheduled Keyword Search","text":"<pre><code>async with Xeepy() as x:\n    # Search for keywords periodically\n    results = await x.monitor.keyword_search(\n        keywords=[\"python tips\", \"learn python\"],\n        since=\"1h\",  # Last hour\n        min_engagement=10\n    )\n\n    for tweet in results:\n        print(f\"@{tweet.author.username}: {tweet.text[:100]}\")\n</code></pre>"},{"location":"guides/monitoring/#engagement-monitoring","title":"Engagement Monitoring","text":"<p>Track engagement on your tweets:</p> <pre><code>async with Xeepy() as x:\n    # Monitor engagement on recent tweets\n    engagement = await x.monitor.my_engagement(period=\"7d\")\n\n    print(f\"\"\"\n    \ud83d\udcac Engagement Report (7d)\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    Total likes: {engagement.total_likes:,}\n    Total retweets: {engagement.total_retweets:,}\n    Total replies: {engagement.total_replies:,}\n\n    Avg per tweet: {engagement.avg_per_tweet:.1f}\n    Engagement rate: {engagement.rate:.2%}\n\n    Top tweet: {engagement.top_tweet.text[:50]}...\n      ({engagement.top_tweet.likes:,} likes)\n    \"\"\")\n</code></pre>"},{"location":"guides/monitoring/#per-tweet-monitoring","title":"Per-Tweet Monitoring","text":"<pre><code>async with Xeepy() as x:\n    # Watch a specific tweet's performance\n    tweet_url = \"https://x.com/you/status/123456789\"\n\n    performance = await x.monitor.tweet_performance(\n        tweet_url,\n        duration=\"24h\",\n        interval=\"1h\"\n    )\n\n    for snapshot in performance.timeline:\n        print(f\"{snapshot.time}: {snapshot.likes} likes, {snapshot.retweets} RTs\")\n</code></pre>"},{"location":"guides/monitoring/#notifications-integration","title":"Notifications Integration","text":"<p>Connect monitoring to notifications:</p> <pre><code>from xeepy import Xeepy\nfrom xeepy.notifications import DiscordNotifier, TelegramNotifier\n\nasync with Xeepy() as x:\n    discord = DiscordNotifier(webhook_url=\"...\")\n    telegram = TelegramNotifier(token=\"...\", chat_id=\"...\")\n\n    # Configure notification triggers\n    x.monitor.on_unfollower(lambda u: discord.send(f\"Lost follower: @{u}\"))\n    x.monitor.on_new_follower(lambda u: telegram.send(f\"New follower: @{u}\"))\n    x.monitor.on_mention(lambda t: discord.send(f\"Mentioned: {t.text[:100]}\"))\n\n    # Start monitoring\n    await x.monitor.start()\n</code></pre>"},{"location":"guides/monitoring/#cli-commands","title":"CLI Commands","text":"<pre><code># Check unfollowers\nxeepy monitor unfollowers\n\n# Track growth\nxeepy monitor growth --period 7d\n\n# Watch keywords\nxeepy monitor keywords \"python,automation\" --notify discord\n\n# Monitor account\nxeepy monitor account competitor_username --watch\n\n# Full monitoring daemon\nxeepy monitor start --config monitoring.yaml\n</code></pre>"},{"location":"guides/monitoring/#monitoring-configuration","title":"Monitoring Configuration","text":"<p>Create <code>monitoring.yaml</code>:</p> <pre><code>monitoring:\n  # Unfollower detection\n  unfollowers:\n    enabled: true\n    check_interval: 3600  # seconds\n    notify:\n      - discord\n      - email\n\n  # Growth tracking\n  growth:\n    enabled: true\n    periods: [\"24h\", \"7d\", \"30d\"]\n    snapshot_interval: 86400\n\n  # Keyword monitoring\n  keywords:\n    enabled: true\n    terms:\n      - \"your_brand\"\n      - \"@yourusername\"\n      - \"your_product\"\n    min_engagement: 5\n\n  # Account watching\n  accounts:\n    enabled: true\n    watch:\n      - competitor1\n      - competitor2\n    events:\n      - new_tweet\n      - follower_milestone\n\n  # Notifications\n  notifications:\n    discord:\n      webhook: ${DISCORD_WEBHOOK}\n    telegram:\n      token: ${TELEGRAM_TOKEN}\n      chat_id: ${TELEGRAM_CHAT_ID}\n    email:\n      to: you@example.com\n</code></pre>"},{"location":"guides/monitoring/#best-practices","title":"Best Practices","text":"<ol> <li>Start simple - Begin with unfollower detection</li> <li>Set reasonable intervals - Don't check too frequently</li> <li>Filter noise - Set minimum engagement thresholds</li> <li>Prioritize alerts - Not everything needs a notification</li> <li>Review regularly - Check monitoring reports weekly</li> </ol>"},{"location":"guides/monitoring/#data-storage","title":"Data Storage","text":"<p>Monitoring data is stored automatically:</p> <pre><code>async with Xeepy() as x:\n    # Access historical data\n    history = await x.monitor.get_history(\n        metric=\"followers\",\n        period=\"90d\"\n    )\n\n    # Export for analysis\n    x.export.to_csv(history, \"follower_history.csv\")\n</code></pre>"},{"location":"guides/monitoring/accounts/","title":"Account Change Monitoring","text":"<p>Monitor profile changes, tweet deletions, and following behavior across multiple accounts.</p>"},{"location":"guides/monitoring/accounts/#overview","title":"Overview","text":"<p>Account monitoring helps you:</p> <ul> <li>Detect bio, name, or avatar changes</li> <li>Track tweet deletions</li> <li>Monitor following/unfollowing activity</li> <li>Watch multiple accounts simultaneously</li> <li>Store change history for analysis</li> </ul>"},{"location":"guides/monitoring/accounts/#bionameavatar-change-detection","title":"Bio/Name/Avatar Change Detection","text":""},{"location":"guides/monitoring/accounts/#basic-profile-change-monitor","title":"Basic Profile Change Monitor","text":"<pre><code>import asyncio\nfrom datetime import datetime\nfrom xtools import Xtools\n\nclass ProfileChangeMonitor:\n    def __init__(self, username: str):\n        self.username = username\n        self.last_profile = None\n\n    async def check_changes(self) -&gt; list:\n        \"\"\"Check for profile changes.\"\"\"\n        async with Xtools() as x:\n            profile = await x.scrape.profile(self.username)\n            changes = []\n\n            if self.last_profile:\n                # Check name change\n                if profile.name != self.last_profile.get(\"name\"):\n                    changes.append({\n                        \"field\": \"name\",\n                        \"old\": self.last_profile.get(\"name\"),\n                        \"new\": profile.name\n                    })\n\n                # Check bio change\n                if profile.bio != self.last_profile.get(\"bio\"):\n                    changes.append({\n                        \"field\": \"bio\",\n                        \"old\": self.last_profile.get(\"bio\"),\n                        \"new\": profile.bio\n                    })\n\n                # Check avatar change\n                if profile.profile_image_url != self.last_profile.get(\"avatar\"):\n                    changes.append({\n                        \"field\": \"avatar\",\n                        \"old\": self.last_profile.get(\"avatar\"),\n                        \"new\": profile.profile_image_url\n                    })\n\n                # Check location change\n                if profile.location != self.last_profile.get(\"location\"):\n                    changes.append({\n                        \"field\": \"location\",\n                        \"old\": self.last_profile.get(\"location\"),\n                        \"new\": profile.location\n                    })\n\n                # Check website change\n                if profile.url != self.last_profile.get(\"url\"):\n                    changes.append({\n                        \"field\": \"website\",\n                        \"old\": self.last_profile.get(\"url\"),\n                        \"new\": profile.url\n                    })\n\n            # Update stored profile\n            self.last_profile = {\n                \"name\": profile.name,\n                \"bio\": profile.bio,\n                \"avatar\": profile.profile_image_url,\n                \"location\": profile.location,\n                \"url\": profile.url\n            }\n\n            return changes\n\n# Usage\nasync def main():\n    monitor = ProfileChangeMonitor(\"elonmusk\")\n\n    # Initial load\n    await monitor.check_changes()\n    print(\"Profile loaded. Monitoring for changes...\")\n\n    while True:\n        await asyncio.sleep(3600)  # Check hourly\n\n        changes = await monitor.check_changes()\n        for change in changes:\n            print(f\"\\n\ud83d\udd14 @{monitor.username} changed their {change['field']}:\")\n            print(f\"   Old: {change['old']}\")\n            print(f\"   New: {change['new']}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/accounts/#comprehensive-profile-tracker","title":"Comprehensive Profile Tracker","text":"<pre><code>import asyncio\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom xtools import Xtools\n\nclass ComprehensiveProfileTracker:\n    TRACKED_FIELDS = [\n        \"name\", \"username\", \"bio\", \"location\", \"url\",\n        \"profile_image_url\", \"profile_banner_url\",\n        \"protected\", \"verified\"\n    ]\n\n    def __init__(self, username: str, storage_dir: str = \"profile_history\"):\n        self.username = username\n        self.storage_dir = Path(storage_dir)\n        self.storage_dir.mkdir(exist_ok=True)\n        self.history_file = self.storage_dir / f\"{username}_history.json\"\n        self.history = self._load_history()\n\n    def _load_history(self) -&gt; list:\n        if self.history_file.exists():\n            return json.loads(self.history_file.read_text())\n        return []\n\n    def _save_history(self):\n        self.history_file.write_text(json.dumps(self.history, indent=2))\n\n    async def snapshot(self) -&gt; dict:\n        \"\"\"Take a profile snapshot and detect changes.\"\"\"\n        async with Xtools() as x:\n            profile = await x.scrape.profile(self.username)\n\n            current = {\n                \"timestamp\": datetime.now().isoformat(),\n                \"name\": profile.name,\n                \"username\": profile.username,\n                \"bio\": profile.bio,\n                \"location\": profile.location,\n                \"url\": profile.url,\n                \"profile_image_url\": profile.profile_image_url,\n                \"profile_banner_url\": profile.profile_banner_url,\n                \"protected\": profile.protected,\n                \"verified\": profile.verified,\n                \"followers_count\": profile.followers_count,\n                \"following_count\": profile.following_count\n            }\n\n            changes = []\n\n            if self.history:\n                last = self.history[-1]\n                for field in self.TRACKED_FIELDS:\n                    if current.get(field) != last.get(field):\n                        changes.append({\n                            \"field\": field,\n                            \"old\": last.get(field),\n                            \"new\": current.get(field),\n                            \"timestamp\": current[\"timestamp\"]\n                        })\n\n            # Always save snapshot\n            self.history.append(current)\n            self._save_history()\n\n            return {\n                \"snapshot\": current,\n                \"changes\": changes\n            }\n\n    def get_field_history(self, field: str) -&gt; list:\n        \"\"\"Get history of changes for a specific field.\"\"\"\n        changes = []\n        last_value = None\n\n        for snapshot in self.history:\n            value = snapshot.get(field)\n            if value != last_value and last_value is not None:\n                changes.append({\n                    \"timestamp\": snapshot[\"timestamp\"],\n                    \"old\": last_value,\n                    \"new\": value\n                })\n            last_value = value\n\n        return changes\n\n# Usage\nasync def main():\n    tracker = ComprehensiveProfileTracker(\"elonmusk\")\n\n    result = await tracker.snapshot()\n\n    if result[\"changes\"]:\n        print(\"\ud83d\udd14 Profile changes detected:\")\n        for change in result[\"changes\"]:\n            print(f\"   {change['field']}: {change['old']} \u2192 {change['new']}\")\n    else:\n        print(\"No changes detected\")\n\n    # View bio history\n    bio_history = tracker.get_field_history(\"bio\")\n    print(f\"\\nBio changed {len(bio_history)} times\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/accounts/#tweet-deletion-monitoring","title":"Tweet Deletion Monitoring","text":""},{"location":"guides/monitoring/accounts/#track-deleted-tweets","title":"Track Deleted Tweets","text":"<pre><code>import asyncio\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom xtools import Xtools\n\nclass TweetDeletionMonitor:\n    def __init__(self, username: str, storage_dir: str = \"tweet_archive\"):\n        self.username = username\n        self.storage_dir = Path(storage_dir)\n        self.storage_dir.mkdir(exist_ok=True)\n        self.archive_file = self.storage_dir / f\"{username}_tweets.json\"\n        self.deleted_file = self.storage_dir / f\"{username}_deleted.json\"\n\n        self.known_tweets = self._load_json(self.archive_file)\n        self.deleted_tweets = self._load_json(self.deleted_file)\n\n    def _load_json(self, path: Path) -&gt; dict:\n        if path.exists():\n            return json.loads(path.read_text())\n        return {}\n\n    def _save_json(self, path: Path, data: dict):\n        path.write_text(json.dumps(data, indent=2))\n\n    async def check_for_deletions(self) -&gt; list:\n        \"\"\"Check for deleted tweets.\"\"\"\n        async with Xtools() as x:\n            # Get current tweets\n            result = await x.scrape.tweets(self.username, limit=200)\n            current_ids = {t.id for t in result.tweets}\n\n            # Find deleted tweets\n            deleted = []\n            for tweet_id, tweet_data in list(self.known_tweets.items()):\n                if tweet_id not in current_ids:\n                    # Tweet was deleted\n                    tweet_data[\"deleted_at\"] = datetime.now().isoformat()\n                    deleted.append(tweet_data)\n                    self.deleted_tweets[tweet_id] = tweet_data\n                    del self.known_tweets[tweet_id]\n\n            # Add new tweets to archive\n            for tweet in result.tweets:\n                if tweet.id not in self.known_tweets:\n                    self.known_tweets[tweet.id] = {\n                        \"id\": tweet.id,\n                        \"text\": tweet.text,\n                        \"created_at\": tweet.created_at.isoformat() if tweet.created_at else None,\n                        \"archived_at\": datetime.now().isoformat(),\n                        \"likes\": tweet.like_count,\n                        \"retweets\": tweet.retweet_count\n                    }\n\n            # Save state\n            self._save_json(self.archive_file, self.known_tweets)\n            self._save_json(self.deleted_file, self.deleted_tweets)\n\n            return deleted\n\n    def get_deleted_tweets(self) -&gt; list:\n        \"\"\"Get all recorded deleted tweets.\"\"\"\n        return list(self.deleted_tweets.values())\n\n# Usage\nasync def main():\n    monitor = TweetDeletionMonitor(\"target_username\")\n\n    # Check for deletions\n    deleted = await monitor.check_for_deletions()\n\n    if deleted:\n        print(f\"\ud83d\uddd1\ufe0f {len(deleted)} tweets deleted:\")\n        for tweet in deleted:\n            print(f\"\\n   [{tweet['id']}]\")\n            print(f\"   {tweet['text'][:100]}...\")\n            print(f\"   Originally posted: {tweet['created_at']}\")\n    else:\n        print(\"No deletions detected\")\n\n    # View all deleted tweets\n    all_deleted = monitor.get_deleted_tweets()\n    print(f\"\\nTotal archived deletions: {len(all_deleted)}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/accounts/#following-changes-monitoring","title":"Following Changes Monitoring","text":""},{"location":"guides/monitoring/accounts/#track-who-user-followsunfollows","title":"Track Who User Follows/Unfollows","text":"<pre><code>import asyncio\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom xtools import Xtools\n\nclass FollowingChangeMonitor:\n    def __init__(self, username: str, storage_dir: str = \"following_data\"):\n        self.username = username\n        self.storage_dir = Path(storage_dir)\n        self.storage_dir.mkdir(exist_ok=True)\n        self.state_file = self.storage_dir / f\"{username}_following.json\"\n        self.history_file = self.storage_dir / f\"{username}_follow_history.json\"\n\n        self.known_following = set(self._load_state())\n        self.history = self._load_history()\n\n    def _load_state(self) -&gt; list:\n        if self.state_file.exists():\n            return json.loads(self.state_file.read_text())\n        return []\n\n    def _load_history(self) -&gt; list:\n        if self.history_file.exists():\n            return json.loads(self.history_file.read_text())\n        return []\n\n    def _save_state(self, following: set):\n        self.state_file.write_text(json.dumps(list(following)))\n\n    def _save_history(self):\n        self.history_file.write_text(json.dumps(self.history, indent=2))\n\n    async def check_changes(self) -&gt; dict:\n        \"\"\"Check for following changes.\"\"\"\n        async with Xtools() as x:\n            result = await x.scrape.following(self.username, limit=5000)\n            current_following = {u.username for u in result.users}\n\n            changes = {\n                \"new_follows\": [],\n                \"unfollows\": [],\n                \"timestamp\": datetime.now().isoformat()\n            }\n\n            if self.known_following:\n                # New follows\n                new_follows = current_following - self.known_following\n                changes[\"new_follows\"] = list(new_follows)\n\n                # Unfollows\n                unfollows = self.known_following - current_following\n                changes[\"unfollows\"] = list(unfollows)\n\n                # Record history\n                if new_follows or unfollows:\n                    self.history.append(changes)\n                    self._save_history()\n\n            # Update state\n            self.known_following = current_following\n            self._save_state(current_following)\n\n            return changes\n\n    def get_follow_history(self, username: str) -&gt; list:\n        \"\"\"Get follow/unfollow history for specific user.\"\"\"\n        events = []\n        for record in self.history:\n            if username in record.get(\"new_follows\", []):\n                events.append({\"action\": \"followed\", \"timestamp\": record[\"timestamp\"]})\n            if username in record.get(\"unfollows\", []):\n                events.append({\"action\": \"unfollowed\", \"timestamp\": record[\"timestamp\"]})\n        return events\n\n# Usage\nasync def main():\n    monitor = FollowingChangeMonitor(\"target_username\")\n\n    changes = await monitor.check_changes()\n\n    if changes[\"new_follows\"]:\n        print(f\"\u2795 Started following ({len(changes['new_follows'])}):\")\n        for user in changes[\"new_follows\"][:10]:\n            print(f\"   @{user}\")\n\n    if changes[\"unfollows\"]:\n        print(f\"\u2796 Stopped following ({len(changes['unfollows'])}):\")\n        for user in changes[\"unfollows\"][:10]:\n            print(f\"   @{user}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/accounts/#multiple-account-tracking","title":"Multiple Account Tracking","text":""},{"location":"guides/monitoring/accounts/#multi-account-monitor","title":"Multi-Account Monitor","text":"<pre><code>import asyncio\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom xtools import Xtools\n\nclass MultiAccountMonitor:\n    def __init__(self, usernames: list, storage_dir: str = \"multi_monitor\"):\n        self.usernames = usernames\n        self.storage_dir = Path(storage_dir)\n        self.storage_dir.mkdir(exist_ok=True)\n        self.state = self._load_state()\n\n    def _load_state(self) -&gt; dict:\n        state_file = self.storage_dir / \"state.json\"\n        if state_file.exists():\n            return json.loads(state_file.read_text())\n        return {}\n\n    def _save_state(self):\n        state_file = self.storage_dir / \"state.json\"\n        state_file.write_text(json.dumps(self.state, indent=2))\n\n    async def check_all(self) -&gt; dict:\n        \"\"\"Check all accounts for changes.\"\"\"\n        async with Xtools() as x:\n            results = {}\n\n            for username in self.usernames:\n                profile = await x.scrape.profile(username)\n\n                current = {\n                    \"name\": profile.name,\n                    \"bio\": profile.bio,\n                    \"followers\": profile.followers_count,\n                    \"following\": profile.following_count,\n                    \"avatar\": profile.profile_image_url,\n                    \"verified\": profile.verified\n                }\n\n                changes = []\n                last = self.state.get(username, {})\n\n                for field in [\"name\", \"bio\", \"avatar\", \"verified\"]:\n                    if field in last and current[field] != last[field]:\n                        changes.append({\n                            \"field\": field,\n                            \"old\": last[field],\n                            \"new\": current[field]\n                        })\n\n                results[username] = {\n                    \"current\": current,\n                    \"changes\": changes\n                }\n\n                self.state[username] = current\n\n                await asyncio.sleep(1)  # Rate limiting\n\n            self._save_state()\n            return results\n\n    async def monitor_loop(self, interval_minutes: int = 60):\n        \"\"\"Continuous monitoring loop.\"\"\"\n        print(f\"\ud83d\udd0d Monitoring {len(self.usernames)} accounts...\")\n\n        while True:\n            results = await self.check_all()\n\n            for username, data in results.items():\n                if data[\"changes\"]:\n                    print(f\"\\n\ud83d\udd14 @{username} changes:\")\n                    for change in data[\"changes\"]:\n                        print(f\"   {change['field']}: {change['old']} \u2192 {change['new']}\")\n\n            await asyncio.sleep(interval_minutes * 60)\n\n# Usage\nasync def main():\n    accounts = [\n        \"elonmusk\",\n        \"naval\",\n        \"paulg\",\n        \"sama\",\n        \"vaborsh\"\n    ]\n\n    monitor = MultiAccountMonitor(accounts)\n    await monitor.monitor_loop(interval_minutes=30)\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/accounts/#change-history-storage","title":"Change History Storage","text":""},{"location":"guides/monitoring/accounts/#structured-change-log","title":"Structured Change Log","text":"<pre><code>import asyncio\nimport sqlite3\nfrom datetime import datetime\nfrom xtools import Xtools\n\nclass ChangeHistoryDB:\n    def __init__(self, db_path: str = \"account_changes.db\"):\n        self.conn = sqlite3.connect(db_path)\n        self._init_db()\n\n    def _init_db(self):\n        self.conn.executescript(\"\"\"\n            CREATE TABLE IF NOT EXISTS profile_changes (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                username TEXT NOT NULL,\n                field TEXT NOT NULL,\n                old_value TEXT,\n                new_value TEXT,\n                detected_at DATETIME DEFAULT CURRENT_TIMESTAMP\n            );\n\n            CREATE TABLE IF NOT EXISTS tweet_deletions (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                username TEXT NOT NULL,\n                tweet_id TEXT NOT NULL,\n                tweet_text TEXT,\n                original_date TEXT,\n                deleted_at DATETIME DEFAULT CURRENT_TIMESTAMP\n            );\n\n            CREATE TABLE IF NOT EXISTS follow_events (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                username TEXT NOT NULL,\n                target_username TEXT NOT NULL,\n                action TEXT NOT NULL,\n                detected_at DATETIME DEFAULT CURRENT_TIMESTAMP\n            );\n\n            CREATE INDEX IF NOT EXISTS idx_profile_user ON profile_changes(username);\n            CREATE INDEX IF NOT EXISTS idx_deletions_user ON tweet_deletions(username);\n            CREATE INDEX IF NOT EXISTS idx_follow_user ON follow_events(username);\n        \"\"\")\n        self.conn.commit()\n\n    def log_profile_change(self, username: str, field: str, old_value, new_value):\n        self.conn.execute(\"\"\"\n            INSERT INTO profile_changes (username, field, old_value, new_value)\n            VALUES (?, ?, ?, ?)\n        \"\"\", (username, field, str(old_value), str(new_value)))\n        self.conn.commit()\n\n    def log_deletion(self, username: str, tweet_id: str, text: str, original_date: str):\n        self.conn.execute(\"\"\"\n            INSERT INTO tweet_deletions (username, tweet_id, tweet_text, original_date)\n            VALUES (?, ?, ?, ?)\n        \"\"\", (username, tweet_id, text, original_date))\n        self.conn.commit()\n\n    def log_follow_event(self, username: str, target: str, action: str):\n        self.conn.execute(\"\"\"\n            INSERT INTO follow_events (username, target_username, action)\n            VALUES (?, ?, ?)\n        \"\"\", (username, target, action))\n        self.conn.commit()\n\n    def get_user_history(self, username: str, days: int = 30) -&gt; dict:\n        \"\"\"Get all changes for a user.\"\"\"\n        from datetime import timedelta\n        since = (datetime.now() - timedelta(days=days)).isoformat()\n\n        profile_changes = self.conn.execute(\"\"\"\n            SELECT field, old_value, new_value, detected_at\n            FROM profile_changes\n            WHERE username = ? AND detected_at &gt;= ?\n            ORDER BY detected_at DESC\n        \"\"\", (username, since)).fetchall()\n\n        deletions = self.conn.execute(\"\"\"\n            SELECT tweet_id, tweet_text, deleted_at\n            FROM tweet_deletions\n            WHERE username = ? AND deleted_at &gt;= ?\n            ORDER BY deleted_at DESC\n        \"\"\", (username, since)).fetchall()\n\n        follow_events = self.conn.execute(\"\"\"\n            SELECT target_username, action, detected_at\n            FROM follow_events\n            WHERE username = ? AND detected_at &gt;= ?\n            ORDER BY detected_at DESC\n        \"\"\", (username, since)).fetchall()\n\n        return {\n            \"profile_changes\": profile_changes,\n            \"deletions\": deletions,\n            \"follow_events\": follow_events\n        }\n\n# Usage\ndb = ChangeHistoryDB()\n\n# Log changes\ndb.log_profile_change(\"elonmusk\", \"bio\", \"Old bio text\", \"New bio text\")\ndb.log_deletion(\"user\", \"123456\", \"Deleted tweet text\", \"2024-01-01\")\ndb.log_follow_event(\"user\", \"target\", \"followed\")\n\n# Get history\nhistory = db.get_user_history(\"elonmusk\", days=7)\nprint(f\"Profile changes: {len(history['profile_changes'])}\")\nprint(f\"Deletions: {len(history['deletions'])}\")\nprint(f\"Follow events: {len(history['follow_events'])}\")\n</code></pre>"},{"location":"guides/monitoring/accounts/#alert-configuration","title":"Alert Configuration","text":""},{"location":"guides/monitoring/accounts/#configurable-alert-system","title":"Configurable Alert System","text":"<pre><code>import asyncio\nfrom dataclasses import dataclass\nfrom typing import Callable, Optional\nfrom xtools import Xtools\nfrom xtools.notifications import DiscordNotifier, TelegramNotifier\n\n@dataclass\nclass AlertConfig:\n    profile_changes: bool = True\n    bio_only: bool = False\n    deletions: bool = True\n    min_deletion_likes: int = 0\n    follow_events: bool = True\n    follower_threshold: Optional[int] = None\n\nclass ConfigurableMonitor:\n    def __init__(\n        self,\n        username: str,\n        config: AlertConfig,\n        discord_webhook: str = None,\n        telegram_token: str = None,\n        telegram_chat: str = None\n    ):\n        self.username = username\n        self.config = config\n        self.notifiers = []\n\n        if discord_webhook:\n            self.notifiers.append(DiscordNotifier(discord_webhook))\n        if telegram_token and telegram_chat:\n            self.notifiers.append(TelegramNotifier(telegram_token, telegram_chat))\n\n    async def send_alert(self, title: str, message: str, priority: str = \"normal\"):\n        \"\"\"Send alert to all configured notifiers.\"\"\"\n        for notifier in self.notifiers:\n            await notifier.send(\n                title=title,\n                message=message,\n                color=0xFF0000 if priority == \"high\" else 0x00FF00\n            )\n\n    async def handle_profile_change(self, changes: list):\n        if not self.config.profile_changes:\n            return\n\n        for change in changes:\n            if self.config.bio_only and change[\"field\"] != \"bio\":\n                continue\n\n            await self.send_alert(\n                title=f\"@{self.username} Profile Change\",\n                message=f\"{change['field']}: {change['new']}\"\n            )\n\n    async def handle_deletion(self, tweet: dict):\n        if not self.config.deletions:\n            return\n\n        if tweet.get(\"likes\", 0) &lt; self.config.min_deletion_likes:\n            return\n\n        await self.send_alert(\n            title=f\"@{self.username} Deleted Tweet\",\n            message=tweet[\"text\"][:200],\n            priority=\"high\" if tweet.get(\"likes\", 0) &gt; 1000 else \"normal\"\n        )\n\n    async def handle_follow_event(self, target: str, action: str):\n        if not self.config.follow_events:\n            return\n\n        await self.send_alert(\n            title=f\"@{self.username} {action.title()}\",\n            message=f\"@{target}\"\n        )\n\n# Usage\nconfig = AlertConfig(\n    profile_changes=True,\n    bio_only=False,\n    deletions=True,\n    min_deletion_likes=100,  # Only alert for popular deletions\n    follow_events=True\n)\n\nmonitor = ConfigurableMonitor(\n    \"target_user\",\n    config,\n    discord_webhook=\"https://discord.com/api/webhooks/...\"\n)\n</code></pre>"},{"location":"guides/monitoring/accounts/#next-steps","title":"Next Steps","text":"<ul> <li>Growth Monitoring - Track follower changes</li> <li>Keyword Monitoring - Monitor mentions and topics</li> <li>Engagement Monitoring - Track interactions</li> </ul>"},{"location":"guides/monitoring/engagement/","title":"Engagement Monitoring","text":"<p>Track likes, replies, and retweets in real-time. Detect viral content and monitor response times.</p>"},{"location":"guides/monitoring/engagement/#overview","title":"Overview","text":"<p>Engagement monitoring helps you:</p> <ul> <li>Track real-time likes and replies on your tweets</li> <li>Detect engagement velocity spikes (potential virality)</li> <li>Identify your most engaging content</li> <li>Monitor response times for customer support</li> <li>Create engagement leaderboards</li> </ul>"},{"location":"guides/monitoring/engagement/#real-time-likereply-tracking","title":"Real-Time Like/Reply Tracking","text":""},{"location":"guides/monitoring/engagement/#basic-engagement-tracker","title":"Basic Engagement Tracker","text":"<pre><code>import asyncio\nfrom datetime import datetime\nfrom xtools import Xtools\n\nclass TweetEngagementTracker:\n    def __init__(self, tweet_id: str):\n        self.tweet_id = tweet_id\n        self.history = []\n        self.last_stats = None\n\n    async def check(self) -&gt; dict:\n        \"\"\"Check current engagement stats.\"\"\"\n        async with Xtools() as x:\n            # Get tweet details\n            tweet = await x.scrape.tweet(self.tweet_id)\n\n            current = {\n                \"timestamp\": datetime.now().isoformat(),\n                \"likes\": tweet.like_count,\n                \"retweets\": tweet.retweet_count,\n                \"replies\": tweet.reply_count,\n                \"quotes\": tweet.quote_count,\n                \"views\": tweet.view_count\n            }\n\n            changes = {}\n            if self.last_stats:\n                changes = {\n                    \"likes_delta\": current[\"likes\"] - self.last_stats[\"likes\"],\n                    \"retweets_delta\": current[\"retweets\"] - self.last_stats[\"retweets\"],\n                    \"replies_delta\": current[\"replies\"] - self.last_stats[\"replies\"]\n                }\n\n            self.history.append(current)\n            self.last_stats = current\n\n            return {\n                \"current\": current,\n                \"changes\": changes\n            }\n\n    async def monitor(self, interval_seconds: int = 60, duration_minutes: int = None):\n        \"\"\"Monitor engagement over time.\"\"\"\n        print(f\"\ud83d\udcca Tracking tweet: {self.tweet_id}\")\n\n        start_time = datetime.now()\n\n        while True:\n            result = await self.check()\n\n            c = result[\"current\"]\n            d = result[\"changes\"]\n\n            print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}]\")\n            print(f\"   \u2764\ufe0f {c['likes']:,} (+{d.get('likes_delta', 0)})\")\n            print(f\"   \ud83d\udd01 {c['retweets']:,} (+{d.get('retweets_delta', 0)})\")\n            print(f\"   \ud83d\udcac {c['replies']:,} (+{d.get('replies_delta', 0)})\")\n\n            if c.get('views'):\n                print(f\"   \ud83d\udc41\ufe0f {c['views']:,} views\")\n\n            # Check duration\n            if duration_minutes:\n                elapsed = (datetime.now() - start_time).total_seconds() / 60\n                if elapsed &gt;= duration_minutes:\n                    break\n\n            await asyncio.sleep(interval_seconds)\n\n        return self.history\n\n# Usage\ntracker = TweetEngagementTracker(\"1234567890123456789\")\nasyncio.run(tracker.monitor(interval_seconds=30, duration_minutes=60))\n</code></pre>"},{"location":"guides/monitoring/engagement/#multi-tweet-engagement-monitor","title":"Multi-Tweet Engagement Monitor","text":"<pre><code>import asyncio\nfrom datetime import datetime\nfrom xtools import Xtools\n\nclass MultiTweetMonitor:\n    def __init__(self, tweet_ids: list):\n        self.tweet_ids = tweet_ids\n        self.stats = {tid: [] for tid in tweet_ids}\n\n    async def check_all(self) -&gt; dict:\n        \"\"\"Check engagement for all tweets.\"\"\"\n        async with Xtools() as x:\n            results = {}\n\n            for tweet_id in self.tweet_ids:\n                tweet = await x.scrape.tweet(tweet_id)\n\n                stats = {\n                    \"timestamp\": datetime.now().isoformat(),\n                    \"likes\": tweet.like_count,\n                    \"retweets\": tweet.retweet_count,\n                    \"replies\": tweet.reply_count,\n                    \"text\": tweet.text[:50]\n                }\n\n                # Calculate change from last check\n                if self.stats[tweet_id]:\n                    last = self.stats[tweet_id][-1]\n                    stats[\"likes_delta\"] = stats[\"likes\"] - last[\"likes\"]\n                    stats[\"retweets_delta\"] = stats[\"retweets\"] - last[\"retweets\"]\n                else:\n                    stats[\"likes_delta\"] = 0\n                    stats[\"retweets_delta\"] = 0\n\n                self.stats[tweet_id].append(stats)\n                results[tweet_id] = stats\n\n                await asyncio.sleep(1)  # Rate limiting\n\n            return results\n\n    def get_top_performers(self) -&gt; list:\n        \"\"\"Get tweets ranked by engagement.\"\"\"\n        rankings = []\n\n        for tweet_id, history in self.stats.items():\n            if history:\n                latest = history[-1]\n                total = latest[\"likes\"] + latest[\"retweets\"] * 2\n                rankings.append({\n                    \"id\": tweet_id,\n                    \"text\": latest[\"text\"],\n                    \"engagement\": total,\n                    \"likes\": latest[\"likes\"],\n                    \"retweets\": latest[\"retweets\"]\n                })\n\n        return sorted(rankings, key=lambda x: x[\"engagement\"], reverse=True)\n\n# Usage\nasync def main():\n    tweets = [\n        \"1234567890123456789\",\n        \"1234567890123456790\",\n        \"1234567890123456791\"\n    ]\n\n    monitor = MultiTweetMonitor(tweets)\n\n    for _ in range(10):  # 10 checks\n        results = await monitor.check_all()\n\n        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Engagement Update:\")\n        for tweet_id, stats in results.items():\n            print(f\"   {stats['text'][:30]}... \"\n                  f\"\u2764\ufe0f {stats['likes']} (+{stats['likes_delta']})\")\n\n        await asyncio.sleep(60)\n\n    # Final rankings\n    print(\"\\n\ud83c\udfc6 Top Performers:\")\n    for i, t in enumerate(monitor.get_top_performers(), 1):\n        print(f\"   {i}. {t['text']}... - {t['engagement']} engagement\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/engagement/#engagement-velocity-alerts","title":"Engagement Velocity Alerts","text":""},{"location":"guides/monitoring/engagement/#velocity-detection","title":"Velocity Detection","text":"<pre><code>import asyncio\nfrom datetime import datetime, timedelta\nfrom collections import deque\nfrom xtools import Xtools\n\nclass VelocityMonitor:\n    def __init__(self, tweet_id: str, alert_threshold: float = 10.0):\n        self.tweet_id = tweet_id\n        self.alert_threshold = alert_threshold  # Likes per minute\n        self.history = deque(maxlen=60)  # Keep 60 data points\n        self.last_likes = 0\n\n    def calculate_velocity(self) -&gt; float:\n        \"\"\"Calculate likes per minute over recent history.\"\"\"\n        if len(self.history) &lt; 2:\n            return 0.0\n\n        recent = list(self.history)[-5:]  # Last 5 readings\n\n        if len(recent) &lt; 2:\n            return 0.0\n\n        total_change = recent[-1][\"likes\"] - recent[0][\"likes\"]\n        time_diff = (\n            datetime.fromisoformat(recent[-1][\"timestamp\"]) -\n            datetime.fromisoformat(recent[0][\"timestamp\"])\n        ).total_seconds() / 60\n\n        if time_diff == 0:\n            return 0.0\n\n        return total_change / time_diff\n\n    async def check(self) -&gt; dict:\n        \"\"\"Check engagement and velocity.\"\"\"\n        async with Xtools() as x:\n            tweet = await x.scrape.tweet(self.tweet_id)\n\n            data = {\n                \"timestamp\": datetime.now().isoformat(),\n                \"likes\": tweet.like_count,\n                \"retweets\": tweet.retweet_count\n            }\n\n            self.history.append(data)\n\n            velocity = self.calculate_velocity()\n\n            result = {\n                \"current\": data,\n                \"velocity\": velocity,\n                \"alert\": velocity &gt;= self.alert_threshold\n            }\n\n            if result[\"alert\"]:\n                result[\"message\"] = (\n                    f\"\ud83d\ude80 VELOCITY ALERT! Tweet gaining \"\n                    f\"{velocity:.1f} likes/minute\"\n                )\n\n            self.last_likes = data[\"likes\"]\n\n            return result\n\n    async def monitor(self, interval_seconds: int = 30):\n        \"\"\"Monitor velocity continuously.\"\"\"\n        print(f\"\ud83d\udcc8 Monitoring velocity for tweet {self.tweet_id}\")\n        print(f\"   Alert threshold: {self.alert_threshold} likes/min\")\n\n        while True:\n            result = await self.check()\n\n            velocity_bar = \"\u2588\" * min(int(result[\"velocity\"]), 20)\n            print(f\"\\r   Velocity: {result['velocity']:5.1f}/min [{velocity_bar:&lt;20}]\", end=\"\")\n\n            if result[\"alert\"]:\n                print(f\"\\n{result['message']}\")\n                # Send notification here\n\n            await asyncio.sleep(interval_seconds)\n\n# Usage\nmonitor = VelocityMonitor(\"1234567890123456789\", alert_threshold=5.0)\nasyncio.run(monitor.monitor())\n</code></pre>"},{"location":"guides/monitoring/engagement/#viral-content-detection","title":"Viral Content Detection","text":""},{"location":"guides/monitoring/engagement/#viral-detector","title":"Viral Detector","text":"<pre><code>import asyncio\nfrom datetime import datetime, timedelta\nfrom xtools import Xtools\nfrom xtools.notifications import DiscordNotifier\n\nclass ViralDetector:\n    def __init__(\n        self,\n        username: str,\n        viral_threshold_likes: int = 1000,\n        viral_threshold_velocity: float = 50.0,  # likes/min\n        discord_webhook: str = None\n    ):\n        self.username = username\n        self.threshold_likes = viral_threshold_likes\n        self.threshold_velocity = viral_threshold_velocity\n        self.tracked_tweets = {}\n        self.notifier = DiscordNotifier(discord_webhook) if discord_webhook else None\n        self.alerted_tweets = set()\n\n    async def check_user_tweets(self) -&gt; list:\n        \"\"\"Check recent tweets for viral potential.\"\"\"\n        async with Xtools() as x:\n            result = await x.scrape.tweets(self.username, limit=20)\n            viral_tweets = []\n\n            for tweet in result.tweets:\n                # Skip if already alerted\n                if tweet.id in self.alerted_tweets:\n                    continue\n\n                # Calculate age in minutes\n                if tweet.created_at:\n                    age_minutes = (datetime.now() - tweet.created_at).total_seconds() / 60\n                else:\n                    age_minutes = 60  # Default\n\n                # Calculate velocity\n                velocity = tweet.like_count / max(age_minutes, 1)\n\n                # Check viral conditions\n                is_viral = (\n                    tweet.like_count &gt;= self.threshold_likes or\n                    velocity &gt;= self.threshold_velocity\n                )\n\n                if is_viral:\n                    viral_tweets.append({\n                        \"tweet\": tweet,\n                        \"velocity\": velocity,\n                        \"age_minutes\": age_minutes\n                    })\n\n                # Update tracking\n                if tweet.id not in self.tracked_tweets:\n                    self.tracked_tweets[tweet.id] = {\n                        \"first_seen\": datetime.now(),\n                        \"initial_likes\": tweet.like_count\n                    }\n\n            return viral_tweets\n\n    async def alert_viral(self, viral_tweets: list):\n        \"\"\"Send alerts for viral content.\"\"\"\n        for item in viral_tweets:\n            tweet = item[\"tweet\"]\n\n            if tweet.id in self.alerted_tweets:\n                continue\n\n            self.alerted_tweets.add(tweet.id)\n\n            message = (\n                f\"\ud83d\udd25 **VIRAL TWEET DETECTED**\\n\\n\"\n                f\"@{self.username}\\n\"\n                f\"{tweet.text[:200]}...\\n\\n\"\n                f\"\u2764\ufe0f {tweet.like_count:,} likes\\n\"\n                f\"\ud83d\udd01 {tweet.retweet_count:,} retweets\\n\"\n                f\"\ud83d\udcc8 {item['velocity']:.1f} likes/minute\\n\"\n                f\"\u23f1\ufe0f Posted {item['age_minutes']:.0f} minutes ago\"\n            )\n\n            print(message)\n\n            if self.notifier:\n                await self.notifier.send(\n                    title=\"\ud83d\udd25 Viral Tweet!\",\n                    message=message,\n                    color=0xFF6600\n                )\n\n    async def monitor(self, interval_minutes: int = 5):\n        \"\"\"Monitor for viral content.\"\"\"\n        print(f\"\ud83d\udd0d Viral Monitor: @{self.username}\")\n        print(f\"   Thresholds: {self.threshold_likes} likes or \"\n              f\"{self.threshold_velocity} likes/min\")\n\n        while True:\n            viral_tweets = await self.check_user_tweets()\n\n            if viral_tweets:\n                await self.alert_viral(viral_tweets)\n\n            await asyncio.sleep(interval_minutes * 60)\n\n# Usage\ndetector = ViralDetector(\n    \"your_username\",\n    viral_threshold_likes=500,\n    viral_threshold_velocity=20.0,\n    discord_webhook=\"https://discord.com/api/webhooks/...\"\n)\n\nasyncio.run(detector.monitor())\n</code></pre>"},{"location":"guides/monitoring/engagement/#response-time-tracking","title":"Response Time Tracking","text":""},{"location":"guides/monitoring/engagement/#support-response-monitor","title":"Support Response Monitor","text":"<pre><code>import asyncio\nfrom datetime import datetime\nfrom statistics import mean, median\nfrom xtools import Xtools\n\nclass ResponseTimeTracker:\n    def __init__(self, username: str):\n        self.username = username\n        self.response_times = []\n        self.unanswered = []\n\n    async def analyze_responses(self, hours: int = 24) -&gt; dict:\n        \"\"\"Analyze response times to mentions.\"\"\"\n        async with Xtools() as x:\n            # Get mentions\n            mentions = await x.scrape.mentions(self.username, limit=100)\n\n            # Get user's replies\n            user_tweets = await x.scrape.tweets(self.username, limit=200)\n            replies = [t for t in user_tweets.tweets if t.in_reply_to_user_id]\n\n            reply_map = {}\n            for reply in replies:\n                if reply.in_reply_to_status_id:\n                    reply_map[reply.in_reply_to_status_id] = reply\n\n            for mention in mentions.tweets:\n                # Check if we replied\n                if mention.id in reply_map:\n                    reply = reply_map[mention.id]\n\n                    if mention.created_at and reply.created_at:\n                        response_time = (reply.created_at - mention.created_at).total_seconds()\n\n                        self.response_times.append({\n                            \"mention_id\": mention.id,\n                            \"mention_author\": mention.author.username,\n                            \"response_seconds\": response_time,\n                            \"mention_time\": mention.created_at.isoformat()\n                        })\n                else:\n                    self.unanswered.append({\n                        \"id\": mention.id,\n                        \"author\": mention.author.username,\n                        \"text\": mention.text,\n                        \"time\": mention.created_at.isoformat() if mention.created_at else None\n                    })\n\n            # Calculate stats\n            times = [r[\"response_seconds\"] for r in self.response_times]\n\n            return {\n                \"total_mentions\": len(mentions.tweets),\n                \"answered\": len(self.response_times),\n                \"unanswered\": len(self.unanswered),\n                \"response_rate\": len(self.response_times) / max(len(mentions.tweets), 1) * 100,\n                \"avg_response_minutes\": mean(times) / 60 if times else 0,\n                \"median_response_minutes\": median(times) / 60 if times else 0,\n                \"fastest_minutes\": min(times) / 60 if times else 0,\n                \"slowest_minutes\": max(times) / 60 if times else 0\n            }\n\n    def get_unanswered(self) -&gt; list:\n        \"\"\"Get list of unanswered mentions.\"\"\"\n        return self.unanswered\n\n# Usage\nasync def main():\n    tracker = ResponseTimeTracker(\"your_brand_account\")\n\n    stats = await tracker.analyze_responses(hours=24)\n\n    print(f\"\ud83d\udcca Response Time Analysis\")\n    print(f\"   Total mentions: {stats['total_mentions']}\")\n    print(f\"   Answered: {stats['answered']} ({stats['response_rate']:.1f}%)\")\n    print(f\"   Avg response: {stats['avg_response_minutes']:.1f} minutes\")\n    print(f\"   Median: {stats['median_response_minutes']:.1f} minutes\")\n    print(f\"   Fastest: {stats['fastest_minutes']:.1f} minutes\")\n    print(f\"   Slowest: {stats['slowest_minutes']:.1f} minutes\")\n\n    unanswered = tracker.get_unanswered()\n    if unanswered:\n        print(f\"\\n\u26a0\ufe0f {len(unanswered)} unanswered mentions:\")\n        for m in unanswered[:5]:\n            print(f\"   @{m['author']}: {m['text'][:50]}...\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/engagement/#engagement-leaderboards","title":"Engagement Leaderboards","text":""},{"location":"guides/monitoring/engagement/#daily-leaderboard","title":"Daily Leaderboard","text":"<pre><code>import asyncio\nfrom datetime import datetime, timedelta\nfrom xtools import Xtools\n\nclass EngagementLeaderboard:\n    def __init__(self, username: str):\n        self.username = username\n\n    async def get_top_tweets(self, days: int = 7, limit: int = 10) -&gt; list:\n        \"\"\"Get top performing tweets from recent period.\"\"\"\n        async with Xtools() as x:\n            result = await x.scrape.tweets(self.username, limit=100)\n\n            # Filter by date\n            cutoff = datetime.now() - timedelta(days=days)\n            recent_tweets = [\n                t for t in result.tweets\n                if t.created_at and t.created_at &gt;= cutoff\n            ]\n\n            # Calculate engagement score\n            scored = []\n            for tweet in recent_tweets:\n                score = (\n                    tweet.like_count +\n                    tweet.retweet_count * 2 +\n                    tweet.reply_count * 3 +\n                    tweet.quote_count * 2\n                )\n\n                scored.append({\n                    \"id\": tweet.id,\n                    \"text\": tweet.text[:100],\n                    \"score\": score,\n                    \"likes\": tweet.like_count,\n                    \"retweets\": tweet.retweet_count,\n                    \"replies\": tweet.reply_count,\n                    \"created\": tweet.created_at.isoformat() if tweet.created_at else None,\n                    \"url\": f\"https://x.com/{self.username}/status/{tweet.id}\"\n                })\n\n            # Sort by score\n            scored.sort(key=lambda x: x[\"score\"], reverse=True)\n\n            return scored[:limit]\n\n    async def get_top_engagers(self, limit: int = 20) -&gt; list:\n        \"\"\"Get users who engage most with your content.\"\"\"\n        async with Xtools() as x:\n            # Get recent tweets\n            tweets = await x.scrape.tweets(self.username, limit=50)\n\n            engager_counts = {}\n\n            for tweet in tweets.tweets:\n                # Get replies\n                replies = await x.scrape.replies(\n                    f\"https://x.com/{self.username}/status/{tweet.id}\",\n                    limit=50\n                )\n\n                for reply in replies.tweets:\n                    author = reply.author.username\n                    if author not in engager_counts:\n                        engager_counts[author] = {\n                            \"username\": author,\n                            \"replies\": 0,\n                            \"profile\": reply.author\n                        }\n                    engager_counts[author][\"replies\"] += 1\n\n                await asyncio.sleep(1)  # Rate limiting\n\n            # Sort by engagement\n            sorted_engagers = sorted(\n                engager_counts.values(),\n                key=lambda x: x[\"replies\"],\n                reverse=True\n            )\n\n            return sorted_engagers[:limit]\n\n# Usage\nasync def main():\n    leaderboard = EngagementLeaderboard(\"your_username\")\n\n    # Top tweets\n    print(\"\ud83c\udfc6 TOP TWEETS (Last 7 Days)\\n\")\n    top_tweets = await leaderboard.get_top_tweets(days=7, limit=5)\n\n    for i, tweet in enumerate(top_tweets, 1):\n        print(f\"{i}. Score: {tweet['score']:,}\")\n        print(f\"   {tweet['text']}...\")\n        print(f\"   \u2764\ufe0f {tweet['likes']} | \ud83d\udd01 {tweet['retweets']} | \ud83d\udcac {tweet['replies']}\")\n        print()\n\n    # Top engagers\n    print(\"\\n\ud83d\udc65 TOP ENGAGERS\\n\")\n    top_engagers = await leaderboard.get_top_engagers(limit=10)\n\n    for i, engager in enumerate(top_engagers, 1):\n        print(f\"{i}. @{engager['username']} - {engager['replies']} replies\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/engagement/#weekly-report-generator","title":"Weekly Report Generator","text":"<pre><code>import asyncio\nfrom datetime import datetime, timedelta\nfrom xtools import Xtools\n\nclass WeeklyEngagementReport:\n    def __init__(self, username: str):\n        self.username = username\n\n    async def generate(self) -&gt; dict:\n        \"\"\"Generate weekly engagement report.\"\"\"\n        async with Xtools() as x:\n            # Get profile\n            profile = await x.scrape.profile(self.username)\n\n            # Get tweets from last week\n            tweets = await x.scrape.tweets(self.username, limit=200)\n\n            cutoff = datetime.now() - timedelta(days=7)\n            weekly_tweets = [\n                t for t in tweets.tweets\n                if t.created_at and t.created_at &gt;= cutoff\n            ]\n\n            # Calculate totals\n            total_likes = sum(t.like_count for t in weekly_tweets)\n            total_retweets = sum(t.retweet_count for t in weekly_tweets)\n            total_replies = sum(t.reply_count for t in weekly_tweets)\n\n            # Find best performer\n            best = max(weekly_tweets, key=lambda t: t.like_count, default=None)\n\n            report = {\n                \"period\": {\n                    \"start\": cutoff.isoformat(),\n                    \"end\": datetime.now().isoformat()\n                },\n                \"profile\": {\n                    \"followers\": profile.followers_count,\n                    \"following\": profile.following_count\n                },\n                \"activity\": {\n                    \"tweets_posted\": len(weekly_tweets),\n                    \"avg_per_day\": len(weekly_tweets) / 7\n                },\n                \"engagement\": {\n                    \"total_likes\": total_likes,\n                    \"total_retweets\": total_retweets,\n                    \"total_replies\": total_replies,\n                    \"avg_likes_per_tweet\": total_likes / max(len(weekly_tweets), 1),\n                    \"avg_retweets_per_tweet\": total_retweets / max(len(weekly_tweets), 1)\n                },\n                \"best_tweet\": {\n                    \"id\": best.id if best else None,\n                    \"text\": best.text[:100] if best else None,\n                    \"likes\": best.like_count if best else 0\n                }\n            }\n\n            return report\n\n    def format_report(self, report: dict) -&gt; str:\n        \"\"\"Format report as readable text.\"\"\"\n        return f\"\"\"\n\ud83d\udcca WEEKLY ENGAGEMENT REPORT\n{'='*40}\n\ud83d\udcc5 {report['period']['start'][:10]} to {report['period']['end'][:10]}\n\n\ud83d\udc64 Profile Stats\n   Followers: {report['profile']['followers']:,}\n   Following: {report['profile']['following']:,}\n\n\u270d\ufe0f Activity\n   Tweets Posted: {report['activity']['tweets_posted']}\n   Avg Per Day: {report['activity']['avg_per_day']:.1f}\n\n\u2764\ufe0f Engagement\n   Total Likes: {report['engagement']['total_likes']:,}\n   Total Retweets: {report['engagement']['total_retweets']:,}\n   Total Replies: {report['engagement']['total_replies']:,}\n   Avg Likes/Tweet: {report['engagement']['avg_likes_per_tweet']:.1f}\n\n\ud83c\udfc6 Best Tweet\n   {report['best_tweet']['text']}...\n   \u2764\ufe0f {report['best_tweet']['likes']:,} likes\n\"\"\"\n\n# Usage\nasync def main():\n    reporter = WeeklyEngagementReport(\"your_username\")\n\n    report = await reporter.generate()\n    formatted = reporter.format_report(report)\n\n    print(formatted)\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/engagement/#next-steps","title":"Next Steps","text":"<ul> <li>Growth Monitoring - Track follower changes</li> <li>Account Monitoring - Track profile changes</li> <li>Keyword Monitoring - Monitor mentions</li> <li>Analytics Guide - Deep data analysis</li> </ul>"},{"location":"guides/monitoring/growth/","title":"Growth Monitoring","text":"<p>Track your account growth, detect follower changes, and set up alerts for milestones and anomalies.</p>"},{"location":"guides/monitoring/growth/#overview","title":"Overview","text":"<p>Growth monitoring helps you:</p> <ul> <li>Track daily follower/following counts</li> <li>Detect sudden changes (gains or drops)</li> <li>Celebrate milestones automatically</li> <li>Identify growth patterns and trends</li> <li>Store historical data for analysis</li> </ul>"},{"location":"guides/monitoring/growth/#setting-up-daily-tracking","title":"Setting Up Daily Tracking","text":""},{"location":"guides/monitoring/growth/#basic-growth-tracker","title":"Basic Growth Tracker","text":"<pre><code>import asyncio\nfrom datetime import datetime\nfrom xtools import Xtools\n\nasync def track_daily_growth(username: str):\n    \"\"\"Record daily growth metrics.\"\"\"\n    async with Xtools() as x:\n        profile = await x.scrape.profile(username)\n\n        metrics = {\n            \"date\": datetime.now().isoformat(),\n            \"username\": username,\n            \"followers\": profile.followers_count,\n            \"following\": profile.following_count,\n            \"tweets\": profile.tweet_count,\n            \"likes\": profile.likes_count\n        }\n\n        print(f\"\ud83d\udcca {username} Stats for {metrics['date'][:10]}\")\n        print(f\"   Followers: {metrics['followers']:,}\")\n        print(f\"   Following: {metrics['following']:,}\")\n        print(f\"   Tweets: {metrics['tweets']:,}\")\n\n        return metrics\n\n# Run daily\nasyncio.run(track_daily_growth(\"your_username\"))\n</code></pre>"},{"location":"guides/monitoring/growth/#automated-daily-tracking-with-storage","title":"Automated Daily Tracking with Storage","text":"<pre><code>import asyncio\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom xtools import Xtools\n\nclass GrowthTracker:\n    def __init__(self, username: str, data_file: str = \"growth_data.json\"):\n        self.username = username\n        self.data_file = Path(data_file)\n        self.history = self._load_history()\n\n    def _load_history(self) -&gt; list:\n        \"\"\"Load historical data from file.\"\"\"\n        if self.data_file.exists():\n            return json.loads(self.data_file.read_text())\n        return []\n\n    def _save_history(self):\n        \"\"\"Save history to file.\"\"\"\n        self.data_file.write_text(json.dumps(self.history, indent=2))\n\n    async def record_snapshot(self) -&gt; dict:\n        \"\"\"Take a growth snapshot.\"\"\"\n        async with Xtools() as x:\n            profile = await x.scrape.profile(self.username)\n\n            snapshot = {\n                \"timestamp\": datetime.now().isoformat(),\n                \"followers\": profile.followers_count,\n                \"following\": profile.following_count,\n                \"tweets\": profile.tweet_count,\n                \"likes\": profile.likes_count,\n                \"listed\": profile.listed_count\n            }\n\n            self.history.append(snapshot)\n            self._save_history()\n\n            return snapshot\n\n    def get_growth_rate(self, days: int = 7) -&gt; dict:\n        \"\"\"Calculate growth rate over period.\"\"\"\n        if len(self.history) &lt; 2:\n            return {\"error\": \"Not enough data\"}\n\n        # Get snapshots from period\n        recent = self.history[-1]\n\n        # Find snapshot from N days ago\n        target_date = datetime.now().timestamp() - (days * 86400)\n        old_snapshot = None\n\n        for snap in self.history:\n            snap_time = datetime.fromisoformat(snap[\"timestamp\"]).timestamp()\n            if snap_time &gt;= target_date:\n                old_snapshot = snap\n                break\n\n        if not old_snapshot:\n            old_snapshot = self.history[0]\n\n        return {\n            \"period_days\": days,\n            \"follower_change\": recent[\"followers\"] - old_snapshot[\"followers\"],\n            \"following_change\": recent[\"following\"] - old_snapshot[\"following\"],\n            \"tweet_change\": recent[\"tweets\"] - old_snapshot[\"tweets\"],\n            \"daily_avg_followers\": (recent[\"followers\"] - old_snapshot[\"followers\"]) / days\n        }\n\n# Usage\nasync def main():\n    tracker = GrowthTracker(\"elonmusk\")\n\n    # Record today's snapshot\n    snapshot = await tracker.record_snapshot()\n    print(f\"Today: {snapshot['followers']:,} followers\")\n\n    # Get weekly growth\n    growth = tracker.get_growth_rate(days=7)\n    print(f\"Weekly change: {growth['follower_change']:+,} followers\")\n    print(f\"Daily average: {growth['daily_avg_followers']:+.1f}/day\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/growth/#follower-change-detection","title":"Follower Change Detection","text":""},{"location":"guides/monitoring/growth/#real-time-change-detection","title":"Real-Time Change Detection","text":"<pre><code>import asyncio\nfrom xtools import Xtools\n\nclass FollowerChangeDetector:\n    def __init__(self, username: str):\n        self.username = username\n        self.last_followers = set()\n        self.last_count = 0\n\n    async def check_changes(self) -&gt; dict:\n        \"\"\"Detect follower changes since last check.\"\"\"\n        async with Xtools() as x:\n            # Get current followers\n            result = await x.scrape.followers(self.username, limit=5000)\n            current_followers = {u.username for u in result.users}\n            current_count = len(current_followers)\n\n            changes = {\n                \"new_followers\": [],\n                \"unfollowers\": [],\n                \"net_change\": 0\n            }\n\n            if self.last_followers:\n                # Find new followers\n                new = current_followers - self.last_followers\n                changes[\"new_followers\"] = list(new)\n\n                # Find unfollowers\n                lost = self.last_followers - current_followers\n                changes[\"unfollowers\"] = list(lost)\n\n                # Net change\n                changes[\"net_change\"] = current_count - self.last_count\n\n            # Update state\n            self.last_followers = current_followers\n            self.last_count = current_count\n\n            return changes\n\n    async def monitor(self, interval_minutes: int = 60):\n        \"\"\"Continuously monitor for changes.\"\"\"\n        print(f\"\ud83d\udd0d Monitoring @{self.username} for follower changes...\")\n\n        # Initial load\n        await self.check_changes()\n        print(f\"   Loaded {self.last_count:,} followers\")\n\n        while True:\n            await asyncio.sleep(interval_minutes * 60)\n\n            changes = await self.check_changes()\n\n            if changes[\"new_followers\"]:\n                print(f\"\\n\u2705 New followers ({len(changes['new_followers'])}):\")\n                for user in changes[\"new_followers\"][:10]:\n                    print(f\"   @{user}\")\n\n            if changes[\"unfollowers\"]:\n                print(f\"\\n\u274c Unfollowers ({len(changes['unfollowers'])}):\")\n                for user in changes[\"unfollowers\"][:10]:\n                    print(f\"   @{user}\")\n\n            if changes[\"net_change\"]:\n                emoji = \"\ud83d\udcc8\" if changes[\"net_change\"] &gt; 0 else \"\ud83d\udcc9\"\n                print(f\"\\n{emoji} Net change: {changes['net_change']:+,}\")\n\n# Usage\nasync def main():\n    detector = FollowerChangeDetector(\"your_username\")\n    await detector.monitor(interval_minutes=30)\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/growth/#growth-alerts","title":"Growth Alerts","text":""},{"location":"guides/monitoring/growth/#milestone-alerts","title":"Milestone Alerts","text":"<pre><code>import asyncio\nfrom xtools import Xtools\nfrom xtools.notifications import DiscordNotifier\n\nMILESTONES = [100, 500, 1000, 2500, 5000, 10000, 25000, 50000, 100000]\n\nclass MilestoneTracker:\n    def __init__(self, username: str, webhook_url: str = None):\n        self.username = username\n        self.last_count = 0\n        self.achieved_milestones = set()\n        self.notifier = DiscordNotifier(webhook_url) if webhook_url else None\n\n    def _check_milestones(self, count: int) -&gt; list:\n        \"\"\"Check if any milestones were crossed.\"\"\"\n        crossed = []\n        for milestone in MILESTONES:\n            if self.last_count &lt; milestone &lt;= count:\n                if milestone not in self.achieved_milestones:\n                    crossed.append(milestone)\n                    self.achieved_milestones.add(milestone)\n        return crossed\n\n    async def check_and_alert(self):\n        \"\"\"Check for milestone achievements.\"\"\"\n        async with Xtools() as x:\n            profile = await x.scrape.profile(self.username)\n            count = profile.followers_count\n\n            crossed = self._check_milestones(count)\n            self.last_count = count\n\n            for milestone in crossed:\n                message = f\"\ud83c\udf89 @{self.username} hit {milestone:,} followers!\"\n                print(message)\n\n                if self.notifier:\n                    await self.notifier.send(\n                        title=\"Milestone Achieved!\",\n                        message=message,\n                        color=0x00FF00\n                    )\n\n            return crossed\n\n# Usage\nasync def main():\n    tracker = MilestoneTracker(\n        \"your_username\",\n        webhook_url=\"https://discord.com/api/webhooks/...\"\n    )\n\n    while True:\n        await tracker.check_and_alert()\n        await asyncio.sleep(3600)  # Check hourly\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/growth/#drop-alerts","title":"Drop Alerts","text":"<pre><code>import asyncio\nfrom datetime import datetime\nfrom xtools import Xtools\n\nclass DropAlertMonitor:\n    def __init__(self, username: str, drop_threshold_percent: float = 5.0):\n        self.username = username\n        self.threshold = drop_threshold_percent\n        self.baseline = None\n        self.history = []\n\n    async def check_for_drops(self) -&gt; dict:\n        \"\"\"Detect unusual follower drops.\"\"\"\n        async with Xtools() as x:\n            profile = await x.scrape.profile(self.username)\n            current = profile.followers_count\n\n            result = {\n                \"current\": current,\n                \"alert\": False,\n                \"drop_percent\": 0,\n                \"message\": None\n            }\n\n            if self.baseline is None:\n                self.baseline = current\n                return result\n\n            # Calculate drop percentage\n            if self.baseline &gt; 0:\n                drop_percent = ((self.baseline - current) / self.baseline) * 100\n\n                if drop_percent &gt;= self.threshold:\n                    result[\"alert\"] = True\n                    result[\"drop_percent\"] = drop_percent\n                    result[\"message\"] = (\n                        f\"\u26a0\ufe0f ALERT: @{self.username} lost \"\n                        f\"{self.baseline - current:,} followers \"\n                        f\"({drop_percent:.1f}% drop)\"\n                    )\n\n            # Update baseline (rolling average)\n            self.history.append(current)\n            if len(self.history) &gt; 24:  # Keep last 24 readings\n                self.history.pop(0)\n            self.baseline = sum(self.history) / len(self.history)\n\n            return result\n\n# Usage\nasync def main():\n    monitor = DropAlertMonitor(\"your_username\", drop_threshold_percent=3.0)\n\n    while True:\n        result = await monitor.check_for_drops()\n\n        if result[\"alert\"]:\n            print(result[\"message\"])\n            # Send notification, email, etc.\n\n        await asyncio.sleep(1800)  # Check every 30 minutes\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/growth/#historical-tracking-database","title":"Historical Tracking Database","text":""},{"location":"guides/monitoring/growth/#sqlite-based-tracking","title":"SQLite-Based Tracking","text":"<pre><code>import asyncio\nimport sqlite3\nfrom datetime import datetime, timedelta\nfrom xtools import Xtools\n\nclass GrowthDatabase:\n    def __init__(self, db_path: str = \"growth.db\"):\n        self.conn = sqlite3.connect(db_path)\n        self._init_db()\n\n    def _init_db(self):\n        \"\"\"Initialize database schema.\"\"\"\n        self.conn.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS growth_metrics (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                username TEXT NOT NULL,\n                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,\n                followers INTEGER,\n                following INTEGER,\n                tweets INTEGER,\n                likes INTEGER,\n                listed INTEGER\n            )\n        \"\"\")\n\n        self.conn.execute(\"\"\"\n            CREATE INDEX IF NOT EXISTS idx_username_time \n            ON growth_metrics(username, timestamp)\n        \"\"\")\n\n        self.conn.commit()\n\n    async def record(self, username: str):\n        \"\"\"Record current metrics for username.\"\"\"\n        async with Xtools() as x:\n            profile = await x.scrape.profile(username)\n\n            self.conn.execute(\"\"\"\n                INSERT INTO growth_metrics \n                (username, followers, following, tweets, likes, listed)\n                VALUES (?, ?, ?, ?, ?, ?)\n            \"\"\", (\n                username,\n                profile.followers_count,\n                profile.following_count,\n                profile.tweet_count,\n                profile.likes_count,\n                profile.listed_count\n            ))\n\n            self.conn.commit()\n\n    def get_history(self, username: str, days: int = 30) -&gt; list:\n        \"\"\"Get historical data for username.\"\"\"\n        since = datetime.now() - timedelta(days=days)\n\n        cursor = self.conn.execute(\"\"\"\n            SELECT timestamp, followers, following, tweets\n            FROM growth_metrics\n            WHERE username = ? AND timestamp &gt;= ?\n            ORDER BY timestamp ASC\n        \"\"\", (username, since.isoformat()))\n\n        return cursor.fetchall()\n\n    def get_growth_summary(self, username: str, days: int = 7) -&gt; dict:\n        \"\"\"Get growth summary for period.\"\"\"\n        history = self.get_history(username, days)\n\n        if len(history) &lt; 2:\n            return {\"error\": \"Insufficient data\"}\n\n        first = history[0]\n        last = history[-1]\n\n        return {\n            \"period_days\": days,\n            \"start_followers\": first[1],\n            \"end_followers\": last[1],\n            \"change\": last[1] - first[1],\n            \"percent_change\": ((last[1] - first[1]) / first[1]) * 100 if first[1] else 0,\n            \"data_points\": len(history)\n        }\n\n# Usage\nasync def main():\n    db = GrowthDatabase()\n\n    # Record metrics for multiple accounts\n    accounts = [\"account1\", \"account2\", \"account3\"]\n\n    for account in accounts:\n        await db.record(account)\n        print(f\"Recorded metrics for @{account}\")\n\n    # Get growth summary\n    summary = db.get_growth_summary(\"account1\", days=7)\n    print(f\"\\n7-day Growth Summary:\")\n    print(f\"  Change: {summary['change']:+,} followers\")\n    print(f\"  Percent: {summary['percent_change']:+.2f}%\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/growth/#growth-dashboards","title":"Growth Dashboards","text":""},{"location":"guides/monitoring/growth/#terminal-dashboard","title":"Terminal Dashboard","text":"<pre><code>import asyncio\nfrom datetime import datetime\nfrom xtools import Xtools\n\nasync def display_dashboard(usernames: list):\n    \"\"\"Display growth dashboard for multiple accounts.\"\"\"\n    async with Xtools() as x:\n        print(\"\\n\" + \"=\" * 60)\n        print(f\"\ud83d\udcca GROWTH DASHBOARD - {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n        print(\"=\" * 60)\n\n        for username in usernames:\n            profile = await x.scrape.profile(username)\n\n            print(f\"\\n@{username}\")\n            print(f\"  {'Followers:':&lt;12} {profile.followers_count:&gt;10,}\")\n            print(f\"  {'Following:':&lt;12} {profile.following_count:&gt;10,}\")\n            print(f\"  {'Tweets:':&lt;12} {profile.tweet_count:&gt;10,}\")\n            print(f\"  {'Ratio:':&lt;12} {profile.followers_count/max(1,profile.following_count):&gt;10.2f}\")\n\n        print(\"\\n\" + \"=\" * 60)\n\n# Usage\nasyncio.run(display_dashboard([\"elonmusk\", \"naval\", \"paulg\"]))\n</code></pre>"},{"location":"guides/monitoring/growth/#anomaly-detection","title":"Anomaly Detection","text":""},{"location":"guides/monitoring/growth/#statistical-anomaly-detection","title":"Statistical Anomaly Detection","text":"<pre><code>import asyncio\nfrom statistics import mean, stdev\nfrom xtools import Xtools\n\nclass AnomalyDetector:\n    def __init__(self, username: str, sensitivity: float = 2.0):\n        self.username = username\n        self.sensitivity = sensitivity  # Standard deviations\n        self.daily_changes = []\n\n    def _is_anomaly(self, value: float) -&gt; bool:\n        \"\"\"Check if value is anomalous based on history.\"\"\"\n        if len(self.daily_changes) &lt; 7:\n            return False\n\n        avg = mean(self.daily_changes)\n        std = stdev(self.daily_changes) if len(self.daily_changes) &gt; 1 else 0\n\n        if std == 0:\n            return False\n\n        z_score = abs(value - avg) / std\n        return z_score &gt; self.sensitivity\n\n    async def add_data_point(self, change: int) -&gt; dict:\n        \"\"\"Add daily change and check for anomaly.\"\"\"\n        is_anomaly = self._is_anomaly(change)\n\n        self.daily_changes.append(change)\n        if len(self.daily_changes) &gt; 30:  # Keep 30 days\n            self.daily_changes.pop(0)\n\n        result = {\n            \"change\": change,\n            \"is_anomaly\": is_anomaly,\n            \"average\": mean(self.daily_changes) if self.daily_changes else 0\n        }\n\n        if is_anomaly:\n            direction = \"gain\" if change &gt; 0 else \"loss\"\n            result[\"message\"] = (\n                f\"\ud83d\udea8 Anomaly detected: {abs(change):,} follower {direction} \"\n                f\"(avg: {result['average']:.0f})\"\n            )\n\n        return result\n\n# Usage\nasync def main():\n    detector = AnomalyDetector(\"your_username\", sensitivity=2.0)\n\n    # Simulate daily changes\n    daily_changes = [50, 45, 52, 48, 500, 47, 51]  # 500 is anomaly\n\n    for change in daily_changes:\n        result = await detector.add_data_point(change)\n        if result[\"is_anomaly\"]:\n            print(result[\"message\"])\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/growth/#complete-growth-monitoring-system","title":"Complete Growth Monitoring System","text":"<pre><code>import asyncio\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom xtools import Xtools\nfrom xtools.notifications import DiscordNotifier\n\nclass ComprehensiveGrowthMonitor:\n    def __init__(\n        self,\n        username: str,\n        data_dir: str = \"growth_data\",\n        discord_webhook: str = None\n    ):\n        self.username = username\n        self.data_dir = Path(data_dir)\n        self.data_dir.mkdir(exist_ok=True)\n        self.notifier = DiscordNotifier(discord_webhook) if discord_webhook else None\n\n        self.metrics_file = self.data_dir / f\"{username}_metrics.json\"\n        self.followers_file = self.data_dir / f\"{username}_followers.json\"\n\n        self.metrics_history = self._load_json(self.metrics_file)\n        self.known_followers = set(self._load_json(self.followers_file))\n\n    def _load_json(self, path: Path) -&gt; list:\n        if path.exists():\n            return json.loads(path.read_text())\n        return []\n\n    def _save_json(self, path: Path, data):\n        path.write_text(json.dumps(data, indent=2, default=str))\n\n    async def run_check(self) -&gt; dict:\n        \"\"\"Run comprehensive growth check.\"\"\"\n        async with Xtools() as x:\n            # Get profile metrics\n            profile = await x.scrape.profile(self.username)\n\n            # Get current followers\n            result = await x.scrape.followers(self.username, limit=10000)\n            current_followers = {u.username for u in result.users}\n\n            # Calculate changes\n            new_followers = current_followers - self.known_followers\n            unfollowers = self.known_followers - current_followers\n\n            # Record metrics\n            snapshot = {\n                \"timestamp\": datetime.now().isoformat(),\n                \"followers\": profile.followers_count,\n                \"following\": profile.following_count,\n                \"tweets\": profile.tweet_count,\n                \"new_followers\": list(new_followers),\n                \"unfollowers\": list(unfollowers)\n            }\n\n            self.metrics_history.append(snapshot)\n            self._save_json(self.metrics_file, self.metrics_history[-1000:])\n\n            # Update known followers\n            self.known_followers = current_followers\n            self._save_json(self.followers_file, list(current_followers))\n\n            # Send notifications\n            await self._send_notifications(snapshot)\n\n            return snapshot\n\n    async def _send_notifications(self, snapshot: dict):\n        \"\"\"Send notifications for significant events.\"\"\"\n        if not self.notifier:\n            return\n\n        if snapshot[\"new_followers\"]:\n            await self.notifier.send(\n                title=f\"New Followers for @{self.username}\",\n                message=f\"+{len(snapshot['new_followers'])} new followers\",\n                color=0x00FF00\n            )\n\n        if snapshot[\"unfollowers\"]:\n            await self.notifier.send(\n                title=f\"Unfollowers for @{self.username}\",\n                message=f\"-{len(snapshot['unfollowers'])} unfollowers\",\n                color=0xFF0000\n            )\n\n# Usage\nasync def main():\n    monitor = ComprehensiveGrowthMonitor(\n        \"your_username\",\n        discord_webhook=\"https://discord.com/api/webhooks/...\"\n    )\n\n    # Run every hour\n    while True:\n        result = await monitor.run_check()\n        print(f\"[{datetime.now()}] Followers: {result['followers']:,}\")\n        await asyncio.sleep(3600)\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/growth/#next-steps","title":"Next Steps","text":"<ul> <li>Account Monitoring - Track profile changes</li> <li>Keyword Monitoring - Monitor mentions and topics</li> <li>Engagement Monitoring - Track interactions</li> <li>Analytics Guide - Deep dive into data analysis</li> </ul>"},{"location":"guides/monitoring/keywords/","title":"Keyword Monitoring","text":"<p>Monitor X/Twitter in real-time for specific keywords, hashtags, and phrases with sentiment filtering and engagement thresholds.</p>"},{"location":"guides/monitoring/keywords/#overview","title":"Overview","text":"<p>Keyword monitoring enables you to:</p> <ul> <li>Track brand mentions and keywords in real-time</li> <li>Filter by sentiment (positive, negative, neutral)</li> <li>Set engagement thresholds for alerts</li> <li>Monitor multiple keywords simultaneously</li> <li>Receive notifications via Discord, Telegram, or email</li> </ul>"},{"location":"guides/monitoring/keywords/#real-time-keyword-tracking","title":"Real-Time Keyword Tracking","text":""},{"location":"guides/monitoring/keywords/#basic-keyword-monitor","title":"Basic Keyword Monitor","text":"<pre><code>import asyncio\nfrom datetime import datetime\nfrom xtools import Xtools\n\nasync def monitor_keyword(keyword: str, interval_seconds: int = 60):\n    \"\"\"Monitor for new tweets containing keyword.\"\"\"\n    seen_ids = set()\n\n    async with Xtools() as x:\n        print(f\"\ud83d\udd0d Monitoring for: {keyword}\")\n\n        while True:\n            # Search for keyword\n            result = await x.scrape.search(\n                keyword,\n                search_type=\"Latest\",\n                limit=50\n            )\n\n            new_tweets = []\n            for tweet in result.tweets:\n                if tweet.id not in seen_ids:\n                    seen_ids.add(tweet.id)\n                    new_tweets.append(tweet)\n\n            if new_tweets:\n                print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] \"\n                      f\"{len(new_tweets)} new tweets:\")\n\n                for tweet in new_tweets[:5]:\n                    print(f\"\\n  @{tweet.author.username}:\")\n                    print(f\"  {tweet.text[:100]}...\")\n                    print(f\"  \u2764\ufe0f {tweet.like_count} | \ud83d\udd01 {tweet.retweet_count}\")\n\n            await asyncio.sleep(interval_seconds)\n\n# Usage\nasyncio.run(monitor_keyword(\"python programming\", interval_seconds=30))\n</code></pre>"},{"location":"guides/monitoring/keywords/#advanced-keyword-tracker-with-storage","title":"Advanced Keyword Tracker with Storage","text":"<pre><code>import asyncio\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom xtools import Xtools\n\nclass KeywordTracker:\n    def __init__(self, keyword: str, storage_file: str = None):\n        self.keyword = keyword\n        self.storage_file = Path(storage_file or f\"keyword_{keyword.replace(' ', '_')}.json\")\n        self.seen_ids = self._load_seen()\n        self.matches = []\n\n    def _load_seen(self) -&gt; set:\n        if self.storage_file.exists():\n            data = json.loads(self.storage_file.read_text())\n            return set(data.get(\"seen_ids\", []))\n        return set()\n\n    def _save_state(self):\n        data = {\n            \"keyword\": self.keyword,\n            \"seen_ids\": list(self.seen_ids),\n            \"last_check\": datetime.now().isoformat()\n        }\n        self.storage_file.write_text(json.dumps(data, indent=2))\n\n    async def check(self) -&gt; list:\n        \"\"\"Check for new keyword matches.\"\"\"\n        async with Xtools() as x:\n            result = await x.scrape.search(\n                self.keyword,\n                search_type=\"Latest\",\n                limit=100\n            )\n\n            new_tweets = []\n            for tweet in result.tweets:\n                if tweet.id not in self.seen_ids:\n                    self.seen_ids.add(tweet.id)\n                    new_tweets.append({\n                        \"id\": tweet.id,\n                        \"text\": tweet.text,\n                        \"author\": tweet.author.username,\n                        \"created_at\": tweet.created_at.isoformat() if tweet.created_at else None,\n                        \"likes\": tweet.like_count,\n                        \"retweets\": tweet.retweet_count,\n                        \"url\": f\"https://x.com/{tweet.author.username}/status/{tweet.id}\"\n                    })\n\n            self._save_state()\n            return new_tweets\n\n    async def monitor(self, interval_seconds: int = 60, callback=None):\n        \"\"\"Continuous monitoring with optional callback.\"\"\"\n        print(f\"\ud83d\udd0d Monitoring: '{self.keyword}'\")\n\n        while True:\n            new_tweets = await self.check()\n\n            if new_tweets:\n                if callback:\n                    await callback(self.keyword, new_tweets)\n                else:\n                    print(f\"\\n\ud83d\udce2 {len(new_tweets)} new matches for '{self.keyword}'\")\n\n            await asyncio.sleep(interval_seconds)\n\n# Usage\nasync def main():\n    tracker = KeywordTracker(\"machine learning\")\n    await tracker.monitor(interval_seconds=60)\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/keywords/#multiple-keyword-support","title":"Multiple Keyword Support","text":""},{"location":"guides/monitoring/keywords/#multi-keyword-monitor","title":"Multi-Keyword Monitor","text":"<pre><code>import asyncio\nfrom datetime import datetime\nfrom xtools import Xtools\n\nclass MultiKeywordMonitor:\n    def __init__(self, keywords: list):\n        self.keywords = keywords\n        self.seen_ids = {kw: set() for kw in keywords}\n\n    async def check_all(self) -&gt; dict:\n        \"\"\"Check all keywords for new matches.\"\"\"\n        async with Xtools() as x:\n            results = {}\n\n            for keyword in self.keywords:\n                result = await x.scrape.search(\n                    keyword,\n                    search_type=\"Latest\",\n                    limit=50\n                )\n\n                new_tweets = []\n                for tweet in result.tweets:\n                    if tweet.id not in self.seen_ids[keyword]:\n                        self.seen_ids[keyword].add(tweet.id)\n                        new_tweets.append(tweet)\n\n                if new_tweets:\n                    results[keyword] = new_tweets\n\n                await asyncio.sleep(2)  # Rate limiting between keywords\n\n            return results\n\n    async def monitor(self, interval_seconds: int = 120):\n        \"\"\"Monitor all keywords.\"\"\"\n        print(f\"\ud83d\udd0d Monitoring {len(self.keywords)} keywords:\")\n        for kw in self.keywords:\n            print(f\"   - {kw}\")\n\n        while True:\n            results = await self.check_all()\n\n            for keyword, tweets in results.items():\n                print(f\"\\n\ud83d\udce2 [{keyword}] {len(tweets)} new tweets\")\n                for tweet in tweets[:3]:\n                    print(f\"   @{tweet.author.username}: {tweet.text[:60]}...\")\n\n            await asyncio.sleep(interval_seconds)\n\n# Usage\nasync def main():\n    keywords = [\n        \"python tips\",\n        \"machine learning\",\n        \"web development\",\n        \"#coding\",\n        \"#100DaysOfCode\"\n    ]\n\n    monitor = MultiKeywordMonitor(keywords)\n    await monitor.monitor(interval_seconds=60)\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/keywords/#keyword-categories","title":"Keyword Categories","text":"<pre><code>import asyncio\nfrom dataclasses import dataclass\nfrom typing import List, Dict\nfrom xtools import Xtools\n\n@dataclass\nclass KeywordCategory:\n    name: str\n    keywords: List[str]\n    priority: str = \"normal\"  # low, normal, high\n\nclass CategorizedKeywordMonitor:\n    def __init__(self, categories: List[KeywordCategory]):\n        self.categories = categories\n        self.seen_ids = {}\n\n        for cat in categories:\n            for kw in cat.keywords:\n                self.seen_ids[kw] = set()\n\n    async def check_category(self, category: KeywordCategory) -&gt; Dict:\n        \"\"\"Check all keywords in a category.\"\"\"\n        async with Xtools() as x:\n            results = {}\n\n            for keyword in category.keywords:\n                result = await x.scrape.search(\n                    keyword,\n                    search_type=\"Latest\",\n                    limit=30\n                )\n\n                new_tweets = [\n                    t for t in result.tweets\n                    if t.id not in self.seen_ids[keyword]\n                ]\n\n                for t in new_tweets:\n                    self.seen_ids[keyword].add(t.id)\n\n                if new_tweets:\n                    results[keyword] = new_tweets\n\n                await asyncio.sleep(1)\n\n            return results\n\n    async def monitor(self, interval_seconds: int = 120):\n        \"\"\"Monitor all categories.\"\"\"\n        while True:\n            for category in self.categories:\n                results = await self.check_category(category)\n\n                if results:\n                    priority_emoji = {\n                        \"high\": \"\ud83d\udd34\",\n                        \"normal\": \"\ud83d\udfe1\",\n                        \"low\": \"\ud83d\udfe2\"\n                    }\n\n                    print(f\"\\n{priority_emoji[category.priority]} [{category.name}]\")\n                    for keyword, tweets in results.items():\n                        print(f\"   '{keyword}': {len(tweets)} new\")\n\n            await asyncio.sleep(interval_seconds)\n\n# Usage\ncategories = [\n    KeywordCategory(\n        name=\"Brand Mentions\",\n        keywords=[\"@mycompany\", \"MyCompany\", \"my company\"],\n        priority=\"high\"\n    ),\n    KeywordCategory(\n        name=\"Industry\",\n        keywords=[\"#saas\", \"#startup\", \"product launch\"],\n        priority=\"normal\"\n    ),\n    KeywordCategory(\n        name=\"Competitors\",\n        keywords=[\"competitor1\", \"competitor2\"],\n        priority=\"low\"\n    )\n]\n\nmonitor = CategorizedKeywordMonitor(categories)\nasyncio.run(monitor.monitor())\n</code></pre>"},{"location":"guides/monitoring/keywords/#sentiment-filtering","title":"Sentiment Filtering","text":""},{"location":"guides/monitoring/keywords/#basic-sentiment-filter","title":"Basic Sentiment Filter","text":"<pre><code>import asyncio\nfrom xtools import Xtools\nfrom xtools.ai import SentimentAnalyzer\n\nclass SentimentFilteredMonitor:\n    def __init__(self, keyword: str, target_sentiment: str = None):\n        self.keyword = keyword\n        self.target_sentiment = target_sentiment  # positive, negative, neutral, or None for all\n        self.analyzer = SentimentAnalyzer()\n        self.seen_ids = set()\n\n    async def check(self) -&gt; list:\n        \"\"\"Check for keyword with sentiment filter.\"\"\"\n        async with Xtools() as x:\n            result = await x.scrape.search(\n                self.keyword,\n                search_type=\"Latest\",\n                limit=50\n            )\n\n            matches = []\n\n            for tweet in result.tweets:\n                if tweet.id in self.seen_ids:\n                    continue\n\n                self.seen_ids.add(tweet.id)\n\n                # Analyze sentiment\n                sentiment = await self.analyzer.analyze(tweet.text)\n\n                # Filter by sentiment if specified\n                if self.target_sentiment and sentiment.label != self.target_sentiment:\n                    continue\n\n                matches.append({\n                    \"tweet\": tweet,\n                    \"sentiment\": sentiment.label,\n                    \"confidence\": sentiment.score\n                })\n\n            return matches\n\n# Usage\nasync def main():\n    # Monitor for negative mentions only\n    monitor = SentimentFilteredMonitor(\n        \"MyProduct\",\n        target_sentiment=\"negative\"\n    )\n\n    while True:\n        matches = await monitor.check()\n\n        for match in matches:\n            tweet = match[\"tweet\"]\n            print(f\"\\n\u26a0\ufe0f Negative mention detected:\")\n            print(f\"   @{tweet.author.username}: {tweet.text[:100]}\")\n            print(f\"   Sentiment: {match['sentiment']} ({match['confidence']:.2f})\")\n\n        await asyncio.sleep(60)\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/keywords/#sentiment-analytics","title":"Sentiment Analytics","text":"<pre><code>import asyncio\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom xtools import Xtools\nfrom xtools.ai import SentimentAnalyzer\n\nclass SentimentAnalytics:\n    def __init__(self, keyword: str):\n        self.keyword = keyword\n        self.analyzer = SentimentAnalyzer()\n        self.stats = defaultdict(int)\n        self.hourly_stats = defaultdict(lambda: defaultdict(int))\n\n    async def analyze_batch(self, limit: int = 100) -&gt; dict:\n        \"\"\"Analyze sentiment distribution for keyword.\"\"\"\n        async with Xtools() as x:\n            result = await x.scrape.search(\n                self.keyword,\n                search_type=\"Latest\",\n                limit=limit\n            )\n\n            batch_stats = defaultdict(int)\n\n            for tweet in result.tweets:\n                sentiment = await self.analyzer.analyze(tweet.text)\n                batch_stats[sentiment.label] += 1\n                self.stats[sentiment.label] += 1\n\n                # Track hourly\n                hour = datetime.now().strftime(\"%Y-%m-%d %H:00\")\n                self.hourly_stats[hour][sentiment.label] += 1\n\n            total = sum(batch_stats.values())\n\n            return {\n                \"keyword\": self.keyword,\n                \"total\": total,\n                \"positive\": batch_stats[\"positive\"],\n                \"negative\": batch_stats[\"negative\"],\n                \"neutral\": batch_stats[\"neutral\"],\n                \"positive_pct\": (batch_stats[\"positive\"] / total * 100) if total else 0,\n                \"negative_pct\": (batch_stats[\"negative\"] / total * 100) if total else 0\n            }\n\n    def get_summary(self) -&gt; dict:\n        \"\"\"Get overall sentiment summary.\"\"\"\n        total = sum(self.stats.values())\n        return {\n            \"total_analyzed\": total,\n            \"positive_pct\": (self.stats[\"positive\"] / total * 100) if total else 0,\n            \"negative_pct\": (self.stats[\"negative\"] / total * 100) if total else 0,\n            \"neutral_pct\": (self.stats[\"neutral\"] / total * 100) if total else 0\n        }\n\n# Usage\nasync def main():\n    analytics = SentimentAnalytics(\"Python programming\")\n\n    result = await analytics.analyze_batch(limit=100)\n\n    print(f\"\ud83d\udcca Sentiment Analysis for '{result['keyword']}'\")\n    print(f\"   Total: {result['total']} tweets\")\n    print(f\"   \u2705 Positive: {result['positive_pct']:.1f}%\")\n    print(f\"   \u274c Negative: {result['negative_pct']:.1f}%\")\n    print(f\"   \u2796 Neutral: {100 - result['positive_pct'] - result['negative_pct']:.1f}%\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/keywords/#engagement-thresholds","title":"Engagement Thresholds","text":""},{"location":"guides/monitoring/keywords/#high-engagement-filter","title":"High-Engagement Filter","text":"<pre><code>import asyncio\nfrom dataclasses import dataclass\nfrom xtools import Xtools\n\n@dataclass\nclass EngagementThreshold:\n    min_likes: int = 0\n    min_retweets: int = 0\n    min_replies: int = 0\n    min_total: int = 0\n\nclass HighEngagementMonitor:\n    def __init__(self, keyword: str, threshold: EngagementThreshold):\n        self.keyword = keyword\n        self.threshold = threshold\n        self.seen_ids = set()\n\n    def _meets_threshold(self, tweet) -&gt; bool:\n        \"\"\"Check if tweet meets engagement threshold.\"\"\"\n        total = tweet.like_count + tweet.retweet_count + tweet.reply_count\n\n        return (\n            tweet.like_count &gt;= self.threshold.min_likes and\n            tweet.retweet_count &gt;= self.threshold.min_retweets and\n            tweet.reply_count &gt;= self.threshold.min_replies and\n            total &gt;= self.threshold.min_total\n        )\n\n    async def check(self) -&gt; list:\n        \"\"\"Find high-engagement tweets.\"\"\"\n        async with Xtools() as x:\n            result = await x.scrape.search(\n                self.keyword,\n                search_type=\"Top\",  # Top results have higher engagement\n                limit=50\n            )\n\n            high_engagement = []\n\n            for tweet in result.tweets:\n                if tweet.id in self.seen_ids:\n                    continue\n\n                if self._meets_threshold(tweet):\n                    self.seen_ids.add(tweet.id)\n                    high_engagement.append(tweet)\n\n            return high_engagement\n\n    async def monitor(self, interval_seconds: int = 300):\n        \"\"\"Monitor for high-engagement content.\"\"\"\n        print(f\"\ud83d\udd0d Monitoring '{self.keyword}' for high engagement\")\n        print(f\"   Thresholds: {self.threshold.min_likes}+ likes, \"\n              f\"{self.threshold.min_retweets}+ RTs\")\n\n        while True:\n            matches = await self.check()\n\n            for tweet in matches:\n                print(f\"\\n\ud83d\udd25 High-engagement tweet found:\")\n                print(f\"   @{tweet.author.username} ({tweet.author.followers_count:,} followers)\")\n                print(f\"   {tweet.text[:100]}...\")\n                print(f\"   \u2764\ufe0f {tweet.like_count:,} | \ud83d\udd01 {tweet.retweet_count:,}\")\n\n            await asyncio.sleep(interval_seconds)\n\n# Usage\nthreshold = EngagementThreshold(\n    min_likes=100,\n    min_retweets=20,\n    min_total=150\n)\n\nmonitor = HighEngagementMonitor(\"artificial intelligence\", threshold)\nasyncio.run(monitor.monitor())\n</code></pre>"},{"location":"guides/monitoring/keywords/#notification-callbacks","title":"Notification Callbacks","text":""},{"location":"guides/monitoring/keywords/#callback-system","title":"Callback System","text":"<pre><code>import asyncio\nfrom typing import Callable, List\nfrom xtools import Xtools\nfrom xtools.notifications import DiscordNotifier, TelegramNotifier\n\nclass CallbackKeywordMonitor:\n    def __init__(self, keyword: str):\n        self.keyword = keyword\n        self.seen_ids = set()\n        self.callbacks: List[Callable] = []\n\n    def add_callback(self, callback: Callable):\n        \"\"\"Add a callback function.\"\"\"\n        self.callbacks.append(callback)\n\n    async def _trigger_callbacks(self, tweets: list):\n        \"\"\"Trigger all callbacks with new tweets.\"\"\"\n        for callback in self.callbacks:\n            try:\n                if asyncio.iscoroutinefunction(callback):\n                    await callback(self.keyword, tweets)\n                else:\n                    callback(self.keyword, tweets)\n            except Exception as e:\n                print(f\"Callback error: {e}\")\n\n    async def check(self):\n        \"\"\"Check and trigger callbacks for new tweets.\"\"\"\n        async with Xtools() as x:\n            result = await x.scrape.search(\n                self.keyword,\n                search_type=\"Latest\",\n                limit=50\n            )\n\n            new_tweets = [\n                t for t in result.tweets\n                if t.id not in self.seen_ids\n            ]\n\n            for t in new_tweets:\n                self.seen_ids.add(t.id)\n\n            if new_tweets:\n                await self._trigger_callbacks(new_tweets)\n\n# Define callbacks\nasync def discord_callback(keyword: str, tweets: list):\n    notifier = DiscordNotifier(\"https://discord.com/api/webhooks/...\")\n    await notifier.send(\n        title=f\"New mentions: {keyword}\",\n        message=f\"{len(tweets)} new tweets found\"\n    )\n\nasync def log_callback(keyword: str, tweets: list):\n    with open(\"keyword_log.txt\", \"a\") as f:\n        for t in tweets:\n            f.write(f\"{keyword}|{t.id}|{t.author.username}|{t.text[:100]}\\n\")\n\ndef print_callback(keyword: str, tweets: list):\n    print(f\"[{keyword}] {len(tweets)} new tweets\")\n\n# Usage\nasync def main():\n    monitor = CallbackKeywordMonitor(\"python tips\")\n\n    monitor.add_callback(discord_callback)\n    monitor.add_callback(log_callback)\n    monitor.add_callback(print_callback)\n\n    while True:\n        await monitor.check()\n        await asyncio.sleep(60)\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/keywords/#use-case-brand-monitoring","title":"Use Case: Brand Monitoring","text":""},{"location":"guides/monitoring/keywords/#complete-brand-monitoring-system","title":"Complete Brand Monitoring System","text":"<pre><code>import asyncio\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom typing import List, Optional\nfrom xtools import Xtools\nfrom xtools.ai import SentimentAnalyzer\nfrom xtools.notifications import DiscordNotifier\n\n@dataclass\nclass BrandConfig:\n    brand_name: str\n    keywords: List[str]\n    competitors: List[str] = None\n    alert_on_negative: bool = True\n    alert_threshold_likes: int = 50\n    discord_webhook: Optional[str] = None\n\nclass BrandMonitor:\n    def __init__(self, config: BrandConfig):\n        self.config = config\n        self.analyzer = SentimentAnalyzer()\n        self.seen_ids = set()\n        self.storage_dir = Path(f\"brand_monitor_{config.brand_name}\")\n        self.storage_dir.mkdir(exist_ok=True)\n\n        if config.discord_webhook:\n            self.notifier = DiscordNotifier(config.discord_webhook)\n        else:\n            self.notifier = None\n\n    async def check_mentions(self) -&gt; dict:\n        \"\"\"Check all brand keywords.\"\"\"\n        async with Xtools() as x:\n            all_keywords = self.config.keywords.copy()\n            if self.config.competitors:\n                all_keywords.extend(self.config.competitors)\n\n            results = {\n                \"brand_mentions\": [],\n                \"competitor_mentions\": [],\n                \"negative_alerts\": [],\n                \"high_engagement\": []\n            }\n\n            for keyword in all_keywords:\n                is_competitor = keyword in (self.config.competitors or [])\n\n                result = await x.scrape.search(\n                    keyword,\n                    search_type=\"Latest\",\n                    limit=30\n                )\n\n                for tweet in result.tweets:\n                    if tweet.id in self.seen_ids:\n                        continue\n\n                    self.seen_ids.add(tweet.id)\n\n                    # Analyze sentiment\n                    sentiment = await self.analyzer.analyze(tweet.text)\n\n                    mention = {\n                        \"id\": tweet.id,\n                        \"text\": tweet.text,\n                        \"author\": tweet.author.username,\n                        \"followers\": tweet.author.followers_count,\n                        \"likes\": tweet.like_count,\n                        \"sentiment\": sentiment.label,\n                        \"keyword\": keyword,\n                        \"url\": f\"https://x.com/{tweet.author.username}/status/{tweet.id}\",\n                        \"timestamp\": datetime.now().isoformat()\n                    }\n\n                    # Categorize\n                    if is_competitor:\n                        results[\"competitor_mentions\"].append(mention)\n                    else:\n                        results[\"brand_mentions\"].append(mention)\n\n                    # Check for alerts\n                    if sentiment.label == \"negative\" and self.config.alert_on_negative:\n                        results[\"negative_alerts\"].append(mention)\n\n                    if tweet.like_count &gt;= self.config.alert_threshold_likes:\n                        results[\"high_engagement\"].append(mention)\n\n                await asyncio.sleep(2)\n\n            return results\n\n    async def send_alerts(self, results: dict):\n        \"\"\"Send alerts for important mentions.\"\"\"\n        if not self.notifier:\n            return\n\n        # Negative mentions alert\n        for mention in results[\"negative_alerts\"]:\n            await self.notifier.send(\n                title=f\"\u26a0\ufe0f Negative Mention: {self.config.brand_name}\",\n                message=(\n                    f\"@{mention['author']} ({mention['followers']:,} followers)\\n\"\n                    f\"{mention['text'][:200]}\\n\"\n                    f\"[View Tweet]({mention['url']})\"\n                ),\n                color=0xFF0000\n            )\n\n        # High engagement alert\n        for mention in results[\"high_engagement\"]:\n            if mention not in results[\"negative_alerts\"]:  # Avoid duplicates\n                await self.notifier.send(\n                    title=f\"\ud83d\udd25 High Engagement: {self.config.brand_name}\",\n                    message=(\n                        f\"@{mention['author']}\\n\"\n                        f\"{mention['text'][:200]}\\n\"\n                        f\"\u2764\ufe0f {mention['likes']} likes\"\n                    ),\n                    color=0x00FF00\n                )\n\n    def save_report(self, results: dict):\n        \"\"\"Save daily report.\"\"\"\n        date_str = datetime.now().strftime(\"%Y-%m-%d\")\n        report_file = self.storage_dir / f\"report_{date_str}.json\"\n\n        # Load existing or create new\n        if report_file.exists():\n            report = json.loads(report_file.read_text())\n        else:\n            report = {\"date\": date_str, \"mentions\": []}\n\n        report[\"mentions\"].extend(results[\"brand_mentions\"])\n        report_file.write_text(json.dumps(report, indent=2))\n\n    async def monitor(self, interval_minutes: int = 15):\n        \"\"\"Run continuous monitoring.\"\"\"\n        print(f\"\ud83d\udd0d Brand Monitor: {self.config.brand_name}\")\n        print(f\"   Keywords: {', '.join(self.config.keywords)}\")\n        if self.config.competitors:\n            print(f\"   Competitors: {', '.join(self.config.competitors)}\")\n\n        while True:\n            results = await self.check_mentions()\n\n            # Summary\n            total = len(results[\"brand_mentions\"]) + len(results[\"competitor_mentions\"])\n            if total:\n                print(f\"\\n[{datetime.now().strftime('%H:%M')}] \"\n                      f\"{len(results['brand_mentions'])} brand, \"\n                      f\"{len(results['competitor_mentions'])} competitor, \"\n                      f\"{len(results['negative_alerts'])} negative\")\n\n            # Send alerts\n            await self.send_alerts(results)\n\n            # Save report\n            self.save_report(results)\n\n            await asyncio.sleep(interval_minutes * 60)\n\n# Usage\nconfig = BrandConfig(\n    brand_name=\"MyStartup\",\n    keywords=[\"MyStartup\", \"@mystartup\", \"mystartup.com\"],\n    competitors=[\"Competitor1\", \"Competitor2\"],\n    alert_on_negative=True,\n    alert_threshold_likes=100,\n    discord_webhook=\"https://discord.com/api/webhooks/...\"\n)\n\nmonitor = BrandMonitor(config)\nasyncio.run(monitor.monitor(interval_minutes=10))\n</code></pre>"},{"location":"guides/monitoring/keywords/#next-steps","title":"Next Steps","text":"<ul> <li>Growth Monitoring - Track follower changes</li> <li>Account Monitoring - Track profile changes</li> <li>Engagement Monitoring - Track interactions</li> </ul>"},{"location":"guides/monitoring/unfollowers/","title":"Unfollower Detection","text":"<p>Track exactly who unfollowed you, when, and get insights on why.</p>"},{"location":"guides/monitoring/unfollowers/#basic-unfollower-check","title":"Basic Unfollower Check","text":"<pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    # Get unfollower report\n    report = await x.monitor.unfollowers()\n\n    print(f\"\ud83d\udcc9 Unfollowers: {len(report.unfollowers)}\")\n    print(f\"\ud83d\udcc8 New Followers: {len(report.new_followers)}\")\n    print(f\"\ud83d\udcca Net Change: {report.net_change:+d}\")\n\n# Detailed unfollower info\nasync with Xeepy() as x:\n    report = await x.monitor.unfollowers(detailed=True)\n\n    for user in report.unfollowers:\n        print(f\"\\n@{user.username}\")\n        print(f\"  Followers: {user.followers_count:,}\")\n        print(f\"  Was following you since: {user.followed_since}\")\n        print(f\"  Last interaction: {user.last_interaction}\")\n</code></pre>"},{"location":"guides/monitoring/unfollowers/#how-it-works","title":"How It Works","text":"<p>Xeepy tracks your followers by:</p> <ol> <li>First Run: Saves complete follower list as baseline</li> <li>Subsequent Runs: Compares current followers to baseline</li> <li>Detection: Identifies additions and removals</li> <li>Storage: Updates baseline for next comparison</li> </ol> <pre><code># First run creates baseline\nasync with Xeepy() as x:\n    report = await x.monitor.unfollowers()\n    # report.unfollowers = []  (no baseline yet)\n    # report.message = \"Baseline created with 5,432 followers\"\n\n# Second run (24 hours later)\nasync with Xeepy() as x:\n    report = await x.monitor.unfollowers()\n    # report.unfollowers = [user1, user2, user3]  # Detected!\n    # report.new_followers = [user4, user5]\n</code></pre>"},{"location":"guides/monitoring/unfollowers/#data-storage-options","title":"Data Storage Options","text":""},{"location":"guides/monitoring/unfollowers/#file-based-default","title":"File-Based (Default)","text":"<pre><code># Stores in ~/.xeepy/followers_baseline.json\nasync with Xeepy() as x:\n    report = await x.monitor.unfollowers()\n</code></pre>"},{"location":"guides/monitoring/unfollowers/#custom-storage-location","title":"Custom Storage Location","text":"<pre><code>async with Xeepy(storage_path=\"/custom/path\") as x:\n    report = await x.monitor.unfollowers()\n</code></pre>"},{"location":"guides/monitoring/unfollowers/#sqlite-database","title":"SQLite Database","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.storage import SQLiteStorage\n\n# Use SQLite for better querying\nstorage = SQLiteStorage(\"followers.db\")\n\nasync with Xeepy(storage=storage) as x:\n    report = await x.monitor.unfollowers()\n\n    # Query historical data\n    history = storage.get_unfollower_history(days=30)\n    print(f\"Total unfollowers (30 days): {len(history)}\")\n</code></pre>"},{"location":"guides/monitoring/unfollowers/#advanced-analysis","title":"Advanced Analysis","text":""},{"location":"guides/monitoring/unfollowers/#unfollower-patterns","title":"Unfollower Patterns","text":"<pre><code>async def analyze_unfollower_patterns():\n    \"\"\"Understand why people unfollow\"\"\"\n\n    async with Xeepy() as x:\n        report = await x.monitor.unfollowers(detailed=True)\n\n        # Categorize unfollowers\n        categories = {\n            \"high_value\": [],    # 10k+ followers\n            \"recent\": [],        # Followed &lt; 7 days\n            \"inactive\": [],      # No tweets in 30 days\n            \"mass_unfollower\": [], # Unfollows many people\n            \"competitor\": [],    # Follows competitors\n        }\n\n        for user in report.unfollowers:\n            # High value (potential influencer)\n            if user.followers_count &gt;= 10000:\n                categories[\"high_value\"].append(user)\n\n            # Recent follower (didn't stick)\n            if user.followed_days &lt; 7:\n                categories[\"recent\"].append(user)\n\n            # Inactive account\n            if user.days_since_last_tweet &gt; 30:\n                categories[\"inactive\"].append(user)\n\n            # Mass unfollower\n            if user.recent_unfollows &gt; 50:\n                categories[\"mass_unfollower\"].append(user)\n\n        # Print analysis\n        print(\"\ud83d\udcca Unfollower Analysis\")\n        print(\"=\" * 50)\n\n        for category, users in categories.items():\n            if users:\n                print(f\"\\n{category}: {len(users)}\")\n                for u in users[:3]:\n                    print(f\"  @{u.username} ({u.followers_count:,})\")\n\n        return categories\n</code></pre>"},{"location":"guides/monitoring/unfollowers/#high-value-unfollower-alerts","title":"High-Value Unfollower Alerts","text":"<pre><code>async def alert_vip_unfollowers(min_followers: int = 10000):\n    \"\"\"Alert when important people unfollow\"\"\"\n\n    async with Xeepy() as x:\n        report = await x.monitor.unfollowers()\n\n        vips = [u for u in report.unfollowers if u.followers_count &gt;= min_followers]\n\n        if vips:\n            message = f\"\ud83d\udea8 {len(vips)} VIP unfollower(s):\\n\\n\"\n\n            for user in vips:\n                message += f\"@{user.username} ({user.followers_count:,} followers)\\n\"\n                message += f\"Bio: {user.bio[:100]}...\\n\\n\"\n\n            # Send alert\n            await x.notify.discord(\n                webhook_url=\"...\",\n                content=message\n            )\n\n            await x.notify.telegram(\n                bot_token=\"...\",\n                chat_id=\"...\",\n                message=message\n            )\n\n        return vips\n</code></pre>"},{"location":"guides/monitoring/unfollowers/#scheduled-monitoring","title":"Scheduled Monitoring","text":""},{"location":"guides/monitoring/unfollowers/#daily-check-script","title":"Daily Check Script","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"daily_unfollower_check.py - Run via cron\"\"\"\n\nimport asyncio\nfrom datetime import datetime\nfrom xeepy import Xeepy\n\nasync def main():\n    async with Xeepy() as x:\n        report = await x.monitor.unfollowers()\n\n        # Format report\n        date = datetime.now().strftime(\"%Y-%m-%d\")\n\n        summary = f\"\"\"\n\ud83d\udcca Daily Follower Report - {date}\n{'='*40}\n\n\ud83d\udcc9 Lost: {len(report.unfollowers)}\n\ud83d\udcc8 Gained: {len(report.new_followers)}  \n\ud83d\udcca Net: {report.net_change:+d}\n\n\"\"\"\n\n        if report.unfollowers:\n            summary += \"Top Unfollowers:\\n\"\n            for u in sorted(report.unfollowers, key=lambda x: -x.followers_count)[:5]:\n                summary += f\"  \u2022 @{u.username} ({u.followers_count:,})\\n\"\n\n        if report.new_followers:\n            summary += \"\\nTop New Followers:\\n\"\n            for u in sorted(report.new_followers, key=lambda x: -x.followers_count)[:5]:\n                summary += f\"  \u2022 @{u.username} ({u.followers_count:,})\\n\"\n\n        # Send via Discord\n        await x.notify.discord(webhook_url=\"...\", content=summary)\n\n        # Save to log\n        with open(\"unfollower_log.txt\", \"a\") as f:\n            f.write(summary + \"\\n\")\n\n        print(summary)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/monitoring/unfollowers/#cron-setup","title":"Cron Setup","text":"<pre><code># Run daily at 9 AM\n0 9 * * * /usr/bin/python3 /path/to/daily_unfollower_check.py\n\n# Run every 6 hours\n0 */6 * * * /usr/bin/python3 /path/to/daily_unfollower_check.py\n</code></pre>"},{"location":"guides/monitoring/unfollowers/#unfollower-recovery","title":"Unfollower Recovery","text":""},{"location":"guides/monitoring/unfollowers/#win-back-campaign","title":"Win-Back Campaign","text":"<pre><code>async def unfollower_winback(days: int = 7, message_template: str = None):\n    \"\"\"Attempt to win back recent unfollowers\"\"\"\n\n    async with Xeepy() as x:\n        # Get recent unfollowers from database\n        recent_unfollowers = await x.monitor.get_unfollower_history(days=days)\n\n        # Filter for recoverable accounts\n        targets = [\n            u for u in recent_unfollowers\n            if u.followers_count &gt; 100  # Not a bot\n            and u.followed_days &gt; 30    # Was a real follower\n            and not u.is_protected      # Can interact\n        ]\n\n        print(f\"Found {len(targets)} potential win-back targets\")\n\n        for user in targets[:10]:  # Limit to avoid spam\n            # Engage with their recent content\n            tweets = await x.scrape.tweets(user.username, limit=5)\n\n            if tweets:\n                # Like their tweet\n                await x.engage.like(tweets[0].url)\n                print(f\"\u2713 Liked @{user.username}'s tweet\")\n\n            await asyncio.sleep(60)  # Be gentle\n\n        return targets\n</code></pre>"},{"location":"guides/monitoring/unfollowers/#metrics-analytics","title":"Metrics &amp; Analytics","text":""},{"location":"guides/monitoring/unfollowers/#historical-trends","title":"Historical Trends","text":"<pre><code>from datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\n\nasync def plot_unfollower_trends(days: int = 30):\n    \"\"\"Visualize unfollower patterns\"\"\"\n\n    async with Xeepy() as x:\n        history = await x.monitor.get_unfollower_history(days=days)\n\n        # Group by date\n        daily_counts = {}\n        for record in history:\n            date = record.timestamp.date()\n            daily_counts[date] = daily_counts.get(date, 0) + 1\n\n        # Plot\n        dates = list(daily_counts.keys())\n        counts = list(daily_counts.values())\n\n        plt.figure(figsize=(12, 6))\n        plt.bar(dates, counts, color='red', alpha=0.7)\n        plt.xlabel('Date')\n        plt.ylabel('Unfollowers')\n        plt.title(f'Daily Unfollowers - Last {days} Days')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        plt.savefig('unfollower_trends.png')\n\n        print(f\"Chart saved to unfollower_trends.png\")\n\n        # Summary stats\n        total = sum(counts)\n        avg = total / days\n        max_day = max(daily_counts.items(), key=lambda x: x[1])\n\n        print(f\"\\nSummary:\")\n        print(f\"  Total unfollowers: {total}\")\n        print(f\"  Average per day: {avg:.1f}\")\n        print(f\"  Worst day: {max_day[0]} ({max_day[1]} unfollowers)\")\n</code></pre>"},{"location":"guides/monitoring/unfollowers/#correlation-analysis","title":"Correlation Analysis","text":"<pre><code>async def correlate_unfollows_with_content():\n    \"\"\"Find if specific content causes unfollows\"\"\"\n\n    async with Xeepy() as x:\n        # Get unfollower timestamps\n        unfollowers = await x.monitor.get_unfollower_history(days=30)\n\n        # Get your tweets\n        my_tweets = await x.scrape.tweets(\"your_username\", limit=100)\n\n        # Correlate\n        correlations = []\n\n        for tweet in my_tweets:\n            # Count unfollows within 24 hours of tweet\n            tweet_time = tweet.created_at\n            unfollows_after = [\n                u for u in unfollowers\n                if tweet_time &lt;= u.timestamp &lt;= tweet_time + timedelta(hours=24)\n            ]\n\n            if unfollows_after:\n                correlations.append({\n                    \"tweet\": tweet.text[:100],\n                    \"unfollows\": len(unfollows_after),\n                    \"posted_at\": tweet_time\n                })\n\n        # Sort by unfollows\n        correlations.sort(key=lambda x: -x[\"unfollows\"])\n\n        print(\"Tweets potentially causing unfollows:\")\n        for c in correlations[:5]:\n            print(f\"\\n  {c['unfollows']} unfollows after:\")\n            print(f\"  '{c['tweet']}...'\")\n</code></pre>"},{"location":"guides/monitoring/unfollowers/#export-reporting","title":"Export &amp; Reporting","text":"<pre><code>async def export_unfollower_report(format: str = \"all\"):\n    \"\"\"Export detailed unfollower reports\"\"\"\n\n    async with Xeepy() as x:\n        report = await x.monitor.unfollowers(detailed=True)\n        history = await x.monitor.get_unfollower_history(days=30)\n\n        if format in [\"csv\", \"all\"]:\n            x.export.to_csv(report.unfollowers, \"unfollowers.csv\")\n            x.export.to_csv(history, \"unfollower_history.csv\")\n\n        if format in [\"json\", \"all\"]:\n            x.export.to_json({\n                \"current\": [u.__dict__ for u in report.unfollowers],\n                \"history\": [u.__dict__ for u in history],\n                \"summary\": {\n                    \"total_unfollowers\": len(report.unfollowers),\n                    \"new_followers\": len(report.new_followers),\n                    \"net_change\": report.net_change\n                }\n            }, \"unfollower_report.json\")\n\n        if format in [\"excel\", \"all\"]:\n            x.export.to_excel({\n                \"Current Unfollowers\": report.unfollowers,\n                \"30-Day History\": history,\n                \"New Followers\": report.new_followers\n            }, \"unfollower_report.xlsx\")\n\n        print(\"Reports exported!\")\n</code></pre>"},{"location":"guides/monitoring/unfollowers/#best-practices","title":"Best Practices","text":"<p>Monitoring Tips</p> <ul> <li>Run checks at consistent times for accurate comparison</li> <li>Don't obsess over every unfollower</li> <li>Focus on high-value unfollowers</li> <li>Look for patterns, not individual events</li> </ul> <p>Avoid Over-Reaction</p> <ul> <li>Some unfollows are bots/spam being cleaned</li> <li>Inactive accounts get deleted</li> <li>Focus on net growth, not individual losses</li> <li>Don't harass unfollowers</li> </ul> <p>Database Recommendation</p> <p>For serious monitoring, use SQLite storage: <pre><code>from xeepy.storage import SQLiteStorage\nstorage = SQLiteStorage(\"followers.db\")\n</code></pre> This enables historical queries and trend analysis.</p>"},{"location":"guides/monitoring/unfollowers/#next-steps","title":"Next Steps","text":"<p> Growth Analytics - Track overall growth</p> <p> Account Monitoring - Watch specific accounts</p> <p> Non-Follower Unfollow - Unfollow non-followers</p>"},{"location":"guides/notifications/","title":"Notifications Guide","text":"<p>Xeepy can send notifications through multiple channels to keep you informed about your X/Twitter activity, alerts, and automation results.</p>"},{"location":"guides/notifications/#overview","title":"Overview","text":"<ul> <li> <p>:material-discord:{ .lg .middle } Discord</p> <p>Webhook notifications to Discord channels</p> </li> <li> <p> Telegram</p> <p>Bot messages to Telegram chats</p> </li> <li> <p> Email</p> <p>SMTP email notifications</p> </li> <li> <p> Webhooks</p> <p>Custom HTTP webhook integrations</p> </li> <li> <p> Console</p> <p>Terminal output for local monitoring</p> </li> </ul>"},{"location":"guides/notifications/#quick-start","title":"Quick Start","text":"<pre><code>from xeepy.notifications import DiscordNotifier\n\n# Create notifier\ndiscord = DiscordNotifier(\n    webhook_url=\"https://discord.com/api/webhooks/...\"\n)\n\n# Send a message\nawait discord.send(\"\ud83d\ude80 Xeepy automation started!\")\n</code></pre>"},{"location":"guides/notifications/#discord-notifications","title":"Discord Notifications","text":""},{"location":"guides/notifications/#setup","title":"Setup","text":"<ol> <li>Create a webhook in your Discord server:</li> <li>Server Settings \u2192 Integrations \u2192 Webhooks \u2192 New Webhook</li> <li> <p>Copy the webhook URL</p> </li> <li> <p>Use in Xeepy:</p> </li> </ol> <pre><code>from xeepy.notifications import DiscordNotifier\n\ndiscord = DiscordNotifier(\n    webhook_url=\"https://discord.com/api/webhooks/123/abc...\"\n)\n</code></pre>"},{"location":"guides/notifications/#send-messages","title":"Send Messages","text":"<pre><code># Simple message\nawait discord.send(\"Hello from Xeepy!\")\n\n# With formatting\nawait discord.send(\"\"\"\n**Daily Report** \ud83d\udcca\n\n\u2705 Followers: 12,456 (+23)\n\u274c Unfollowers: 5\n\ud83d\udcac Engagement: 2.3%\n\"\"\")\n</code></pre>"},{"location":"guides/notifications/#send-rich-embeds","title":"Send Rich Embeds","text":"<pre><code>await discord.send_embed({\n    \"title\": \"\ud83d\udcca Daily Analytics Report\",\n    \"color\": 0x1DA1F2,  # Twitter blue\n    \"fields\": [\n        {\"name\": \"Followers\", \"value\": \"12,456 (+23)\", \"inline\": True},\n        {\"name\": \"Engagement\", \"value\": \"2.3%\", \"inline\": True},\n        {\"name\": \"Top Tweet\", \"value\": \"My latest thread got 500 likes!\", \"inline\": False}\n    ],\n    \"footer\": {\"text\": \"Generated by Xeepy\"},\n    \"timestamp\": datetime.now().isoformat()\n})\n</code></pre>"},{"location":"guides/notifications/#with-images","title":"With Images","text":"<pre><code>await discord.send_embed({\n    \"title\": \"Growth Chart\",\n    \"image\": {\"url\": \"https://your-chart-url.png\"}\n})\n\n# Or upload file\nawait discord.send_file(\"reports/growth_chart.png\", \"This week's growth\")\n</code></pre>"},{"location":"guides/notifications/#telegram-notifications","title":"Telegram Notifications","text":""},{"location":"guides/notifications/#setup_1","title":"Setup","text":"<ol> <li>Create a bot via @BotFather</li> <li>Get your chat ID (message @userinfobot)</li> </ol> <pre><code>from xeepy.notifications import TelegramNotifier\n\ntelegram = TelegramNotifier(\n    bot_token=\"123456:ABC-DEF...\",\n    chat_id=\"your_chat_id\"\n)\n</code></pre>"},{"location":"guides/notifications/#send-messages_1","title":"Send Messages","text":"<pre><code># Simple message\nawait telegram.send(\"\ud83d\udd14 New follower: @username\")\n\n# With Markdown formatting\nawait telegram.send(\"\"\"\n*Daily Report* \ud83d\udcca\n\n\u2705 Followers: `12,456` (+23)\n\u274c Unfollowers: `5`\n\ud83d\udcac Engagement: `2.3%`\n\"\"\", parse_mode=\"Markdown\")\n\n# With HTML\nawait telegram.send(\n    \"&lt;b&gt;Alert!&lt;/b&gt; Lost 10 followers in the last hour\",\n    parse_mode=\"HTML\"\n)\n</code></pre>"},{"location":"guides/notifications/#send-to-groups","title":"Send to Groups","text":"<pre><code># For group chats, use the group chat ID\ntelegram = TelegramNotifier(\n    bot_token=\"...\",\n    chat_id=\"-100123456789\"  # Group IDs are negative\n)\n</code></pre>"},{"location":"guides/notifications/#interactive-buttons","title":"Interactive Buttons","text":"<pre><code>await telegram.send_with_buttons(\n    \"New mention detected! How should I respond?\",\n    buttons=[\n        [{\"text\": \"\ud83d\udc4d Like\", \"callback_data\": \"like\"}],\n        [{\"text\": \"\ud83d\udcac Reply\", \"callback_data\": \"reply\"}],\n        [{\"text\": \"\ud83d\udeab Ignore\", \"callback_data\": \"ignore\"}]\n    ]\n)\n</code></pre>"},{"location":"guides/notifications/#email-notifications","title":"Email Notifications","text":""},{"location":"guides/notifications/#setup_2","title":"Setup","text":"<pre><code>from xeepy.notifications import EmailNotifier\n\nemail = EmailNotifier(\n    smtp_host=\"smtp.gmail.com\",\n    smtp_port=587,\n    username=\"your-email@gmail.com\",\n    password=\"your-app-password\",  # Use app password for Gmail\n    from_email=\"your-email@gmail.com\"\n)\n</code></pre>"},{"location":"guides/notifications/#send-emails","title":"Send Emails","text":"<pre><code># Simple email\nawait email.send(\n    to=\"recipient@example.com\",\n    subject=\"Xeepy Daily Report\",\n    body=\"Your daily stats are ready...\"\n)\n\n# HTML email\nawait email.send(\n    to=\"recipient@example.com\",\n    subject=\"\ud83d\udcca Weekly Analytics\",\n    html=\"\"\"\n    &lt;h1&gt;Weekly Report&lt;/h1&gt;\n    &lt;p&gt;Followers: &lt;strong&gt;12,456&lt;/strong&gt; (+150)&lt;/p&gt;\n    &lt;p&gt;Engagement Rate: &lt;strong&gt;2.3%&lt;/strong&gt;&lt;/p&gt;\n    \"\"\"\n)\n\n# With attachments\nawait email.send(\n    to=\"recipient@example.com\",\n    subject=\"Monthly Report\",\n    body=\"See attached report.\",\n    attachments=[\"reports/monthly_report.pdf\"]\n)\n</code></pre>"},{"location":"guides/notifications/#multiple-recipients","title":"Multiple Recipients","text":"<pre><code>await email.send(\n    to=[\"user1@example.com\", \"user2@example.com\"],\n    cc=[\"manager@example.com\"],\n    subject=\"Team Report\",\n    body=\"Weekly metrics...\"\n)\n</code></pre>"},{"location":"guides/notifications/#webhook-notifications","title":"Webhook Notifications","text":""},{"location":"guides/notifications/#generic-webhook","title":"Generic Webhook","text":"<pre><code>from xeepy.notifications import WebhookNotifier\n\nwebhook = WebhookNotifier(\n    url=\"https://your-api.com/webhook\",\n    headers={\"Authorization\": \"Bearer token123\"}\n)\n\nawait webhook.send({\n    \"event\": \"unfollower_detected\",\n    \"data\": {\n        \"username\": \"unfollowed_user\",\n        \"timestamp\": datetime.now().isoformat()\n    }\n})\n</code></pre>"},{"location":"guides/notifications/#slack-integration","title":"Slack Integration","text":"<pre><code>from xeepy.notifications import SlackNotifier\n\nslack = SlackNotifier(\n    webhook_url=\"https://hooks.slack.com/services/...\"\n)\n\nawait slack.send(\"New follower milestone: 10,000! \ud83c\udf89\")\n\n# Rich message\nawait slack.send_block({\n    \"blocks\": [\n        {\n            \"type\": \"header\",\n            \"text\": {\"type\": \"plain_text\", \"text\": \"\ud83d\udcca Daily Report\"}\n        },\n        {\n            \"type\": \"section\",\n            \"fields\": [\n                {\"type\": \"mrkdwn\", \"text\": \"*Followers:*\\n12,456\"},\n                {\"type\": \"mrkdwn\", \"text\": \"*Change:*\\n+23\"}\n            ]\n        }\n    ]\n})\n</code></pre>"},{"location":"guides/notifications/#zapierifttt-integration","title":"Zapier/IFTTT Integration","text":"<pre><code># Zapier webhook\nzapier = WebhookNotifier(\n    url=\"https://hooks.zapier.com/hooks/catch/...\"\n)\n\nawait zapier.send({\n    \"trigger\": \"new_follower_milestone\",\n    \"followers\": 10000\n})\n\n# IFTTT webhook\nifttt = WebhookNotifier(\n    url=\"https://maker.ifttt.com/trigger/xeepy_event/with/key/...\"\n)\n\nawait ifttt.send({\n    \"value1\": \"10000\",\n    \"value2\": \"follower milestone\"\n})\n</code></pre>"},{"location":"guides/notifications/#notification-manager","title":"Notification Manager","text":"<p>Manage multiple notification channels:</p> <pre><code>from xeepy.notifications import NotificationManager\n\n# Create manager\nnotifications = NotificationManager()\n\n# Add channels\nnotifications.add_channel(\"discord\", DiscordNotifier(webhook_url=\"...\"))\nnotifications.add_channel(\"telegram\", TelegramNotifier(token=\"...\", chat_id=\"...\"))\nnotifications.add_channel(\"email\", EmailNotifier(...))\n\n# Send to all channels\nawait notifications.broadcast(\"\ud83d\ude80 Important announcement!\")\n\n# Send to specific channels\nawait notifications.send(\n    message=\"Daily report ready\",\n    channels=[\"discord\", \"email\"]\n)\n</code></pre>"},{"location":"guides/notifications/#priority-based-routing","title":"Priority-Based Routing","text":"<pre><code># Configure priority routing\nnotifications.configure_routing({\n    \"critical\": [\"telegram\", \"email\", \"discord\"],\n    \"warning\": [\"discord\", \"telegram\"],\n    \"info\": [\"discord\"],\n    \"debug\": [\"console\"]\n})\n\n# Send with priority\nawait notifications.send(\"Server error!\", priority=\"critical\")\nawait notifications.send(\"New follower!\", priority=\"info\")\n</code></pre>"},{"location":"guides/notifications/#integration-with-xeepy","title":"Integration with Xeepy","text":""},{"location":"guides/notifications/#automatic-notifications","title":"Automatic Notifications","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.notifications import DiscordNotifier\n\nasync with Xeepy() as x:\n    # Set up notifications\n    x.notifications.add(DiscordNotifier(webhook_url=\"...\"))\n\n    # Enable automatic notifications\n    x.notifications.on_error = True        # Notify on errors\n    x.notifications.on_milestone = True    # Follower milestones\n    x.notifications.on_unfollower = True   # When someone unfollows\n\n    # Run operations - notifications happen automatically\n    await x.unfollow.non_followers(max_unfollows=50)\n</code></pre>"},{"location":"guides/notifications/#event-based-notifications","title":"Event-Based Notifications","text":"<pre><code>async with Xeepy() as x:\n    discord = DiscordNotifier(webhook_url=\"...\")\n\n    # Register event handlers\n    @x.events.on(\"unfollower\")\n    async def on_unfollower(user):\n        await discord.send(f\"\ud83d\ude22 @{user.username} unfollowed you\")\n\n    @x.events.on(\"new_follower\")\n    async def on_new_follower(user):\n        await discord.send(f\"\ud83c\udf89 New follower: @{user.username}\")\n\n    @x.events.on(\"milestone\")\n    async def on_milestone(count):\n        await discord.send(f\"\ud83c\udfc6 Milestone reached: {count:,} followers!\")\n\n    # Start monitoring\n    await x.monitor.start()\n</code></pre>"},{"location":"guides/notifications/#configuration","title":"Configuration","text":""},{"location":"guides/notifications/#environment-variables","title":"Environment Variables","text":"<pre><code># Discord\nexport DISCORD_WEBHOOK=\"https://discord.com/api/webhooks/...\"\n\n# Telegram\nexport TELEGRAM_BOT_TOKEN=\"123456:ABC...\"\nexport TELEGRAM_CHAT_ID=\"123456789\"\n\n# Email\nexport SMTP_HOST=\"smtp.gmail.com\"\nexport SMTP_PORT=\"587\"\nexport SMTP_USER=\"you@gmail.com\"\nexport SMTP_PASSWORD=\"app-password\"\n</code></pre>"},{"location":"guides/notifications/#config-file","title":"Config File","text":"<pre><code># xeepy.toml\n[xeepy.notifications]\nenabled = true\n\n[xeepy.notifications.discord]\nwebhook_url = \"${DISCORD_WEBHOOK}\"\nusername = \"Xeepy Bot\"\navatar_url = \"https://...\"\n\n[xeepy.notifications.telegram]\nbot_token = \"${TELEGRAM_BOT_TOKEN}\"\nchat_id = \"${TELEGRAM_CHAT_ID}\"\n\n[xeepy.notifications.email]\nsmtp_host = \"smtp.gmail.com\"\nsmtp_port = 587\nusername = \"${SMTP_USER}\"\npassword = \"${SMTP_PASSWORD}\"\nfrom_email = \"xeepy@yourdomain.com\"\ndefault_to = \"you@example.com\"\n</code></pre>"},{"location":"guides/notifications/#cli-commands","title":"CLI Commands","text":"<pre><code># Test notifications\nxeepy notify test --channel discord --message \"Test message\"\n\n# Send notification\nxeepy notify send \"Hello!\" --channels discord,telegram\n\n# Configure notifications\nxeepy notify setup discord\nxeepy notify setup telegram\nxeepy notify setup email\n</code></pre>"},{"location":"guides/notifications/#best-practices","title":"Best Practices","text":"<ol> <li>Don't over-notify - Only alert on important events</li> <li>Use appropriate channels - Critical alerts to multiple channels</li> <li>Include context - Add relevant data to messages</li> <li>Test your setup - Verify notifications work before relying on them</li> <li>Secure credentials - Use environment variables for tokens</li> <li>Rate limit - Don't spam notification channels</li> </ol>"},{"location":"guides/scraping/","title":"Scraping Guide","text":"<p>Xeepy provides powerful, flexible scraping capabilities for X/Twitter. This guide covers all scraping features with detailed examples.</p>"},{"location":"guides/scraping/#overview","title":"Overview","text":"<p>Xeepy can scrape virtually any public data from X/Twitter:</p> <ul> <li> <p> Replies</p> <p>Scrape all replies to any tweet</p> </li> <li> <p> Profiles</p> <p>Get detailed user profile information</p> </li> <li> <p> Followers</p> <p>Extract follower lists with metadata</p> </li> <li> <p> Following</p> <p>Get who a user follows</p> </li> <li> <p> Tweets</p> <p>Scrape user tweets and timelines</p> </li> <li> <p> Threads</p> <p>Unroll and extract full threads</p> </li> <li> <p> Search</p> <p>Search tweets with advanced filters</p> </li> <li> <p> Hashtags</p> <p>Scrape tweets by hashtag</p> </li> <li> <p> Media</p> <p>Extract images and videos</p> </li> <li> <p> Lists</p> <p>Scrape list members and tweets</p> </li> </ul>"},{"location":"guides/scraping/#quick-start","title":"Quick Start","text":"<pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    # Scrape 100 replies to a tweet\n    replies = await x.scrape.replies(\n        \"https://x.com/elonmusk/status/1234567890\",\n        limit=100\n    )\n\n    # Export to CSV\n    x.export.to_csv(replies, \"replies.csv\")\n</code></pre>"},{"location":"guides/scraping/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/scraping/#scrape-with-progress","title":"Scrape with Progress","text":"<pre><code>async with Xeepy() as x:\n    async for tweet in x.scrape.tweets_stream(\"username\", limit=1000):\n        print(f\"Got tweet: {tweet.text[:50]}...\")\n\n        # Process each tweet as it comes\n        await process_tweet(tweet)\n</code></pre>"},{"location":"guides/scraping/#scrape-multiple-users","title":"Scrape Multiple Users","text":"<pre><code>async with Xeepy() as x:\n    users = [\"user1\", \"user2\", \"user3\"]\n\n    for user in users:\n        tweets = await x.scrape.tweets(user, limit=100)\n        x.export.to_csv(tweets, f\"{user}_tweets.csv\")\n</code></pre>"},{"location":"guides/scraping/#handle-large-datasets","title":"Handle Large Datasets","text":"<pre><code>async with Xeepy() as x:\n    # Scrape in batches to avoid memory issues\n    async for batch in x.scrape.followers_batched(\"popular_user\", batch_size=100):\n        # Process and save each batch\n        x.export.append_csv(batch, \"followers.csv\")\n        print(f\"Processed {len(batch)} followers\")\n</code></pre>"},{"location":"guides/scraping/#rate-limiting","title":"Rate Limiting","text":"<p>Xeepy automatically handles rate limiting to protect your account:</p> <pre><code>async with Xeepy() as x:\n    # Default: 20 requests/minute (safe)\n    replies = await x.scrape.replies(url, limit=1000)\n\n    # Customize rate limit\n    x.config.rate_limit.requests_per_minute = 30\n</code></pre> <p>Be Respectful</p> <p>Higher rate limits increase detection risk. Stick to defaults unless you have a specific need.</p>"},{"location":"guides/scraping/#data-models","title":"Data Models","text":"<p>All scraped data uses typed models for consistency:</p> <pre><code># Tweet model\nreply.id           # Tweet ID\nreply.text         # Tweet content\nreply.author       # User model\nreply.created_at   # Datetime\nreply.likes        # Like count\nreply.retweets     # Retweet count\nreply.replies      # Reply count\nreply.url          # Tweet URL\n\n# User model\nuser.id            # User ID\nuser.username      # Handle (without @)\nuser.name          # Display name\nuser.bio           # Bio/description\nuser.followers_count\nuser.following_count\nuser.tweet_count\nuser.verified\nuser.created_at\n</code></pre>"},{"location":"guides/scraping/#export-options","title":"Export Options","text":"<p>Every scraping function integrates with export:</p> <pre><code>async with Xeepy() as x:\n    data = await x.scrape.replies(url, limit=100)\n\n    # Multiple export formats\n    x.export.to_csv(data, \"data.csv\")\n    x.export.to_json(data, \"data.json\")\n    x.export.to_excel(data, \"data.xlsx\")\n    x.export.to_parquet(data, \"data.parquet\")\n\n    # Database export\n    await x.export.to_database(data, \"sqlite:///data.db\")\n</code></pre>"},{"location":"guides/scraping/#best-practices","title":"Best Practices","text":"<ol> <li>Start small - Test with <code>limit=10</code> before scaling up</li> <li>Use caching - Avoid re-scraping the same data</li> <li>Respect rate limits - Don't disable built-in protections</li> <li>Handle errors - Network issues happen; use try/except</li> <li>Store incrementally - Save data as you scrape for large jobs</li> </ol>"},{"location":"guides/scraping/#detailed-guides","title":"Detailed Guides","text":"<p>Choose a specific scraping topic:</p> <ul> <li>Replies Scraping - Extract conversation threads</li> <li>Profile Scraping - Get user details</li> <li>Followers Scraping - Build follower lists</li> <li>Tweet Scraping - Get user timelines</li> <li>Search Scraping - Find tweets by query</li> <li>Hashtag Scraping - Monitor hashtag activity</li> <li>Thread Unrolling - Extract full threads</li> <li>Media Scraping - Download images/videos</li> </ul>"},{"location":"guides/scraping/followers/","title":"Followers Scraping","text":"<p>Extract and analyze follower lists with filtering and export options.</p>"},{"location":"guides/scraping/followers/#basic-follower-scraping","title":"Basic Follower Scraping","text":"<pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    # Get followers with limit\n    followers = await x.scrape.followers(\"elonmusk\", limit=1000)\n\n    print(f\"Scraped {len(followers)} followers\")\n\n    for follower in followers[:10]:\n        print(f\"@{follower.username}: {follower.followers_count:,} followers\")\n</code></pre>"},{"location":"guides/scraping/followers/#follower-data-model","title":"Follower Data Model","text":"<pre><code>@dataclass\nclass Follower:\n    user_id: str\n    username: str\n    name: str\n    bio: str\n    followers_count: int\n    following_count: int\n    verified: bool\n    profile_image_url: str\n    followed_at: datetime  # When they followed (if available)\n</code></pre>"},{"location":"guides/scraping/followers/#advanced-scraping-options","title":"Advanced Scraping Options","text":""},{"location":"guides/scraping/followers/#with-filtering","title":"With Filtering","text":"<pre><code>async with Xeepy() as x:\n    # Scrape all followers (no limit)\n    all_followers = await x.scrape.followers(\"target_user\")\n\n    # Filter by follower count\n    verified_only = await x.scrape.followers(\n        \"target_user\",\n        min_followers=10000,\n        verified_only=True\n    )\n\n    # Filter by account age\n    established = await x.scrape.followers(\n        \"target_user\",\n        min_account_age_days=365\n    )\n</code></pre>"},{"location":"guides/scraping/followers/#with-progress-tracking","title":"With Progress Tracking","text":"<pre><code>async def scrape_with_progress(username: str, target: int = 10000):\n    \"\"\"Scrape large follower lists with progress updates\"\"\"\n\n    async with Xeepy() as x:\n        followers = []\n        cursor = None\n\n        while len(followers) &lt; target:\n            batch, cursor = await x.scrape.followers_batch(\n                username,\n                cursor=cursor,\n                batch_size=200\n            )\n\n            followers.extend(batch)\n            progress = min(len(followers) / target * 100, 100)\n            print(f\"Progress: {progress:.1f}% ({len(followers):,} followers)\")\n\n            if not cursor:  # No more results\n                break\n\n        return followers[:target]\n\nfollowers = await scrape_with_progress(\"target_user\", 50000)\n</code></pre>"},{"location":"guides/scraping/followers/#resumable-scraping","title":"Resumable Scraping","text":"<pre><code>import json\n\nasync def resumable_follower_scrape(username: str, state_file: str = None):\n    \"\"\"Resume scraping from where you left off\"\"\"\n\n    state_file = state_file or f\"{username}_followers_state.json\"\n\n    # Load previous state\n    try:\n        with open(state_file) as f:\n            state = json.load(f)\n            followers = state[\"followers\"]\n            cursor = state[\"cursor\"]\n            print(f\"Resuming from {len(followers)} followers\")\n    except FileNotFoundError:\n        followers = []\n        cursor = None\n\n    async with Xeepy() as x:\n        try:\n            while True:\n                batch, cursor = await x.scrape.followers_batch(\n                    username,\n                    cursor=cursor,\n                    batch_size=200\n                )\n\n                if not batch:\n                    break\n\n                followers.extend([{\n                    \"username\": f.username,\n                    \"name\": f.name,\n                    \"followers_count\": f.followers_count\n                } for f in batch])\n\n                print(f\"Total: {len(followers)}\")\n\n                # Save state periodically\n                if len(followers) % 1000 == 0:\n                    with open(state_file, \"w\") as f:\n                        json.dump({\"followers\": followers, \"cursor\": cursor}, f)\n\n        except KeyboardInterrupt:\n            print(\"Interrupted, saving state...\")\n        finally:\n            with open(state_file, \"w\") as f:\n                json.dump({\"followers\": followers, \"cursor\": cursor}, f)\n\n    return followers\n</code></pre>"},{"location":"guides/scraping/followers/#follower-analysis","title":"Follower Analysis","text":""},{"location":"guides/scraping/followers/#audience-segmentation","title":"Audience Segmentation","text":"<pre><code>async def segment_followers(username: str, limit: int = 1000):\n    \"\"\"Segment followers by size and characteristics\"\"\"\n\n    async with Xeepy() as x:\n        followers = await x.scrape.followers(username, limit=limit)\n\n        segments = {\n            \"mega_influencers\": [],      # 1M+\n            \"macro_influencers\": [],     # 100k-1M\n            \"micro_influencers\": [],     # 10k-100k\n            \"nano_influencers\": [],      # 1k-10k\n            \"regular_users\": [],         # &lt;1k\n            \"verified_users\": [],\n            \"new_accounts\": [],          # &lt;30 days old\n        }\n\n        for f in followers:\n            # By follower count\n            if f.followers_count &gt;= 1_000_000:\n                segments[\"mega_influencers\"].append(f)\n            elif f.followers_count &gt;= 100_000:\n                segments[\"macro_influencers\"].append(f)\n            elif f.followers_count &gt;= 10_000:\n                segments[\"micro_influencers\"].append(f)\n            elif f.followers_count &gt;= 1_000:\n                segments[\"nano_influencers\"].append(f)\n            else:\n                segments[\"regular_users\"].append(f)\n\n            # Additional segments\n            if f.verified:\n                segments[\"verified_users\"].append(f)\n\n        # Print summary\n        print(f\"Audience Segmentation for @{username}\")\n        print(\"=\" * 50)\n\n        for segment, users in segments.items():\n            pct = len(users) / len(followers) * 100\n            print(f\"{segment}: {len(users)} ({pct:.1f}%)\")\n\n        return segments\n</code></pre>"},{"location":"guides/scraping/followers/#geographic-analysis-from-bios","title":"Geographic Analysis (from bios)","text":"<pre><code>import re\nfrom collections import Counter\n\nasync def analyze_follower_locations(username: str, limit: int = 1000):\n    \"\"\"Analyze follower locations from their bios\"\"\"\n\n    async with Xeepy() as x:\n        followers = await x.scrape.followers(username, limit=limit)\n\n        # Common location patterns\n        cities = Counter()\n        countries = Counter()\n\n        city_patterns = [\n            r'(?i)new york|nyc|manhattan',\n            r'(?i)los angeles|la|hollywood',\n            r'(?i)san francisco|sf|bay area',\n            r'(?i)london|uk',\n            r'(?i)toronto|canada',\n            r'(?i)paris|france',\n            r'(?i)tokyo|japan',\n            r'(?i)berlin|germany',\n            r'(?i)sydney|australia',\n        ]\n\n        for f in followers:\n            location = f\"{f.bio} {f.location}\".lower() if f.bio else \"\"\n\n            for pattern in city_patterns:\n                if re.search(pattern, location):\n                    city = pattern.split('|')[0].replace(r'(?i)', '').title()\n                    cities[city] += 1\n\n        print(\"Top Locations in Follower Base:\")\n        for city, count in cities.most_common(10):\n            pct = count / len(followers) * 100\n            print(f\"  {city}: {count} ({pct:.1f}%)\")\n\n        return cities\n</code></pre>"},{"location":"guides/scraping/followers/#interest-analysis","title":"Interest Analysis","text":"<pre><code>async def analyze_follower_interests(username: str, limit: int = 500):\n    \"\"\"Analyze common interests from follower bios\"\"\"\n\n    interest_keywords = {\n        \"tech\": [\"developer\", \"engineer\", \"coding\", \"programmer\", \"software\", \"tech\", \"ai\", \"ml\"],\n        \"crypto\": [\"crypto\", \"bitcoin\", \"web3\", \"nft\", \"blockchain\", \"defi\", \"ethereum\"],\n        \"business\": [\"founder\", \"ceo\", \"entrepreneur\", \"startup\", \"investor\", \"vc\"],\n        \"marketing\": [\"marketing\", \"growth\", \"seo\", \"brand\", \"content\", \"social media\"],\n        \"design\": [\"designer\", \"ux\", \"ui\", \"creative\", \"art\", \"visual\"],\n        \"finance\": [\"finance\", \"trading\", \"stocks\", \"investment\", \"fintech\"],\n        \"writing\": [\"writer\", \"author\", \"blogger\", \"journalist\", \"content creator\"],\n    }\n\n    async with Xeepy() as x:\n        followers = await x.scrape.followers(username, limit=limit)\n\n        interest_counts = Counter()\n\n        for f in followers:\n            bio = f.bio.lower() if f.bio else \"\"\n\n            for interest, keywords in interest_keywords.items():\n                if any(kw in bio for kw in keywords):\n                    interest_counts[interest] += 1\n\n        print(f\"Interest Distribution for @{username}'s followers:\")\n        print(\"=\" * 50)\n\n        for interest, count in interest_counts.most_common():\n            pct = count / len(followers) * 100\n            bar = \"\u2588\" * int(pct / 2)\n            print(f\"{interest:12} {bar} {pct:.1f}%\")\n\n        return interest_counts\n</code></pre>"},{"location":"guides/scraping/followers/#comparison-operations","title":"Comparison Operations","text":""},{"location":"guides/scraping/followers/#find-mutual-followers","title":"Find Mutual Followers","text":"<pre><code>async def find_mutual_followers(user1: str, user2: str, limit: int = 1000):\n    \"\"\"Find users who follow both accounts\"\"\"\n\n    async with Xeepy() as x:\n        followers1 = await x.scrape.followers(user1, limit=limit)\n        followers2 = await x.scrape.followers(user2, limit=limit)\n\n        set1 = {f.username for f in followers1}\n        set2 = {f.username for f in followers2}\n\n        mutual = set1 &amp; set2\n        only_user1 = set1 - set2\n        only_user2 = set2 - set1\n\n        print(f\"Follower Comparison: @{user1} vs @{user2}\")\n        print(\"=\" * 50)\n        print(f\"@{user1} followers: {len(set1)}\")\n        print(f\"@{user2} followers: {len(set2)}\")\n        print(f\"Mutual followers: {len(mutual)}\")\n        print(f\"Overlap: {len(mutual) / min(len(set1), len(set2)) * 100:.1f}%\")\n\n        return {\n            \"mutual\": list(mutual),\n            \"only_user1\": list(only_user1),\n            \"only_user2\": list(only_user2)\n        }\n</code></pre>"},{"location":"guides/scraping/followers/#find-influencer-overlap","title":"Find Influencer Overlap","text":"<pre><code>async def find_influential_shared_followers(accounts: list, limit: int = 500):\n    \"\"\"Find influential users who follow multiple accounts\"\"\"\n\n    async with Xeepy() as x:\n        all_followers = {}\n\n        for account in accounts:\n            followers = await x.scrape.followers(account, limit=limit)\n            for f in followers:\n                if f.username not in all_followers:\n                    all_followers[f.username] = {\n                        \"user\": f,\n                        \"follows\": []\n                    }\n                all_followers[f.username][\"follows\"].append(account)\n\n        # Find users following multiple accounts\n        multi_followers = {\n            username: data\n            for username, data in all_followers.items()\n            if len(data[\"follows\"]) &gt;= 2\n        }\n\n        # Sort by follower count\n        influential = sorted(\n            multi_followers.items(),\n            key=lambda x: -x[1][\"user\"].followers_count\n        )\n\n        print(f\"Influential users following multiple accounts:\")\n        for username, data in influential[:20]:\n            user = data[\"user\"]\n            accounts_followed = \", \".join(data[\"follows\"])\n            print(f\"  @{username} ({user.followers_count:,} followers)\")\n            print(f\"    Follows: {accounts_followed}\")\n\n        return influential\n</code></pre>"},{"location":"guides/scraping/followers/#export-options","title":"Export Options","text":"<pre><code>async with Xeepy() as x:\n    followers = await x.scrape.followers(\"target_user\", limit=1000)\n\n    # Basic export\n    x.export.to_csv(followers, \"followers.csv\")\n    x.export.to_json(followers, \"followers.json\")\n    x.export.to_excel(followers, \"followers.xlsx\")\n\n    # Custom fields\n    x.export.to_csv(\n        followers,\n        \"followers_simple.csv\",\n        fields=[\"username\", \"followers_count\", \"bio\"]\n    )\n\n    # With analytics\n    x.export.to_csv(\n        followers,\n        \"followers_analyzed.csv\",\n        fields=[\"username\", \"followers_count\", \"following_count\", \"ratio\"],\n        computed_fields={\n            \"ratio\": lambda f: f.followers_count / max(f.following_count, 1)\n        }\n    )\n</code></pre>"},{"location":"guides/scraping/followers/#best-practices","title":"Best Practices","text":"<p>Large Scrapes</p> <ul> <li>Use resumable scraping for 10k+ followers</li> <li>Save state frequently to handle interruptions</li> <li>Consider overnight runs for massive accounts</li> <li>Use cursor-based pagination</li> </ul> <p>Rate Limits</p> <ul> <li>Built-in rate limiting protects your account</li> <li>Large accounts may take hours to fully scrape</li> <li>Sample smaller sets for quick analysis</li> </ul> <p>Data Freshness</p> <ul> <li>Follower data changes constantly</li> <li>Cache results with timestamps</li> <li>Re-scrape periodically for tracking</li> </ul>"},{"location":"guides/scraping/followers/#next-steps","title":"Next Steps","text":"<p> Following Scraping - Scrape who users follow</p> <p> Unfollower Tracking - Detect unfollows</p> <p> Network Analysis - Map influence networks</p>"},{"location":"guides/scraping/following/","title":"Scraping Following Lists","text":"<p>Retrieve and analyze the accounts a user follows to understand their interests, find potential connections, and build targeted outreach lists.</p>"},{"location":"guides/scraping/following/#overview","title":"Overview","text":"<p>The following scraper extracts comprehensive data about accounts a user follows, including profile information, follower counts, and activity metrics. This is invaluable for competitive analysis, lead generation, and understanding network dynamics.</p>"},{"location":"guides/scraping/following/#use-cases","title":"Use Cases","text":"<ul> <li>Competitive Analysis: See who your competitors follow for partnership insights</li> <li>Lead Generation: Build lists of potential customers from relevant accounts</li> <li>Network Mapping: Understand industry connections and influencer relationships</li> <li>Content Discovery: Find accounts posting content your audience engages with</li> </ul>"},{"location":"guides/scraping/following/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\n\nasync def scrape_following():\n    async with Xeepy() as x:\n        # Get accounts a user follows\n        following = await x.scrape.following(\"elonmusk\", limit=500)\n\n        for user in following:\n            print(f\"@{user.username}: {user.followers_count} followers\")\n\n        # Export to CSV for analysis\n        x.export.to_csv(following, \"following_list.csv\")\n\nasyncio.run(scrape_following())\n</code></pre>"},{"location":"guides/scraping/following/#advanced-options","title":"Advanced Options","text":"<pre><code>async def advanced_following_scrape():\n    async with Xeepy() as x:\n        # Scrape with filtering options\n        following = await x.scrape.following(\n            username=\"techcrunch\",\n            limit=1000,\n            include_protected=False,  # Skip private accounts\n            min_followers=1000,       # Only accounts with 1k+ followers\n            verified_only=False,      # Include non-verified accounts\n            active_days=30            # Only accounts active in last 30 days\n        )\n\n        # Filter by account type\n        influencers = [u for u in following if u.followers_count &gt; 10000]\n        brands = [u for u in following if u.verified and u.followers_count &gt; 50000]\n\n        return following\n\nasyncio.run(advanced_following_scrape())\n</code></pre>"},{"location":"guides/scraping/following/#batch-processing-multiple-accounts","title":"Batch Processing Multiple Accounts","text":"<pre><code>async def batch_following_analysis():\n    async with Xeepy() as x:\n        target_accounts = [\"user1\", \"user2\", \"user3\"]\n        all_following = {}\n\n        for account in target_accounts:\n            following = await x.scrape.following(account, limit=500)\n            all_following[account] = following\n            print(f\"@{account} follows {len(following)} accounts\")\n\n        # Find common follows across accounts\n        sets = [set(u.username for u in f) for f in all_following.values()]\n        common = sets[0].intersection(*sets[1:])\n        print(f\"Common follows: {len(common)}\")\n\nasyncio.run(batch_following_analysis())\n</code></pre>"},{"location":"guides/scraping/following/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>username</code> str required Target username to scrape <code>limit</code> int 100 Maximum accounts to retrieve <code>include_protected</code> bool True Include private accounts <code>min_followers</code> int 0 Minimum follower count filter <code>verified_only</code> bool False Only verified accounts <code>active_days</code> int None Filter by recent activity <p>Rate Limiting</p> <p>The scraper automatically handles rate limits. For large following lists (10k+), expect the operation to take several minutes. Use <code>limit</code> parameter to control batch sizes.</p> <p>Private Accounts</p> <p>You can only scrape following lists from public accounts or accounts that follow you back.</p>"},{"location":"guides/scraping/following/#best-practices","title":"Best Practices","text":"<ol> <li>Start Small: Test with <code>limit=100</code> before running large scrapes</li> <li>Use Filters: Apply <code>min_followers</code> to focus on influential accounts</li> <li>Schedule Off-Peak: Run large scrapes during low-traffic hours</li> <li>Cache Results: Store results locally to avoid repeated scraping</li> <li>Respect Privacy: Only scrape public information for legitimate purposes</li> </ol>"},{"location":"guides/scraping/following/#data-export","title":"Data Export","text":"<pre><code>async def export_following_data():\n    async with Xeepy() as x:\n        following = await x.scrape.following(\"username\", limit=500)\n\n        # Multiple export formats\n        x.export.to_csv(following, \"following.csv\")\n        x.export.to_json(following, \"following.json\")\n        x.export.to_excel(following, \"following.xlsx\")\n\nasyncio.run(export_following_data())\n</code></pre>"},{"location":"guides/scraping/following/#related-guides","title":"Related Guides","text":"<ul> <li>Scraping Followers</li> <li>Audience Insights</li> <li>Competitor Analysis</li> </ul>"},{"location":"guides/scraping/hashtags/","title":"Hashtag Tweet Scraping","text":"<p>Collect and analyze tweets containing specific hashtags to track trends, measure campaign performance, and understand community conversations.</p>"},{"location":"guides/scraping/hashtags/#overview","title":"Overview","text":"<p>Hashtag scraping retrieves tweets using specific hashtags, providing insights into trending topics, campaign reach, and community engagement. This is essential for marketers, researchers, and community managers.</p>"},{"location":"guides/scraping/hashtags/#use-cases","title":"Use Cases","text":"<ul> <li>Campaign Tracking: Measure hashtag campaign performance</li> <li>Trend Analysis: Monitor trending hashtags in your industry</li> <li>Community Research: Study conversations around specific topics</li> <li>Event Monitoring: Track event hashtags in real-time</li> <li>Content Discovery: Find shareable content in your niche</li> </ul>"},{"location":"guides/scraping/hashtags/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\n\nasync def scrape_hashtag():\n    async with Xeepy() as x:\n        # Scrape tweets with a hashtag\n        tweets = await x.scrape.hashtag(\"#Python\", limit=100)\n\n        for tweet in tweets:\n            print(f\"@{tweet.author.username}: {tweet.text[:60]}...\")\n            print(f\"  Likes: {tweet.likes} | RTs: {tweet.retweets}\\n\")\n\n        # Export results\n        x.export.to_csv(tweets, \"python_hashtag.csv\")\n\nasyncio.run(scrape_hashtag())\n</code></pre>"},{"location":"guides/scraping/hashtags/#advanced-hashtag-scraping","title":"Advanced Hashtag Scraping","text":"<pre><code>async def advanced_hashtag_scrape():\n    async with Xeepy() as x:\n        # Scrape with filtering options\n        tweets = await x.scrape.hashtag(\n            hashtag=\"#MachineLearning\",\n            limit=500,\n            sort_by=\"Latest\",          # Latest, Top, People, Media\n            lang=\"en\",                 # Language filter\n            since=\"2024-01-01\",        # Date range\n            until=\"2024-12-31\",\n            min_likes=10,              # Quality filter\n            include_retweets=False,    # Original tweets only\n            verified_only=False        # All users\n        )\n\n        print(f\"Found {len(tweets)} tweets with #MachineLearning\")\n\n        # Engagement metrics\n        total_likes = sum(t.likes for t in tweets)\n        total_rts = sum(t.retweets for t in tweets)\n        print(f\"Total engagement: {total_likes} likes, {total_rts} RTs\")\n\nasyncio.run(advanced_hashtag_scrape())\n</code></pre>"},{"location":"guides/scraping/hashtags/#multiple-hashtag-analysis","title":"Multiple Hashtag Analysis","text":"<pre><code>async def multi_hashtag_analysis():\n    async with Xeepy() as x:\n        hashtags = [\"#Python\", \"#JavaScript\", \"#Rust\", \"#Go\"]\n        results = {}\n\n        for tag in hashtags:\n            tweets = await x.scrape.hashtag(tag, limit=200)\n\n            # Calculate metrics\n            results[tag] = {\n                \"count\": len(tweets),\n                \"total_likes\": sum(t.likes for t in tweets),\n                \"total_retweets\": sum(t.retweets for t in tweets),\n                \"unique_authors\": len(set(t.author.username for t in tweets)),\n                \"avg_engagement\": sum(t.likes + t.retweets for t in tweets) / len(tweets) if tweets else 0\n            }\n\n        # Compare hashtags\n        print(\"Hashtag Performance Comparison:\")\n        print(\"-\" * 60)\n        for tag, metrics in results.items():\n            print(f\"{tag}:\")\n            print(f\"  Tweets: {metrics['count']}\")\n            print(f\"  Avg engagement: {metrics['avg_engagement']:.1f}\")\n            print(f\"  Unique authors: {metrics['unique_authors']}\\n\")\n\nasyncio.run(multi_hashtag_analysis())\n</code></pre>"},{"location":"guides/scraping/hashtags/#campaign-hashtag-tracking","title":"Campaign Hashtag Tracking","text":"<pre><code>async def track_campaign_hashtag():\n    async with Xeepy() as x:\n        campaign_tag = \"#YourCampaign2024\"\n\n        tweets = await x.scrape.hashtag(\n            campaign_tag,\n            limit=1000,\n            include_retweets=True\n        )\n\n        # Campaign metrics\n        original_tweets = [t for t in tweets if not t.is_retweet]\n        retweets = [t for t in tweets if t.is_retweet]\n\n        print(f\"Campaign: {campaign_tag}\")\n        print(f\"Total mentions: {len(tweets)}\")\n        print(f\"Original tweets: {len(original_tweets)}\")\n        print(f\"Retweets: {len(retweets)}\")\n\n        # Top contributors\n        from collections import Counter\n        contributors = Counter(t.author.username for t in original_tweets)\n        print(\"\\nTop contributors:\")\n        for user, count in contributors.most_common(10):\n            print(f\"  @{user}: {count} tweets\")\n\n        # Reach estimation\n        total_followers = sum(t.author.followers_count for t in original_tweets)\n        print(f\"\\nEstimated reach: {total_followers:,} followers\")\n\nasyncio.run(track_campaign_hashtag())\n</code></pre>"},{"location":"guides/scraping/hashtags/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>hashtag</code> str required Hashtag to search (with or without #) <code>limit</code> int 100 Maximum tweets to retrieve <code>sort_by</code> str \"Top\" Latest, Top, People, Media <code>lang</code> str None Language code filter <code>since</code> str None Start date (YYYY-MM-DD) <code>until</code> str None End date (YYYY-MM-DD) <code>min_likes</code> int 0 Minimum likes filter <code>include_retweets</code> bool True Include retweets <p>Hashtag Formatting</p> <p>You can pass the hashtag with or without the <code>#</code> symbol. Both <code>\"#Python\"</code> and <code>\"Python\"</code> work correctly.</p> <p>Popular Hashtags</p> <p>Very popular hashtags may return thousands of results. Use <code>limit</code> and date filters to manage data volume.</p>"},{"location":"guides/scraping/hashtags/#real-time-hashtag-monitoring","title":"Real-Time Hashtag Monitoring","text":"<pre><code>async def monitor_hashtag():\n    async with Xeepy() as x:\n        hashtag = \"#BreakingNews\"\n        seen_ids = set()\n\n        print(f\"Monitoring {hashtag}...\")\n\n        while True:\n            tweets = await x.scrape.hashtag(\n                hashtag,\n                limit=50,\n                sort_by=\"Latest\"\n            )\n\n            new_tweets = [t for t in tweets if t.id not in seen_ids]\n\n            for tweet in new_tweets:\n                seen_ids.add(tweet.id)\n                print(f\"\\n[NEW] @{tweet.author.username}\")\n                print(f\"  {tweet.text[:100]}...\")\n                print(f\"  Likes: {tweet.likes} | RTs: {tweet.retweets}\")\n\n            await asyncio.sleep(30)  # Check every 30 seconds\n\n# asyncio.run(monitor_hashtag())\n</code></pre>"},{"location":"guides/scraping/hashtags/#hashtag-trend-analysis","title":"Hashtag Trend Analysis","text":"<pre><code>async def analyze_hashtag_trends():\n    async with Xeepy() as x:\n        tweets = await x.scrape.hashtag(\"#AI\", limit=500)\n\n        # Group by date\n        from collections import defaultdict\n        daily_counts = defaultdict(int)\n        daily_engagement = defaultdict(int)\n\n        for tweet in tweets:\n            date = tweet.created_at.strftime(\"%Y-%m-%d\")\n            daily_counts[date] += 1\n            daily_engagement[date] += tweet.likes + tweet.retweets\n\n        print(\"Daily hashtag activity:\")\n        for date in sorted(daily_counts.keys()):\n            print(f\"  {date}: {daily_counts[date]} tweets, {daily_engagement[date]} engagement\")\n\nasyncio.run(analyze_hashtag_trends())\n</code></pre>"},{"location":"guides/scraping/hashtags/#best-practices","title":"Best Practices","text":"<ol> <li>Start Recent: Use <code>sort_by=\"Latest\"</code> for real-time monitoring</li> <li>Filter Quality: Set <code>min_likes</code> to focus on engaging content</li> <li>Track Variations: Monitor hashtag variations (e.g., #AI, #ArtificialIntelligence)</li> <li>Exclude Retweets: Use <code>include_retweets=False</code> for unique content analysis</li> <li>Set Date Ranges: Bound searches for campaign-specific analysis</li> <li>Monitor Competitors: Track competitor campaign hashtags</li> </ol>"},{"location":"guides/scraping/hashtags/#related-guides","title":"Related Guides","text":"<ul> <li>Search Results Scraping</li> <li>Keyword Monitoring</li> <li>Engagement Analysis</li> </ul>"},{"location":"guides/scraping/lists/","title":"Twitter List Scraping","text":"<p>Extract members and tweets from Twitter Lists to discover curated communities, track industry experts, and monitor focused content streams.</p>"},{"location":"guides/scraping/lists/#overview","title":"Overview","text":"<p>Twitter Lists are curated collections of accounts organized around themes, industries, or interests. The list scraper extracts list metadata, members, and recent tweets, providing access to pre-filtered high-quality content streams.</p>"},{"location":"guides/scraping/lists/#use-cases","title":"Use Cases","text":"<ul> <li>Expert Discovery: Find industry experts curated by trusted sources</li> <li>Community Research: Study curated communities in your niche</li> <li>Content Streams: Access focused content without noise</li> <li>Lead Generation: Build prospect lists from relevant lists</li> <li>Competitive Analysis: See who competitors have curated</li> </ul>"},{"location":"guides/scraping/lists/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\n\nasync def scrape_list():\n    async with Xeepy() as x:\n        # Scrape members from a Twitter list\n        members = await x.scrape.list_members(\n            list_url=\"https://x.com/i/lists/123456789\"\n        )\n\n        for user in members:\n            print(f\"@{user.username}: {user.followers_count} followers\")\n            print(f\"  Bio: {user.bio[:60]}...\\n\")\n\n        # Export to CSV\n        x.export.to_csv(members, \"list_members.csv\")\n\nasyncio.run(scrape_list())\n</code></pre>"},{"location":"guides/scraping/lists/#scraping-list-tweets","title":"Scraping List Tweets","text":"<pre><code>async def scrape_list_tweets():\n    async with Xeepy() as x:\n        # Get recent tweets from list timeline\n        tweets = await x.scrape.list_tweets(\n            list_url=\"https://x.com/i/lists/123456789\",\n            limit=200\n        )\n\n        print(f\"Retrieved {len(tweets)} tweets from list\")\n\n        for tweet in tweets:\n            print(f\"@{tweet.author.username}: {tweet.text[:50]}...\")\n            print(f\"  Engagement: {tweet.likes} likes\\n\")\n\nasyncio.run(scrape_list_tweets())\n</code></pre>"},{"location":"guides/scraping/lists/#scraping-users-lists","title":"Scraping User's Lists","text":"<pre><code>async def scrape_user_lists():\n    async with Xeepy() as x:\n        # Get all lists created by a user\n        lists = await x.scrape.user_lists(\n            username=\"elonmusk\",\n            include_subscribed=False  # Only created lists\n        )\n\n        for lst in lists:\n            print(f\"List: {lst.name}\")\n            print(f\"  Description: {lst.description}\")\n            print(f\"  Members: {lst.member_count}\")\n            print(f\"  Subscribers: {lst.subscriber_count}\")\n            print(f\"  URL: {lst.url}\\n\")\n\nasyncio.run(scrape_user_lists())\n</code></pre>"},{"location":"guides/scraping/lists/#advanced-list-member-scraping","title":"Advanced List Member Scraping","text":"<pre><code>async def advanced_list_scraping():\n    async with Xeepy() as x:\n        # Scrape with filtering\n        members = await x.scrape.list_members(\n            list_url=\"https://x.com/i/lists/123456789\",\n            limit=500,\n            min_followers=1000,       # Filter by followers\n            verified_only=False,       # Include non-verified\n            include_profile=True       # Full profile data\n        )\n\n        # Analyze the list\n        total_followers = sum(u.followers_count for u in members)\n        avg_followers = total_followers / len(members) if members else 0\n        verified_count = sum(1 for u in members if u.verified)\n\n        print(f\"List Analysis:\")\n        print(f\"  Total members: {len(members)}\")\n        print(f\"  Verified members: {verified_count}\")\n        print(f\"  Average followers: {avg_followers:,.0f}\")\n        print(f\"  Combined reach: {total_followers:,}\")\n\nasyncio.run(advanced_list_scraping())\n</code></pre>"},{"location":"guides/scraping/lists/#discovering-public-lists","title":"Discovering Public Lists","text":"<pre><code>async def discover_lists():\n    async with Xeepy() as x:\n        # Search for public lists by topic\n        lists = await x.scrape.search_lists(\n            query=\"machine learning\",\n            limit=20\n        )\n\n        print(\"Top ML-related lists:\")\n        for lst in lists:\n            print(f\"\\n{lst.name} by @{lst.owner.username}\")\n            print(f\"  Members: {lst.member_count}\")\n            print(f\"  Subscribers: {lst.subscriber_count}\")\n            print(f\"  {lst.url}\")\n\nasyncio.run(discover_lists())\n</code></pre>"},{"location":"guides/scraping/lists/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>list_url</code> str required List URL or ID <code>limit</code> int 100 Maximum items to retrieve <code>min_followers</code> int 0 Minimum follower filter <code>verified_only</code> bool False Only verified accounts <code>include_profile</code> bool True Include full profile data <code>include_subscribed</code> bool True Include subscribed lists <p>Finding List IDs</p> <p>You can use either the full URL (<code>https://x.com/i/lists/123456789</code>) or just the list ID (<code>123456789</code>). Find list IDs by navigating to the list on Twitter and copying from the URL.</p> <p>Private Lists</p> <p>You can only scrape public lists or private lists you own. Member data is limited based on list visibility settings.</p>"},{"location":"guides/scraping/lists/#batch-list-processing","title":"Batch List Processing","text":"<pre><code>async def batch_process_lists():\n    async with Xeepy() as x:\n        list_urls = [\n            \"https://x.com/i/lists/111\",\n            \"https://x.com/i/lists/222\",\n            \"https://x.com/i/lists/333\",\n        ]\n\n        all_members = {}\n\n        for url in list_urls:\n            members = await x.scrape.list_members(url, limit=200)\n            list_id = url.split(\"/\")[-1]\n            all_members[list_id] = members\n            print(f\"List {list_id}: {len(members)} members\")\n\n        # Find members appearing in multiple lists\n        from collections import Counter\n        all_usernames = []\n        for members in all_members.values():\n            all_usernames.extend(u.username for u in members)\n\n        common = Counter(all_usernames)\n        multi_list = [(u, c) for u, c in common.items() if c &gt; 1]\n\n        print(f\"\\nUsers in multiple lists: {len(multi_list)}\")\n\nasyncio.run(batch_process_lists())\n</code></pre>"},{"location":"guides/scraping/lists/#list-comparison-analysis","title":"List Comparison Analysis","text":"<pre><code>async def compare_lists():\n    async with Xeepy() as x:\n        # Compare two curated lists\n        list1_members = await x.scrape.list_members(\n            \"https://x.com/i/lists/111\",\n            limit=500\n        )\n        list2_members = await x.scrape.list_members(\n            \"https://x.com/i/lists/222\",\n            limit=500\n        )\n\n        set1 = set(u.username for u in list1_members)\n        set2 = set(u.username for u in list2_members)\n\n        overlap = set1 &amp; set2\n        only_list1 = set1 - set2\n        only_list2 = set2 - set1\n\n        print(f\"List 1 members: {len(set1)}\")\n        print(f\"List 2 members: {len(set2)}\")\n        print(f\"Overlap: {len(overlap)} members\")\n        print(f\"Unique to List 1: {len(only_list1)}\")\n        print(f\"Unique to List 2: {len(only_list2)}\")\n\nasyncio.run(compare_lists())\n</code></pre>"},{"location":"guides/scraping/lists/#building-custom-lists","title":"Building Custom Lists","text":"<pre><code>async def build_custom_list():\n    async with Xeepy() as x:\n        # Scrape multiple lists and combine unique members\n        source_lists = [\n            \"https://x.com/i/lists/111\",\n            \"https://x.com/i/lists/222\",\n        ]\n\n        unique_members = {}\n\n        for url in source_lists:\n            members = await x.scrape.list_members(url, limit=200)\n            for member in members:\n                if member.username not in unique_members:\n                    unique_members[member.username] = member\n\n        # Filter for high-value accounts\n        quality_members = [\n            u for u in unique_members.values()\n            if u.followers_count &gt; 5000 and u.tweet_count &gt; 100\n        ]\n\n        print(f\"Quality accounts found: {len(quality_members)}\")\n        x.export.to_csv(quality_members, \"curated_list.csv\")\n\nasyncio.run(build_custom_list())\n</code></pre>"},{"location":"guides/scraping/lists/#best-practices","title":"Best Practices","text":"<ol> <li>Verify List Quality: Check list descriptions and subscriber counts</li> <li>Check Last Updated: Old lists may have inactive members</li> <li>Cross-Reference: Compare multiple lists for validation</li> <li>Filter Strategically: Use follower counts to focus on influential members</li> <li>Respect Curation: Lists represent someone's curation effort</li> <li>Monitor Changes: Track list membership changes over time</li> </ol>"},{"location":"guides/scraping/lists/#related-guides","title":"Related Guides","text":"<ul> <li>Following Scraping</li> <li>Audience Insights</li> <li>Competitor Analysis</li> </ul>"},{"location":"guides/scraping/media/","title":"Media Post Scraping","text":"<p>Extract and download media content from tweets including photos, videos, GIFs, and high-quality image variants.</p>"},{"location":"guides/scraping/media/#overview","title":"Overview","text":"<p>The media scraper retrieves visual content from tweets, enabling archival, analysis, and content curation. It supports downloading photos at various resolutions, videos in multiple qualities, and animated GIFs.</p>"},{"location":"guides/scraping/media/#use-cases","title":"Use Cases","text":"<ul> <li>Content Archival: Preserve media before potential deletion</li> <li>Asset Collection: Build image libraries for inspiration</li> <li>Research: Analyze visual content trends</li> <li>Backup: Archive your own media posts</li> <li>Curation: Collect media for newsletters or compilations</li> </ul>"},{"location":"guides/scraping/media/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\n\nasync def scrape_media():\n    async with Xeepy() as x:\n        # Get media posts from a user\n        media_tweets = await x.scrape.media(\"nasa\", limit=50)\n\n        for tweet in media_tweets:\n            print(f\"Tweet: {tweet.text[:50]}...\")\n            for media in tweet.media:\n                print(f\"  Type: {media.type}\")\n                print(f\"  URL: {media.url}\")\n\nasyncio.run(scrape_media())\n</code></pre>"},{"location":"guides/scraping/media/#downloading-media-files","title":"Downloading Media Files","text":"<pre><code>async def download_media():\n    async with Xeepy() as x:\n        # Download media from specific tweets\n        result = await x.media.download(\n            tweet_ids=[\"123456789\", \"987654321\"],\n            output_dir=\"downloads/media\",\n            photos=True,\n            videos=True,\n            gifs=True,\n            hq_images=True  # Highest quality images\n        )\n\n        print(f\"Downloaded {len(result.files)} files\")\n        for file in result.files:\n            print(f\"  {file.filename}: {file.size_mb:.1f} MB\")\n\nasyncio.run(download_media())\n</code></pre>"},{"location":"guides/scraping/media/#scraping-user-media-gallery","title":"Scraping User Media Gallery","text":"<pre><code>async def scrape_user_gallery():\n    async with Xeepy() as x:\n        # Get all media from a user's media tab\n        media_tweets = await x.scrape.media(\n            username=\"natgeo\",\n            limit=200,\n            media_type=\"all\",       # all, photos, videos\n            include_retweets=False  # Original content only\n        )\n\n        # Categorize by media type\n        photos = []\n        videos = []\n        gifs = []\n\n        for tweet in media_tweets:\n            for media in tweet.media:\n                if media.type == \"photo\":\n                    photos.append(media)\n                elif media.type == \"video\":\n                    videos.append(media)\n                elif media.type == \"animated_gif\":\n                    gifs.append(media)\n\n        print(f\"Photos: {len(photos)}\")\n        print(f\"Videos: {len(videos)}\")\n        print(f\"GIFs: {len(gifs)}\")\n\nasyncio.run(scrape_user_gallery())\n</code></pre>"},{"location":"guides/scraping/media/#batch-media-download","title":"Batch Media Download","text":"<pre><code>async def batch_download():\n    async with Xeepy() as x:\n        users = [\"nasa\", \"natgeo\", \"bbcearth\"]\n\n        for username in users:\n            print(f\"\\nProcessing @{username}...\")\n\n            # Download user's media\n            paths = await x.media.download_user_media(\n                username=username,\n                output_dir=f\"media/{username}\",\n                limit=100,\n                photos=True,\n                videos=True,\n                hq_images=True\n            )\n\n            print(f\"  Downloaded {len(paths)} files\")\n\nasyncio.run(batch_download())\n</code></pre>"},{"location":"guides/scraping/media/#video-quality-options","title":"Video Quality Options","text":"<pre><code>async def download_videos():\n    async with Xeepy() as x:\n        # Download videos with quality preference\n        result = await x.media.download(\n            tweet_ids=[\"123456789\"],\n            output_dir=\"videos\",\n            videos=True,\n            video_quality=\"highest\",  # highest, 720p, 480p, lowest\n            include_thumbnail=True    # Save video thumbnails\n        )\n\n        for file in result.files:\n            if file.type == \"video\":\n                print(f\"Video: {file.filename}\")\n                print(f\"  Resolution: {file.width}x{file.height}\")\n                print(f\"  Duration: {file.duration_seconds}s\")\n                print(f\"  Size: {file.size_mb:.1f} MB\")\n\nasyncio.run(download_videos())\n</code></pre>"},{"location":"guides/scraping/media/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>username</code> str required Target username <code>limit</code> int 50 Maximum media tweets <code>media_type</code> str \"all\" all, photos, videos <code>include_retweets</code> bool True Include retweeted media <code>output_dir</code> str \"media\" Download directory <code>photos</code> bool True Download photos <code>videos</code> bool True Download videos <code>gifs</code> bool True Download GIFs <code>hq_images</code> bool False Highest quality images <code>video_quality</code> str \"highest\" Video quality preference <p>High Quality Images</p> <p>Use <code>hq_images=True</code> to download the largest available image variants. Twitter stores multiple resolutions; this option fetches the original upload quality.</p> <p>Video Limitations</p> <p>Some videos may have DRM or playback restrictions. The scraper downloads the best available quality that's publicly accessible.</p>"},{"location":"guides/scraping/media/#image-quality-variants","title":"Image Quality Variants","text":"<pre><code>async def get_image_variants():\n    async with Xeepy() as x:\n        media_tweets = await x.scrape.media(\"username\", limit=10)\n\n        for tweet in media_tweets:\n            for media in tweet.media:\n                if media.type == \"photo\":\n                    print(f\"\\nImage variants:\")\n                    print(f\"  Thumb: {media.thumb_url}\")\n                    print(f\"  Small: {media.small_url}\")\n                    print(f\"  Medium: {media.medium_url}\")\n                    print(f\"  Large: {media.large_url}\")\n                    print(f\"  Original: {media.original_url}\")\n\nasyncio.run(get_image_variants())\n</code></pre>"},{"location":"guides/scraping/media/#media-metadata-extraction","title":"Media Metadata Extraction","text":"<pre><code>async def extract_media_metadata():\n    async with Xeepy() as x:\n        media_tweets = await x.scrape.media(\"username\", limit=50)\n\n        for tweet in media_tweets:\n            print(f\"\\nTweet ID: {tweet.id}\")\n            print(f\"Posted: {tweet.created_at}\")\n\n            for media in tweet.media:\n                print(f\"\\n  Media ID: {media.id}\")\n                print(f\"  Type: {media.type}\")\n                print(f\"  Dimensions: {media.width}x{media.height}\")\n\n                if media.type == \"video\":\n                    print(f\"  Duration: {media.duration_ms}ms\")\n                    print(f\"  Views: {media.view_count}\")\n\n                # Alt text if available\n                if media.alt_text:\n                    print(f\"  Alt text: {media.alt_text}\")\n\nasyncio.run(extract_media_metadata())\n</code></pre>"},{"location":"guides/scraping/media/#organizing-downloaded-media","title":"Organizing Downloaded Media","text":"<pre><code>async def organized_download():\n    async with Xeepy() as x:\n        from datetime import datetime\n\n        media_tweets = await x.scrape.media(\"username\", limit=100)\n\n        for tweet in media_tweets:\n            # Organize by date and type\n            date_str = tweet.created_at.strftime(\"%Y/%m\")\n\n            for media in tweet.media:\n                type_dir = media.type  # photo, video, animated_gif\n                output_path = f\"media/{date_str}/{type_dir}\"\n\n                await x.media.download_single(\n                    media_url=media.original_url,\n                    output_dir=output_path,\n                    filename=f\"{tweet.id}_{media.id}\"\n                )\n\nasyncio.run(organized_download())\n</code></pre>"},{"location":"guides/scraping/media/#best-practices","title":"Best Practices","text":"<ol> <li>Respect Copyright: Only download media you have rights to use</li> <li>Use HQ Sparingly: High-quality downloads consume more bandwidth and storage</li> <li>Organize by Date: Structure directories by date for easy navigation</li> <li>Check File Sizes: Videos can be large; monitor disk space</li> <li>Handle Errors: Some media may be unavailable; implement error handling</li> <li>Rate Limiting: Space out bulk downloads to avoid rate limits</li> </ol>"},{"location":"guides/scraping/media/#related-guides","title":"Related Guides","text":"<ul> <li>User Tweets Scraping</li> <li>Thread Unrolling</li> <li>Search Scraping</li> </ul>"},{"location":"guides/scraping/profiles/","title":"Profile Scraping","text":"<p>Get detailed user profile information including bios, stats, and metadata.</p>"},{"location":"guides/scraping/profiles/#basic-profile-scraping","title":"Basic Profile Scraping","text":"<pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    # Get single profile\n    profile = await x.scrape.profile(\"elonmusk\")\n\n    print(f\"Name: {profile.name}\")\n    print(f\"Username: @{profile.username}\")\n    print(f\"Bio: {profile.bio}\")\n    print(f\"Followers: {profile.followers_count:,}\")\n    print(f\"Following: {profile.following_count:,}\")\n    print(f\"Tweets: {profile.tweets_count:,}\")\n    print(f\"Verified: {profile.verified}\")\n    print(f\"Created: {profile.created_at}\")\n    print(f\"Location: {profile.location}\")\n    print(f\"Website: {profile.website}\")\n</code></pre>"},{"location":"guides/scraping/profiles/#profile-data-model","title":"Profile Data Model","text":"<pre><code>@dataclass\nclass UserProfile:\n    user_id: str\n    username: str\n    name: str\n    bio: str\n    location: str\n    website: str\n    created_at: datetime\n    followers_count: int\n    following_count: int\n    tweets_count: int\n    likes_count: int\n    verified: bool\n    profile_image_url: str\n    profile_banner_url: str\n    pinned_tweet_id: str\n    is_protected: bool\n</code></pre>"},{"location":"guides/scraping/profiles/#batch-profile-scraping","title":"Batch Profile Scraping","text":"<pre><code>async def scrape_multiple_profiles(usernames: list):\n    \"\"\"Scrape profiles for multiple users\"\"\"\n\n    async with Xeepy() as x:\n        profiles = []\n\n        for username in usernames:\n            try:\n                profile = await x.scrape.profile(username)\n                profiles.append(profile)\n                print(f\"\u2713 @{username}\")\n            except Exception as e:\n                print(f\"\u2717 @{username}: {e}\")\n\n        # Export to CSV\n        x.export.to_csv(profiles, \"profiles.csv\")\n\n        return profiles\n\n# Usage\nusernames = [\"user1\", \"user2\", \"user3\", \"user4\"]\nprofiles = await scrape_multiple_profiles(usernames)\n</code></pre>"},{"location":"guides/scraping/profiles/#profile-analysis","title":"Profile Analysis","text":""},{"location":"guides/scraping/profiles/#follower-to-following-ratio","title":"Follower-to-Following Ratio","text":"<pre><code>async def analyze_profile_quality(username: str):\n    \"\"\"Calculate engagement and influence metrics\"\"\"\n\n    async with Xeepy() as x:\n        profile = await x.scrape.profile(username)\n\n        # Follower/following ratio\n        ratio = profile.followers_count / max(profile.following_count, 1)\n\n        # Account age\n        age_days = (datetime.now() - profile.created_at).days\n\n        # Growth rate (followers per day)\n        growth_rate = profile.followers_count / max(age_days, 1)\n\n        # Engagement estimate\n        tweets_per_day = profile.tweets_count / max(age_days, 1)\n\n        analysis = {\n            \"username\": username,\n            \"followers\": profile.followers_count,\n            \"following\": profile.following_count,\n            \"ratio\": round(ratio, 2),\n            \"account_age_days\": age_days,\n            \"followers_per_day\": round(growth_rate, 2),\n            \"tweets_per_day\": round(tweets_per_day, 2),\n            \"quality_score\": min(ratio * 10, 100)  # Simple score\n        }\n\n        return analysis\n\n# Usage\nanalysis = await analyze_profile_quality(\"target_user\")\nprint(f\"Quality Score: {analysis['quality_score']}\")\n</code></pre>"},{"location":"guides/scraping/profiles/#detect-botfake-accounts","title":"Detect Bot/Fake Accounts","text":"<pre><code>async def detect_suspicious_profile(username: str):\n    \"\"\"Flag potentially fake or bot accounts\"\"\"\n\n    async with Xeepy() as x:\n        profile = await x.scrape.profile(username)\n\n        red_flags = []\n        score = 0\n\n        # Check account age\n        age_days = (datetime.now() - profile.created_at).days\n        if age_days &lt; 30:\n            red_flags.append(\"Account less than 30 days old\")\n            score += 20\n\n        # Check ratio\n        ratio = profile.followers_count / max(profile.following_count, 1)\n        if ratio &lt; 0.1:\n            red_flags.append(\"Very low follower ratio\")\n            score += 25\n\n        # Check tweet count\n        if profile.tweets_count &lt; 10 and age_days &gt; 30:\n            red_flags.append(\"Very few tweets for account age\")\n            score += 15\n\n        # Check for default profile\n        if not profile.bio or len(profile.bio) &lt; 10:\n            red_flags.append(\"Missing or minimal bio\")\n            score += 10\n\n        if not profile.profile_banner_url:\n            red_flags.append(\"No profile banner\")\n            score += 10\n\n        # Check for suspicious patterns\n        if profile.following_count &gt; 5000 and profile.followers_count &lt; 100:\n            red_flags.append(\"Mass following with few followers\")\n            score += 30\n\n        return {\n            \"username\": username,\n            \"suspicion_score\": score,\n            \"red_flags\": red_flags,\n            \"likely_bot\": score &gt;= 50\n        }\n\n# Check a list of users\nasync def audit_followers(username: str):\n    async with Xeepy() as x:\n        followers = await x.scrape.followers(username, limit=100)\n\n        suspicious = []\n        for follower in followers:\n            result = await detect_suspicious_profile(follower.username)\n            if result[\"likely_bot\"]:\n                suspicious.append(result)\n\n        print(f\"Found {len(suspicious)} suspicious accounts out of {len(followers)}\")\n        return suspicious\n</code></pre>"},{"location":"guides/scraping/profiles/#profile-comparison","title":"Profile Comparison","text":"<pre><code>async def compare_profiles(usernames: list):\n    \"\"\"Compare multiple profiles side by side\"\"\"\n\n    async with Xeepy() as x:\n        profiles = []\n\n        for username in usernames:\n            profile = await x.scrape.profile(username)\n            profiles.append({\n                \"username\": username,\n                \"followers\": profile.followers_count,\n                \"following\": profile.following_count,\n                \"tweets\": profile.tweets_count,\n                \"ratio\": profile.followers_count / max(profile.following_count, 1),\n                \"age_days\": (datetime.now() - profile.created_at).days\n            })\n\n        # Print comparison table\n        print(f\"{'Username':&lt;20} {'Followers':&gt;12} {'Following':&gt;12} {'Ratio':&gt;8}\")\n        print(\"-\" * 60)\n\n        for p in sorted(profiles, key=lambda x: -x[\"followers\"]):\n            print(f\"@{p['username']:&lt;19} {p['followers']:&gt;12,} {p['following']:&gt;12,} {p['ratio']:&gt;8.1f}\")\n\n        return profiles\n\n# Compare competitors\nawait compare_profiles([\"competitor1\", \"competitor2\", \"your_account\"])\n</code></pre>"},{"location":"guides/scraping/profiles/#profile-changes-tracking","title":"Profile Changes Tracking","text":"<pre><code>import json\nfrom datetime import datetime\n\nasync def track_profile_changes(username: str, history_file: str = None):\n    \"\"\"Track changes to a profile over time\"\"\"\n\n    history_file = history_file or f\"{username}_history.json\"\n\n    async with Xeepy() as x:\n        profile = await x.scrape.profile(username)\n\n        current = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"followers\": profile.followers_count,\n            \"following\": profile.following_count,\n            \"tweets\": profile.tweets_count,\n            \"bio\": profile.bio,\n            \"name\": profile.name\n        }\n\n        # Load history\n        try:\n            with open(history_file) as f:\n                history = json.load(f)\n        except FileNotFoundError:\n            history = []\n\n        # Check for changes\n        if history:\n            last = history[-1]\n\n            changes = []\n            if current[\"followers\"] != last[\"followers\"]:\n                diff = current[\"followers\"] - last[\"followers\"]\n                changes.append(f\"Followers: {'+' if diff &gt; 0 else ''}{diff}\")\n\n            if current[\"following\"] != last[\"following\"]:\n                diff = current[\"following\"] - last[\"following\"]\n                changes.append(f\"Following: {'+' if diff &gt; 0 else ''}{diff}\")\n\n            if current[\"bio\"] != last[\"bio\"]:\n                changes.append(\"Bio changed\")\n\n            if current[\"name\"] != last[\"name\"]:\n                changes.append(f\"Name: {last['name']} \u2192 {current['name']}\")\n\n            if changes:\n                print(f\"Changes detected for @{username}:\")\n                for change in changes:\n                    print(f\"  \u2022 {change}\")\n\n        # Save current state\n        history.append(current)\n        with open(history_file, \"w\") as f:\n            json.dump(history, f, indent=2)\n\n        return current\n\n# Run periodically (e.g., via cron)\nawait track_profile_changes(\"competitor\")\n</code></pre>"},{"location":"guides/scraping/profiles/#export-options","title":"Export Options","text":"<pre><code>async with Xeepy() as x:\n    profiles = []\n    for user in [\"user1\", \"user2\", \"user3\"]:\n        profiles.append(await x.scrape.profile(user))\n\n    # Export to various formats\n    x.export.to_csv(profiles, \"profiles.csv\")\n    x.export.to_json(profiles, \"profiles.json\")\n    x.export.to_excel(profiles, \"profiles.xlsx\")\n\n    # Custom fields only\n    x.export.to_csv(\n        profiles,\n        \"profiles_simple.csv\",\n        fields=[\"username\", \"followers_count\", \"bio\"]\n    )\n</code></pre>"},{"location":"guides/scraping/profiles/#best-practices","title":"Best Practices","text":"<p>Efficiency Tips</p> <ul> <li>Cache profiles to avoid repeated requests</li> <li>Batch similar operations together</li> <li>Use GraphQL API for bulk operations (faster)</li> </ul> <p>Privacy Considerations</p> <ul> <li>Respect protected/private accounts</li> <li>Don't scrape personal information excessively</li> <li>Follow X's Terms of Service</li> <li>Use data responsibly</li> </ul>"},{"location":"guides/scraping/profiles/#next-steps","title":"Next Steps","text":"<p> Followers Scraping - Get follower lists</p> <p> Bot Detection - AI-powered fake account detection</p> <p> Competitive Analysis - Profile comparison techniques</p>"},{"location":"guides/scraping/replies/","title":"Scraping Replies","text":"<p>Learn how to scrape all replies to any tweet, with filtering, threading, and export options.</p>"},{"location":"guides/scraping/replies/#basic-usage","title":"Basic Usage","text":"<pre><code>from xeepy import Xeepy\n\nasync with Xeepy() as x:\n    replies = await x.scrape.replies(\n        \"https://x.com/elonmusk/status/1234567890\"\n    )\n\n    for reply in replies:\n        print(f\"@{reply.author.username}: {reply.text}\")\n</code></pre>"},{"location":"guides/scraping/replies/#parameters","title":"Parameters","text":"<pre><code>replies = await x.scrape.replies(\n    url: str,              # Tweet URL or ID\n    limit: int = 100,      # Max replies to scrape\n    include_author_replies: bool = True,  # Include OP's replies\n    sort_by: str = \"top\",  # \"top\", \"recent\", \"controversial\"\n    min_likes: int = 0,    # Filter by minimum likes\n    verified_only: bool = False,  # Only verified accounts\n)\n</code></pre>"},{"location":"guides/scraping/replies/#advanced-examples","title":"Advanced Examples","text":""},{"location":"guides/scraping/replies/#get-all-replies-no-limit","title":"Get All Replies (No Limit)","text":"<pre><code>async with Xeepy() as x:\n    # Warning: Can be slow for viral tweets\n    replies = await x.scrape.replies(\n        tweet_url,\n        limit=None  # Get all available replies\n    )\n</code></pre>"},{"location":"guides/scraping/replies/#filter-high-engagement-replies","title":"Filter High-Engagement Replies","text":"<pre><code>async with Xeepy() as x:\n    replies = await x.scrape.replies(\n        tweet_url,\n        limit=500,\n        min_likes=10,  # Only replies with 10+ likes\n        sort_by=\"top\"\n    )\n</code></pre>"},{"location":"guides/scraping/replies/#stream-replies-in-real-time","title":"Stream Replies in Real-Time","text":"<pre><code>async with Xeepy() as x:\n    async for reply in x.scrape.replies_stream(tweet_url):\n        print(f\"New reply: {reply.text[:50]}...\")\n\n        # Process each reply as it arrives\n        if \"question\" in reply.text.lower():\n            await notify_about_question(reply)\n</code></pre>"},{"location":"guides/scraping/replies/#get-reply-threads","title":"Get Reply Threads","text":"<pre><code>async with Xeepy() as x:\n    # Get replies with their nested replies\n    replies = await x.scrape.replies(\n        tweet_url,\n        include_nested=True,  # Get replies to replies\n        max_depth=3           # How deep to go\n    )\n\n    for reply in replies:\n        print(f\"@{reply.author.username}: {reply.text}\")\n        for nested in reply.replies:\n            print(f\"  \u2514\u2500 @{nested.author.username}: {nested.text}\")\n</code></pre>"},{"location":"guides/scraping/replies/#analyze-sentiment-of-replies","title":"Analyze Sentiment of Replies","text":"<pre><code>from xeepy import Xeepy\nfrom xeepy.ai import ContentGenerator\n\nasync with Xeepy() as x:\n    replies = await x.scrape.replies(tweet_url, limit=100)\n\n    ai = ContentGenerator(provider=\"openai\")\n\n    sentiments = {\"positive\": 0, \"negative\": 0, \"neutral\": 0}\n\n    for reply in replies:\n        result = await ai.analyze_sentiment(reply.text)\n        sentiments[result.label] += 1\n\n    print(f\"Sentiment breakdown: {sentiments}\")\n</code></pre>"},{"location":"guides/scraping/replies/#export-replies","title":"Export Replies","text":"<pre><code>async with Xeepy() as x:\n    replies = await x.scrape.replies(tweet_url, limit=500)\n\n    # CSV with all fields\n    x.export.to_csv(replies, \"replies.csv\")\n\n    # JSON for programmatic use\n    x.export.to_json(replies, \"replies.json\")\n\n    # Excel with formatting\n    x.export.to_excel(replies, \"replies.xlsx\")\n</code></pre>"},{"location":"guides/scraping/replies/#customize-export-fields","title":"Customize Export Fields","text":"<pre><code>async with Xeepy() as x:\n    replies = await x.scrape.replies(tweet_url)\n\n    # Export specific fields only\n    x.export.to_csv(\n        replies,\n        \"replies.csv\",\n        fields=[\"author.username\", \"text\", \"likes\", \"created_at\"]\n    )\n</code></pre>"},{"location":"guides/scraping/replies/#cli-usage","title":"CLI Usage","text":"<pre><code># Basic scrape\nxeepy scrape replies https://x.com/user/status/123\n\n# With options\nxeepy scrape replies https://x.com/user/status/123 \\\n    --limit 500 \\\n    --min-likes 5 \\\n    --sort top \\\n    --output replies.csv\n\n# JSON output\nxeepy scrape replies https://x.com/user/status/123 \\\n    --format json \\\n    --output replies.json\n</code></pre>"},{"location":"guides/scraping/replies/#reply-data-model","title":"Reply Data Model","text":"<pre><code>@dataclass\nclass Reply:\n    id: str                    # Reply tweet ID\n    text: str                  # Reply content\n    author: User               # Author details\n    created_at: datetime       # When posted\n    likes: int                 # Like count\n    retweets: int              # Retweet count\n    replies: int               # Reply count\n    url: str                   # Reply URL\n    in_reply_to: str           # Parent tweet ID\n    conversation_id: str       # Thread root ID\n    is_author_reply: bool      # Is OP replying?\n    media: List[Media]         # Attached media\n    hashtags: List[str]        # Hashtags used\n    mentions: List[str]        # Users mentioned\n    nested_replies: List[Reply]  # Replies to this reply\n</code></pre>"},{"location":"guides/scraping/replies/#use-cases","title":"Use Cases","text":""},{"location":"guides/scraping/replies/#find-questions-to-answer","title":"Find Questions to Answer","text":"<pre><code>async with Xeepy() as x:\n    replies = await x.scrape.replies(my_tweet_url)\n\n    questions = [r for r in replies if \"?\" in r.text]\n\n    print(f\"Found {len(questions)} questions to answer:\")\n    for q in questions:\n        print(f\"  @{q.author.username}: {q.text}\")\n</code></pre>"},{"location":"guides/scraping/replies/#identify-influencer-replies","title":"Identify Influencer Replies","text":"<pre><code>async with Xeepy() as x:\n    replies = await x.scrape.replies(tweet_url, limit=500)\n\n    # Find replies from accounts with 10k+ followers\n    influencer_replies = [\n        r for r in replies \n        if r.author.followers_count &gt;= 10000\n    ]\n\n    print(f\"Influencer replies ({len(influencer_replies)}):\")\n    for r in influencer_replies:\n        print(f\"  @{r.author.username} ({r.author.followers_count:,} followers)\")\n</code></pre>"},{"location":"guides/scraping/replies/#build-community-list-from-engagers","title":"Build Community List from Engagers","text":"<pre><code>async with Xeepy() as x:\n    # Get people who engage with your content\n    replies = await x.scrape.replies(my_tweet_url)\n\n    engaged_users = [r.author for r in replies]\n\n    # Export for follow-up\n    x.export.to_csv(engaged_users, \"engaged_community.csv\")\n</code></pre>"},{"location":"guides/scraping/replies/#troubleshooting","title":"Troubleshooting","text":"Why am I getting fewer replies than expected? <p>X/Twitter's API doesn't return all replies. Hidden replies, spam-filtered replies, and very old replies may not be accessible.</p> Replies are loading slowly <p>Large reply threads require pagination. Consider:</p> <ul> <li>Using <code>limit</code> to cap results</li> <li>Using <code>min_likes</code> to filter</li> <li>Using <code>sort_by=\"top\"</code> to get best replies first</li> </ul> How do I get quote tweets? <p>Quote tweets are separate from replies. Use: <pre><code>quotes = await x.scrape.quotes(tweet_url)\n</code></pre></p>"},{"location":"guides/scraping/search/","title":"Search Results Scraping","text":"<p>Scrape Twitter search results to find relevant conversations, monitor topics, and gather data for research and analysis.</p>"},{"location":"guides/scraping/search/#overview","title":"Overview","text":"<p>The search scraper leverages Twitter's search functionality to find tweets matching specific queries, filters, and criteria. This is essential for social listening, trend monitoring, and targeted data collection.</p>"},{"location":"guides/scraping/search/#use-cases","title":"Use Cases","text":"<ul> <li>Social Listening: Monitor brand mentions and sentiment</li> <li>Trend Research: Study emerging topics and conversations</li> <li>Lead Generation: Find potential customers discussing relevant topics</li> <li>Competitive Intelligence: Track competitor mentions and campaigns</li> <li>Content Discovery: Find viral content in your niche</li> </ul>"},{"location":"guides/scraping/search/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\n\nasync def basic_search():\n    async with Xeepy() as x:\n        # Simple keyword search\n        results = await x.scrape.search(\"python programming\", limit=100)\n\n        for tweet in results:\n            print(f\"@{tweet.author.username}: {tweet.text[:60]}...\")\n            print(f\"  Engagement: {tweet.likes} likes, {tweet.retweets} RTs\\n\")\n\n        # Export results\n        x.export.to_csv(results, \"search_results.csv\")\n\nasyncio.run(basic_search())\n</code></pre>"},{"location":"guides/scraping/search/#advanced-search-queries","title":"Advanced Search Queries","text":"<pre><code>async def advanced_search():\n    async with Xeepy() as x:\n        # Use Twitter advanced search operators\n        results = await x.scrape.search(\n            query=\"machine learning -course -tutorial\",  # Exclude terms\n            limit=200,\n            search_type=\"Latest\",      # Latest, Top, People, Photos, Videos\n            lang=\"en\",                 # Language filter\n            since=\"2024-01-01\",        # Date range\n            until=\"2024-12-31\",\n            min_likes=50,              # Engagement filters\n            min_retweets=10,\n            has_media=True,            # Only tweets with media\n            is_verified=False          # Include all users\n        )\n\n        print(f\"Found {len(results)} tweets matching criteria\")\n\nasyncio.run(advanced_search())\n</code></pre>"},{"location":"guides/scraping/search/#search-operators","title":"Search Operators","text":"<pre><code>async def search_with_operators():\n    async with Xeepy() as x:\n        # From specific user\n        from_user = await x.scrape.search(\"from:elonmusk AI\", limit=50)\n\n        # To specific user (replies)\n        to_user = await x.scrape.search(\"to:OpenAI feedback\", limit=50)\n\n        # Mentions of user\n        mentions = await x.scrape.search(\"@github copilot\", limit=50)\n\n        # Exact phrase\n        exact = await x.scrape.search('\"artificial intelligence\"', limit=50)\n\n        # OR operator\n        either = await x.scrape.search(\"Python OR JavaScript\", limit=50)\n\n        # Filter by engagement\n        popular = await x.scrape.search(\n            \"startup funding min_faves:1000\",\n            limit=50\n        )\n\n        # Near location (approximate)\n        local = await x.scrape.search(\n            \"coffee shop near:NYC within:10mi\",\n            limit=50\n        )\n\nasyncio.run(search_with_operators())\n</code></pre>"},{"location":"guides/scraping/search/#real-time-monitoring","title":"Real-Time Monitoring","text":"<pre><code>async def monitor_search_results():\n    async with Xeepy() as x:\n        seen_ids = set()\n\n        while True:\n            # Search for latest mentions\n            results = await x.scrape.search(\n                query=\"your_brand_name\",\n                search_type=\"Latest\",\n                limit=50\n            )\n\n            # Find new tweets\n            new_tweets = [t for t in results if t.id not in seen_ids]\n\n            for tweet in new_tweets:\n                seen_ids.add(tweet.id)\n                print(f\"New mention: @{tweet.author.username}\")\n                print(f\"  {tweet.text[:100]}...\\n\")\n\n            # Wait before next check\n            await asyncio.sleep(60)  # Check every minute\n\n# asyncio.run(monitor_search_results())\n</code></pre>"},{"location":"guides/scraping/search/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>query</code> str required Search query with operators <code>limit</code> int 100 Maximum results <code>search_type</code> str \"Top\" Latest, Top, People, Photos, Videos <code>lang</code> str None Language code (en, es, etc.) <code>since</code> str None Start date (YYYY-MM-DD) <code>until</code> str None End date (YYYY-MM-DD) <code>min_likes</code> int 0 Minimum likes filter <code>min_retweets</code> int 0 Minimum retweets filter <code>has_media</code> bool False Only tweets with media <p>Search Operators Reference</p> <ul> <li><code>from:user</code> - Tweets from specific user</li> <li><code>to:user</code> - Replies to specific user</li> <li><code>@user</code> - Mentions of user</li> <li><code>\"exact phrase\"</code> - Exact match</li> <li><code>-word</code> - Exclude word</li> <li><code>OR</code> - Match either term</li> <li><code>min_faves:N</code> - Minimum likes</li> <li><code>min_retweets:N</code> - Minimum retweets</li> <li><code>filter:media</code> - Has media</li> <li><code>filter:links</code> - Has links</li> </ul> <p>Rate Limits</p> <p>Search scraping is rate-limited. For high-volume monitoring, implement delays between requests and consider caching results.</p>"},{"location":"guides/scraping/search/#aggregating-search-data","title":"Aggregating Search Data","text":"<pre><code>async def aggregate_search_data():\n    async with Xeepy() as x:\n        results = await x.scrape.search(\"AI startups\", limit=500)\n\n        # Analyze results\n        total_engagement = sum(t.likes + t.retweets for t in results)\n        unique_authors = len(set(t.author.username for t in results))\n\n        # Top authors by tweet count\n        from collections import Counter\n        author_counts = Counter(t.author.username for t in results)\n        top_authors = author_counts.most_common(10)\n\n        print(f\"Total tweets: {len(results)}\")\n        print(f\"Unique authors: {unique_authors}\")\n        print(f\"Total engagement: {total_engagement}\")\n        print(f\"\\nTop authors:\")\n        for author, count in top_authors:\n            print(f\"  @{author}: {count} tweets\")\n\nasyncio.run(aggregate_search_data())\n</code></pre>"},{"location":"guides/scraping/search/#export-and-analysis","title":"Export and Analysis","text":"<pre><code>async def export_search_analysis():\n    async with Xeepy() as x:\n        results = await x.scrape.search(\"your_keyword\", limit=200)\n\n        # Export raw data\n        x.export.to_json(results, \"search_raw.json\")\n        x.export.to_csv(results, \"search_results.csv\")\n\n        # Export with custom fields\n        x.export.to_csv(\n            results,\n            \"search_custom.csv\",\n            fields=[\"author.username\", \"text\", \"likes\", \"created_at\"]\n        )\n\nasyncio.run(export_search_analysis())\n</code></pre>"},{"location":"guides/scraping/search/#best-practices","title":"Best Practices","text":"<ol> <li>Refine Queries: Use operators to reduce noise and improve relevance</li> <li>Set Date Ranges: Bound searches to relevant time periods</li> <li>Filter by Engagement: Use <code>min_likes</code> to focus on impactful tweets</li> <li>Monitor Continuously: Set up scheduled searches for ongoing monitoring</li> <li>Deduplicate Results: Track seen tweet IDs to avoid processing duplicates</li> </ol>"},{"location":"guides/scraping/search/#related-guides","title":"Related Guides","text":"<ul> <li>Hashtag Scraping</li> <li>Keyword Monitoring</li> <li>Sentiment Analysis</li> </ul>"},{"location":"guides/scraping/threads/","title":"Unrolling Twitter Threads","text":"<p>Extract complete Twitter threads in order, preserving the narrative flow and all associated media and engagement data.</p>"},{"location":"guides/scraping/threads/#overview","title":"Overview","text":"<p>Twitter threads are powerful storytelling tools, but reading them on the platform can be fragmented. The thread unroller extracts entire threads as a cohesive document, perfect for archiving, analysis, or content repurposing.</p>"},{"location":"guides/scraping/threads/#use-cases","title":"Use Cases","text":"<ul> <li>Content Archiving: Save valuable thread content before deletion</li> <li>Research Documentation: Preserve educational threads for reference</li> <li>Content Repurposing: Convert threads to blog posts or newsletters</li> <li>Engagement Analysis: Study which thread tweets perform best</li> </ul>"},{"location":"guides/scraping/threads/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\n\nasync def unroll_thread():\n    async with Xeepy() as x:\n        # Unroll a thread from any tweet in the chain\n        thread = await x.scrape.thread(\n            \"https://x.com/naval/status/1002103360646823936\"\n        )\n\n        print(f\"Thread by @{thread.author.username}\")\n        print(f\"Total tweets: {len(thread.tweets)}\")\n        print(f\"Total engagement: {thread.total_likes} likes\\n\")\n\n        for i, tweet in enumerate(thread.tweets, 1):\n            print(f\"{i}. {tweet.text}\\n\")\n\nasyncio.run(unroll_thread())\n</code></pre>"},{"location":"guides/scraping/threads/#advanced-thread-extraction","title":"Advanced Thread Extraction","text":"<pre><code>async def advanced_thread_unroll():\n    async with Xeepy() as x:\n        thread = await x.scrape.thread(\n            url=\"https://x.com/user/status/123456789\",\n            include_replies=True,      # Include author's replies\n            include_quotes=True,       # Include quote tweets\n            include_media=True,        # Download media attachments\n            output_dir=\"threads\"       # Save media to directory\n        )\n\n        # Access thread metadata\n        print(f\"Thread ID: {thread.id}\")\n        print(f\"Author: @{thread.author.username}\")\n        print(f\"Created: {thread.created_at}\")\n        print(f\"Tweet count: {len(thread.tweets)}\")\n\n        # Engagement summary\n        print(f\"\\nEngagement Summary:\")\n        print(f\"  Total likes: {thread.total_likes}\")\n        print(f\"  Total retweets: {thread.total_retweets}\")\n        print(f\"  Total replies: {thread.total_replies}\")\n\nasyncio.run(advanced_thread_unroll())\n</code></pre>"},{"location":"guides/scraping/threads/#converting-threads-to-documents","title":"Converting Threads to Documents","text":"<pre><code>async def thread_to_document():\n    async with Xeepy() as x:\n        thread = await x.scrape.thread(\n            \"https://x.com/user/status/123456789\"\n        )\n\n        # Generate markdown document\n        markdown = f\"# {thread.tweets[0].text[:50]}...\\n\\n\"\n        markdown += f\"*By @{thread.author.username} on {thread.created_at}*\\n\\n\"\n        markdown += \"---\\n\\n\"\n\n        for tweet in thread.tweets:\n            markdown += f\"{tweet.text}\\n\\n\"\n            if tweet.media:\n                for media in tweet.media:\n                    markdown += f\"![Media]({media.url})\\n\\n\"\n\n        # Save to file\n        with open(\"thread_export.md\", \"w\") as f:\n            f.write(markdown)\n\n        print(\"Thread exported to markdown!\")\n\nasyncio.run(thread_to_document())\n</code></pre>"},{"location":"guides/scraping/threads/#batch-thread-processing","title":"Batch Thread Processing","text":"<pre><code>async def batch_unroll_threads():\n    async with Xeepy() as x:\n        thread_urls = [\n            \"https://x.com/user1/status/111\",\n            \"https://x.com/user2/status/222\",\n            \"https://x.com/user3/status/333\",\n        ]\n\n        threads = []\n        for url in thread_urls:\n            thread = await x.scrape.thread(url)\n            threads.append(thread)\n            print(f\"Unrolled: {len(thread.tweets)} tweets from @{thread.author.username}\")\n\n        # Export all threads\n        for thread in threads:\n            filename = f\"thread_{thread.id}.json\"\n            x.export.to_json(thread, filename)\n\nasyncio.run(batch_unroll_threads())\n</code></pre>"},{"location":"guides/scraping/threads/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>url</code> str required URL of any tweet in thread <code>include_replies</code> bool False Include author replies <code>include_quotes</code> bool False Include quote tweets <code>include_media</code> bool True Extract media URLs <code>output_dir</code> str None Directory for media downloads <p>Finding the Thread Start</p> <p>You can pass any tweet URL from the thread - the unroller automatically finds and retrieves the entire thread from the beginning.</p> <p>Long Threads</p> <p>Very long threads (100+ tweets) may take longer to process. The scraper handles pagination automatically.</p>"},{"location":"guides/scraping/threads/#analyzing-thread-performance","title":"Analyzing Thread Performance","text":"<pre><code>async def analyze_thread_performance():\n    async with Xeepy() as x:\n        thread = await x.scrape.thread(\n            \"https://x.com/user/status/123456789\"\n        )\n\n        # Find best performing tweet in thread\n        best_tweet = max(thread.tweets, key=lambda t: t.likes)\n        print(f\"Best tweet: {best_tweet.text[:50]}...\")\n        print(f\"Likes: {best_tweet.likes}\")\n\n        # Engagement dropoff analysis\n        print(\"\\nEngagement by position:\")\n        for i, tweet in enumerate(thread.tweets, 1):\n            pct = (tweet.likes / thread.tweets[0].likes * 100) if thread.tweets[0].likes else 0\n            print(f\"  Tweet {i}: {tweet.likes} likes ({pct:.1f}% of first)\")\n\nasyncio.run(analyze_thread_performance())\n</code></pre>"},{"location":"guides/scraping/threads/#export-formats","title":"Export Formats","text":"<pre><code>async def export_thread_formats():\n    async with Xeepy() as x:\n        thread = await x.scrape.thread(\"https://x.com/user/status/123\")\n\n        # Multiple export options\n        x.export.to_json(thread, \"thread.json\")\n        x.export.to_csv(thread.tweets, \"thread_tweets.csv\")\n        x.export.thread_to_markdown(thread, \"thread.md\")\n        x.export.thread_to_html(thread, \"thread.html\")\n\nasyncio.run(export_thread_formats())\n</code></pre>"},{"location":"guides/scraping/threads/#best-practices","title":"Best Practices","text":"<ol> <li>Save Thread URLs: Threads can be deleted; save URLs of valuable threads</li> <li>Download Media: Use <code>include_media=True</code> to preserve images and videos</li> <li>Check Completeness: Verify tweet count matches what you see on Twitter</li> <li>Archive Regularly: Important threads should be archived promptly</li> <li>Cite Sources: When repurposing content, credit the original author</li> </ol>"},{"location":"guides/scraping/threads/#related-guides","title":"Related Guides","text":"<ul> <li>Scraping User Tweets</li> <li>Media Scraping</li> <li>Content Analysis</li> </ul>"},{"location":"guides/scraping/tweets/","title":"Scraping User Tweets","text":"<p>Extract tweets from any public Twitter profile to analyze content strategy, engagement patterns, and posting behavior.</p>"},{"location":"guides/scraping/tweets/#overview","title":"Overview","text":"<p>The tweets scraper retrieves a user's timeline including tweet text, media, engagement metrics, and metadata. This data powers content analysis, competitive research, and historical tweet archiving.</p>"},{"location":"guides/scraping/tweets/#use-cases","title":"Use Cases","text":"<ul> <li>Content Analysis: Study what content performs best for an account</li> <li>Competitive Intelligence: Monitor competitor posting strategies</li> <li>Historical Archive: Back up tweets for research or compliance</li> <li>Engagement Research: Identify high-performing tweet patterns</li> </ul>"},{"location":"guides/scraping/tweets/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom xeepy import Xeepy\n\nasync def scrape_user_tweets():\n    async with Xeepy() as x:\n        # Get recent tweets from a user\n        tweets = await x.scrape.tweets(\"elonmusk\", limit=100)\n\n        for tweet in tweets:\n            print(f\"{tweet.created_at}: {tweet.text[:50]}...\")\n            print(f\"  Likes: {tweet.likes} | RTs: {tweet.retweets}\")\n\n        # Export for analysis\n        x.export.to_csv(tweets, \"user_tweets.csv\")\n\nasyncio.run(scrape_user_tweets())\n</code></pre>"},{"location":"guides/scraping/tweets/#advanced-filtering","title":"Advanced Filtering","text":"<pre><code>async def filtered_tweet_scrape():\n    async with Xeepy() as x:\n        # Scrape with advanced filters\n        tweets = await x.scrape.tweets(\n            username=\"techcrunch\",\n            limit=500,\n            include_retweets=False,    # Original tweets only\n            include_replies=False,      # No reply tweets\n            min_likes=100,              # Only popular tweets\n            media_only=False,           # Include text-only tweets\n            since=\"2024-01-01\",         # Date range start\n            until=\"2024-12-31\"          # Date range end\n        )\n\n        # Analyze engagement\n        total_engagement = sum(t.likes + t.retweets for t in tweets)\n        avg_engagement = total_engagement / len(tweets) if tweets else 0\n        print(f\"Average engagement: {avg_engagement:.1f}\")\n\nasyncio.run(filtered_tweet_scrape())\n</code></pre>"},{"location":"guides/scraping/tweets/#extracting-tweet-components","title":"Extracting Tweet Components","text":"<pre><code>async def extract_tweet_details():\n    async with Xeepy() as x:\n        tweets = await x.scrape.tweets(\"username\", limit=50)\n\n        for tweet in tweets:\n            # Access all tweet properties\n            print(f\"ID: {tweet.id}\")\n            print(f\"Text: {tweet.text}\")\n            print(f\"Created: {tweet.created_at}\")\n            print(f\"Likes: {tweet.likes}\")\n            print(f\"Retweets: {tweet.retweets}\")\n            print(f\"Replies: {tweet.reply_count}\")\n            print(f\"Quotes: {tweet.quote_count}\")\n            print(f\"Views: {tweet.views}\")\n\n            # Media attachments\n            if tweet.media:\n                for media in tweet.media:\n                    print(f\"Media: {media.type} - {media.url}\")\n\n            # Hashtags and mentions\n            print(f\"Hashtags: {tweet.hashtags}\")\n            print(f\"Mentions: {tweet.mentions}\")\n            print(f\"URLs: {tweet.urls}\")\n\nasyncio.run(extract_tweet_details())\n</code></pre>"},{"location":"guides/scraping/tweets/#batch-processing","title":"Batch Processing","text":"<pre><code>async def batch_tweet_analysis():\n    async with Xeepy() as x:\n        accounts = [\"user1\", \"user2\", \"user3\"]\n        all_tweets = []\n\n        for account in accounts:\n            tweets = await x.scrape.tweets(account, limit=200)\n            all_tweets.extend(tweets)\n            print(f\"Scraped {len(tweets)} tweets from @{account}\")\n\n        # Aggregate analysis\n        top_tweets = sorted(all_tweets, key=lambda t: t.likes, reverse=True)[:10]\n        print(\"\\nTop 10 tweets by likes:\")\n        for t in top_tweets:\n            print(f\"@{t.author.username}: {t.likes} likes - {t.text[:40]}...\")\n\nasyncio.run(batch_tweet_analysis())\n</code></pre>"},{"location":"guides/scraping/tweets/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>username</code> str required Target username <code>limit</code> int 100 Maximum tweets to retrieve <code>include_retweets</code> bool True Include retweets <code>include_replies</code> bool True Include reply tweets <code>min_likes</code> int 0 Minimum likes filter <code>media_only</code> bool False Only tweets with media <code>since</code> str None Start date (YYYY-MM-DD) <code>until</code> str None End date (YYYY-MM-DD) <p>Performance Optimization</p> <p>For accounts with thousands of tweets, use date ranges to paginate through history efficiently. This prevents timeout issues and reduces memory usage.</p> <p>Tweet Availability</p> <p>Twitter limits how far back you can retrieve tweets. Very old tweets may not be accessible through the standard timeline.</p>"},{"location":"guides/scraping/tweets/#content-analysis-example","title":"Content Analysis Example","text":"<pre><code>async def analyze_content_strategy():\n    async with Xeepy() as x:\n        tweets = await x.scrape.tweets(\"username\", limit=500)\n\n        # Categorize by content type\n        with_media = [t for t in tweets if t.media]\n        with_links = [t for t in tweets if t.urls]\n        questions = [t for t in tweets if \"?\" in t.text]\n\n        print(f\"Tweets with media: {len(with_media)} ({len(with_media)/len(tweets)*100:.1f}%)\")\n        print(f\"Tweets with links: {len(with_links)} ({len(with_links)/len(tweets)*100:.1f}%)\")\n        print(f\"Questions asked: {len(questions)}\")\n\nasyncio.run(analyze_content_strategy())\n</code></pre>"},{"location":"guides/scraping/tweets/#best-practices","title":"Best Practices","text":"<ol> <li>Use Date Ranges: For historical analysis, specify <code>since</code> and <code>until</code> parameters</li> <li>Filter Retweets: Set <code>include_retweets=False</code> for original content analysis</li> <li>Monitor Rate Limits: Large scrapes may hit rate limits; use delays</li> <li>Store Raw Data: Export raw JSON for future analysis flexibility</li> <li>Respect ToS: Use scraped data responsibly and ethically</li> </ol>"},{"location":"guides/scraping/tweets/#related-guides","title":"Related Guides","text":"<ul> <li>Thread Unrolling</li> <li>Search Scraping</li> <li>Engagement Analysis</li> </ul>"}]}